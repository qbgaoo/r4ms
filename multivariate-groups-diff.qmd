# Multivariate data

```{r}
#| echo: false

source("_common.R")
```

Multivariate data usually refers to data involving more than one variable, but here specially refers to datasets where there are multiple outcome or response variables (also called dependent variables) that are measured simultaneously for each observation or experimental unit. This type of data is common in many fields such as psychology, medicine, biology, and social sciences, where researchers want to study how one or more predictors (independent variables) affect multiple outcomes.

## Prerequisite

```{r}
#| message: false

library(tidyverse)
library(GGally)
library(DescTools)
library(emmeans)
library(nlme)
library(profileR)
```

## Descriptive statistics

Descriptive statistics for multivariate data help summarize the properties of several variables simultaneously. Some key statistics include:

### Mean vector

The mean vector is a multivariate extension of the concept of a mean in univariate statistics. It provides the average or central value for each variable in a multivariate dataset. For multiple variables, it is represented as a vector:

$$\mu = (\mu_1, \mu_2, \dots, \mu_p)$$

where $p$ is the number of variables.

### Deviation matrix

The deviation matrix (also known as the mean deviation matrix or matrix of deviations) is a matrix that represents how much each observation in a dataset deviates from the mean of the corresponding variable. It is often used in multivariate data analysis as a step before calculating other statistics, like the covariance matrix.

If the dataset is represented by the matrix $\mathbf{X}$ with $n$ rows (observations) and $p$ columns (variables), and $v$ is the mean vector of the variables, the deviation matrix $\mathbf{D}$ is calculated as:

$$
\mathbf{D} = \mathbf{X} - \mathbf{1} \mu^T
$$

where $\mathbf{X}$ is the $n \times p$ data matrix, $\mathbf{1}$ is an $n \times 1$ column vector of ones, $\mu^T$ is the $1 \times p$ mean vector transposed to match the dimensions, $\mathbf{D}$ is the $n \times p$ deviation matrix.

### Covariance matrix

The covariance matrix describes the relationships between pairs of variables in a dataset. It captures how much two variables change together (covary) and generalizes the concept of variance (which measures how much a single variable varies) to multiple variables. The covariance matrix $\mathbf{S}$ contains variances on the diagonal and covariances off-diagonal:

$$
\mathbf{S} = \begin{pmatrix}S_{11} & S_{12} & \cdots & S_{1p} \\S_{21} & S_{22} & \cdots & S_{2p} \\\vdots & \vdots & \ddots & \vdots \\S_{p1} & S_{p2} & \cdots & S_{pp}\end{pmatrix}
$$

Where $S_{ii}$ is the variance of the $i$-th variable, $S_{ij}$ (for $i \neq j$) is the covariance between the $i$-th and $j$-th variables.

The covariance between $X_i$ and $X_j$ is calculated as:

$$
S_{ij} = \frac{1}{n-1} \sum_{k=1}^{n} (X_{ik} - \bar{X}_i)(X_{jk} - \bar{X}_j)
$$

where $n$ is the number of observations, $X_{ik}$ and $X_{jk}$ are the $k -th$ observations of the variables $X_i$ and $X_j$, respectively, $\bar{X}_i$ and $\bar{X}_j$ are the means of the variables $X_i$ and $X_j$.

### Correlation matrix

The correlation matrix provides the pairwise correlation coefficients between variables in a dataset. It is a normalized version of the covariance matrix, giving a measure of how strongly variables are linearly related, with values ranging from -1 to 1.

For a dataset with $p$ variables, the correlation matrix is a $p \times p$ matrix where each element $r_{ij}$ represents the correlation coefficient between the $i -th$ and$j -th$ variables. The diagonal elements are always 1, as a variable is perfectly correlated with itself.

Mathematically, the correlation matrix $R$ is:

$$
R = \begin{pmatrix}    1 & r_{12} & r_{13} & \dots & r_{1p} \\    r_{21} & 1 & r_{23} & \dots & r_{2p} \\    r_{31} & r_{32} & 1 & \dots & r_{3p} \\    \vdots & \vdots & \vdots & \ddots & \vdots \\    r_{p1} & r_{p2} & r_{p3} & \dots & 1    \end{pmatrix}
$$

where $r_{ij}$ is the Pearson correlation coefficient between variable $i$ and variable $j$, defined as:

$$
r_{ij} = \frac{\text{Cov}(X_i, X_j)}{\sigma_{X_i} \sigma_{X_j}}
$$

$\text{Cov}(X_i, X_j)$ is the covariance between $X_i$ and $X_j$, $\sigma_{X_i}$ and$\sigma_{X_j}$are the standard deviations of $X_i$ and $X_j$, respectively.

### Scatterplot matrix

A scatterplot matrix (also called a pairs plot) is a grid of scatterplots that visually represents the relationships between pairs of variables in a multivariate dataset. It is particularly useful for examining potential correlations or patterns among multiple variables simultaneously.

For a dataset with $p$ variables, a scatterplot matrix will contain a $p \times p$ grid of plots, where the plot in the $i -th$ and $j -th$ column displays a scatterplot of the $i -th$ variable versus the $j -th$ variable. The diagonal often shows the distribution of each variable, typically as a histogram or a density plot. Each off-diagonal plot shows the relationship (scatterplot) between two variables, helping identify patterns like linearity, clusters, or outliers.

::: example
Pulmonary function measurements were performed on 15 patients with acute lower respiratory tract infections admitted to the respiratory department of a hospital. The measurements included forced vital capacity (FVC), forced expiratory volume in the first second (FEV1), and peak expiratory flow (PEF). Please perform a multivariate description of these three response variables.
:::

<div>

<a href="datasets/ex17-01.csv" download="ex17-01.csv" class="btn btn-success"> <i class="bi bi-database-fill-down"></i> Download data </a>

</div>

```{r}
df <- read_csv("datasets/ex17-01.csv", show_col_types = F) |> 
  select(FVC, FEV1, PEF)
```

**Mean vector:**

You can easily calculate a correlation matrix using the `colMeans()` function:

```{r}
colMeans(df)
```

**Covariance matrix:**

You can easily calculate a correlation matrix using the `cov()` function:

```{r}
cov(df)
```

**Correlation matrix:**

You can easily calculate a correlation matrix using the `cor()` function:

```{r}
cor(df) 
```

**Scatterplot matrix**

Tools like `pairs()` in base R or `ggpairs()` in GGally can generate scatterplot matrices for visual exploration of data relationships.

```{r}
pairs(df, main = "Scatterplot matrix with pairs()")
```

```{r}
ggpairs(df, title = "Scatterplot matrix with ggpairs()")
```

### Multivariate normal distribution

Multivariate normal distribution (also known as multivariate Gaussian distribution) is a generalization of the univariate normal distribution to multiple dimensions. It describes a set of $p$-dimensional random variables, where each variable is normally distributed and their joint distribution follows a normal pattern.

A random vector $\mathbf{X} = (X_1, X_2, \dots, X_p)^T$ is said to follow a multivariate normal distribution if every linear combination of its components is normally distributed. It is denoted as:

$$
\mathbf{X} \sim \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma}) 
$$

where $\boldsymbol{\mu} = (\mu_1, \mu_2, \dots, \mu_p)^T$ is the mean vector, representing the expected values of the variables, $\boldsymbol{\Sigma}$ is the $p \times p$ covariance matrix, representing the variances of and covariances between the variables.

The probability density function of a multivariate normal distribution is given by:

$$
f(\mathbf{X}) = \frac{1}{(2\pi)^{p/2} |\boldsymbol{\Sigma}|^{1/2}} \exp\left( -\frac{1}{2} (\mathbf{X} - \boldsymbol{\mu})^T \boldsymbol{\Sigma}^{-1} (\mathbf{X} - \boldsymbol{\mu}) \right) 
$$

where $\mathbf{X}$ is the $p$-dimensional vector of variables, $\boldsymbol{\Sigma}$ is the determinant of the covariance matrix, $\boldsymbol{\Sigma}^{-1}$ is the inverse of the covariance matrix.

## Comparison of differences between groups

When comparing differences between groups with multivariate data, we deal with situations where there are multiple dependent variables or outcomes that need to be analyzed simultaneously. Traditional univariate methods (such as t-tests or ANOVA) only test differences for one outcome variable at a time. For multivariate data, we use techniques that account for the interrelationships between the multiple outcomes.

Here are the common methods for comparing group differences with multivariate data:

### Hotelling’s T² test: one-sample

Hotelling’s T² test is the multivariate extension of the univariate t-test. It can be used in two different contexts: one-sample and two-sample. Both tests are multivariate versions of t-tests, designed for situations where there are multiple dependent variables.

For one-sample, it tests whether the sample’s mean vector differs from a known mean vector. The hypothesis tested is:

-   $H_0$: The mean vector of the sample is equal to the known mean vector.
-   $H_1$: The mean vector of the sample is different from the known mean vector.

Let $\mathbf{X}$ be the sample data matrix of n observations and $p$ variables. The mean vector $\bar{\mathbf{X}}$ is compared to a hypothesized mean vector $\mathbf{\mu}_0$. The test statistic is given by:

$$
T^2 = n (\bar{\mathbf{X}} - \mathbf{\mu}_0)^{\prime} \mathbf{S}^{-1} (\bar{\mathbf{X}} - \mathbf{\mu}_0)
$$

where $n$ is the sample size, $\bar{\mathbf{X}}$ is the sample mean vector, $\mathbf{S}$ is the sample covariance matrix, $\mathbf{\mu}_0$ is the hypothesized mean vector.

Under the null hypothesis, the test statistic $T^2$ follows an $F$-distribution after scaling:

$$
F = \frac{(n - p)}{p(n - 1)} T^2 \sim F(p, n - p)
$$

where $p$ is the number of variables (dimensions) in the data.

::: example
In a battery factory, 15 workers engaged in lead-related tasks were randomly selected for measurement of liver function indicators, including alanine aminotransferase (ALT), aspartate aminotransferase (AST), and gamma-glutamyl transferase (GGT). It is known that the mean values for ALT, AST, and GGT in the general population in that area are 23.9 U/L, 25.7 U/L, and 26.7 U/L, respectively. The question is whether the liver function of the lead-exposed workers differs from that of the normal population.
:::

<div>

<a href="datasets/ex17-02.csv" download="ex17-02.csv" class="btn btn-success"> <i class="bi bi-database-fill-down"></i> Download data </a>

</div>

```{r}
df <- read_csv("datasets/ex17-02.csv", show_col_types = F)
```

To perform a one-sample Hotelling’s T² test, you can use the `HotellingsT2Test()` function from the `DescTools` package.

```{r}
normal_mean <- matrix(c(23.9, 25.7, 26.7), nrow = 3)
df |> 
  HotellingsT2Test(mu = normal_mean)
```

The results show that the Hotelling’s T² statistic is 5.3391, p-value is 0.01439, less than 0.05, reject the null hypothesis. This suggest that the liver function (ALT, AST, and GGT) of the lead-exposed workers is different from the normal population values (\[23.9, 25.7, 26.7\]), implying a potential impact of lead exposure on the workers’ liver function.

### Hotelling’s T² test: two-sample

For two groups, it tests whether the mean vectors of two independent samples are equal. The hypothesis tested is:

-   $H_0$: The mean vectors of the two groups are equal: $\boldsymbol{\mu}_1 = \boldsymbol{\mu}_2$.
-   $H_1$: The mean vectors of the two groups are not equal: $\boldsymbol{\mu}_1 \neq \boldsymbol{\mu}_2$.

The test statistic for Hotelling’s $T^2$ is computed similarly to a multivariate version of the t-test. When comparing two independent groups with sample sizes $n_1$ and $n_2$, and sample mean vectors $\bar{\mathbf{X}}_1$ and $\bar{\mathbf{X}}_2$, the test statistic is:

$$
T^2 = \frac{n_1 n_2}{n_1 + n_2} (\bar{\mathbf{X}}_1 - \bar{\mathbf{X}}_2)^{\prime} \mathbf{S}_p^{-1} (\bar{\mathbf{X}}_1 - \bar{\mathbf{X}}_2)
$$

where $\mathbf{S}_p$ is the pooled covariance matrix.

Specifically, the Hotelling’s $T^2$ statistic can be transformed into an F-statistic as:

$$
F = \frac{(n_1 + n_2 - p - 1) T^2}{(n_1 + n_2 - 2)p}
$$

where $p$ is the number of variables.

$F$ follows an $F$-distribution with $p$ and $n_1 + n_2 - p - 1$ degrees of freedom under the null hypothesis.

::: example
A hospital conducted a study to investigate the therapeutic effect of drug A on heart failure. 20 heart failure patients were randomly divided into two groups: one group received standard treatment, while the other group received drug A in addition to the standard treatment. The post-treatment heart function indicators, left ventricular ejection fraction (LVEF) and the 6-minute walk test (6MWT) were measured. It is known that lower values of LVEF and 6MWT indicate more severe heart function impairment. The question is whether drug A is effective in treating heart failure.
:::

<div>

<a href="datasets/ex17-03.csv" download="ex17-03.csv" class="btn btn-success"> <i class="bi bi-database-fill-down"></i> Download data </a>

</div>

```{r}
df <- read_csv("datasets/ex17-03.csv", col_types = list(group = col_factor()))
```

The `HotellingsT2Test()` function from `DescTools` package is used to perform Hotelling’s T² test for two groups.

```{r}
HotellingsT2Test(cbind(LVEF, MWT) ~ group, data = df)
```

An alternative method is using the `manova()` function to perform multivariate ANOVA, just as univariate two-sample mean difference can be tested either by t-test or ANOVA.

```{r}
manova(cbind(LVEF, MWT) ~ group, data = df) |> 
  summary(test = "Wilks")      # <1>
```

1.  This provides a summary of the MANOVA results using Wilks’ Lambda as the test statistic.

In this case, the p-value is 0.3009, which is higher than the significance level of 0.05, not reject the null hypothesis. There is no significant evidence to suggest that drug A has a effect on improving heart function (as measured by LVEF and 6MWT) when compared to standard treatment alone.

### MANOVA for multiple groups

MANOVA (Multivariate analysis of variance) is an extension of ANOVA that allows for comparing the mean vectors of multiple groups across several dependent variables while considering the correlation between them.

::: example
The children with chronic gastritis were randomly divided into 3 groups, with group I and group II as treatment groups, and another group serving as the control. The aim is to compare the effect of the treatment drugs on T-cell immune function (percentages of peripheral blood T3, T4, and T5 cells).
:::

<div>

<a href="datasets/ex17-04.csv" download="ex17-04.csv" class="btn btn-success"> <i class="bi bi-database-fill-down"></i> Download data </a>

</div>

```{r}
df <- read_csv("datasets/ex17-04.csv", col_types = list(group = col_factor()))
df |> 
  group_by(group)|> 
  summarise_all(.funs = list(mean = mean))
```

Use the `manova()` function to test whether the treatment drugs have a significant effect on T-cell immune function.

```{r}
model <- manova(cbind(t3, t4, t8) ~ group, data = df)
summary(model, test = "Wilks")
```

The MANOVA results indicate the following:

1.  Wilks’ Lambda (0.088735): This is a test statistic used in multivariate analysis of variance. A lower Wilks’ Lambda value indicates that the group differences explain a large portion of the variance in the dependent variables (T3, T4, T8). In this case, the value is relatively low, which suggests that the groups differ significantly in the combined T-cell percentages.
2.  Approximate F-statistic (5.4997): The F-value is used to determine whether the differences between the groups’ means are statistically significant. A larger F-value typically suggests stronger evidence against the null hypothesis (i.e., no difference between groups).
3.  Degrees of Freedom (num Df = 6, den Df = 14): num Df refers to the numerator degrees of freedom, associated with the number of dependent variables and groups. den Df refers to the denominator degrees of freedom, related to the residuals or error in the model.
4.  P-value: This is the most critical value for interpreting the test. A p-value of 0.004104 indicates that there is a statistically significant difference in the combined T-cell immune function measurements (T3, T4, and T8 cell percentages) between the groups at the 0.05 significance level.

Since the p-value is significant (0.004104), we can reject the null hypothesis and conclude that the treatments have a significant impact on the T-cell immune function across the groups. To determine which specific variables (T3, T4, or T8) are driving these differences, you can run follow-up univariate ANOVAs for each dependent variable.

```{r}
# Univariate ANOVA
df |> 
  select(t3, t4, t8) |> 
  lapply(\(x) aov(x ~ group, data = df)) |> 
  map(summary)
```

Another convenient tool is `summary.aov()`, which can extract the univariate ANOVAs directly from the model for each dependent variable.

```{r}
summary.aov(model)
```

This will give you the individual ANOVA tables for each dependent variable. If any of these ANOVAs are significant, you should conduct post-hoc pairwise comparisons to see which two groups are significantly different.

```{r}
df |> 
  select(t3, t4, t8) |> 
  lapply(\(x) aov(x ~ group, data = df)) |> 
  map(~ TukeyHSD(.)) 
```

The results show that group 1 and group 3 do not show a significant difference in each response variable mean. This method performs pairwise comparisons on each response variable independently, which can be simpler but loses the multivariate aspect.

The Hotelling’s T² test is a multivariate generalization of the t-test for comparing mean vectors. Here we define a customized function named `pairs.hotellings.T2.test()` to perform pairwise comparisons of the multivariate means between groups.

```{r}
# Define the pairwise_hotelling function
pairs.hotellings.T2.test <- function(formula, data, p.adjust.method = p.adjust.methods) {
  vars <- all.vars(formula[[2]])  # Get the response variables
  g <- all.vars(formula[[3]])  # Get the group variable 
  p.adjust.method <- match.arg(p.adjust.method)
  
  group <- unique(data[[g]])  # Get unique groups
  n_group <- length(group)
  pairwise_table <- c()
  METHOD <- "Pairwise comparison using Hotelling's two sample T2-test"
  
  # Loop through all unique pairwise combinations of groups
  for (i in 1:(n_group - 1)) {
    for (j in (i + 1):n_group) {
      x <- data[data[[g]] == group[i], vars]
      y <- data[data[[g]] == group[j], vars]
      
      # Perform Hotelling's T² test
      htest <- HotellingsT2Test(x, y)
      tb <- tibble(
        pairs = paste0(group[i], "~", group[j]),
        T.2 = htest$statistic,
        df1 = htest$parameter[1],
        df2 = htest$parameter[2],
        p_value = htest$p.value
      )
      # Store the result in a list with descriptive names
      pairwise_table <- rbind(pairwise_table, tb)
    }
  }
  pairwise_table <- pairwise_table |> 
    mutate(p_value = p.adjust(p_value, method = p.adjust.method))
  
  list(tbl = pairwise_table, method = METHOD, p.adjust.method=p.adjust.method)
}
```

```{r}
pairs.hotellings.T2.test(cbind(t3, t4, t8) ~ group, data = df) 
```

The results show that group 1 and group 2 do not show a significant difference in their mean vectors (p = 0.1676). Both group 1 and group 3, group 2 and group 3 show significant differences (p = 0.0316 and p = 0.0218, respectively), indicating that group 3 is notably different from both group 1 and group 2.

::: callout-caution
You may have noticed that the conclusions made by `pairs.hotellings.T2.test()` (multivariate test) and by `TukeyHSD()` (multiple univariate tests) are not consistent.
:::

### Multivariate analysis vs univariate analysis

The conclusions of a multivariate hypothesis test (MANOVA or Hotelling’s T² test) and performing multiple univariate hypothesis tests (ANOVA or t-test) may differ due to the following key reasons:

1.  **Correlations between variables**

    -   Multivariate Tests: Multivariate hypothesis tests take into account the correlations between the m response variables. This means that these tests evaluate the joint distribution of the variables, assessing if there is a significant difference in the combined response vector.
    -   Univariate Tests: Univariate tests treat each response variable independently. They ignore any relationships or correlations between the variables.
    -   Impact: If the response variables are correlated, a multivariate test may detect significant differences that individual univariate tests would miss, and vice versa. Ignoring the correlation structure in univariate tests can lead to inaccurate conclusions or lower statistical power.

2.  **Inflation of type I error**

    -   Multivariate Tests: A multivariate test, such as MANOVA, tests the overall difference across multiple response variables at once, and hence controls for the overall type I error (false positive rate) at the desired significance level.
    -   Univariate Tests: Performing multiple univariate tests increases the risk of type I error, especially without adjustment. If you perform multiple individual tests, the cumulative probability of making at least one false positive error increases. This problem can be addressed by adjusting the p-values using correction methods, but this reduces the power of the tests.
    -   Impact: Without correction, univariate tests might show more significant results due to an inflated type I error rate. After correction, the results of multiple univariate tests might be more conservative compared to a multivariate approach.

3.  **Power of the test**

    -   Multivariate Tests: Multivariate tests tend to have higher statistical power when the variables are correlated, meaning that they are better at detecting differences when response variables are related.
    -   Univariate Tests: Separate univariate tests can lose power because they do not take advantage of the relationships between variables. Each test only considers one variable at a time, and the overall pattern of change across multiple variables may be missed.
    -   Impact: Multivariate tests may detect a significant overall difference even when none of the univariate tests do.

**Conclusion**

If the response variables are uncorrelated and you adjust for multiple comparisons in the univariate tests, the conclusions from the multivariate test and the separate univariate tests may be similar. If the response variables are correlated, or if no correction is applied to the univariate tests, the conclusions can differ significantly. Multivariate tests provide a more comprehensive picture by accounting for relationships between variables, whereas univariate tests focus on one variable at a time.

::: example
A researcher collected data of birth weight and length for two groups of newborns. Perform univariate and multivariate analysis separately.
:::

<div>

<a href="datasets/ex17-05.csv" download="ex17-05.csv" class="btn btn-success"> <i class="bi bi-database-fill-down"></i> Download data </a>

</div>

```{r}
df <- read_csv(
  "datasets/ex17-05.csv", 
  col_types = list(group = col_factor(), no = col_factor()))
```

1.  **Univariate analysis**

Conduct two-sample t-test separately for birth weight and length to compare if there are significant differences between the two groups.

```{r}
df |> 
  select(weight, height) |> 
  lapply(\(x) t.test(x ~ group, data = df)) 
```

2.  **Multivariate analysis**

For multivariate analysis, you can use Hotelling’s T² test to simultaneously compare the mean vectors of birth weight and length between the two groups.

```{r}
HotellingsT2Test(cbind(weight, height) ~ group, data = df)
```

It is clear that the results of univariate and multivariate analysis are not consistent. The multivariate analysis reveals the difference between groups, while the univariate analysis not. Below is a visualization of difference based on multivariate data.

```{r}
ggplot(df, aes(height, weight, color = group)) + 
  geom_point()
```

## Repeated measures MANOVA

Repeated measures MANOVA is an extension of MANOVA that accounts for multiple measurements of the same subjects over time. It’s a robust technique used when there are several dependent variables and repeated measurements for each subject, considering the correlations between these repeated measurements.

::: example
The weight of 10 obese patients was recorded before and during the 1st to 4th weeks of taking weight-loss medication under the guidance of a doctor, following a unified standard. Analyze the effectiveness of the weight-loss medication.
:::

<div>

<a href="datasets/ex17-06.csv" download="ex17-06.csv" class="btn btn-success"> <i class="bi bi-database-fill-down"></i> Download data </a>

</div>

```{r}
df <- read_csv("datasets/ex17-06.csv", col_types = list(no = col_factor()))

tb <- df |> 
  pivot_longer(
    contains("t"),
    names_to = "time",
    names_prefix = "t",
    values_to = "weight",
    names_transform = as.ordered
  ) 
```

Since there are multiple time points, a repeated measures ANOVA is suitable. This will test whether there is a significant difference in the weights across time points.

```{r}
aov(weight ~ time + Error(no), data = tb) |> summary()
```

If you have more complex data, such as if patients drop out at different times, you can use a linear mixed-effects model to account for missing data or individual differences.

```{r}
lme(weight ~ time, random = ~ 1 | no, data = tb) |> anova()
```

Specifically, you can treat the measurements at each time point as separate variables (since they represent the same outcome measured at different time points). This is common in repeated measures analysis, but in this case, we will treat the time points as distinct variables for a multivariate analysis.

```{r}
mean0 <- rep(0, times = 4)
df |> 
  mutate(t2 = t2 - t1, t3 = t3 - t1, t4 = t4 - t1, t5 = t5 - t1, .keep = "none") |>
  HotellingsT2Test(mu = mean0) 
```

```{r}
lme(weight ~ time, random = ~ 1 | no, data = tb) |> 
  emmeans(specs = ~ time) |> 
  contrast(method = "poly")      # <1>
```

1.  This type of contrast can be useful for examining trends over ordered factors like time.

The linear trend is significant (p \< 0.0001), indicating that there is a significant linear weight reduction over time.

Finally, to visualize the weight reduction trend, you can plot the average weight over time:

```{r}
ggplot(tb, aes(x = time, y = weight, group = 1)) + 
  geom_point(stat = "summary", fun = "mean") +
  geom_line(stat = "summary", fun = "mean") 
```

If you want to plot individual patient data, you can adjust the code to explicitly set the group, color and linetype aesthetics to `no`:

```{r}
ggplot(tb, aes(time, weight, color = no, linetype = no, group = no)) +
  geom_point(size = 0.9) +
  geom_line() +
  labs(color = "", linetype = "")
```

## Profile analysis

Profile Analysis is a multivariate technique that examines the relationship between two or more groups across multiple related variables. It is commonly used when the same set of measurements (variables) is collected for different groups, and the goal is to assess whether these groups differ in their “profiles” across those variables.

In essence, profile analysis is a multivariate extension of repeated measures ANOVA where each variable is treated as a repeated measure.

Profile analysis typically focuses on three main hypothesis:

1.  **Test of parallelism (interaction between factors of group and variable/time)**

    Do the profiles of the groups (mean values across the variables) have the same shape? If the test is significant, it means that the two or more groups differ in the pattern or trend across the variables.

2.  **Test of levels (main effect of group)**

    Are the overall mean levels of the variables different between groups (i.e., are the profiles at different heights, but still parallel)? A significant result indicates that there is a difference in the average levels of the variables across groups, although their trends are similar.

3.  **Test of flatness (main effect of variables)**

    Is the profile flat, meaning there is no significant difference between the variables? A significant test means that there are significant differences between the variables for each group.

::: example
A health status survey was conducted on 50 master’s students and 30 doctoral students. The questionnaire included 7 questions, and responses to each question were rated on a 4-point scale from good to poor (scored as 1, 2, 3, and 4, respectively). The question is: are the responses to each question the same between the master’s students and the doctoral students?
:::

<div>

<a href="datasets/ex17-07.csv" download="ex17-07.csv" class="btn btn-success"> <i class="bi bi-database-fill-down"></i> Download data </a>

</div>

```{r}
df <- read_csv("datasets/ex17-07.csv", col_types = list(group = col_factor()))

tb <- df |> 
  pivot_longer(
    cols = contains("x"),
    names_to = "ques",
    names_prefix = "x",
    values_to = "y"
  )
```

Plot profiles using ggplot2

```{r}
df |> 
  group_by(group) |> 
  summarise(across(contains("x"), mean)) |> 
  pivot_longer(                # <1>
    cols = contains("x"),
    names_to = "ques",
    names_prefix = "x",
    values_to = "y"
  ) |> 
  ggplot(aes(x = ques, y = y, group = group, linetype = group, shape = group, color = group)) + 
  geom_point(size = 2) +
  geom_line() +
  labs(x = "Question", y = "Average score") +
  scale_color_manual(
    values = c("blue", "red"),
    labels = c("Master", "Doctor")
  ) +
  scale_shape_manual(
    values = c(1, 5), 
    labels = c("Master", "Doctor")
  ) +
  scale_linetype_manual(
    values = c(1, 2), 
    labels = c("Master", "Doctor")
  ) +
  ylim(1.5, 2.8) +
  theme(legend.title = element_blank())
```

1.  Reshape data to long format for visualization

The `pbg()` function of `profileR` package implements the three hypothesis tests. These tests are whether the profiles are parallel, have equal levels, and are flat across groups defined by the grouping variable. If parallelism is rejected, the other two tests are not necessary. In that case, flatness may be assessed within each group, and various within- and between-group contrasts may be analyzed.

```{r}
with(df, pbg(cbind(x1, x2, x3 ,x4, x5, x6, x7), group, original.names = T)) |>
  summary()
```

The output summarizes the results of three hypothesis tests from a profile analysis, which tests different aspects of the profiles between two groups:

1.  Test of parallelism
    -   Statistics: Four tests were conducted: Wilks’ Lambda, Pillai’s Trace, Hotelling-Lawley Trace, and Roy’s Largest Root, p-value: 0.8205697 for all four tests.
    -   Interpretation: Since the p-value is 0.8205697, larger than the typical significance level 0.05, fail to reject the null hypothesis. This means the profiles of the two groups are parallel, indicating that the trend in scores across the seven questions is similar between the groups.
2.  Test of equal levels
    -   Statistics: F-value is 0.276, with a p-value of 0.601.
    -   Interpretation: Since p-value is 0.601, greater than 0.05, fail to reject the null hypothesis. This suggests that the overall levels of the profiles are not significantly different between the two groups, indicating similar average scores.
3.  Test of flatness
    -   Statistics: F-value is 10.83958, and the p-value is 1.36e-08.
    -   Interpretation: The p-value is less than 0.05, so reject the null hypothesis. This means the profiles are not flat, suggesting that the scores vary significantly across the seven questions within each group.

**Conclusion:**

The profiles are parallel, meaning the trends across the variables are similar for both groups.

The overall levels of the profiles are not significantly different, meaning the average scores are comparable between the groups. The profiles are not flat, indicating that within each group, there is significant variation in scores across the variables.

In summary, while both groups show similar trends across the questions, the scores for each question vary within the groups.
