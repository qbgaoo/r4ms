[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R for Medical Statistics",
    "section": "",
    "text": "Welcome\nThe topics of this book line up closely with traditional teaching progression; however, the book also highlights computer-intensive approaches to motivate the more traditional approach. The authors emphasize realistic data and examples and rely on visualization techniques to gather insight. They introduce statistics and R seamlessly, giving students the tools they need to use R and the information they need to navigate the sometimes complex world of statistical computing.\nThis book is created by Quarto and R in RStudio IDE。Quarto is an open-source publishing system that integrates well with R, enabling users to create dynamic documents that combine text, code, and output (like tables and plots) in a single document. It supports R Markdown, allowing the execution of R code within documents and rendering outputs in various formats, such as HTML, PDF, and Word. Quarto is ideal for creating reproducible reports, presentations, and books, especially in academic and research settings where R is extensively used. You can also manage bibliographies, citations, and cross-references easily. Quarto is highly customizable, allowing users to create complex documents with ease, and is often used with GitHub Actions for continuous integration and automated publishing.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "1  Preface",
    "section": "",
    "text": "1.1 Prerequisites\nWe’ve made a few assumptions about what you already know to get the most out of this book. You should have some basic knowledge about medical statistics, and it’s helpful if you have some basic R programming experience already.\nYou need some things to run the code in this book: R, RStudio and some preinstalled R packages. Packages are the fundamental units of reproducible R code. They include reusable functions, documentation that describes how to use them, and sample data.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "preface.html#prerequisites",
    "href": "preface.html#prerequisites",
    "title": "1  Preface",
    "section": "",
    "text": "1.1.1 R\nTo download R, go to CRAN, the comprehensive R archive network, https://cloud.r-project.org. A new major version of R comes out once a year, and there are 2-3 minor releases each year. It’s a good idea to update regularly. Upgrading can be a bit of a hassle, especially for major versions that require you to re-install all your packages, but putting it off only makes it worse. We recommend R 4.4.1 or later for this book.\n\n\n1.1.2 RStudio\nRStudio is an integrated development environment, or IDE, for R programming, which you can download from https://posit.co/download/rstudio-desktop/. RStudio is updated a couple of times a year, and it will automatically let you know when a new version is out, so there’s no need to check back. It’s a good idea to upgrade regularly to take advantage of the latest and greatest features. For this book, make sure you have at least RStudio 2024.04.2.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "preface.html#running-r-code",
    "href": "preface.html#running-r-code",
    "title": "1  Preface",
    "section": "1.2 Running R code",
    "text": "1.2 Running R code\nThe previous section showed you several examples of running R code. The code in the book looks like this:\n\n1 + 2\n\n#&gt; [1] 3\n\n\nIf you run the same code in your local console, it will look like this:\n&gt; 1 + 2\n[1] 3\nThere are two main differences. In your console, you type after the &gt;, called the prompt; we don’t show the prompt in the book. In the book, the output is commented out with #&gt;; in your console, it appears directly after your code. These two differences mean that if you’re working with an electronic version of the book, you can easily copy code out of the book and paste it into the console.\nThroughout the book, we use a consistent set of conventions to refer to code:\n\nFunctions are displayed in a code font and followed by parentheses, like sum() or mean().\nOther R objects (such as data or function arguments) are in a code font, without parentheses, like flights or x.\nSometimes, to make it clear which package an object comes from, we’ll use the package name followed by two colons, like dplyr::mutate() or nycflights13::flights. This is also valid R code.\nTo improves readability, variable names and function names are named using snake case.\n\nIn this course an introduction to basic statistical methods useful for biomedical data analysis will be given. Concepts are taught in an intuitive manner, alternating between short lectures and practicals. This allows for plenty of interaction and illustration with examples of practical interest. Participants who aim to use more complex methods can use the concepts and skills learned during the course as basis, as the vast majority of statistical methods are implemented in R.\nParticipants must be able to work with R and R packages to follow the course. Those with little or no experience in R must follow an introductory R course prior to following this course.\nIn addition, it is strongly advised to learn to work with RStudio and RMarkdown. Those with no prior knowledge of RMarkdown can follow the tutorials here. During the course we will practice further, and the RMarkdown cheatsheets may be useful.\nWho should attend\nResearchers who need to run their own statistical analyses, and want to do it in a transparent and reproducible manner. While most participants tend to be PhD students and postdocs, more senior researchers can also benefit from the course.\nR is a free, open-source software for statistical computing and graphics. In this course, you will learn the programming techniques to make the most of this powerful tool for processing, analysing and presenting your data. Please note that basic statistical knowledge is required. You will analyse data by writing functions and scripts to get reproducible and well documented results. You will learn how to create excellent graphics and how to adapt them to your needs. We will use data from clinical background to work throughout the course. Main topics covered during this course will be: Basics of the R language, data accessing, data manipulations, explore and summarise your data using descriptive statistics, graphics in R, data analysis with focus on basic statistics (e.g. hypothesis testing, linear and logistic regression).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "preface.html#colophon",
    "href": "preface.html#colophon",
    "title": "1  Preface",
    "section": "1.3 Colophon",
    "text": "1.3 Colophon\nThe book is written by Quarto, an online version of it is available at https://qbgaoo.github.io/r4ms/.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "2  Introduction",
    "section": "",
    "text": "2.1 Medical statistics\nMedical statistics is the science of applying statistical techniques and principles to analyze data related to health, disease, treatment effectiveness, and public health issues. It involves the systematic collection, organization, description, and inference of health-related data to assist healthcare professionals in making evidence-based, scientific decisions.\nKey Components:\n1. Data Collection: Gathering health-related data systematically through experiments, observational studies, or clinical trials.\n2. Data Organization and Description: Using statistical measures, such as means, variances, and frequency distributions, to describe and summarize the basic characteristics of the data.\n3. Data Analysis: Applying inferential statistical methods, such as hypothesis testing, regression analysis, and analysis of variance (ANOVA), to uncover patterns and causal relationships in the data.\n4. Interpretation of Results: Applying the results of statistical analyses to the medical field, explaining the occurrence and progression of diseases, the effectiveness of treatments, and providing evidence-based support for clinical decisions and public health policies.\nMedical statistics is widely used in medical research, the design of clinical trials, the evaluation of diagnostic tests, the comparison of treatment outcomes, and the epidemiological study of diseases.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#population-and-sample",
    "href": "intro.html#population-and-sample",
    "title": "2  Introduction",
    "section": "2.2 Population and sample",
    "text": "2.2 Population and sample\nPopulation and sample are fundamental concepts in statistics, and they are key to understanding data analysis, particularly in the context of statistical inference.\n\n2.2.1 Population\nA population includes all members of a defined group that we are studying or collecting information on for data-driven decisions. The population represents the entire group of interest, which could be people, objects, events, or measurements.\nExamples: All adults living in a particular country. All patients treated for a particular condition in a hospital over a decade.\nCharacteristics:\nParameters: The numerical characteristics of a population are called parameters (e.g., population mean, population standard deviation).\nSize: The population can be finite (e.g., all students in a school) or infinite (e.g., all possible outcomes of rolling a die).\n\n\n2.2.2 Sample\nA sample is a subset of the population selected for study. It is often impractical or impossible to study an entire population, so researchers use a sample to draw conclusions about the population.\nExamples: 1,000 randomly selected adults from a country. 200 patients selected from the hospital’s records.\nStatistics: The numerical characteristics of a sample are called statistics (e.g., sample mean, sample standard deviation).\nSize: The size of the sample (denoted as n) is always smaller than the population size (denoted as N).\n\n\n2.2.3 Relationship between population and sample\nSampling: The process of selecting a sample from a population is called sampling. It is crucial that the sample is representative of the population to ensure that the conclusions drawn from the sample can be generalized to the population.\nInference: Statistical inference involves making predictions or generalizations about a population based on information from a sample. This is done using various statistical methods, including estimation and hypothesis testing.\nBias and Variability: A sample might not perfectly represent the population due to sampling bias or variability. Researchers use random sampling techniques to minimize these issues and improve the reliability of the inference.\nPractical Example\nImagine you want to study the average height of all adult women in a country (population). Measuring the height of every woman in the country would be impractical, so instead, you randomly select 500 women (sample) and measure their heights. The average height of these 500 women is a statistic. You then use this statistic to estimate the parameter—the average height of all adult women in the country.\nKey Points to Remember\nPopulation: Entire group of interest; parameters describe it.\nSample: Subset of the population; statistics describe it.\nInference: Drawing conclusions about the population based on sample data.\nRepresentativeness: A sample should represent the population to ensure valid inferences.\nUnderstanding the distinction between population and sample is essential for conducting valid and reliable statistical analyses.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#data",
    "href": "intro.html#data",
    "title": "2  Introduction",
    "section": "2.3 Data",
    "text": "2.3 Data\nIn statistics, data can be classified into different types based on various characteristics. Understanding these data types is crucial for selecting appropriate statistical methods and accurately interpreting results. Below are the main types of data:\n\n2.3.1 Quantitative data\nQuantitative data (numerical data) refers to data that can be measured and expressed numerically. This type of data is critical in the medical field as it allows for precise measurements, comparisons, and statistical analysis, ultimately leading to better understanding, diagnosis, treatment, and prevention of diseases. Quantitative data can be further classified into two main types:\n1. Discrete Data\nDiscrete data consist of distinct, separate values that are countable. These values are typically whole numbers, and there is no intermediate value between them.\nExamples:\nNumber of patients: The number of patients admitted to a hospital.\nNumber of medications: The number of medications a patient is taking.\nNumber of occurrences: The number of heartbeats per minute (heart rate).\n2. Continuous Data\nContinuous data can take any value within a given range. These data represent measurements and can have decimal places, meaning they are not restricted to whole numbers.\nExamples:\nBlood pressure: Systolic and diastolic blood pressure measurements (e.g., 120/80 mmHg).\nBody temperature: Body temperature measured in degrees Celsius or Fahrenheit (e.g., 36.6°C).\nCholesterol levels: Blood cholesterol levels measured in milligrams per deciliter (e.g., 180 mg/dL).\nQuantitative data is fundamental in many areas of medicine:\n\nDiagnosis: • Quantitative measurements such as blood pressure, blood glucose levels, and cholesterol levels are used to diagnose conditions like hypertension, diabetes, and hyperlipidemia. 2. Treatment Monitoring: • Quantitative data is crucial for monitoring treatment effectiveness. For example, changes in tumor size (measured in millimeters or centimeters) can indicate whether cancer treatment is effective. 3. Epidemiology: • Quantitative data is used to track disease incidence and prevalence rates, mortality rates, and other public health indicators. • Example: Tracking the number of new COVID-19 cases per day during a pandemic. 4. Clinical Research: • In clinical trials, quantitative data is collected to assess the safety and efficacy of new drugs or treatments. • Example: Measuring the reduction in blood pressure in participants taking an antihypertensive drug. 5. Medical Imaging: • Quantitative data is used in medical imaging to measure organ size, tumor volume, and other anatomical features. • Example: Measuring the size of a kidney stone on an ultrasound image.\n\nStatistical Analysis of Quantitative Data in Medicine\nQuantitative data allows for a variety of statistical analyses, including:\n\nDescriptive Statistics: • Mean: The average value (e.g., average heart rate of a group of patients). • Median: The middle value when data is ordered (e.g., median age of patients in a study). • Standard Deviation: A measure of the variability or spread of data (e.g., variability in blood pressure readings).\nInferential Statistics: • t-tests and ANOVA: Used to compare means between two or more groups. • Regression Analysis: Used to model the relationship between a dependent variable and one or more independent variables. • Correlation: Used to assess the strength and direction of the relationship between two continuous variables.\nProbability and Distributions: • Quantitative data often follows specific statistical distributions (e.g., normal distribution), which are used in probability and inferential statistics.\n\nExamples of Quantitative Data in Medical Practice\nVital Signs Monitoring: • Continuous monitoring of heart rate, blood pressure, and oxygen saturation levels in critically ill patients. • Laboratory Tests: • Quantitative measurement of blood glucose levels, complete blood count (CBC), and electrolyte levels. • Growth and Development: • Tracking height, weight, and head circumference in pediatric patients over time to assess growth and development. • Pharmacokinetics: • Measuring drug concentrations in the blood over time to understand absorption, distribution, metabolism, and excretion.\nApplication of Quantitative Data in Medical Research\n\nRandomized Controlled Trials (RCTs): • Quantitative data is used to compare the effectiveness of new treatments or interventions with standard care or placebo. • Example: Measuring the reduction in blood pressure in patients receiving a new antihypertensive drug versus a placebo.\nLongitudinal Studies: • Quantitative data is collected over time to study changes in health outcomes, risk factors, or disease progression. • Example: Tracking changes in lung function in smokers versus non-smokers over several years.\nHealth Economics: • Quantitative data is used to evaluate the cost-effectiveness of medical interventions, including cost per quality-adjusted life year (QALY) gained. • Example: Calculating the cost-effectiveness of a new cancer treatment based on survival rates and costs.\n\nQuantitative data is a cornerstone of medical science, providing the foundation for diagnosis, treatment, research, and public health. Its ability to be precisely measured and analyzed makes it indispensable for advancing medical knowledge, improving patient care, and making informed decisions in healthcare.\n\n\n2.3.2 Qualitative data (Categorical data)\nQualitative data in medicine refers to non-numerical data that represent categories or groups. Unlike quantitative data, which deals with numbers and measurements, qualitative data describes characteristics or attributes that can be used to classify individuals, objects, or events into distinct groups. This type of data is crucial in medical research and practice for understanding patient demographics, disease classifications, and treatment outcomes. ain types:\n1. Nominal Data\nNominal data consists of categories that do not have a natural order or ranking. These categories are mutually exclusive, meaning that an individual or event can belong to only one category.\nBlood Type: Categories like A, B, AB, and O.\nGender: Male, Female, Other.\nPresence or Absence of a Condition: Yes/No responses (e.g., presence of hypertension).\n\n\n2.3.3 Ordinal Data\nOrdinal data consists of categories that have a meaningful order or ranking, but the intervals between the categories are not necessarily equal or meaningful.\nSeverity of Disease: Mild, Moderate, Severe.\nPain Scale: Numeric rating scale (e.g., 0–10), where 0 is no pain and 10 is the worst pain imaginable.\nStage of Cancer: Stages I, II, III, IV.\nQualitative data plays a vital role in various aspects:\n\nPatient Demographics:\n\nCategorization: Patients are often categorized based on attributes like age group, gender, race, and ethnicity, which are critical for understanding the distribution of diseases and tailoring medical interventions.\nExample: A study may categorize patients by age group (e.g., children, adults, elderly) to analyze how a particular disease affects different age groups.\n\nDisease Classification:\n\nDiagnosis: Diseases are classified into categories based on symptoms, genetic markers, or other clinical criteria. This classification helps in diagnosing, treating, and researching diseases.\nExample: Types of diabetes (Type 1, Type 2, Gestational Diabetes) are categorical variables that guide treatment plans.\n\nTreatment Outcomes:\n\nOutcome Measures: Treatment outcomes can be categorized as successful/unsuccessful, improved/no improvement, or recurrence/no recurrence. These qualitative outcomes are essential for evaluating the effectiveness of treatments.\nExample: A study on cancer treatment might categorize outcomes as “complete remission,” “partial remission,” or “no response.”\n\nQuality of Life and Patient Satisfaction:\n\nSurveys and Questionnaires: Patient satisfaction surveys and quality of life assessments often use ordinal scales (e.g., very satisfied, satisfied, neutral, dissatisfied, very dissatisfied) to gather qualitative data on patient experiences.\nExample: A patient satisfaction survey may categorize responses to a question about hospital care quality into five levels, from “very poor” to “excellent.”\n\n\nStatistical Analysis of Qualitative Data\n\nFrequency and Proportion: The most basic analysis involves counting the number of occurrences in each category and expressing them as frequencies or proportions (percentages).\nExample: Proportion of patients with a specific blood type in a population.\nChi-Square Tests: Used to determine if there is a significant association between two categorical variables.\nExample: Assessing the relationship between smoking status (smoker, non-smoker) and lung cancer diagnosis (yes, no).\n\n\n\nLogistic Regression: Used for modeling the probability of a binary outcome based on one or more predictor variables.\nExample: Predicting the likelihood of a disease relapse based on treatment type and other categorical factors.\nCross-Tabulation: A method used to examine the relationship between two or more categorical variables by displaying the data in a matrix format.\nExample: Cross-tabulating the relationship between gender and the prevalence of a specific medical condition.\n\nApplications in Medical Research\n\nEpidemiology:\n\nResearchers often use qualitative data to study the distribution and determinants of health-related states in populations.\nExample: Categorizing patients by exposure to a risk factor (e.g., exposure to asbestos: yes/no) and examining its relationship with disease occurrence.\n\nClinical Trials:\n\nClinical trials use qualitative data to classify participants, monitor treatment adherence, and assess treatment responses.\nExample: Categorizing patients based on their response to a new drug as “responsive,” “partially responsive,” or “non-responsive.”\n\nPublic Health:\n\nPublic health studies often categorize populations based on demographic factors to identify health disparities and target interventions.\nExample: Identifying vaccination rates across different ethnic groups to improve immunization programs.\nQualitative data is essential in the medical field for categorizing and analyzing various aspects of health and disease. It provides the basis for understanding patterns, making clinical decisions, and conducting research that ultimately improves patient care. Properly collecting, analyzing, and interpreting qualitative data ensures that medical interventions are tailored to the specific needs of patients and populations.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "quat-data-stat-desc.html",
    "href": "quat-data-stat-desc.html",
    "title": "3  Statistical description of quantitative data",
    "section": "",
    "text": "3.1 Prerequisite\nSetting up the required R packages.\nlibrary(tidyverse)\nDownload data\nClick on the download button above to download as a .csv file. Save it in your working directory and import the data file into R using the code below.\nrbc &lt;- read_csv(\"datasets/ex03-01.csv\")\n\n#&gt; Rows: 138 Columns: 1\n#&gt; ── Column specification ─────────────────────────────────────────────────────\n#&gt; Delimiter: \",\"\n#&gt; dbl (1): rbc\n#&gt; \n#&gt; ℹ Use `spec()` to retrieve the full column specification for this data.\n#&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nHere we use the read_csv() function. The first argument is the most important: the path to the file. You can think about the path as the address of the file. The code above will work if you have a ex02-01.csv file in the datasets folder of your project.\nWhen you run read_csv(), it prints out a message telling you the number of rows and columns of data, the delimiter that was used, and the column specifications (names of columns organized by the type of data the column contains). It also prints out some information about retrieving the full column specification and how to quiet this message.\nThe data file has only one column with name rbc. Let’s check if there are some missing values present in it.\nrbc |&gt; \n  anyNA()\n\n#&gt; [1] FALSE\nThe output FALSE indicates no missing values is present.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Statistical description of quantitative data</span>"
    ]
  },
  {
    "objectID": "quat-data-stat-desc.html#prerequisite",
    "href": "quat-data-stat-desc.html#prerequisite",
    "title": "3  Statistical description of quantitative data",
    "section": "",
    "text": "Example 1: \nA researcher used a random sampling method to examine the red blood cell counts of 138 normal adult women. The measuring results are saved in a data file. Please use the data to create a frequency distribution table.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Statistical description of quantitative data</span>"
    ]
  },
  {
    "objectID": "quat-data-stat-desc.html#visualization",
    "href": "quat-data-stat-desc.html#visualization",
    "title": "3  Statistical description of quantitative data",
    "section": "3.2 Visualization",
    "text": "3.2 Visualization\n\n3.2.1 Frequency table\nA frequency table mentioned here is a statistical tool that organizes data into intervals and lists the number of frequency in each interval. It helps summarize large dataset by showing how often each value or range of values occurs, making it easier to identify trends and the overall distribution of the data. This table is often used in conjunction with visual tools like histogram to enhance data interpretation.\nHere is the steps for creating a frequency table for continuous variables.\n\nComputing the minimum and maximum of the variable.\n\n\nmin_rbc &lt;- rbc |&gt; \n  min()\n\nmax_rbc &lt;- rbc |&gt; \n  max()\n\n\nDetermining the number of intervals. The number of intervals is typically between 10 and 15; here, we adopt 10. The seq() function is used to obtain upper and lower limits of the intervals.\n\n\nbins = 12\nbreaks &lt;- seq(min_rbc, max_rbc, length.out = bins + 1)\nbreaks\n\n#&gt;  [1] 3.070000 3.269167 3.468333 3.667500 3.866667 4.065833 4.265000 4.464167\n#&gt;  [9] 4.663333 4.862500 5.061667 5.260833 5.460000\n\n\n\nUsing the cut() function to dive each data into their respective intervals.\n\n\ncut(pull(rbc), breaks = breaks, include.lowest = T, right = F) |&gt; \n  head(40)\n\n#&gt;  [1] [3.87,4.07) [4.07,4.26) [4.26,4.46) [3.47,3.67) [5.06,5.26) [3.87,4.07)\n#&gt;  [7] [4.26,4.46) [3.67,3.87) [4.66,4.86) [4.07,4.26) [4.46,4.66) [4.07,4.26)\n#&gt; [13] [3.67,3.87) [4.07,4.26) [4.26,4.46) [3.07,3.27) [4.86,5.06) [3.87,4.07)\n#&gt; [19] [4.26,4.46) [3.47,3.67) [4.46,4.66) [3.87,4.07) [4.46,4.66) [4.07,4.26)\n#&gt; [25] [4.46,4.66) [3.87,4.07) [4.26,4.46) [3.47,3.67) [4.86,5.06) [3.87,4.07)\n#&gt; [31] [4.26,4.46) [4.07,4.26) [4.66,4.86) [4.07,4.26) [4.46,4.66) [4.07,4.26)\n#&gt; [37] [4.46,4.66) [3.67,3.87) [3.87,4.07) [4.07,4.26)\n#&gt; 12 Levels: [3.07,3.27) [3.27,3.47) [3.47,3.67) [3.67,3.87) ... [5.26,5.46]\n\n\n\nGenerating the frequency table.\n\n\ncut(pull(rbc), breaks = breaks, include.lowest = T, right = F) |&gt;\n  table() |&gt; \n  knitr::kable(col.names = c(\"interval\", \"freq\"), align = \"c\")\n\n\n\n\ninterval\nfreq\n\n\n\n\n[3.07,3.27)\n2\n\n\n[3.27,3.47)\n3\n\n\n[3.47,3.67)\n9\n\n\n[3.67,3.87)\n14\n\n\n[3.87,4.07)\n22\n\n\n[4.07,4.26)\n30\n\n\n[4.26,4.46)\n21\n\n\n[4.46,4.66)\n15\n\n\n[4.66,4.86)\n10\n\n\n[4.86,5.06)\n6\n\n\n[5.06,5.26)\n4\n\n\n[5.26,5.46]\n2\n\n\n\n\n\n\n\n3.2.2 Frequency histogram\nA frequency histogram is a graphical representation of a frequency table. It displays the distribution of numerical variales by showing the frequency (count) of a value within specific intervals (bins) on the x-axis, with the y-axis representing the frequency. Each bar in the histogram corresponds to an interval, and the height of the bar indicates how many valuess fall within that range. This visual tool is useful for quickly assessing the shape, spread, and central tendency of the data distribution.\nHere we supply two methods to plot a histogram.\n\nbaseggplot2\n\n\n\nhist(\n  x              = pull(rbc), \n  breaks         = breaks, \n  freq           = T,\n  right          = F, \n  col            = \"skyblue\", \n  include.lowest = T,\n  main           = \" \",\n  xlab           = \"Maximum heart rate\",\n  ylab           = \"Frequency\",\n  ylim           = c(0, 32),\n  labels         = T\n)\n\n\n\n\n\n\n\n\n\n\n\nrbc |&gt; \n  ggplot(aes(x = rbc)) +\n  geom_histogram(\n    fill   = \"skyblue\", \n    stat   = \"bin\",\n    color  = \"black\",\n    breaks = breaks,\n    closed = \"left\"\n  ) +\n  stat_bin(\n    geom   = \"text\", \n    aes(label = after_stat(count)),\n    breaks = breaks, \n    closed = \"left\",\n    size   = 4,\n    vjust  = - 0.3\n  ) +\n  labs(x = \"Maximum heart rate\", y = \"Frequency\") +\n  theme(\n    axis.title.x     = element_text(size = 12), \n    axis.title.y     = element_text(size = 12), \n    axis.text.x      = element_text(size = 11),  \n    axis.text.y      = element_text(size = 11),\n    panel.background = element_blank(),        \n    axis.line        = element_line(color = \"black\") \n  )",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Statistical description of quantitative data</span>"
    ]
  },
  {
    "objectID": "quat-data-stat-desc.html#measures-of-central-tendency",
    "href": "quat-data-stat-desc.html#measures-of-central-tendency",
    "title": "3  Statistical description of quantitative data",
    "section": "3.3 Measures of central tendency",
    "text": "3.3 Measures of central tendency\nCentral tendency is a statistical concept that refers to the measure of the center or typical value in a data set. It provides a summary of the data with a single value that represents the middle or average of the data. The most common measures of central tendency are:\n\n3.3.1 Mean\nThe arithmetic average of all values. It’s calculated by summing all the values and dividing by the number of values.\nFor a population:\n\n\\mu = \\frac{\\sum x_i}{N}\n\nFor a sample:\n\n\\bar{X} = \\frac{\\sum x_i}{n}\n\nwhere x_i represents values of a random variable X, and N and n are the sizes of the population and sample, respectively.\n\nrbc |&gt; \n  pull() |&gt; \n  mean()\n\n#&gt; [1] 4.227029\n\n\n\n\n3.3.2 Median\nThe middle value in a data set when the values are sorted in ascending order. If there is an even number of values, the median is the average of the two middle values. Unlike the mean, the median is not affected by outliers or skewed data, making it a robust indicator of central tendency. To find the median:\n\nSort the data set.\nIf the number of observations N is odd, the median is the middle value.\nIf N is even, the median is the average of the two central values.\n\n\nrbc |&gt; \n  pull() |&gt; \n  median()\n\n#&gt; [1] 4.23",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Statistical description of quantitative data</span>"
    ]
  },
  {
    "objectID": "quat-data-stat-desc.html#measures-of-dispersion",
    "href": "quat-data-stat-desc.html#measures-of-dispersion",
    "title": "3  Statistical description of quantitative data",
    "section": "3.4 Measures of dispersion",
    "text": "3.4 Measures of dispersion\nDispersion or variability, describe the spread or dispersion of data points in a data set. They provide insight into how much individual data points differ from the central value (mean, median, etc.). Common measures of dispersion include:\n\n3.4.1 Range\nThe difference between the maximum and minimum values in the data set.\n\n\\text{range} = \\text{max} - \\text{min}\n\n\nrange(rbc) |&gt; \n  diff()\n\n#&gt; [1] 2.39\n\n\n\n\n3.4.2 Interquartile range\nInterquartile range (IQR) is the range of the middle 50% of the data, calculated as the difference between the 75th percentile (Q3) and the 25th percentile (Q1).\n\n\\text{IQR} = Q3 - Q1\n\nYou can directly use the IQR() function to get IQR.\n\nrbc |&gt; \n  pull() |&gt; \n  IQR()\n\n#&gt; [1] 0.565\n\n\n\n\n3.4.3 Variance\nMeasures the average squared deviation of each data point from the mean.\nFor a population:\n\n\\sigma^2 = \\frac{1}{N} \\sum_{i=1}^N (x_i - \\mu)^2\n\nFor a sample:\n\nS^2 = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{X})^2\n\nwhere \\mu is the population mean, \\bar{X} is the sample mean, x_i represents each value, and N and n are are the sizes of the population and sample, respectively.\nYou can directly use the var() function to get variance.\n\nrbc |&gt; \n  pull() |&gt; \n  var()\n\n#&gt; [1] 0.1986751\n\n\n\n\n3.4.4 Standard deviation\nThe square root of the variance, providing a measure of spread in the same units as the data.\nFor a population:\n\n\\sigma = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N (x_i - \\mu)^2}\n\nFor a sample:\n\nS = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{X})^2}\n\nYou can directly use the sd() function to get standard deviation.\n\nrbc |&gt; \n  pull() |&gt; \n  sd()\n\n#&gt; [1] 0.4457298\n\n\n\n\n3.4.5 Coefficient of variation\nThe ratio of the standard deviation to the mean, expressed as a percentage, useful for comparing variability between variables with different units or scales.\nFor a population:\n\n\\text{CV} = \\frac{\\sigma}{\\mu} \\times 100%\n\nwhere \\sigma is the standard deviation and \\mu is the mean of a population.\nFor a sample:\n\n\\text{CV} = \\frac{S}{\\bar{X}} \\times 100%\n\nwhere S is the standard deviation and \\bar{X} is the mean of a sample.\n\nmean &lt;- rbc |&gt; \n  pull() |&gt; \n  mean()\n\nsd &lt;- rbc |&gt; \n  pull() |&gt; \n  sd()\n\nsd / mean * 100\n\n#&gt; [1] 10.54475",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Statistical description of quantitative data</span>"
    ]
  },
  {
    "objectID": "normal-distribution.html",
    "href": "normal-distribution.html",
    "title": "4  Normal distribution",
    "section": "",
    "text": "4.1 Prerequisite\nSetting up the required R packages in this chapter.\nlibrary(tidyverse)\nlibrary(nortest)\nlibrary(scales)\nlibrary(e1071)",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Normal distribution</span>"
    ]
  },
  {
    "objectID": "normal-distribution.html#normal-distribution",
    "href": "normal-distribution.html#normal-distribution",
    "title": "4  Normal distribution",
    "section": "4.2 Normal distribution",
    "text": "4.2 Normal distribution\nThe probability density function of a normal distribution is given by the formula:\nf(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}}\nwhere x is the variable, \\mu is the mean, and \\sigma is the standard deviation. The \\mu and \\sigma are two parameters of the distribution. The mean \\mu is a location parameter, which defines the central position, as shown in Figure 4.1 . The standard deviation \\sigma is the shape parameter, which defines the width and height of the distribution, as shown in Figure 4.2 .\n\n\n\n\n\n\n\n\nFigure 4.1: The normal curve with different means\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.2: The normal curve with different standard deviation\n\n\n\n\n\n\n4.2.1 Standard normal distribution\nAs shown above, the change of \\mu and \\sigma affects the shape of a normal distribution. For convenience, let\nz = \\frac{x - \\mu}{\\sigma}\nthen the above mentioned probability density function will become as:\nf(z) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{1}{2}z^2} which is called the standard normal distribution, where of \\mu = 0 and \\sigma = 1.\n\n\n4.2.2 Area under curve\nThe cumulative distribution function is the probability that a normal random variable X will be less than or equal to a given value x, which is defined by the formula:\nF(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\int_{-\\infty}^x e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}}The area under the curve of a normal distribution over a specific interval represents the probability that the random variable falls within that interval. This is computed using the cumulative distribution function.\nIn R, you can calculate the area under the curve between two points using the pnorm() function.\n\nExample 1: \nCalculating the area under the standard normal curve between -1.96 and 1.96.\n\n\narea_under_curve &lt;- pnorm(1.96, mean = 0, sd = 1) - pnorm(-1.96, mean = 0, sd = 1)\narea_under_curve\n\n#&gt; [1] 0.9500042\n\n\n\n\n4.2.3 Visualizing the area\nYou can use ggplot2 package in R to plot the normal distribution curve and shade the area under the curve. Here is example to shade the area of the left and right tails.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Normal distribution</span>"
    ]
  },
  {
    "objectID": "normal-distribution.html#sec-normality-test",
    "href": "normal-distribution.html#sec-normality-test",
    "title": "4  Normal distribution",
    "section": "4.3 Normality test",
    "text": "4.3 Normality test\nNormality test is used to determine whether a data follows a normal distribution. This is important because many statistical tests, including the t-test, assume that the data follows a normal distribution.\n\nExample 2: \nA scientist used a random sampling method to examine the red blood cell count of 29 normal adult men. The measuring results are saved in the below file. Please analyze its normality.\n\n  Download data \nYou can click on the download button above to download and the save it in your own folder. Here we import the data file into R and assign to a tibble named rbc.\n\nrbc &lt;- read_csv(\"datasets/ex04-01.csv\")\n\n#&gt; Rows: 29 Columns: 1\n#&gt; ── Column specification ─────────────────────────────────────────────────────\n#&gt; Delimiter: \",\"\n#&gt; dbl (1): rbc\n#&gt; \n#&gt; ℹ Use `spec()` to retrieve the full column specification for this data.\n#&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nTest of normality is used to determine if a variable is well-modeled by a normal distribution. This is an important assumption for many statistical tests. Histogram (Section 3.2.2) can be used as a visual tool to assess whether a variable follows a normal distribution, but it should be used with caution due to the reasons below:\n\nFor a small sample data, the histogram might not provide a clear picture of the distribution, making it harder to assess normality. With larger samples, the histogram gives a better indication but may still be misleading.\nThe appearance of the histogram can change significantly depending on the number of bins (or the bin width). Too few bins might obscure important features of the data, while too many bins might introduce noise.\nThe interpretation of a histogram is somewhat subjective. Two people might look at the same histogram and draw different conclusions about normality.\n\nSince histogram can sometimes be misleading or ambiguous, it’s a good practice to use it alongside other methods:\n\n4.3.1 Normality test method\nStatistical tests can provide a more formal assessment of normality, though they also have limitations and can be sensitive to sample size. Here we only show a few commonly used:\n\nShapiro-Wilk test: Best for small to medium-sized data.\nShapiro-Francia test: A variation of the Shapiro-Wilk test. It is generally more appropriate for dealing with larger sample size data compared to the Shapiro-Wilk test, particularly for data that is expected to be normally distributed.\nAnderson-Darling test: Gives more weight to the tails of the distribution.\n\nWe perform the Shapiro-Wilk test using shapiro.test(), which lies in the stats package of base R. The Shapiro-Francia test and Anderson-Darling test are performed by sf.test() and ad.test(), respectively. Both of them come from the nortest package, which need to be installed beforehand.\n\nShapiro-WilkShapiro-FranciaAnderson-Darling\n\n\n\nrbc |&gt; \n  pull() |&gt; \n  shapiro.test()\n\n#&gt; \n#&gt;  Shapiro-Wilk normality test\n#&gt; \n#&gt; data:  pull(rbc)\n#&gt; W = 0.98382, p-value = 0.9228\n\n\n\n\n\nrbc |&gt; \n  pull() |&gt; \n  sf.test()\n\n#&gt; \n#&gt;  Shapiro-Francia normality test\n#&gt; \n#&gt; data:  pull(rbc)\n#&gt; W = 0.97756, p-value = 0.682\n\n\n\n\n\nrbc |&gt; \n  pull() |&gt; \n  ad.test()\n\n#&gt; \n#&gt;  Anderson-Darling normality test\n#&gt; \n#&gt; data:  pull(rbc)\n#&gt; A = 0.22677, p-value = 0.7975\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nIf the p-value is greater than 0.05, the data is normally distributed (fail to reject the null hypothesis).\nIf the p-value is less than 0.05, the data is not normally distributed (reject the null hypothesis).\n\n\n\n\n\n4.3.2 Visualization method\nQ-Q Plot (Quantile-Quantile Plot) is a more reliable visual tool that plots the quantiles of the data against the quantiles of a theoretical normal distribution. Visual inspection of the data can be very informative and is always useful as a supplementary method.\nYou can create a Q-Q Plot using qqnorm() function. More, you can add a line to a theoretically normal quantile-quantile plot by qqline(), which passes through the first and third quartiles by default. These two functions can be found from the stats package of base R.\n\nrbc |&gt; \n  pull() |&gt; \n  qqnorm(main = \"\", datax = T)\n\nrbc |&gt; \n  pull() |&gt; \n  qqline(datax = T)\n\n\n\n\n\n\n\n\nAlternatively, you can use the ggplot2 package to create a Q-Q plot, which has more customization and flexibility. Here is an example.\n\nrbc |&gt;\n  ggplot(aes(sample = rbc)) +\n  geom_qq(shape = 1, size = 2.3) +\n  geom_qq_line() +\n  labs(x = \"Theoretical Quantiles\", y = \"Sample Quantiles\") +\n  coord_flip() +\n  theme(\n    axis.text  = element_text(size = 12),\n    axis.title = element_text(size = 12)\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nIf the data points fall approximately along the reference line in the Q-Q plot, the data is likely normally distributed.\nSignificant deviations from the line indicate departures from normality.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Normal distribution</span>"
    ]
  },
  {
    "objectID": "normal-distribution.html#medical-reference-range",
    "href": "normal-distribution.html#medical-reference-range",
    "title": "4  Normal distribution",
    "section": "4.4 Medical reference range",
    "text": "4.4 Medical reference range\nA medical reference range is the set of values that a medical test result falls within for a healthy population. These ranges are used by healthcare providers to interpret laboratory results and determine whether a patient’s test results are normal or indicate a potential health issue.\n\n4.4.1 Establishment\nReference ranges are typically established by testing a large sample of healthy individuals and determining the range within which a certain percentage (often 95%) of results fall. The middle 95% of the population’s values are considered the reference range, meaning that 2.5% of healthy individuals might naturally have results slightly below this range and another 2.5% slightly above it.\n\nExample 3: \nA investigator randomly sampled 180 normal adult males in a region, and measured the fibrinogen levels (g/L) in their venous blood. The data is saved in a file below. Have a try to establish the 95% medical reference range for fibrinogen level of normal adult males in that region.\n\n  Download data \nThe code chunk below can print the results directly.\n\nmed_ref_range &lt;- read_csv(\"datasets/ex04-02.csv\") |&gt; \n  pull() |&gt; \n  quantile(probs = c(0.025, 0.975)) |&gt; \n  round(digits = 2) |&gt; \n  print()\n\n#&gt; Rows: 180 Columns: 1\n#&gt; ── Column specification ─────────────────────────────────────────────────────\n#&gt; Delimiter: \",\"\n#&gt; dbl (1): fibrinogen\n#&gt; \n#&gt; ℹ Use `spec()` to retrieve the full column specification for this data.\n#&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n#&gt;  2.5% 97.5% \n#&gt;  1.77  3.63\n\n\nThe result indicates that the 95% medical reference range for the fibrinogen level of normal adult males is: (1.77, 3.63).\n\n\n4.4.2 Result interpretation\nA test result that falls within the reference range is usually considered normal. A result outside the reference range might indicate an abnormal condition, but this must be interpreted in the context of the patient’s overall health, symptoms, and medical history. Not all out-of-range results indicate disease; they might be normal for a specific individual due to factors like temporary stress, diet, or exercise.\n\n\n4.4.3 Considerations\nReference ranges are not absolute; what is normal for one individual may not be normal for another, especially at the edges of the range. Doctors consider a variety of factors, including patient history and symptoms, when interpreting test results. An out-of-range result may warrant further testing or a different interpretation based on the clinical context.\nReference ranges may be updated as new research and technologies emerge, so staying informed about the latest standards is important for accurate diagnosis and treatment.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Normal distribution</span>"
    ]
  },
  {
    "objectID": "statistical-inference.html",
    "href": "statistical-inference.html",
    "title": "5  Statistical inference",
    "section": "",
    "text": "5.1 Prerequisites\nlibrary(tidyverse)\nlibrary(rmarkdown)",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistical inference</span>"
    ]
  },
  {
    "objectID": "statistical-inference.html#distribution-of-sample-means",
    "href": "statistical-inference.html#distribution-of-sample-means",
    "title": "5  Statistical inference",
    "section": "5.2 Distribution of sample means",
    "text": "5.2 Distribution of sample means\nThe distribution of sample means is a fundamental concept in statistics, describing the distribution of sample means obtained from multiple samples drawn from the same population. Here is a example to demonstrate the distribution of sample means.\n\nExample 1: \nAssume thrombin time follow the normal distribution of \\mu=17.5, \\sigma=1.2. A researcher randomly drew 100 samples from a population of size 60000 , with each sample n=10 observations. The data is saved the the data below. The mean \\bar{X} and standard deviation S for each sample are shown in Table 5.1, please analyze the distribution of the 120 means.\n\n  Download data \n\ndf &lt;- read_csv(\"datasets/ex05-01.csv\", show_col_types = F) |&gt; \n  summarise_all(\n    list(\n1      n       = ~ n(),\n      median  = median,\n      mean    = mean,\n      sd      = sd        \n    )\n  ) |&gt; \n  pivot_longer(\n    cols      = everything(),\n    names_to  = c(\"sample\", \".value\"),\n    names_sep = \"_\"\n  ) |&gt; \n  mutate(across(c(mean, sd), \\(x)round(x, 2)))\n\ndf |&gt; paged_table()\n\n\n1\n\nHere n() must use a style of anonymous function, unlike the other three ones.\n\n\n\n\n\n\nTable 5.1: The n, median, mean and sd of the 100 random samples (s)\n\n\n\n\n  \n\n\n\n\n\n\nIt is clear that the mean of each sample is different, and also may not equal to the mean of the population. The difference between a population parameter and a corresponding sample statistic is called sampling error. It arises because only a random sample of the entire population is observed, which may not perfectly represent the entire population.\nTo visualize the distribution of the 120 sample means, here we create a histogram:\n\ndf |&gt; \n  select(mean) |&gt; \n  pull() |&gt; \n  hist(\n  freq           = T,\n  right          = F, \n  col            = \"lightblue\", \n  include.lowest = T,\n  main           = \" \",\n  xlab           = \"The average of the thrombin time\",\n  ylab           = \"Frequency\",\n  ylim           = c(0, 26),\n  labels         = T\n)\n\n\n\n\n\n\n\n\nFrom the figure above we can say that the distribution of the sample mean can be approximated by a normal distribution. The two sample statistics of mean \\bar{X} and standard deviation S can be achieved by the following code:\n\ndf |&gt; \n  select(x = mean) |&gt; \n  summarise(\n    mean = mean(x),\n    sd   = sd(x)\n  ) |&gt; \n  round(digits = 2) |&gt; \n  knitr::kable(align = \"c\")\n\n\n\n\nmean\nsd\n\n\n\n\n17.46\n0.36\n\n\n\n\n\nThe mean of the sample means is 17.5, equal to the population mean \\mu=1.75, while the standard deviation of the sample mean is 0.4, less than the standard deviation of population \\sigma = 1.25.\n\n5.2.1 Standard error\nThe standard deviation of the distribution of the sample means is called the standard error (SE). It reflects the typical distance between a sample mean and the population mean. The standard error of sample means is given by:\n\\sigma_{\\bar{X}} = \\frac{\\sigma}{\\sqrt{n}}\nHowever, the population parameter \\sigma is usually unknown, therefore, the sample standard deviation S is used as an estimator for the population standard deviation. Therefore, an estimation of the standard error above mentioned is defined by:\nS_{\\bar{X}} = \\frac{S}{\\sqrt{n}}\nAs the sample size n increases, the standard error decreases, meaning the sample means will be closer to the population mean.\nIt is need to be pointed out that the above formula for calculating the sampling error of the mean is only adapt to simple random sampling. For other sampling methods, there are corresponding formulas exist.\n\n\n5.2.2 Central Limit Theorem\nRegardless of the population distribution, as the sample size n becomes large (typically n \\geq 30 is considered sufficient), the sampling distribution of the sample mean will approximate a normal distribution. This is the essence of the Central Limit Theorem in statistics.\nFor small sample sizes, if the population itself is normally distributed, the distribution of the sample means will also be normally distributed.\nBecause the sampling distribution of the sample means can be approximated by a normal distribution (especially for large n), it allows for the construction of confidence intervals for the population mean and the conducting of hypothesis tests. The normality of the sampling distribution justifies the use of z-test or t-test depending on whether the population variance is known or unknown and the sample size.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistical inference</span>"
    ]
  },
  {
    "objectID": "statistical-inference.html#t-distribution",
    "href": "statistical-inference.html#t-distribution",
    "title": "5  Statistical inference",
    "section": "5.3 t distribution",
    "text": "5.3 t distribution\nThe sample mean \\bar{X} follows a normal distribution when the underlying population is normally distributed, or when the sample size is sufficiently large due to the Central Limit Theorem. However, when you’re converting the sample mean into a standardized form, the distribution you use depends on whether or not you know the population standard deviation \\sigma.\n\nIf \\sigma is known, the sample mean \\bar{X} can be standardized using the formula:\n\n\nz = \\frac{\\bar{X} - \\mu}{\\sigma_{\\bar{X}}} = \\frac{\\bar{X} - \\mu}{\\sigma / \\sqrt{n}}\n\nIn this case, the standardized variable Z follows a standard normal distribution N(0, 1).\n\nIf \\sigma is unknown, which is often the case in practice, you need to estimate it using the sample standard deviation S. The formula for standardizing the sample mean becomes:\n\n\nt = \\frac{\\bar{X} - \\mu}{S_{\\bar{X}}} = \\frac{\\bar{X} - \\mu}{S / \\sqrt{n}}\n\nHere, t doesn’t not follow a standard normal distribution anymore, it follows a t-distribution with degrees of freedom \\nu = n - 1 (n is the sample size).\nThe t-distribution, also known as Student’s t-distribution, is used in statistics when estimating population parameters when the sample size is small and/or the population variance is unknown. It’s especially important in confidence intervals, hypothesis testing, and regression analysis.\n\n5.3.1 Visual representation\nThe t-distribution can be plotted to show how it compares with the normal distribution.\n\n\n\n\n\n\n\n\n\nThe t-distribution is similar to the standard normal distribution, but it has heavier tails. This means a higher probability of values further from the mean, especially with a smaller degree of freedom. As degrees of freedom increase, the t-distribution curve will converge toward the standard normal distribution curve.\nThe shape of the t-distribution depends on the degrees of freedom \\nu, which is typically related to the sample size (\\nu = n - 1). As the degrees of freedom increase, the t-distribution approaches the standard normal distribution. For large sample sizes (\\nu &gt; 60), the t-distribution and normal distribution are almost indistinguishable.\n\n\n5.3.2 Common uses\n\nConfidence intervals\nThe t-distribution is used to construct confidence intervals for the population mean when the population variance is unknown and the sample size is small.\nt-test: Used to compare the means of two groups when the sample size is small and the population standard deviation is unknown.\nRegression analysis: In linear regression, t-test are used to determine whether the coefficients of the independent variables are significantly different from zero.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistical inference</span>"
    ]
  },
  {
    "objectID": "statistical-inference.html#estimation-of-population-mean",
    "href": "statistical-inference.html#estimation-of-population-mean",
    "title": "5  Statistical inference",
    "section": "5.4 Estimation of population mean",
    "text": "5.4 Estimation of population mean\nParameter estimation is a concept in statistics that involves using statistic off a sample to estimate parameters of the corresponding population. The sample should be random and large enough to provide an accurate estimate. A parameter is a numerical characteristic about a population, such as a mean, proportion, or standard deviation. Here we just talk about the estimation of population mean.\n\n5.4.1 Point estimation\nThis involves using sample data to calculate a single value that serves as the best guess for a population mean. However, obtaining an exact point estimate of the population mean from just one random sample is almost unattainable.\nCalculate the sample mean \\bar{X}: The sample mean is calculated using the formula:\n\n\\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\n\nwhere n is the sample size, and x_i are the individual sample values. The sample mean (\\bar{X}) serves as an estimate of the population mean (\\mu).\nHowever, a point estimate does not consider the sampling error and cannot tell you how close the estimate is likely to be to the true population mean. Instead of relying on a single point estimate, interval estimation are often used because they provide a range of values within which the true population mean is likely to lie along with a confidence level.\n\n\n5.4.2 Confidence interval\nThe method for calculating the confidence interval of the population mean varies depending on whether the population standard deviation \\sigma is known and the size of sample n. Typically, there are two types of methods: the t-distribution and the z-distribution (also known as the u-distribution). The following will introduce the methods for calculating the confidence interval of a single population mean and the confidence interval of the difference between two population means.\nSingle population mean\nConfidence interval (CI) is associated with a confidence level, typically 95%, which indicates the degree of certainty that the true population mean falls in the range. The two-sided 1-\\alpha confidence interval of single population mean can be formulated as:\n\n\\text{CI} = \\bar{X} \\pm t_{\\nu,\\alpha/2} \\cdot \\frac{S}{\\sqrt{n}} \\quad \\text{or} \\quad \\text{CI} = \\bar{X} \\pm z_{\\alpha/2} \\cdot \\frac{S}{\\sqrt{n}}\n\nSimilarly, the one-sided 1-\\alpha confidence interval for the population mean is given by：\n\n\\mu&gt; \\bar{X} - t_{\\nu,\\alpha} \\cdot \\frac{S}{\\sqrt{n}} \\quad \\text{or} \\quad \\mu&gt; \\bar{X} - z_\\alpha \\cdot \\frac{S}{\\sqrt{n}}\n\n\n\\mu &lt; \\bar{X} + t_{\\nu,\\alpha} \\cdot \\frac{S}{\\sqrt{n}} \\quad \\text{or} \\quad \\mu &lt; \\bar{X} + z_\\alpha \\cdot \\frac{S}{\\sqrt{n}}\n\nwhere t is the critical value from the t-distribution, it depends on the degree of freedom \\nu and the level of confidence \\alpha; z is the critical value from the standard normal distribution, it depends on the level of confidence \\alpha.\n\nExample 2: \nA research group randomly selected 20 adult men in a area and measured their red blood cell count. Please calculate the 95% confidence interval for the population mean.\n\n  Download data \nSince the sample size is small, we would use the t-distribution to estimate the population mean. In practice, you can always use the t-distribution regardless of the size of samples, because as the increase of degrees of freedom , t-distribution approaches the standard normal distribution.\nIn this chapter, we write a function named mean_CI() to tackle this problem.\n\n\nmean_CI  &lt;- function(x, alternative = \"two.sided\", conf.level = 0.95){\n  x_bar  &lt;- mean(x, na.rm = T)\n  s      &lt;-  sd(x, na.rm = T)\n  n      &lt;- length(x)  \n  stderr &lt;- s / sqrt(n)\n  \n  type  &lt;- alternative\n  alpha &lt;- 1- conf.level\n  \n  case_when(\n    type == \"two.sided\" ~ {\n      t_stat &lt;- qt(1 - alpha / 2, df = n - 1)\n      tibble( \n        mean       = x_bar,\n        lower_ci   = x_bar - stderr * t_stat,\n        upper_ci   = x_bar + stderr * t_stat,\n        conf_level = conf.level\n      )\n    },\n    \n    type == \"greater\"   ~ {\n      t_stat &lt;- qt(alpha, df = n - 1)\n      tibble(\n        mean       = x_bar,\n        lower_ci   = x_bar + stderr * t_stat,\n        upper_ci   = Inf,\n        conf_level = conf.level\n      )\n    },\n      \n    type == \"less\"      ~ {\n      t_stat &lt;- qt(alpha, df = n - 1)\n      tibble( \n        mean       = x_bar,\n        lower_ci   = -Inf,\n        upper_ci   = x_bar - stderr * t_stat,\n        conf_level = conf.level\n     )\n    }\n  )\n}\n\n\n\nread_csv(\"datasets/ex05-02.csv\", show_col_types = F) |&gt; \n  pull() |&gt; \n  mean_CI() \n\n#&gt; # A tibble: 1 × 4\n#&gt;    mean lower_ci upper_ci conf_level\n#&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n#&gt; 1  5.31     5.04     5.59       0.95\n\n\nAlternatively, you can use a function MeanCI() from DescTools package.\n\nread_csv(\"datasets/ex05-02.csv\", show_col_types = F) |&gt; \n  pull() |&gt;\n  DescTools::MeanCI()\n\n#&gt;     mean   lwr.ci   upr.ci \n#&gt; 5.312500 5.035249 5.589751\n\n\nDifference between two population means\nThe confidence interval for the difference between two population means is used to estimate the range within which the true difference between the means of two populations lies, based on sample data.\nRandomly sample from two normal populations N(\\mu_1, \\sigma^2) and N(\\mu_2, \\sigma^2) with equal population variances but unequal population means. The sample sizes, means, and standard deviations of the two samples are denoted by n_1, \\bar{X}_1, S_1 and n_2, \\bar{X}_2, S_2 respectively. If the population variances are unknown and the sample sizes are small, then the two-sided (1-\\alpha) confidence interval for the difference between the two population means (\\mu_1 - \\mu_2) is given by:\n\n(\\bar{X}_1 - \\bar{X}2) \\pm t_{\\alpha/2, \\nu} \\cdot \\sqrt{S_c^2(\\frac{1}{n_1} + \\frac{1}{n_2})} \\ , \\quad S_c^2 = \\frac{(n_1 -1)S_1^2 + (n_2 -1)S_2^2}{n_1 + n_2 -2}\n\nWhere t_{\\alpha/2, \\nu} is the t-score with \\nu = n_1 + n_2 - 2 degrees of freedom, \\alpha is the level of confidence, S_c^2 is called the pooled variance.\nIf thevariances of the two populations mentiond above are unknown and unequal, then the two-sided (1-\\alpha) confidence interval for the difference between the two population means (\\mu_1 - \\mu_2) is given by:\n\n(\\bar{X}_1 - \\bar{X}2) \\pm t_{\\alpha/2, \\nu} \\cdot \\sqrt{\\frac{S_1^2}{n_1} + \\frac{S_2^2}{n_2}}\\ , \\\n\\nu \\approx \\frac{\\left(\\frac{S_1^2}{n_1} + \\frac{S_2^2}{n_2}\\right)^2}{\\frac{\\left(\\frac{S_1^2}{n_1}\\right)^2}{n_1 - 1} + \\frac{\\left(\\frac{S_2^2}{n_2}\\right)^2}{n_2 - 1}}\n\nIf the sample sizes of both samples are large (e.g., both are greater than 60)， the confidence interval for the difference between two population means \\mu_1 - \\mu_2 is given by:\n\n(\\bar{X}_1 - \\bar{X}2) \\pm z_{\\alpha/2} \\cdot \\sqrt{\\frac{S_1^2}{n_1} + \\frac{S_2^2}{n_2}}\n\nWhere \\bar{X}_1 and \\bar{X}_2 are the sample means；n_1 and n_2 are the sample sizes. S_1^2 and S_2^2are the sample variances. z_{\\alpha/2} is the z-score corresponding to the desired confidence level.\nHere is a custom function named mean_diff_CI() to resolve this issue.\n\n\nmean_diff_CI &lt;- function(x, y, alternative = \"two.sided\", conf.level = 0.95, \n                         var.equal = FALSE){\n  x &lt;- na.omit(x)\n  y &lt;- na.omit(y)\n  \n  x_bar &lt;- mean(x)\n  v1    &lt;- var(x)\n  n1    &lt;- length(x)  \n  \n  y_bar &lt;- mean(y)\n  v2    &lt;- var(y)\n  n2    &lt;- length(y)  \n  \n  type  &lt;- alternative\n  alpha &lt;- 1- conf.level\n  diff  &lt;-  x_bar - y_bar \n  \n  if(var.equal) {\n    df     &lt;- n1 + n2 - 2\n    var    &lt;- ((n1 - 1) * v1 + (n2 - 1) * v2) / df\n    stderr &lt;- sqrt(var * (1 / n1 + 1 / n2))\n  } else{\n    stderr1 &lt;- sqrt(v1/n1)\n    stderr2 &lt;- sqrt(v2/n2)\n    stderr  &lt;- sqrt(stderr1^2 + stderr2^2)\n    df      &lt;- stderr^4/(stderr1^4/(n1 - 1) + stderr2^4/(n2 - 1))\n  }\n  \n  case_when(\n    type == \"two.sided\" ~ {\n      t_stat &lt;- qt(1 - alpha / 2, df)\n      tibble( \n        mean_x     = x_bar,\n        mean_y     = y_bar,\n        mean_diff  = diff,\n        lower_ci   = diff - stderr * t_stat,\n        upper_ci   = diff + stderr * t_stat,\n        conf_level = conf.level\n      )\n    },\n    \n    type == \"greater\"   ~ {\n      t_stat &lt;- qt(alpha, df)\n      tibble( \n        mean_x     = x_bar,\n        mean_y     = y_bar,\n        mean_diff  = diff,\n        lower_ci   = diff + stderr * t_stat,\n        upper_ci   = Inf,\n        conf_level = conf.level\n      )\n    },\n      \n    type == \"less\"      ~ {\n      t_stat &lt;- qt(alpha, df)\n      tibble( \n        mean_x     = x_bar,\n        mean_y     = y_bar,\n        mean_diff  = diff,\n        lower_ci   = -Inf,\n        upper_ci   = diff - stderr * t_stat,\n        conf_level = conf.level\n     )\n    }\n  )\n}\n\n\n\nExample 3: \nTo compare the difference of hemoglobin concentration between people with alpha thalassemia trait and silent carrier, a doctor choose a random sample of 42 patients from a patient database. 22 subjects have alpha thalassemia trait, 20 are silent_carrier. Have a try to analyze the 95% confidence interval of the two populations.\n\n  Download data \n\ndf &lt;- read_csv(\"datasets/ex05-03.csv\", show_col_types = F)\nx &lt;- pull(df, 1)\ny &lt;- pull(df, -1)\nmean_diff_CI(x, y, var.equal = T)\n\n#&gt; # A tibble: 1 × 6\n#&gt;   mean_x mean_y mean_diff lower_ci upper_ci conf_level\n#&gt;    &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n#&gt; 1   11.0   11.8    -0.715    -1.67    0.245       0.95\n\n\nSamely, you can use the function MeanDiffCI() from DescTools package, but this function does not have a direct argument to control for whether the variances of the two populations are equal.\n\nDescTools::MeanDiffCI(x, y, na.rm = T, var.equal = T)\n\n#&gt;   meandiff     lwr.ci     upr.ci \n#&gt; -0.7145455 -1.6678398  0.2387489\n\n\nAs you can see, the confidence intervals printed out from DescTools::MeanDiffCI and mean_diff_CI() have a little difference.\n\n\n5.4.3 Interpreting the confidence interval\nThe confidence interval represents a range within which the true population mean is expected to fall with a certain level of confidence (usually 95%).\nAs for the difference between two population means, the truth is the same. Moreover, if the interval of the difference between two population means includes 0, this suggests that there may be no significant difference between the two population means. If the interval does not include 0, it suggests that there is a significant difference.\nConsider the 100 samples drawn from normal populations N(17.5, 1.2) (see Example 1), we can construct the 95% confidence intervals for them. Look up the results in Table 5.2 , flag = TRUE indicates the confidence interval contain the population mean, and the vise is not true.\n\npop.mean = 17.5\n\ndf &lt;- read_csv(\"datasets/ex05-01.csv\", show_col_types = F) |&gt; \n  summarise_all(\n    list(\n      s.mean = mean,\n      ci     = mean_CI\n    )\n  ) |&gt; \n  pivot_longer(\n    cols      = everything(),\n    names_to  = c(\"sample\", \".value\"),\n    names_sep = \"_\"\n  ) |&gt; \n  mutate(\n    lower.ci = ci[[2]],\n    upper.ci = ci[[3]],\n    ci       = NULL\n  ) |&gt;\n  mutate(\n    flag = lower.ci &lt;= pop.mean & upper.ci &gt;= pop.mean,\n    sample = as.factor(c(1:100)),\n    across(c(2:4), ~ round(., digits = 2))\n  )\ndf |&gt; \n  paged_table()\n\n\n\nTable 5.2: The confidence intervals for the 100 samples\n\n\n\n\n  \n\n\n\n\n\n\nTo highlight the confidence intervals that do not contain the population mean with a different color,\nTo see more clearli, we visualize the confidence intervals for the means of 100 samples as below. The blue verticale line represents the population mean, black dot denotes each sample mean, and the short horizontal lines characterize the confidence intervals. The confidence intervals that do not contain the population mean are highlighted with a red color.\n\ndf |&gt; \n  ggplot(aes(x = s.mean, y = sample)) +\n  geom_point(size = 0.4) +\n  geom_vline(xintercept = pop.mean, linetype = \"dashed\", color = \"blue\") +\n  geom_errorbarh(\n    aes(xmin = lower.ci, xmax = upper.ci, color = flag), \n    height = 0.3,\n    linewidth = 0.4) +\n  xlim(15, 20) +\n  labs(x = \"Means of sample and 95% CI\", y = \"Sample index\") +\n  theme_light() +\n  theme(\n    axis.text = element_text(size = 6.5), \n    legend.position = \"none\",\n    axis.title  = element_text(size = 10)\n  )",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistical inference</span>"
    ]
  },
  {
    "objectID": "hypothesis-test.html",
    "href": "hypothesis-test.html",
    "title": "6  Hypothesis test",
    "section": "",
    "text": "6.1 Fundamentals of hypothesis test",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Hypothesis test</span>"
    ]
  },
  {
    "objectID": "hypothesis-test.html#fundamentals-of-hypothesis-test",
    "href": "hypothesis-test.html#fundamentals-of-hypothesis-test",
    "title": "6  Hypothesis test",
    "section": "",
    "text": "6.1.1 The fundamentals\nThe small probability event method or proof by contradiction using small probability event, is a reasoning approach used in hypothesis test based on the principle of probability theory. The core idea is that if an event is highly unlikely to occur under a certain assumption, and it does occur, then the assumption is likely incorrect.\nIt start from the opposite assumption (H_0) to indirectly assess whether the problem of interest (H_1) holds true. This involves assuming that H_0 is true, calculating the test statistic, and then using the resulting p-value to determine whether H_0 can be upheld.\n\n\n6.1.2 Some concepts and notations\nBefore you learn and insight into more about hypothesis test, some concepts and notations need to be noticed.\nNull hypothesis (H_0)\nThis is the default assumption that there is no effect or no difference. It is the hypothesis that we seek to test against. For example: The average height of adult men in a certain city is 175 cm. (H_0: \\mu = 175 cm)\nAlternative hypothesis (H_1)\nThis is what you want to prove. It represents an effect, a difference, or a relationship that contradicts the null hypothesis. The hypothesis is that there is an effect or difference. For example: The average height of adult men in the city is not 175 cm. (H_1: \\mu \\ne 175 cm)\nTest statistic\nA standardized value calculated from the sample data that is used to determine whether to reject H_0, such as the t-statistic, z-statistic, or chi-square statistic.\nSignificance level (\\alpha)\nThe probability threshold for rejecting the null hypothesis. Commonly used values are 0.05, 0.01, or 0.10. α = 0.05 means there’s a 5% risk of rejecting the null hypothesis when it is actually true.\nP-value\nThe probability of obtaining a test statistic at least as extreme as the one observed, assuming the null hypothesis is true. If the p-value is less than α, the null hypothesis is rejected. The smaller the p-value, the stronger the evidence against the null hypothesis. For example, if the p-value is 0.03, there is a 3% chance of observing such a result if H₀ is true.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Hypothesis test</span>"
    ]
  },
  {
    "objectID": "hypothesis-test.html#steps-of-hypothesis-test",
    "href": "hypothesis-test.html#steps-of-hypothesis-test",
    "title": "6  Hypothesis test",
    "section": "6.2 Steps of hypothesis test",
    "text": "6.2 Steps of hypothesis test\nThe process of conducting a hypothesis test involves several key steps. Here’s an outline of the typical procedure:\n\nDefine H_0 and H_1.\nChoose the significance level \\alpha, typically 0.05.\nSelect the appropriate test.\nCompute the test statistic.\nDetermine the p-value.\nMake a decision based on the p-value and significance level.\n\nReject H_0: If the p-value ≤ α.\nFail to reject H_0: If the p-value &gt; α.\n\nDraw conclusions in the context of the research.\n\nIn the subsequet chapters, some typical test method will be introduced.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Hypothesis test</span>"
    ]
  },
  {
    "objectID": "hypothesis-test.html#considerations-for-hypothesis-test",
    "href": "hypothesis-test.html#considerations-for-hypothesis-test",
    "title": "6  Hypothesis test",
    "section": "6.3 Considerations for hypothesis test",
    "text": "6.3 Considerations for hypothesis test\n\n6.3.1 Type I error and type II error\nIt is known that hypothesis test employs the principle of proof by contradiction through small probability events. The conclusions drawn based on the p-value are probabilistic essentially, meaning that the conclusions may not be entirely correct. Two types of errors can occur.\nType I error (α) (false positive)\nA type I error occurs when you reject the null hypothesis (H_0) when it is actually true. For example, imagine that you are testing a new drug to see if it lowers blood pressure. The null hypothesis states that the drug has no effect. If you conclude that the drug does lower blood pressure when in fact it does not, you have made a type I error.\nThe probability of making a type I error is denoted by the significance level α .Common choices for α are 0.05 or 0.01, meaning there is a 5% or 1% risk of rejecting H₀ when it is true.\nType II error (β) (false negative)\nA type II error occurs when you fail to reject the null hypothesis (H_0) when the alternative hypothesis (H_1) is actually true. Continuing with the drug example, if the drug actually does lower blood pressure, but you conclude that it doesn’t, you’ve made a type II error.\nThe probability of making a type II error is denoted by \\beta. The power of a test (1-\\beta) represents the probability of correctly rejecting a false null hypothesis when the alternative hypothesis is true. In other words, it’s the likelihood that the test will detect an effect or difference when one actually exists.\nFor example , in a medical study designed to detect whether a new drug is effective, if the power of the test is 0.9, there is a 90% chance of correctly rejecting H_0 (that the drug has no effect) when H_1 (that the drug is effective) is true.\nFactors that affect β include:.\n\nSample size: Larger sample sizes generally increase the power of a test.\nEffect size: Larger differences or stronger effects are easier to detect, thus increasing power.\nSignificance level (α): Increasing α can increase power, but it also increases the risk of a type I error.\nVariance: Lower variability within the data increases power.\n\nIn hypothesis test, there is often a trade-off between the risks of type I and type II errors. Lowering the significance level α reduces the risk of a type I error but increases the risk of a type II error, and vice versa.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Hypothesis test</span>"
    ]
  },
  {
    "objectID": "hypothesis-test.html#considerations-in-hypothesis-test",
    "href": "hypothesis-test.html#considerations-in-hypothesis-test",
    "title": "6  Hypothesis test",
    "section": "6.4 Considerations in hypothesis test",
    "text": "6.4 Considerations in hypothesis test\n\n6.4.1 Study design\nStudy design is the prerequisite of hypothesis test. The groups to be compared should be balanced and comparable, meaning that aside from the primary factor under investigation (such as a new drug in a clinical trial versus a control drug), other factors (such as age, gender, disease duration, and severity) that could influence the results should be identical or similar between groups. The best way to ensure balance is randomization before treatment.\n\n\n6.4.2 Different tests for different types of data\nThe appropriate test should be selected based on the purpose of the analysis, the type and distribution of the data, the study design, the sample size, and the conditions under which different statistical methods are applicable. For example, paired t-tests should be used for paired design measurement data; for completely randomized design measurement data with small sample sizes (i.e., n ≤ 60) with equal variances, the two-sample t-test should be used. If variances are unequal, an approximate t-test should be used.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Hypothesis test</span>"
    ]
  },
  {
    "objectID": "ttest.html",
    "href": "ttest.html",
    "title": "7  t-test",
    "section": "",
    "text": "7.1 Prerequisites\nlibrary(tidyverse)",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>t-test</span>"
    ]
  },
  {
    "objectID": "ttest.html#one-sample-t-test",
    "href": "ttest.html#one-sample-t-test",
    "title": "7  t-test",
    "section": "7.2 One-sample t-test",
    "text": "7.2 One-sample t-test\nThe one-sample t-test is used to determine whether the mean of a single sample is significantly different from a known or hypothesized population mean. It is commonly used when you have a sample and want to test if its mean differs from a theoretical value or an expected value. Its test statistic t is calculated as follows:\n\nt = \\frac{\\bar{X} - \\mu_0}{S_{\\bar{X}}} = \\frac{\\bar{X} - \\mu_0}{S / \\sqrt{n}}\n\nwhere \\bar{X} si the sample mean , S is the sample standard deviation and n is the sample size, \\mu_0 is the hypothesized population mean.\nThe one-sample t-test is a straightforward but powerful tool for hypothesis testing in many research scenarios.\n\nExample 1: \nA doctor measured the hemoglobin concentration in 36 male workers involved in lead-related jobs. The data can be downloaded below. The question is whether the mean hemoglobin concentration (\\mu) of male workers involved in lead-related jobs differs from the mean of 140 (g/L) for normal adult males.\n\n  Download data \nR codes for one-sample t-test is:\n\nread_csv(\"datasets/ex07-01.csv\", show_col_types = F) |&gt; \n  t.test(mu = 140) \n\n#&gt; \n#&gt;  One Sample t-test\n#&gt; \n#&gt; data:  read_csv(\"datasets/ex07-01.csv\", show_col_types = F)\n#&gt; t = -2.1367, df = 35, p-value = 0.03969\n#&gt; alternative hypothesis: true mean is not equal to 140\n#&gt; 95 percent confidence interval:\n#&gt;  122.1238 139.5428\n#&gt; sample estimates:\n#&gt; mean of x \n#&gt;  130.8333\n\n\nThe results show t = -2.1367 , \\text{p-value} = 0.03969. At the significance level of \\alpha = 0.05 , reject H_0 and accept H_1, indicating that the difference is statistically significant. In the context of this case, it can be concluded that the average hemoglobin concentration of male workers engaged in lead work is lower than the average hemoglobin concentration of normal adult males.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>t-test</span>"
    ]
  },
  {
    "objectID": "ttest.html#paired-t-test",
    "href": "ttest.html#paired-t-test",
    "title": "7  t-test",
    "section": "7.3 Paired t-test",
    "text": "7.3 Paired t-test\nThe paired t-test is used to compare the means of two related groups. This test is typically used when the observations are paired in some meaningful way, such as measurements taken on the same subjects at two different times or under two different conditions.\n\nH_0 : The mean difference between the paired observations is zero (\\mu_d = 0).\nH_1 : The mean difference between the paired observations is not zero (\\mu_d \\neq 0).\n\nThe test statistic for the paired t-test is calculated as:\nt = \\frac{\\bar{d}}{S_d / \\sqrt{n}} Where \\bar{d} is the mean of the differences between the paired observations, S_d is the standard deviation of the differences, and n is the number of pairs.\n\nExample 2: \nTo compare whether the results of fat content measurement in lactic acid beverages differ between two methods, 10 samples of lactic acid beverages were randomly selected. The fat content was measured using both the Gerber-Gottlieb method and the fatty acid hydrolysis method. The data can be downloaded below. The question is whether the measurement results from the two methods are different?\n\n  Download data \nR codes for paired t-test is:\n\nread_csv(\"datasets/ex07-02.csv\", show_col_types = F) |&gt; \n  with(t.test(x1, x2, paired = T))\n\n#&gt; \n#&gt;  Paired t-test\n#&gt; \n#&gt; data:  x1 and x2\n#&gt; t = 7.926, df = 9, p-value = 2.384e-05\n#&gt; alternative hypothesis: true mean difference is not equal to 0\n#&gt; 95 percent confidence interval:\n#&gt;  0.1946542 0.3501458\n#&gt; sample estimates:\n#&gt; mean difference \n#&gt;          0.2724\n\n\nThe results show t = -2.1367 , \\text{p-value} &lt; 0.001. At the significance level of \\alpha = 0.05, reject H_0 and accept H_1, indicating that the difference is statistically significant. It can be concluded that the two methods yield different fat content measurements. As mean difference estimation is 0.2724 &gt; 0, the Gerber-Gottlieb method providing higher results.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>t-test</span>"
    ]
  },
  {
    "objectID": "ttest.html#two-sample-t-test",
    "href": "ttest.html#two-sample-t-test",
    "title": "7  t-test",
    "section": "7.4 Two-sample t-test",
    "text": "7.4 Two-sample t-test\nThe two-sample t-test (also known as the independent t-test) is used to determine whether there is a significant difference between the means of two independent groups. It is commonly used in experiments or studies where researchers want to compare the means of two different populations or conditions.\nIn medical study it is commonly used to compare the means of two samples in a completely randomized design. In this design, subjects are randomly assigned to two different treatment groups, and the researcher is interested in whether the means of these two samples represent different population means. Additionally, in observational studies, the two-sample t-test can be used to compare the means of two samples obtained through independent random sampling from two different populations.\nWhen both samples come from normal populations, and the sample sizes are relatively small, such as n_1 \\leq 60 or/and n_2 \\leq 60 , different testing methods should be used depending on whether the population variances are equal.\n\n7.4.1 The t-test for equal population variances\nThe t-test for equal population variances, often referred to as the pooled variance t-test, is used when comparing the means of two independent samples under the assumption that the two populations have the same variance. This assumption allows the variances of the two samples to be combined (or pooled) into a single estimate, which can then be used in the t-test formula.\nBefore conducting the combined t-test, it’s common to perform an F-test to check if the variances of the two samples are equal. If the p-value from the F-test is not significant, the pooled t-test can be justified. The pooled variance is calculated as a weighted average of the variances from the two samples:\n\nS_c^2 = \\frac{(n_1 - 1)S_1^2 + (n_2 - 1)S_2^2}{n_1 + n_2 - 2}\n\nHere, S_c^2 is the combined variance, S_1^2 and S_2^2 are the variances of the two samples, and n_1 and n_2 are the sample sizes.\nOnce the combined variance is calculated, the t-statistic for the two-sample t-test can be computed as:\n\nt = \\frac{\\bar{X}_1 - \\bar{X}_2}{\\sqrt{S_c^2 \\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}}\n\n\\bar{X}_1 and \\bar{X}_2 are the sample means, and n_1 and n_2 are the sample sizes. The degrees of freedom for the pooled variance t-test are calculated as n_1 + n_2 - 2. 5.\nH_0: There is no difference between the population means (\\mu_1 = \\mu_2).\nH_1: There is a difference between the population means (\\mu_1 \\neq \\mu_2).\nIf the variances are unequal, the Welch t-test (which does not assume equal variances) is recommended.\n\nExample 3: \nTo investigate the anti-atherosclerotic effects of tetrandrine on rabbits, an atherosclerosis rabbit model was established using a high-fat diet over 12 weeks. 16 rabbits successfully modeled after 12 weeks were randomly divided into two groups. The group A was given a high-fat diet of 100g/day, while the group B was given the same high-fat diet along with an additional 109mg/(kg·d) of tetrandrine mixed into the feed. After continuous feeding for 3 weeks, blood was drawn from the heart at the end of the experiment to measure the high-density lipoprotein (mmol/L) levels in the heart blood. The data can be downloaded below. Can it be concluded that the population means of high-density lipoprotein content in the heart blood differ between group A and B?\n\n  Download data \nR codes for two-sample t-test is:\n\nread_csv(\"datasets/ex07-03.csv\", show_col_types = F) |&gt; \n  mutate(grp = factor(grp, labels = c(\"A\", \"B\"))) |&gt; \n1  t.test(x ~ grp, data = _, var.equal = T)\n\n\n1\n\nUse the pipe operator to perform t-test with a group variable and a response variable.\n\n\n\n\n#&gt; \n#&gt;  Two Sample t-test\n#&gt; \n#&gt; data:  x by grp\n#&gt; t = 3.689, df = 14, p-value = 0.00243\n#&gt; alternative hypothesis: true difference in means between group A and group B is not equal to 0\n#&gt; 95 percent confidence interval:\n#&gt;  0.05494115 0.20755885\n#&gt; sample estimates:\n#&gt; mean in group A mean in group B \n#&gt;         0.77000         0.63875\n\n\nThe results show t = 3.689 , \\text{p-value} = 0.00243. At the \\alpha = 0.05 significance level, reject H_0 and accept H_1, indicating that the difference is statistically significant. It can be concluded that the population means of high-density lipoprotein content in the heart blood are different between group A and B. Considering the sample means in this case, it can be inferred that the population mean of high-density lipoprotein content in the heart blood is higher in group A than in the model group.\n\n\n7.4.2 Welch t-test for unequal population variances\nUnlike the combined variance t-test, the Welch t-test does not assume that the two populations have the same variance. This is particularly useful when you suspect or know that the variances are different, or when the sample sizes are quite different, which makes it more flexible than the combined variance t-test.\nThe t-statistic for the Welch t-test is calculated as:\n\nt = \\frac{\\bar{X}_1 - \\bar{X}_2}{\\sqrt{\\frac{S_1^2}{n_1} + \\frac{S_2^2}{n_2}}}\n\nHere, \\bar{X}_1 and \\bar{X}_2 are the sample means, S_1^2 and S_2^2 are the variances of the two samples, and n_1 and n_2 are the sample sizes.\nThe degrees of freedom for the Welch t-test are calculated using the Welch-Satterthwaite equation:\n\n\\nu = \\frac{\\left(\\frac{S_1^2}{n_1} + \\frac{S_2^2}{n_2}\\right)^2}{\\frac{\\left(\\frac{S_1^2}{n_1}\\right)^2}{n_1 - 1} + \\frac{\\left(\\frac{S_2^2}{n_2}\\right)^2}{n_2 - 1}}\n\nThis formula typically results in a non-integer value for the degrees of freedom, which is a characteristic of the Welch t-test. It is generally considered more robust than the combined variance t-test in situations where variances differ.\n\nExample 4: \nTo analyze the effect of blood glucose control on serum total cholesterol levels, a study was conducted on type 2 diabetes patients in a community. Hemoglobin A1c (HbA1c) levels below 7.0% were used as the target for blood glucose control. Total cholesterol levels (mmol/L) were measured in 25 patients with poor blood glucose control (Group A) and 25 patients with good blood glucose control (Group B). The data can be downloaded below. Determine whether the mean total cholesterol levels of those with poor blood glucose control and those with good blood glucose control are equal or not.\n\n  Download data \nR codes for Welch t-test is:\n\nread_csv(\"datasets/ex07-04.csv\", show_col_types = F) |&gt; \n  t.test(x ~ grp, data = _, var.equal = F)    \n\n#&gt; \n#&gt;  Welch Two Sample t-test\n#&gt; \n#&gt; data:  x by grp\n#&gt; t = -2.8322, df = 36.672, p-value = 0.007465\n#&gt; alternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n#&gt; 95 percent confidence interval:\n#&gt;  -1.4177999 -0.2350001\n#&gt; sample estimates:\n#&gt; mean in group 1 mean in group 2 \n#&gt;          4.3176          5.1440\n\n\nThe results show t = -2.8322 , \\text{p-value} = 0.007465. At the \\alpha = 0.05 significance level, reject H_0 and accept H_1, indicating that the difference is statistically significant. Therefore, we can conclude that the mean total cholesterol levels between those with poor and good blood glucose control are different.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>t-test</span>"
    ]
  },
  {
    "objectID": "ttest.html#normality-test-and-f-test",
    "href": "ttest.html#normality-test-and-f-test",
    "title": "7  t-test",
    "section": "7.5 Normality test and F-test",
    "text": "7.5 Normality test and F-test\nWhen conducting a two-sample t-test, especially when comparing the means of two small samples, it is required that the corresponding populations follow a normal distribution and that the population variances are equal, known as homogeneity of variance. For paired t-tests, it is only necessary that the distribution of the differences between each pair of data points follows a normal distribution. Therefore, when performing a two-sample t-test with small samples, it is generally advisable to first conduct a homogeneity of variance test, particularly when there is a noticeable disparity between the sample variances. If the variances are homogeneous, a standard t-test is used; if not, an approximate t-test, such as Welch t-test, is applied. Additionally, it may be necessary to perform a normality test, though normality tests are more commonly used to establish medical reference ranges using normal distribution methods.\n\n7.5.1 Normality test\nFor more details about normality test you can see Section 4.3 .",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>t-test</span>"
    ]
  },
  {
    "objectID": "ttest.html#f-test",
    "href": "ttest.html#f-test",
    "title": "7  t-test",
    "section": "7.6 F-test",
    "text": "7.6 F-test\nThe F-test was traditionally used to determine whether the variances of two populations are equal. However, this test assumes that the data follow a normal distribution, which is often not the case, especially when variances are unequal. Because of this limitation, the Levene’s test has become more popular in recent years. Levene’s test is more robust and does not rely on the specific distribution of the population. Levene’s test can be used to test the homogeneity of variances for both two populations and multiple populations. Here, only the F-test for comparing the variances of two samples is introduced.\nH_0: The variances of the two populations are equal (\\sigma_1^2 = \\sigma_2^2).\nH_1: The variances of the two populations are not equal (\\sigma_1^2 \\neq \\sigma_2^2).\nThe F-statistic is calculated as the ratio of the two sample variances:\n\nF = \\frac{S_1^2}{S_2^2}\n\nwhere S_1^2 and S_2^2 are the variances of the two samples. By convention, S_1^2 should be the larger variance, so the F-statistic is always greater than or equal to 1. The F-distribution depends on two parameters: the degrees of freedom for the numerator (\\nu_1 = n_1 - 1) and the denominator (\\nu_2 = n_2 - 1).\nIf the calculated F-statistic is is close to 1, the variances are similar. If it is much larger or smaller than 1, there may be a significant difference. If the p-value is less than the chosen significance level (e.g., 0.05), reject the null hypothesis, indicating that the variances are significantly different.\nThe F-test is sensitive to non-normality. If the data does not follow a normal distribution, the test may give misleading results. In such cases, non-parametric alternatives, such as the Levene’s test, may be more appropriate.\n\ntibble(\n  F_value   = seq(0, 5, length.out = 1000),\n  y1  = df(F_value, 8, 1),\n  y2  = df(F_value, 8, Inf)\n) |&gt; \n  pivot_longer(\n    cols = contains(\"y\"),\n    names_to = \"grp\",\n    values_to = \"Density\"\n  ) |&gt; \n  ggplot(aes(x = F_value, y = Density, linetype = grp)) + \n  geom_line(linewidth = 0.5) +\n  geom_text(\n    aes(x = 0.8, y = 0.3, hjust = 0,\n        label = paste(\"df1 =\", 8, \", df2 =\", 1))) +\n  geom_text(\n    aes(x = 1, y = 0.8, hjust = 0,\n    label = paste(\"df1 =\", 8, \", df2 =\", Inf))) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nExample 5: \nFor Example 3, use the F-test to determine whether the variances of high-density lipoprotein content in the heart blood of the two populations are unequal.\n\n\nread_csv(\"datasets/ex07-03.csv\", show_col_types = F) |&gt; \n  var.test(x ~ grp, data = _)\n\n#&gt; \n#&gt;  F test to compare two variances\n#&gt; \n#&gt; data:  x by grp\n#&gt; F = 2.299, num df = 7, denom df = 7, p-value = 0.2944\n#&gt; alternative hypothesis: true ratio of variances is not equal to 1\n#&gt; 95 percent confidence interval:\n#&gt;   0.4602708 11.4833515\n#&gt; sample estimates:\n#&gt; ratio of variances \n#&gt;           2.299011\n\n\nThe results show: F = 2.299, \\text{p-value} = 0.2944, at the significant level of \\alpha = 0.1, do not reject H_0. The difference is not statistically significant. Therefore, we cannot conclude that the variances of high-density lipoprotein content in the heart blood of the Qingfujian group and the model group are unequal. Consequently, Example 3 used a two-sample t-test under the assumption of equal variances.\n\nExample 6: \nFor Example 4, Use the F test to determine whether the population variances of serum total cholesterol levels differ between those with poor and good blood glucose control.\n\n\nread_csv(\"datasets/ex07-04.csv\", show_col_types = F) |&gt; \n  var.test(x ~ grp, data = _)\n\n#&gt; \n#&gt;  F test to compare two variances\n#&gt; \n#&gt; data:  x by grp\n#&gt; F = 3.5022, num df = 24, denom df = 24, p-value = 0.0032\n#&gt; alternative hypothesis: true ratio of variances is not equal to 1\n#&gt; 95 percent confidence interval:\n#&gt;  1.543310 7.947462\n#&gt; sample estimates:\n#&gt; ratio of variances \n#&gt;           3.502199\n\n\nThe results show: F = 3.5022, \\text{p-value} = 0.0032, at the significant level of \\alpha = 0.1, reject H_0, accept H_1. The difference is statistically significant. It can be considered that the population variances of total cholesterol levels differ between those with poor and good blood glucose control. Therefore, in Example 4, the two-sample t-test for unequal variances was used.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>t-test</span>"
    ]
  },
  {
    "objectID": "anova.html",
    "href": "anova.html",
    "title": "8  Analysis of Variance",
    "section": "",
    "text": "8.1 Prerequisite\nlibrary(haven)\nlibrary(tidyverse)",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "anova.html#completely-randomized-design",
    "href": "anova.html#completely-randomized-design",
    "title": "8  Analysis of Variance",
    "section": "8.2 Completely randomized design",
    "text": "8.2 Completely randomized design\n\n\n\n\n\n\nExample\n\n\n\nTo evaluate the efficacy of xanomeline in subjects with mild to moderate Alzheimer’s disease, subjects were randomized equally to placebo, xanomeline low dose, or xanomeline high dose. Subjects applied 2 patches daily and were followed for a total of 26 weeks. Primary efficacy endpoints\n\n\n\n# haven::read_dta(\"datasets/例02-05.dta\") |&gt; \n#   write_csv(\"datasets/ex02-05.csv\")",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "anova.html#section",
    "href": "anova.html#section",
    "title": "8  Analysis of Variance",
    "section": "8.3 ",
    "text": "8.3",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "cox-regression.html",
    "href": "cox-regression.html",
    "title": "14  Cox regression",
    "section": "",
    "text": "Cox regression, also known as Cox proportional hazards regression, is a statistical method used to analyze the time until an event occurs, typically survival time. It is widely used in medical research to explore the relationship between the survival time of patients and one or more predictor variables. Unlike other models, Cox regression does not require assumptions about the baseline hazard function, making it a flexible tool for handling censored data where the exact time of the event may not be known for all subjects.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Cox regression</span>"
    ]
  },
  {
    "objectID": "quarto.html",
    "href": "quarto.html",
    "title": "17  Quarto",
    "section": "",
    "text": "17.1 GitHub Action\n呈现和发布内容有几种不同的方法。下面，我们将提供一个使用 GitHub Actions 和 GitHub Pages 发布内容的指南。",
    "crumbs": [
      "Communication",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "quarto.html#github-action",
    "href": "quarto.html#github-action",
    "title": "17  Quarto",
    "section": "",
    "text": "17.1.1 执行代码\n如果你愿意，可以配置一个 GitHub Action 来执行 R 代码作为渲染的一部分。这虽然是最好的方法，但当在 GitHub Actions 等 CI 服务中执行代码时，请考虑以下要求:\n\n你需要在 CI 环境中重新配置所有依赖项（包括 R 以及所需的正确版本的包）。\n如果你的代码需要任何特殊权限（例如访问数据库或网络）， CI 服务器上也需要具备这些权限。\n你的项目可能包含无法轻易执行的文档（例如使用旧版包的几年前的博客文章）。这些文档可能需要单独启用冻结功能，以防止它们在 CI 上执行。\n\n\n\n17.1.2 先决条件\n确保代码可以在 GitHub Action 中执行的最佳方法是为项目使用 renv 虚拟环境。以下是一个完整的 GitHub Action 示例，它安装 R 和 renv.lock 中的包依赖项，然后执行代码并将输出渲染到 GitHub Pages：\n\n\n\n\n\n\nTip\n\n\n\n在 RStudio 左下窗口的 Terminal 选项卡中依次运行以下 Git 命令：\n\ngit remote add origin https://github.com/qbgaoo/r4ms.git\ngit branch -M main\ngit push -u origin main\n\n\n\n在 Quarto 项目中新建文本文件，命名为 publish.yml（当然也可以是其他命字），保存路径为 .github/workflows/publish.yml，在文件中添加如下内容：\non:\n  push:\n    branches: main\n  pull_request:\n    branches: main\n  # to be able to trigger a manual build\n  workflow_dispatch:\n  schedule:\n    # run every day at 11 PM\n    - cron: '0 23 * * *'\n\nname: Render and deploy Book to Github\n\njobs:\n  build-deploy:\n    runs-on: ubuntu-latest\n    permissions:\n      contents: write\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v4\n\n      - name: Set up Quarto\n        uses: quarto-dev/quarto-actions/setup@v2\n\n      - name: Install R\n        uses: r-lib/actions/setup-r@v2\n        with:\n          r-version: '4.4.1'\n\n      - name: Install R Dependencies\n        uses: r-lib/actions/setup-renv@v2\n        with:\n          cache-version: 1\n\n      - name: Render and Publish\n        uses: quarto-dev/quarto-actions/publish@v2\n        with:\n          target: gh-pages\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n这是一个 GitHub Actions 工作流配置文件，保存后将文件提交到 GitHub中，触发文件中的工作流。",
    "crumbs": [
      "Communication",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "pilot-project.html",
    "href": "pilot-project.html",
    "title": "18  A case to analyze",
    "section": "",
    "text": "18.1 Datasets\nIn this book, we used publicly available CDISC pilot study data, which is accessible through the CDISC GitHub repository. To streamline the process, we have downloaded all the datasets from the repository and converted them from .xpt format to .sas7bdat format for ease of use and compatibility. Then we stored them in the data/adam/ folder within this project. Additionally, The dataset structure adheres to the CDISC Analysis Data Model (ADaM) standard.\nThe SDTM-ADaM Pilot Project datasets were created to demonstrate the process of converting clinical trial data into formats that comply with the Study Data Tabulation Model (SDTM) and Analysis Data Model (ADaM) standards, which are set by the Clinical Data Interchange Standards Consortium (CDISC). These datasets are used to test, validate, and illustrate how to implement CDISC standards in real-world scenarios, helping pharmaceutical companies and regulatory agencies like the FDA ensure data quality and consistency in clinical trials.\nlibrary(haven)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nadsl &lt;- read_sas(\"data-adam/adsl.sas7bdat\")\n\nadsl |&gt; \n  select(ARM, ITTFL, EFFFL) |&gt; \n  filter(EFFFL == \"Y\") |&gt; \n  group_by(ARM) |&gt; \n  summarise(n = n())\n\n# A tibble: 3 × 2\n  ARM                      n\n  &lt;chr&gt;                &lt;int&gt;\n1 Placebo                 79\n2 Xanomeline High Dose    74\n3 Xanomeline Low Dose     81\nadsl |&gt; \n  glimpse()\n\nRows: 254\nColumns: 48\n$ STUDYID  &lt;chr&gt; \"CDISCPILOT01\", \"CDISCPILOT01\", \"CDISCPILOT01\", \"CDISCPILOT01…\n$ USUBJID  &lt;chr&gt; \"01-701-1015\", \"01-701-1023\", \"01-701-1028\", \"01-701-1033\", \"…\n$ SUBJID   &lt;chr&gt; \"1015\", \"1023\", \"1028\", \"1033\", \"1034\", \"1047\", \"1097\", \"1111…\n$ SITEID   &lt;chr&gt; \"701\", \"701\", \"701\", \"701\", \"701\", \"701\", \"701\", \"701\", \"701\"…\n$ SITEGR1  &lt;chr&gt; \"701\", \"701\", \"701\", \"701\", \"701\", \"701\", \"701\", \"701\", \"701\"…\n$ ARM      &lt;chr&gt; \"Placebo\", \"Placebo\", \"Xanomeline High Dose\", \"Xanomeline Low…\n$ TRT01P   &lt;chr&gt; \"Placebo\", \"Placebo\", \"Xanomeline High Dose\", \"Xanomeline Low…\n$ TRT01PN  &lt;dbl&gt; 0, 0, 81, 54, 81, 0, 54, 54, 54, 0, 0, 81, 81, 81, 0, 81, 81,…\n$ TRT01A   &lt;chr&gt; \"Placebo\", \"Placebo\", \"Xanomeline High Dose\", \"Xanomeline Low…\n$ TRT01AN  &lt;dbl&gt; 0, 0, 81, 54, 81, 0, 54, 54, 54, 0, 0, 81, 81, 81, 0, 81, 81,…\n$ TRTSDT   &lt;date&gt; 2014-01-02, 2012-08-05, 2013-07-19, 2014-03-18, 2014-07-01, …\n$ TRTEDT   &lt;date&gt; 2014-07-02, 2012-09-01, 2014-01-14, 2014-03-31, 2014-12-30, …\n$ TRTDUR   &lt;dbl&gt; 182, 28, 180, 14, 183, 26, 190, 10, 55, 182, 183, 183, 38, 18…\n$ AVGDD    &lt;dbl&gt; 0.0, 0.0, 77.7, 54.0, 76.9, 0.0, 54.0, 54.0, 54.0, 0.0, 0.0, …\n$ CUMDOSE  &lt;dbl&gt; 0, 0, 13986, 756, 14067, 0, 10260, 540, 2970, 0, 0, 14121, 26…\n$ AGE      &lt;dbl&gt; 63, 64, 71, 74, 77, 85, 68, 81, 84, 52, 84, 81, 75, 57, 79, 5…\n$ AGEGR1   &lt;chr&gt; \"&lt;65\", \"&lt;65\", \"65-80\", \"65-80\", \"65-80\", \"&gt;80\", \"65-80\", \"&gt;80…\n$ AGEGR1N  &lt;dbl&gt; 1, 1, 2, 2, 2, 3, 2, 3, 3, 1, 3, 3, 2, 1, 2, 1, 2, 2, 2, 3, 2…\n$ AGEU     &lt;chr&gt; \"YEARS\", \"YEARS\", \"YEARS\", \"YEARS\", \"YEARS\", \"YEARS\", \"YEARS\"…\n$ RACE     &lt;chr&gt; \"WHITE\", \"WHITE\", \"WHITE\", \"WHITE\", \"WHITE\", \"WHITE\", \"WHITE\"…\n$ RACEN    &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1…\n$ SEX      &lt;chr&gt; \"F\", \"M\", \"M\", \"M\", \"F\", \"F\", \"M\", \"F\", \"M\", \"M\", \"M\", \"F\", \"…\n$ ETHNIC   &lt;chr&gt; \"HISPANIC OR LATINO\", \"HISPANIC OR LATINO\", \"NOT HISPANIC OR …\n$ SAFFL    &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"…\n$ ITTFL    &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"…\n$ EFFFL    &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"…\n$ COMP8FL  &lt;chr&gt; \"Y\", \"N\", \"Y\", \"N\", \"Y\", \"N\", \"Y\", \"N\", \"Y\", \"Y\", \"Y\", \"Y\", \"…\n$ COMP16FL &lt;chr&gt; \"Y\", \"N\", \"Y\", \"N\", \"Y\", \"N\", \"Y\", \"N\", \"N\", \"Y\", \"Y\", \"Y\", \"…\n$ COMP24FL &lt;chr&gt; \"Y\", \"N\", \"Y\", \"N\", \"Y\", \"N\", \"Y\", \"N\", \"N\", \"Y\", \"Y\", \"Y\", \"…\n$ DISCONFL &lt;chr&gt; \"\", \"Y\", \"\", \"Y\", \"\", \"Y\", \"\", \"Y\", \"Y\", \"\", \"\", \"\", \"Y\", \"\",…\n$ DSRAEFL  &lt;chr&gt; \"\", \"Y\", \"\", \"\", \"\", \"Y\", \"\", \"Y\", \"Y\", \"\", \"\", \"\", \"Y\", \"\", …\n$ DTHFL    &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"…\n$ BMIBL    &lt;dbl&gt; 25.1, 30.4, 31.4, 28.8, 26.1, 30.4, 27.3, 23.9, 23.9, 21.9, 2…\n$ BMIBLGR1 &lt;chr&gt; \"25-&lt;30\", \"&gt;=30\", \"&gt;=30\", \"25-&lt;30\", \"25-&lt;30\", \"&gt;=30\", \"25-&lt;30…\n$ HEIGHTBL &lt;dbl&gt; 147.3, 162.6, 177.8, 175.3, 154.9, 148.6, 168.9, 158.2, 181.6…\n$ WEIGHTBL &lt;dbl&gt; 54.4, 80.3, 99.3, 88.5, 62.6, 67.1, 78.0, 59.9, 78.9, 71.2, 7…\n$ EDUCLVL  &lt;dbl&gt; 16, 14, 16, 12, 9, 8, 18, 22, 12, 14, 12, 10, 16, 15, 6, 16, …\n$ DISONSDT &lt;date&gt; 2010-04-30, 2006-03-11, 2009-12-16, 2009-08-02, 2011-09-29, …\n$ DURDIS   &lt;dbl&gt; 43.9, 76.4, 42.8, 55.3, 32.9, 42.0, 99.1, 40.7, 101.9, 44.2, …\n$ DURDSGR1 &lt;chr&gt; \"&gt;=12\", \"&gt;=12\", \"&gt;=12\", \"&gt;=12\", \"&gt;=12\", \"&gt;=12\", \"&gt;=12\", \"&gt;=12…\n$ VISIT1DT &lt;date&gt; 2013-12-26, 2012-07-22, 2013-07-11, 2014-03-10, 2014-06-24, …\n$ RFSTDTC  &lt;chr&gt; \"2014-01-02\", \"2012-08-05\", \"2013-07-19\", \"2014-03-18\", \"2014…\n$ RFENDTC  &lt;chr&gt; \"2014-07-02\", \"2012-09-02\", \"2014-01-14\", \"2014-04-14\", \"2014…\n$ VISNUMEN &lt;dbl&gt; 12, 5, 12, 5, 12, 6, 12, 4, 8, 12, 12, 12, 7, 12, 12, 7, 4, 7…\n$ RFENDT   &lt;date&gt; 2014-07-02, 2012-09-02, 2014-01-14, 2014-04-14, 2014-12-30, …\n$ DCDECOD  &lt;chr&gt; \"COMPLETED\", \"ADVERSE EVENT\", \"COMPLETED\", \"STUDY TERMINATED …\n$ DCREASCD &lt;chr&gt; \"Completed\", \"Adverse Event\", \"Completed\", \"Sponsor Decision\"…\n$ MMSETOT  &lt;dbl&gt; 23, 23, 23, 23, 21, 23, 10, 23, 20, 20, 19, 21, 22, 21, 10, 1…\nglimpse() makes it possible to see every column in a data frame. It’s a little like str() applied to a data frame but it tries to show you as much data as possible.\nOnce you read data in, the first step usually involves transforming it in some way to make it easier to work with in the rest of your analysis. Firstly, we use janitor::clean_names() to turn all column names of data frame adsl into snake case.",
    "crumbs": [
      "Communication",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>A case to analyze</span>"
    ]
  },
  {
    "objectID": "basic-stat.html",
    "href": "basic-stat.html",
    "title": "Basic methods",
    "section": "",
    "text": "基本的医学统计方法涵盖了数据的收集、整理、描述、分析和解释，旨在帮助研究者理解疾病的发生、发展、治疗效果及健康状况的分布规律。以下是医学研究中常用的一些基本统计方法：\n\n1. 描述性统计\n\n频率分析：计算各类别或数值出现的频数和百分比。\n集中趋势测量：平均值、中位数和众数，用于描述数据的中心位置。\n离散程度测量：标准差、方差、四分位数间距和极差，反映数据的分散程度。\n\n\n\n2. 推断性统计\n\n参数检验：\n\nt检验：用于比较两组或几组独立或配对样本的均值差异，如学生t检验、配对t检验。\nANOVA（方差分析）：比较多个样本均值是否存在显著差异。\n\n非参数检验：\n\nMann-Whitney U检验（Wilcoxon秩和检验的两样本形式）：适用于连续数据但不符合正态分布的两组样本均值差异检验。\nKruskal-Wallis H检验：多组独立样本的中位数差异检验，适用于非正态分布数据。\n\n卡方检验（Chi-square test）：用于分析分类变量间的关系，如四格表检验。\n\n\n\n3. 回归分析\n\n线性回归：研究一个或多个自变量与一个连续因变量之间的线性关系。\n逻辑回归（Logistic Regression）：适用于因变量为二分类或多元分类的场合，用来估计事件发生概率。\n\n\n\n5. 随机化和抽样\n\n随机化：确保研究的组间可比性。\n抽样技术：包括简单随机抽样、分层抽样、整群抽样等，确保样本代表性。\n\n\n\n6. 误差与功效分析\n\n类型I和II错误的理解，以及统计功效分析，以确定研究设计是否足够强大以检测预期效应。\n\n选择适当的统计方法需依据研究目的、数据类型（定量或定性）、数据分布（正态与否）、样本大小等因素。理解这些基础统计概念和方法对于设计和解读医学研究至关重要。",
    "crumbs": [
      "Basic methods"
    ]
  }
]