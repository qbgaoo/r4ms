[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R for Medical Statistics",
    "section": "",
    "text": "Welcome\nThe topics of this book line up closely with traditional teaching progression; however, the book also highlights computer-intensive approaches to motivate the more traditional approach. The authors emphasize realistic data and examples and rely on visualization techniques to gather insight. They introduce statistics and R seamlessly, giving students the tools they need to use R and the information they need to navigate the sometimes complex world of statistical computing.\nThis book is created by Quarto and R in RStudio IDE。Quarto is an open-source publishing system that integrates well with R, enabling users to create dynamic documents that combine text, code, and output (like tables and plots) in a single document. It supports R Markdown, allowing the execution of R code within documents and rendering outputs in various formats, such as HTML, PDF, and Word. Quarto is ideal for creating reproducible reports, presentations, and books, especially in academic and research settings where R is extensively used. You can also manage bibliographies, citations, and cross-references easily. Quarto is highly customizable, allowing users to create complex documents with ease, and is often used with GitHub Actions for continuous integration and automated publishing.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "1  Preface",
    "section": "",
    "text": "1.1 Prerequisites\nThe books will feature detailed worked examples and R code fully integrated into the text, ensuring their usefulness to researchers, practitioners and students.\nWe’ve made a few assumptions about what you already know to get the most out of this book. You should have some basic knowledge about medical statistics, and it’s helpful if you have some basic R programming experience already.\nYou need some things to run the code in this book: R, RStudio and some preinstalled R packages. Packages are the fundamental units of reproducible R code. They include reusable functions, documentation that describes how to use them, and sample data.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "preface.html#prerequisites",
    "href": "preface.html#prerequisites",
    "title": "1  Preface",
    "section": "",
    "text": "1.1.1 R\nTo download R, go to CRAN, the comprehensive R archive network, https://cloud.r-project.org. A new major version of R comes out once a year, and there are 2-3 minor releases each year. It’s a good idea to update regularly. Upgrading can be a bit of a hassle, especially for major versions that require you to re-install all your packages, but putting it off only makes it worse. We recommend R 4.4.1 or later for this book.\n\n\n1.1.2 RStudio\nRStudio is an integrated development environment, or IDE, for R programming, which you can download from https://posit.co/download/rstudio-desktop/. RStudio is updated a couple of times a year, and it will automatically let you know when a new version is out, so there’s no need to check back. It’s a good idea to upgrade regularly to take advantage of the latest and greatest features. For this book, make sure you have at least RStudio 2024.04.2.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "preface.html#running-r-code",
    "href": "preface.html#running-r-code",
    "title": "1  Preface",
    "section": "1.2 Running R code",
    "text": "1.2 Running R code\nThe previous section showed you several examples of running R code. The code in the book looks like this:\n\n1 + 2\n\n#&gt; [1] 3\n\n\nIf you run the same code in your local console, it will look like this:\n&gt; 1 + 2\n[1] 3\nThere are two main differences. In your console, you type after the &gt;, called the prompt; we don’t show the prompt in the book. In the book, the output is commented out with #&gt;; in your console, it appears directly after your code. These two differences mean that if you’re working with an electronic version of the book, you can easily copy code out of the book and paste it into the console.\nThroughout the book, we use a consistent set of conventions to refer to code:\n\nFunctions are displayed in a code font and followed by parentheses, like sum() or mean().\nOther R objects (such as data or function arguments) are in a code font, without parentheses, like flights or x.\nSometimes, to make it clear which package an object comes from, we’ll use the package name followed by two colons, like dplyr::mutate() or nycflights13::flights. This is also valid R code.\nTo improves readability, variable names and function names are named using snake case.\n\nIn this course an introduction to basic statistical methods useful for biomedical data analysis will be given. Concepts are taught in an intuitive manner, alternating between short lectures and practicals. This allows for plenty of interaction and illustration with examples of practical interest. Participants who aim to use more complex methods can use the concepts and skills learned during the course as basis, as the vast majority of statistical methods are implemented in R.\nParticipants must be able to work with R and R packages to follow the course. Those with little or no experience in R must follow an introductory R course prior to following this course.\nIn addition, it is strongly advised to learn to work with RStudio and RMarkdown. Those with no prior knowledge of RMarkdown can follow the tutorials here. During the course we will practice further, and the RMarkdown cheatsheets may be useful.\nWho should attend\nResearchers who need to run their own statistical analyses, and want to do it in a transparent and reproducible manner. While most participants tend to be PhD students and postdocs, more senior researchers can also benefit from the course.\nR is a free, open-source software for statistical computing and graphics. In this course, you will learn the programming techniques to make the most of this powerful tool for processing, analysing and presenting your data. Please note that basic statistical knowledge is required. You will analyse data by writing functions and scripts to get reproducible and well documented results. You will learn how to create excellent graphics and how to adapt them to your needs. We will use data from clinical background to work throughout the course. Main topics covered during this course will be: Basics of the R language, data accessing, data manipulations, explore and summarise your data using descriptive statistics, graphics in R, data analysis with focus on basic statistics (e.g. hypothesis testing, linear and logistic regression).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "preface.html#colophon",
    "href": "preface.html#colophon",
    "title": "1  Preface",
    "section": "1.3 Colophon",
    "text": "1.3 Colophon\nThe book is written by Quarto, an online version of it is available at https://qbgaoo.github.io/r4ms/.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "2  Introduction",
    "section": "",
    "text": "2.1 Medical statistics\nMedical statistics is the science of applying statistical techniques and principles to analyze data related to health, disease, treatment effectiveness, and public health issues. It involves the systematic collection, organization, description, and inference of health-related data to assist healthcare professionals in making evidence-based, scientific decisions.\nKey Components:\n1. Data Collection: Gathering health-related data systematically through experiments, observational studies, or clinical trials.\n2. Data Organization and Description: Using statistical measures, such as means, variances, and frequency distributions, to describe and summarize the basic characteristics of the data.\n3. Data Analysis: Applying inferential statistical methods, such as hypothesis testing, regression analysis, and analysis of variance (ANOVA), to uncover patterns and causal relationships in the data.\n4. Interpretation of Results: Applying the results of statistical analyses to the medical field, explaining the occurrence and progression of diseases, the effectiveness of treatments, and providing evidence-based support for clinical decisions and public health policies.\nMedical statistics is widely used in medical research, the design of clinical trials, the evaluation of diagnostic tests, the comparison of treatment outcomes, and the epidemiological study of diseases.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "introduction.html#population-and-sample",
    "href": "introduction.html#population-and-sample",
    "title": "2  Introduction",
    "section": "2.2 Population and sample",
    "text": "2.2 Population and sample\nPopulation and sample are fundamental concepts in statistics, and they are key to understanding data analysis, particularly in the context of statistical inference.\n\n2.2.1 Population\nA population includes all members of a defined group that we are studying or collecting information on for data-driven decisions. The population represents the entire group of interest, which could be people, objects, events, or measurements.\nExamples: All adults living in a particular country. All patients treated for a particular condition in a hospital over a decade.\nCharacteristics:\nParameters: The numerical characteristics of a population are called parameters (e.g., population mean, population standard deviation).\nSize: The population can be finite (e.g., all students in a school) or infinite (e.g., all possible outcomes of rolling a die).\n\n\n2.2.2 Sample\nA sample is a subset of the population selected for study. It is often impractical or impossible to study an entire population, so researchers use a sample to draw conclusions about the population.\nExamples: 1,000 randomly selected adults from a country. 200 patients selected from the hospital’s records.\nStatistics: The numerical characteristics of a sample are called statistics (e.g., sample mean, sample standard deviation).\nSize: The size of the sample (denoted as n) is always smaller than the population size (denoted as N).\n\n\n2.2.3 Relationship between population and sample\nSampling: The process of selecting a sample from a population is called sampling. It is crucial that the sample is representative of the population to ensure that the conclusions drawn from the sample can be generalized to the population.\nInference: Statistical inference involves making predictions or generalizations about a population based on information from a sample. This is done using various statistical methods, including estimation and hypothesis testing.\nBias and Variability: A sample might not perfectly represent the population due to sampling bias or variability. Researchers use random sampling techniques to minimize these issues and improve the reliability of the inference.\nPractical Example\nImagine you want to study the average height of all adult women in a country (population). Measuring the height of every woman in the country would be impractical, so instead, you randomly select 500 women (sample) and measure their heights. The average height of these 500 women is a statistic. You then use this statistic to estimate the parameter—the average height of all adult women in the country.\nKey Points to Remember\nPopulation: Entire group of interest; parameters describe it.\nSample: Subset of the population; statistics describe it.\nInference: Drawing conclusions about the population based on sample data.\nRepresentativeness: A sample should represent the population to ensure valid inferences.\nUnderstanding the distinction between population and sample is essential for conducting valid and reliable statistical analyses.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "introduction.html#data",
    "href": "introduction.html#data",
    "title": "2  Introduction",
    "section": "2.3 Data",
    "text": "2.3 Data\nIn statistics, data can be classified into different types based on various characteristics. Understanding these data types is crucial for selecting appropriate statistical methods and accurately interpreting results. Below are the main types of data:\n\n2.3.1 Quantitative data\nQuantitative data (numerical data) refers to data that can be measured and expressed numerically. This type of data is critical in the medical field as it allows for precise measurements, comparisons, and statistical analysis, ultimately leading to better understanding, diagnosis, treatment, and prevention of diseases. Quantitative data can be further classified into two main types:\n1. Discrete Data\nDiscrete data consist of distinct, separate values that are countable. These values are typically whole numbers, and there is no intermediate value between them.\nExamples:\nNumber of patients: The number of patients admitted to a hospital.\nNumber of medications: The number of medications a patient is taking.\nNumber of occurrences: The number of heartbeats per minute (heart rate).\nCharacteristics of count data\n\nDiscrete: The data are non-negative integers, as they represent counts of occurrences.\nOverdispersion: Count data in medical settings often show overdispersion, where the variance exceeds the mean (e.g., some patients may have a high number of hospital visits while others have very few).\nSkewed Distribution: Count data are often positively skewed, with many individuals having low counts (e.g., no hospital visits) and fewer individuals having higher counts.\n\n2. Continuous Data\nContinuous data can take any value within a given range. These data represent measurements and can have decimal places, meaning they are not restricted to whole numbers.\nExamples:\nBlood pressure: Systolic and diastolic blood pressure measurements (e.g., 120/80 mmHg).\nBody temperature: Body temperature measured in degrees Celsius or Fahrenheit (e.g., 36.6°C).\nCholesterol levels: Blood cholesterol levels measured in milligrams per deciliter (e.g., 180 mg/dL).\nQuantitative data is fundamental in many areas of medicine:\n\nDiagnosis: • Quantitative measurements such as blood pressure, blood glucose levels, and cholesterol levels are used to diagnose conditions like hypertension, diabetes, and hyperlipidemia. 2. Treatment Monitoring: • Quantitative data is crucial for monitoring treatment effectiveness. For example, changes in tumor size (measured in millimeters or centimeters) can indicate whether cancer treatment is effective. 3. Epidemiology: • Quantitative data is used to track disease incidence and prevalence rates, mortality rates, and other public health indicators. • Example: Tracking the number of new COVID-19 cases per day during a pandemic. 4. Clinical Research: • In clinical trials, quantitative data is collected to assess the safety and efficacy of new drugs or treatments. • Example: Measuring the reduction in blood pressure in participants taking an antihypertensive drug. 5. Medical Imaging: • Quantitative data is used in medical imaging to measure organ size, tumor volume, and other anatomical features. • Example: Measuring the size of a kidney stone on an ultrasound image.\n\nStatistical Analysis of Quantitative Data in Medicine\nQuantitative data allows for a variety of statistical analyses, including:\n\nDescriptive Statistics: • Mean: The average value (e.g., average heart rate of a group of patients). • Median: The middle value when data is ordered (e.g., median age of patients in a study). • Standard Deviation: A measure of the variability or spread of data (e.g., variability in blood pressure readings).\nInferential Statistics: • t-tests and ANOVA: Used to compare means between two or more groups. • Regression Analysis: Used to model the relationship between a dependent variable and one or more independent variables. • Correlation: Used to assess the strength and direction of the relationship between two continuous variables.\nProbability and Distributions: • Quantitative data often follows specific statistical distributions (e.g., normal distribution), which are used in probability and inferential statistics.\n\nExamples of Quantitative Data in Medical Practice\nVital Signs Monitoring: • Continuous monitoring of heart rate, blood pressure, and oxygen saturation levels in critically ill patients. • Laboratory Tests: • Quantitative measurement of blood glucose levels, complete blood count (CBC), and electrolyte levels. • Growth and Development: • Tracking height, weight, and head circumference in pediatric patients over time to assess growth and development. • Pharmacokinetics: • Measuring drug concentrations in the blood over time to understand absorption, distribution, metabolism, and excretion.\nApplication of Quantitative Data in Medical Research\n\nRandomized Controlled Trials (RCTs): • Quantitative data is used to compare the effectiveness of new treatments or interventions with standard care or placebo. • Example: Measuring the reduction in blood pressure in patients receiving a new antihypertensive drug versus a placebo.\nLongitudinal Studies: • Quantitative data is collected over time to study changes in health outcomes, risk factors, or disease progression. • Example: Tracking changes in lung function in smokers versus non-smokers over several years.\nHealth Economics: • Quantitative data is used to evaluate the cost-effectiveness of medical interventions, including cost per quality-adjusted life year (QALY) gained. • Example: Calculating the cost-effectiveness of a new cancer treatment based on survival rates and costs.\n\nQuantitative data is a cornerstone of medical science, providing the foundation for diagnosis, treatment, research, and public health. Its ability to be precisely measured and analyzed makes it indispensable for advancing medical knowledge, improving patient care, and making informed decisions in healthcare.\n\n\n2.3.2 Qualitative data (Categorical data)\nQualitative data in medicine refers to non-numerical data that represent categories or groups. Unlike quantitative data, which deals with numbers and measurements, qualitative data describes characteristics or attributes that can be used to classify individuals, objects, or events into distinct groups. This type of data is crucial in medical research and practice for understanding patient demographics, disease classifications, and treatment outcomes. ain types:\n1. Nominal Data\nNominal data consists of categories that do not have a natural order or ranking. These categories are mutually exclusive, meaning that an individual or event can belong to only one category.\nBlood Type: Categories like A, B, AB, and O.\nGender: Male, Female, Other.\nPresence or Absence of a Condition: Yes/No responses (e.g., presence of hypertension).\n\n\n2.3.3 Ordinal Data\nOrdinal data consists of categories that have a meaningful order or ranking, but the intervals between the categories are not necessarily equal or meaningful.\nSeverity of Disease: Mild, Moderate, Severe.\nPain Scale: Numeric rating scale (e.g., 0–10), where 0 is no pain and 10 is the worst pain imaginable.\nStage of Cancer: Stages I, II, III, IV.\nQualitative data plays a vital role in various aspects:\n\nPatient Demographics:\n\nCategorization: Patients are often categorized based on attributes like age group, gender, race, and ethnicity, which are critical for understanding the distribution of diseases and tailoring medical interventions.\nExample: A study may categorize patients by age group (e.g., children, adults, elderly) to analyze how a particular disease affects different age groups.\n\nDisease Classification:\n\nDiagnosis: Diseases are classified into categories based on symptoms, genetic markers, or other clinical criteria. This classification helps in diagnosing, treating, and researching diseases.\nExample: Types of diabetes (Type 1, Type 2, Gestational Diabetes) are categorical variables that guide treatment plans.\n\nTreatment Outcomes:\n\nOutcome Measures: Treatment outcomes can be categorized as successful/unsuccessful, improved/no improvement, or recurrence/no recurrence. These qualitative outcomes are essential for evaluating the effectiveness of treatments.\nExample: A study on cancer treatment might categorize outcomes as “complete remission,” “partial remission,” or “no response.”\n\nQuality of Life and Patient Satisfaction:\n\nSurveys and Questionnaires: Patient satisfaction surveys and quality of life assessments often use ordinal scales (e.g., very satisfied, satisfied, neutral, dissatisfied, very dissatisfied) to gather qualitative data on patient experiences.\nExample: A patient satisfaction survey may categorize responses to a question about hospital care quality into five levels, from “very poor” to “excellent.”\n\n\nStatistical Analysis of Qualitative Data\n\nFrequency and Proportion: The most basic analysis involves counting the number of occurrences in each category and expressing them as frequencies or proportions (percentages).\nExample: Proportion of patients with a specific blood type in a population.\nChi-Square Tests: Used to determine if there is a significant association between two categorical variables.\nExample: Assessing the relationship between smoking status (smoker, non-smoker) and lung cancer diagnosis (yes, no).\n\n\n\nLogistic Regression: Used for modeling the probability of a binary outcome based on one or more predictor variables.\nExample: Predicting the likelihood of a disease relapse based on treatment type and other categorical factors.\nCross-Tabulation: A method used to examine the relationship between two or more categorical variables by displaying the data in a matrix format.\nExample: Cross-tabulating the relationship between gender and the prevalence of a specific medical condition.\n\nApplications in Medical Research\n\nEpidemiology:\n\nResearchers often use qualitative data to study the distribution and determinants of health-related states in populations.\nExample: Categorizing patients by exposure to a risk factor (e.g., exposure to asbestos: yes/no) and examining its relationship with disease occurrence.\n\nClinical Trials:\n\nClinical trials use qualitative data to classify participants, monitor treatment adherence, and assess treatment responses.\nExample: Categorizing patients based on their response to a new drug as “responsive,” “partially responsive,” or “non-responsive.”\n\nPublic Health:\n\nPublic health studies often categorize populations based on demographic factors to identify health disparities and target interventions.\nExample: Identifying vaccination rates across different ethnic groups to improve immunization programs.\nQualitative data is essential in the medical field for categorizing and analyzing various aspects of health and disease. It provides the basis for understanding patterns, making clinical decisions, and conducting research that ultimately improves patient care. Properly collecting, analyzing, and interpreting qualitative data ensures that medical interventions are tailored to the specific needs of patients and populations.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "basic-stat.html",
    "href": "basic-stat.html",
    "title": "Basic methods",
    "section": "",
    "text": "基本的医学统计方法涵盖了数据的收集、整理、描述、分析和解释，旨在帮助研究者理解疾病的发生、发展、治疗效果及健康状况的分布规律。以下是医学研究中常用的一些基本统计方法：\n\n1. 描述性统计\n\n频率分析：计算各类别或数值出现的频数和百分比。\n集中趋势测量：平均值、中位数和众数，用于描述数据的中心位置。\n离散程度测量：标准差、方差、四分位数间距和极差，反映数据的分散程度。\n\n\n\n2. 推断性统计\n\n参数检验：\n\nt检验：用于比较两组或几组独立或配对样本的均值差异，如学生t检验、配对t检验。\nANOVA（方差分析）：比较多个样本均值是否存在显著差异。\n\n非参数检验：\n\nMann-Whitney U检验（Wilcoxon秩和检验的两样本形式）：适用于连续数据但不符合正态分布的两组样本均值差异检验。\nKruskal-Wallis H检验：多组独立样本的中位数差异检验，适用于非正态分布数据。\n\n卡方检验（Chi-square test）：用于分析分类变量间的关系，如四格表检验。\n\n\n\n3. 回归分析\n\n线性回归：研究一个或多个自变量与一个连续因变量之间的线性关系。\n逻辑回归（Logistic Regression）：适用于因变量为二分类或多元分类的场合，用来估计事件发生概率。\n\n\n\n5. 随机化和抽样\n\n随机化：确保研究的组间可比性。\n抽样技术：包括简单随机抽样、分层抽样、整群抽样等，确保样本代表性。\n\n\n\n6. 误差与功效分析\n\n类型I和II错误的理解，以及统计功效分析，以确定研究设计是否足够强大以检测预期效应。\n\n选择适当的统计方法需依据研究目的、数据类型（定量或定性）、数据分布（正态与否）、样本大小等因素。理解这些基础统计概念和方法对于设计和解读医学研究至关重要。",
    "crumbs": [
      "Basic methods"
    ]
  },
  {
    "objectID": "quat-data-statdesc.html",
    "href": "quat-data-statdesc.html",
    "title": "3  Statistical description of quantitative data",
    "section": "",
    "text": "3.1 Prerequisite\nThe statistical description of quantitative data involves summarizing and analyzing numerical variable to understand its distribution and characteristics. Key measures include:\nTogether, these measures help describe the data’s overall distribution, identify outliers, and inform statistical analysis in clinical and epidemiological research.Prerequisites\nSetting up the required R packages.\nlibrary(tidyverse)\nClick on the download button above to download the data file. Save it in your working directory and import the data file into R using the code below.\nrbc &lt;- read_csv(\"datasets/ex03-01.csv\")\n\n#&gt; Rows: 138 Columns: 1\n#&gt; ── Column specification ─────────────────────────────────────────────────────\n#&gt; Delimiter: \",\"\n#&gt; dbl (1): rbc\n#&gt; \n#&gt; ℹ Use `spec()` to retrieve the full column specification for this data.\n#&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nHere we use the read_csv() function. The first argument is the most important: the path to the file. You can think about the path as the address of the file. The code above will work if you have a ex02-01.csv file in the datasets folder of your project.\nWhen you run read_csv(), it prints out a message telling you the number of rows and columns of data, the delimiter that was used, and the column specifications (names of columns organized by the type of data the column contains). It also prints out some information about retrieving the full column specification and how to quiet this message.\nThe data file has only one column with name rbc. Let’s check if there are some missing values present in it.\nrbc |&gt; \n  anyNA()\n\n#&gt; [1] FALSE\nThe output FALSE indicates no missing values is present.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Statistical description of quantitative data</span>"
    ]
  },
  {
    "objectID": "quat-data-statdesc.html#prerequisite",
    "href": "quat-data-statdesc.html#prerequisite",
    "title": "3  Statistical description of quantitative data",
    "section": "",
    "text": "Example 1 \nA researcher used a random sampling method to examine the red blood cell counts of 138 normal adult women. The measuring results are saved in a data file. Please use the data to create a frequency distribution table.\n\n\n  Download data",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Statistical description of quantitative data</span>"
    ]
  },
  {
    "objectID": "quat-data-statdesc.html#visualization",
    "href": "quat-data-statdesc.html#visualization",
    "title": "3  Statistical description of quantitative data",
    "section": "3.2 Visualization",
    "text": "3.2 Visualization\n\n3.2.1 Frequency table\nA frequency table mentioned here is a statistical tool that organizes data into intervals and lists the number of frequency in each interval. It helps summarize large dataset by showing how often each value or range of values occurs, making it easier to identify trends and the overall distribution of the data. This table is often used in conjunction with visual tools like histogram to enhance data interpretation.\nHere is the steps for creating a frequency table for continuous variables.\n\nComputing the minimum and maximum of the variable.\n\n\nmin_rbc &lt;- rbc |&gt; \n  min()\n\nmax_rbc &lt;- rbc |&gt; \n  max()\n\n\nDetermining the number of intervals. The number of intervals is typically between 10 and 15; here, we adopt 10. The seq() function is used to obtain upper and lower limits of the intervals.\n\n\nbins = 12\nbreaks &lt;- seq(min_rbc, max_rbc, length.out = bins + 1)\nbreaks\n\n#&gt;  [1] 3.070000 3.269167 3.468333 3.667500 3.866667 4.065833 4.265000 4.464167\n#&gt;  [9] 4.663333 4.862500 5.061667 5.260833 5.460000\n\n\n\nUsing the cut() function to dive each data into their respective intervals.\n\n\ncut(pull(rbc), breaks = breaks, include.lowest = T, right = F) |&gt; \n  head(40)\n\n#&gt;  [1] [3.87,4.07) [4.07,4.26) [4.26,4.46) [3.47,3.67) [5.06,5.26) [3.87,4.07)\n#&gt;  [7] [4.26,4.46) [3.67,3.87) [4.66,4.86) [4.07,4.26) [4.46,4.66) [4.07,4.26)\n#&gt; [13] [3.67,3.87) [4.07,4.26) [4.26,4.46) [3.07,3.27) [4.86,5.06) [3.87,4.07)\n#&gt; [19] [4.26,4.46) [3.47,3.67) [4.46,4.66) [3.87,4.07) [4.46,4.66) [4.07,4.26)\n#&gt; [25] [4.46,4.66) [3.87,4.07) [4.26,4.46) [3.47,3.67) [4.86,5.06) [3.87,4.07)\n#&gt; [31] [4.26,4.46) [4.07,4.26) [4.66,4.86) [4.07,4.26) [4.46,4.66) [4.07,4.26)\n#&gt; [37] [4.46,4.66) [3.67,3.87) [3.87,4.07) [4.07,4.26)\n#&gt; 12 Levels: [3.07,3.27) [3.27,3.47) [3.47,3.67) [3.67,3.87) ... [5.26,5.46]\n\n\n\nGenerating the frequency table.\n\n\ncut(pull(rbc), breaks = breaks, include.lowest = T, right = F) |&gt;\n  table() |&gt; \n  knitr::kable(col.names = c(\"interval\", \"freq\"), align = \"c\")\n\n\n\n\ninterval\nfreq\n\n\n\n\n[3.07,3.27)\n2\n\n\n[3.27,3.47)\n3\n\n\n[3.47,3.67)\n9\n\n\n[3.67,3.87)\n14\n\n\n[3.87,4.07)\n22\n\n\n[4.07,4.26)\n30\n\n\n[4.26,4.46)\n21\n\n\n[4.46,4.66)\n15\n\n\n[4.66,4.86)\n10\n\n\n[4.86,5.06)\n6\n\n\n[5.06,5.26)\n4\n\n\n[5.26,5.46]\n2\n\n\n\n\n\n\n\n3.2.2 Frequency histogram\nA frequency histogram is a graphical representation of a frequency table. It displays the distribution of numerical variales by showing the frequency (count) of a value within specific intervals (bins) on the x-axis, with the y-axis representing the frequency. Each bar in the histogram corresponds to an interval, and the height of the bar indicates how many valuess fall within that range. This visual tool is useful for quickly assessing the shape, spread, and central tendency of the data distribution.\nHere we supply two methods to plot a histogram.\n\nbaseggplot2\n\n\n\nhist(\n  x              = pull(rbc), \n  breaks         = breaks, \n  freq           = T,\n  right          = F, \n  col            = \"skyblue\", \n  include.lowest = T,\n  main           = \" \",\n  xlab           = \"Maximum heart rate\",\n  ylab           = \"Frequency\",\n  ylim           = c(0, 32),\n  labels         = T\n)\n\n\n\n\n\n\n\n\n\n\n\nrbc |&gt; \n  ggplot(aes(x = rbc)) +\n  geom_histogram(\n    fill   = \"skyblue\", \n    stat   = \"bin\",\n    color  = \"black\",\n    breaks = breaks,\n    closed = \"left\"\n  ) +\n  stat_bin(\n    geom   = \"text\", \n    aes(label = after_stat(count)),\n    breaks = breaks, \n    closed = \"left\",\n    size   = 4,\n    vjust  = - 0.3\n  ) +\n  labs(x = \"Maximum heart rate\", y = \"Frequency\") +\n  theme(\n    axis.title.x     = element_text(size = 12), \n    axis.title.y     = element_text(size = 12), \n    axis.text.x      = element_text(size = 11),  \n    axis.text.y      = element_text(size = 11),\n    panel.background = element_blank(),        \n    axis.line        = element_line(color = \"black\") \n  )",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Statistical description of quantitative data</span>"
    ]
  },
  {
    "objectID": "quat-data-statdesc.html#measures-of-central-tendency",
    "href": "quat-data-statdesc.html#measures-of-central-tendency",
    "title": "3  Statistical description of quantitative data",
    "section": "3.3 Measures of central tendency",
    "text": "3.3 Measures of central tendency\nCentral tendency is a statistical concept that refers to the measure of the center or typical value in a data set. It provides a summary of the data with a single value that represents the middle or average of the data. The most common measures of central tendency are:\n\n3.3.1 Mean\nThe arithmetic average of all values. It’s calculated by summing all the values and dividing by the number of values.\nFor a population:\n\n\\mu = \\frac{\\sum x_i}{N}\n\nFor a sample:\n\n\\bar{X} = \\frac{\\sum x_i}{n}\n\nwhere x_i represents values of a random variable X, and N and n are the sizes of the population and sample, respectively.\n\nrbc |&gt; \n  pull() |&gt; \n  mean()\n\n#&gt; [1] 4.227029\n\n\n\n\n3.3.2 Median\nThe middle value in a data set when the values are sorted in ascending order. If there is an even number of values, the median is the average of the two middle values. Unlike the mean, the median is not affected by outliers or skewed data, making it a robust indicator of central tendency. To find the median:\n\nSort the data set.\nIf the number of observations N is odd, the median is the middle value.\nIf N is even, the median is the average of the two central values.\n\n\nrbc |&gt; \n  pull() |&gt; \n  median()\n\n#&gt; [1] 4.23",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Statistical description of quantitative data</span>"
    ]
  },
  {
    "objectID": "quat-data-statdesc.html#measures-of-dispersion",
    "href": "quat-data-statdesc.html#measures-of-dispersion",
    "title": "3  Statistical description of quantitative data",
    "section": "3.4 Measures of dispersion",
    "text": "3.4 Measures of dispersion\nDispersion or variability, describe the spread or dispersion of data points in a data set. They provide insight into how much individual data points differ from the central value (mean, median, etc.). Common measures of dispersion include:\n\n3.4.1 Range\nThe difference between the maximum and minimum values in the data set.\n\n\\text{range} = \\text{max} - \\text{min}\n\n\nrange(rbc) |&gt; \n  diff()\n\n#&gt; [1] 2.39\n\n\n\n\n3.4.2 Interquartile range\nInterquartile range (IQR) is the range of the middle 50% of the data, calculated as the difference between the 75th percentile (Q3) and the 25th percentile (Q1).\n\n\\text{IQR} = Q3 - Q1\n\nYou can directly use the IQR() function to get IQR.\n\nrbc |&gt; \n  pull() |&gt; \n  IQR()\n\n#&gt; [1] 0.565\n\n\n\n\n3.4.3 Variance\nMeasures the average squared deviation of each data point from the mean.\nFor a population:\n\n\\sigma^2 = \\frac{1}{N} \\sum_{i=1}^N (x_i - \\mu)^2\n\nFor a sample:\n\nS^2 = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{X})^2\n\nwhere \\mu is the population mean, \\bar{X} is the sample mean, x_i represents each value, and N and n are are the sizes of the population and sample, respectively.\nYou can directly use the var() function to get variance.\n\nrbc |&gt; \n  pull() |&gt; \n  var()\n\n#&gt; [1] 0.1986751\n\n\n\n\n3.4.4 Standard deviation\nThe square root of the variance, providing a measure of spread in the same units as the data.\nFor a population:\n\n\\sigma = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N (x_i - \\mu)^2}\n\nFor a sample:\n\nS = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{X})^2}\n\nYou can directly use the sd() function to get standard deviation.\n\nrbc |&gt; \n  pull() |&gt; \n  sd()\n\n#&gt; [1] 0.4457298\n\n\n\n\n3.4.5 Coefficient of variation\nThe ratio of the standard deviation to the mean, expressed as a percentage, useful for comparing variability between variables with different units or scales.\nFor a population:\n\n\\text{CV} = \\frac{\\sigma}{\\mu} \\times 100%\n\nwhere \\sigma is the standard deviation and \\mu is the mean of a population.\nFor a sample:\n\n\\text{CV} = \\frac{S}{\\bar{X}} \\times 100%\n\nwhere S is the standard deviation and \\bar{X} is the mean of a sample.\n\nmean &lt;- rbc |&gt; \n  pull() |&gt; \n  mean()\n\nsd &lt;- rbc |&gt; \n  pull() |&gt; \n  sd()\n\nsd / mean * 100\n\n#&gt; [1] 10.54475",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Statistical description of quantitative data</span>"
    ]
  },
  {
    "objectID": "normal-distribution.html",
    "href": "normal-distribution.html",
    "title": "4  Normal distribution",
    "section": "",
    "text": "4.1 Prerequisite\nThe normal distribution, also known as the Gaussian distribution, is one of the most important probability distributions in statistics. It is characterized by its “bell-shaped” curve, which is symmetric about the mean. The normal distribution is used to model a wide range of real-world phenomena, including heights and weights of adults, blood pressures, reaction times in psychological experiments, and more.\nSetting up the required R packages in this chapter.\nlibrary(tidyverse)\nlibrary(nortest)\nlibrary(scales)\nlibrary(e1071)",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Normal distribution</span>"
    ]
  },
  {
    "objectID": "normal-distribution.html#normal-distribution",
    "href": "normal-distribution.html#normal-distribution",
    "title": "4  Normal distribution",
    "section": "4.2 Normal distribution",
    "text": "4.2 Normal distribution\nThe probability density function of a normal distribution is given by the formula:\nf(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}}\nwhere x is the variable, \\mu is the mean, and \\sigma is the standard deviation. The \\mu and \\sigma are two parameters of the distribution. The mean \\mu is a location parameter, which defines the central position, as shown in Figure 4.1 . The standard deviation \\sigma is the shape parameter, which defines the width and height of the distribution, as shown in Figure 4.2 .\n\n\n\n\n\n\n\n\nFigure 4.1: The normal curve with different means\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.2: The normal curve with different standard deviation\n\n\n\n\n\n\n4.2.1 Standard normal distribution\nAs shown above, the change of \\mu and \\sigma affects the shape of a normal distribution. For convenience, let\nz = \\frac{x - \\mu}{\\sigma}\nthen the above mentioned probability density function will become as:\nf(z) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{1}{2}z^2} which is called the standard normal distribution, where of \\mu = 0 and \\sigma = 1.\n\n\n4.2.2 Area under curve\nThe cumulative distribution function is the probability that a normal random variable X will be less than or equal to a given value x, which is defined by the formula:\nF(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\int_{-\\infty}^x e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}}The area under the curve of a normal distribution over a specific interval represents the probability that the random variable falls within that interval. This is computed using the cumulative distribution function.\nIn R, you can calculate the area under the curve between two points using the pnorm() function.\n\nExample 1 \nCalculating the area under the standard normal curve between -1.96 and 1.96.\n\n\narea_under_curve &lt;- pnorm(1.96, mean = 0, sd = 1) - pnorm(-1.96, mean = 0, sd = 1)\narea_under_curve\n\n#&gt; [1] 0.9500042\n\n\n\n\n4.2.3 Visualizing the area\nYou can use ggplot2 package in R to plot the normal distribution curve and shade the area under the curve. Here is example to shade the area of the left and right tails.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Normal distribution</span>"
    ]
  },
  {
    "objectID": "normal-distribution.html#sec-normality-test",
    "href": "normal-distribution.html#sec-normality-test",
    "title": "4  Normal distribution",
    "section": "4.3 Normality test",
    "text": "4.3 Normality test\nNormality test is used to determine whether a data follows a normal distribution. This is important because many statistical tests, including the t-test, assume that the data follows a normal distribution.\n\nExample 2 \nA scientist used a random sampling method to examine the red blood cell count of 29 normal adult men. The measuring results are saved in the below file. Please analyze its normality.\n\n\n  Download data \n\nYou can click on the download button above to download and the save it in your own folder. Here we import the data file into R and assign to a tibble named rbc.\n\nrbc &lt;- read_csv(\"datasets/ex04-01.csv\")\n\n#&gt; Rows: 29 Columns: 1\n#&gt; ── Column specification ─────────────────────────────────────────────────────\n#&gt; Delimiter: \",\"\n#&gt; dbl (1): rbc\n#&gt; \n#&gt; ℹ Use `spec()` to retrieve the full column specification for this data.\n#&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nTest of normality is used to determine if a variable is well-modeled by a normal distribution. This is an important assumption for many statistical tests. Histogram (Section 3.2.2) can be used as a visual tool to assess whether a variable follows a normal distribution, but it should be used with caution due to the reasons below:\n\nFor a small sample data, the histogram might not provide a clear picture of the distribution, making it harder to assess normality. With larger samples, the histogram gives a better indication but may still be misleading.\nThe appearance of the histogram can change significantly depending on the number of bins (or the bin width). Too few bins might obscure important features of the data, while too many bins might introduce noise.\nThe interpretation of a histogram is somewhat subjective. Two people might look at the same histogram and draw different conclusions about normality.\n\nSince histogram can sometimes be misleading or ambiguous, it’s a good practice to use it alongside other methods:\n\n4.3.1 Normality test method\nStatistical tests can provide a more formal assessment of normality, though they also have limitations and can be sensitive to sample size. Here we only show a few commonly used:\n\nShapiro-Wilk test: Best for small to medium-sized data.\nShapiro-Francia test: A variation of the Shapiro-Wilk test. It is generally more appropriate for dealing with larger sample size data compared to the Shapiro-Wilk test, particularly for data that is expected to be normally distributed.\nAnderson-Darling test: Gives more weight to the tails of the distribution.\n\nWe perform the Shapiro-Wilk test using shapiro.test(), which lies in the stats package of base R. The Shapiro-Francia test and Anderson-Darling test are performed by sf.test() and ad.test(), respectively. Both of them come from the nortest package, which need to be installed beforehand.\n\nShapiro-WilkShapiro-FranciaAnderson-Darling\n\n\n\nrbc |&gt; \n  pull() |&gt; \n  shapiro.test()\n\n#&gt; \n#&gt;  Shapiro-Wilk normality test\n#&gt; \n#&gt; data:  pull(rbc)\n#&gt; W = 0.98382, p-value = 0.9228\n\n\n\n\n\nrbc |&gt; \n  pull() |&gt; \n  sf.test()\n\n#&gt; \n#&gt;  Shapiro-Francia normality test\n#&gt; \n#&gt; data:  pull(rbc)\n#&gt; W = 0.97756, p-value = 0.682\n\n\n\n\n\nrbc |&gt; \n  pull() |&gt; \n  ad.test()\n\n#&gt; \n#&gt;  Anderson-Darling normality test\n#&gt; \n#&gt; data:  pull(rbc)\n#&gt; A = 0.22677, p-value = 0.7975\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nIf the p-value is greater than 0.05, the data is normally distributed (fail to reject the null hypothesis).\nIf the p-value is less than 0.05, the data is not normally distributed (reject the null hypothesis).\n\n\n\n\n\n4.3.2 Visualization method\nQ-Q Plot (Quantile-Quantile Plot) is a more reliable visual tool that plots the quantiles of the data against the quantiles of a theoretical normal distribution. Visual inspection of the data can be very informative and is always useful as a supplementary method.\nYou can create a Q-Q Plot using qqnorm() function. More, you can add a line to a theoretically normal quantile-quantile plot by qqline(), which passes through the first and third quartiles by default. These two functions can be found from the stats package of base R.\n\nrbc |&gt; \n  pull() |&gt; \n  qqnorm(main = \"\", datax = T)\n\nrbc |&gt; \n  pull() |&gt; \n  qqline(datax = T)\n\n\n\n\n\n\n\n\nAlternatively, you can use the ggplot2 package to create a Q-Q plot, which has more customization and flexibility. Here is an example.\n\nrbc |&gt;\n  ggplot(aes(sample = rbc)) +\n  geom_qq(shape = 1, size = 2.3) +\n  geom_qq_line() +\n  labs(x = \"Theoretical Quantiles\", y = \"Sample Quantiles\") +\n  coord_flip() +\n  theme(\n    axis.text  = element_text(size = 12),\n    axis.title = element_text(size = 12)\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nIf the data points fall approximately along the reference line in the Q-Q plot, the data is likely normally distributed.\nSignificant deviations from the line indicate departures from normality.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Normal distribution</span>"
    ]
  },
  {
    "objectID": "normal-distribution.html#medical-reference-range",
    "href": "normal-distribution.html#medical-reference-range",
    "title": "4  Normal distribution",
    "section": "4.4 Medical reference range",
    "text": "4.4 Medical reference range\nA medical reference range is the set of values that a medical test result falls within for a healthy population. These ranges are used by healthcare providers to interpret laboratory results and determine whether a patient’s test results are normal or indicate a potential health issue.\n\n4.4.1 Establishment\nReference ranges are typically established by testing a large sample of healthy individuals and determining the range within which a certain percentage (often 95%) of results fall. The middle 95% of the population’s values are considered the reference range, meaning that 2.5% of healthy individuals might naturally have results slightly below this range and another 2.5% slightly above it.\n\nExample 3 \nA investigator randomly sampled 180 normal adult males in a region, and measured the fibrinogen levels (g/L) in their venous blood. The data is saved in a file below. Have a try to establish the 95% medical reference range for fibrinogen level of normal adult males in that region.\n\n\n  Download data \n\nThe code chunk below can print the results directly.\n\nmed_ref_range &lt;- read_csv(\"datasets/ex04-02.csv\") |&gt; \n  pull() |&gt; \n  quantile(probs = c(0.025, 0.975)) |&gt; \n  round(digits = 2) |&gt; \n  print()\n\n#&gt; Rows: 180 Columns: 1\n#&gt; ── Column specification ─────────────────────────────────────────────────────\n#&gt; Delimiter: \",\"\n#&gt; dbl (1): fibrinogen\n#&gt; \n#&gt; ℹ Use `spec()` to retrieve the full column specification for this data.\n#&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n#&gt;  2.5% 97.5% \n#&gt;  1.77  3.63\n\n\nThe result indicates that the 95% medical reference range for the fibrinogen level of normal adult males is: (1.77, 3.63).\n\n\n4.4.2 Result interpretation\nA test result that falls within the reference range is usually considered normal. A result outside the reference range might indicate an abnormal condition, but this must be interpreted in the context of the patient’s overall health, symptoms, and medical history. Not all out-of-range results indicate disease; they might be normal for a specific individual due to factors like temporary stress, diet, or exercise.\n\n\n4.4.3 Considerations\nReference ranges are not absolute; what is normal for one individual may not be normal for another, especially at the edges of the range. Doctors consider a variety of factors, including patient history and symptoms, when interpreting test results. An out-of-range result may warrant further testing or a different interpretation based on the clinical context.\nReference ranges may be updated as new research and technologies emerge, so staying informed about the latest standards is important for accurate diagnosis and treatment.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Normal distribution</span>"
    ]
  },
  {
    "objectID": "statistical-inference.html",
    "href": "statistical-inference.html",
    "title": "5  Statistical inference",
    "section": "",
    "text": "5.1 Prerequisites\nIn statistics we often use samples to infer population characteristics, since it’s impractical or impossible usually to measure the entire population. This process is called statistical inference. It involves using statistical techniques to make estimates, test hypothesis, and make predictions about population parameters. In this chapter, we maily focus on the estimation of population means.\nlibrary(tidyverse)\nlibrary(rmarkdown)",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistical inference</span>"
    ]
  },
  {
    "objectID": "statistical-inference.html#distribution-of-sample-means",
    "href": "statistical-inference.html#distribution-of-sample-means",
    "title": "5  Statistical inference",
    "section": "5.2 Distribution of sample means",
    "text": "5.2 Distribution of sample means\nThe distribution of sample means is a fundamental concept in statistics, describing the distribution of sample means obtained from multiple samples drawn from the same population. Here is a example to demonstrate the distribution of sample means.\n\nExample 1 \nAssume thrombin time follow the normal distribution of \\mu=17.5, \\sigma=1.2. A researcher randomly drew 100 samples from a population of size 60000 , with each sample n=10 observations. The data is saved the the data below. The mean \\bar{X} and standard deviation S for each sample are shown in Table 5.1, please analyze the distribution of the 120 means.\n\n\n  Download data \n\n\ndf &lt;- read_csv(\"datasets/ex05-01.csv\", show_col_types = F) |&gt; \n  summarise_all(\n    list(\n1      n       = ~ n(),\n      median  = median,\n      mean    = mean,\n      sd      = sd        \n    )\n  ) |&gt; \n  pivot_longer(\n    cols      = everything(),\n    names_to  = c(\"sample\", \".value\"),\n    names_sep = \"_\"\n  ) |&gt; \n  mutate(across(c(mean, sd), \\(x)round(x, 2)))\n\ndf |&gt; paged_table()\n\n\n1\n\nHere n() must use a style of anonymous function, unlike the other three ones.\n\n\n\n\n\n\nTable 5.1: The n, median, mean and sd of the 100 random samples (s)\n\n\n\n\n  \n\n\n\n\n\n\nIt is clear that the mean of each sample is different, and also may not equal to the mean of the population. The difference between a population parameter and a corresponding sample statistic is called sampling error. It arises because only a random sample of the entire population is observed, which may not perfectly represent the entire population.\nTo visualize the distribution of the 120 sample means, here we create a histogram:\n\ndf |&gt; \n  select(mean) |&gt; \n  pull() |&gt; \n  hist(\n  freq           = T,\n  right          = F, \n  col            = \"lightblue\", \n  include.lowest = T,\n  main           = \" \",\n  xlab           = \"The average of the thrombin time\",\n  ylab           = \"Frequency\",\n  ylim           = c(0, 26),\n  labels         = T\n)\n\n\n\n\n\n\n\n\nFrom the figure above we can say that the distribution of the sample mean can be approximated by a normal distribution. The two sample statistics of mean \\bar{X} and standard deviation S can be achieved by the following code:\n\ndf |&gt; \n  select(x = mean) |&gt; \n  summarise(\n    mean = mean(x),\n    sd   = sd(x)\n  ) |&gt; \n  round(digits = 2) |&gt; \n  knitr::kable(align = \"c\")\n\n\n\n\nmean\nsd\n\n\n\n\n17.46\n0.36\n\n\n\n\n\nThe mean of the sample means is 17.5, equal to the population mean \\mu=1.75, while the standard deviation of the sample mean is 0.4, less than the standard deviation of population \\sigma = 1.25.\n\n5.2.1 Standard error\nThe standard deviation of the distribution of the sample means is called the standard error (SE). It reflects the typical distance between a sample mean and the population mean. The standard error of sample means is given by:\n\\sigma_{\\bar{X}} = \\frac{\\sigma}{\\sqrt{n}}\nHowever, the population parameter \\sigma is usually unknown, therefore, the sample standard deviation S is used as an estimator for the population standard deviation. Therefore, an estimation of the standard error above mentioned is defined by:\nS_{\\bar{X}} = \\frac{S}{\\sqrt{n}}\nAs the sample size n increases, the standard error decreases, meaning the sample means will be closer to the population mean.\nIt is need to be pointed out that the above formula for calculating the sampling error of the mean is only adapt to simple random sampling. For other sampling methods, there are corresponding formulas exist.\n\n\n5.2.2 Central Limit Theorem\nRegardless of the population distribution, as the sample size n becomes large (typically n \\geq 30 is considered sufficient), the sampling distribution of the sample mean will approximate a normal distribution. This is the essence of the Central Limit Theorem in statistics.\nFor small sample sizes, if the population itself is normally distributed, the distribution of the sample means will also be normally distributed.\nBecause the sampling distribution of the sample means can be approximated by a normal distribution (especially for large n), it allows for the construction of confidence intervals for the population mean and the conducting of hypothesis tests. The normality of the sampling distribution justifies the use of z-test or t-test depending on whether the population variance is known or unknown and the sample size.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistical inference</span>"
    ]
  },
  {
    "objectID": "statistical-inference.html#t-distribution",
    "href": "statistical-inference.html#t-distribution",
    "title": "5  Statistical inference",
    "section": "5.3 t distribution",
    "text": "5.3 t distribution\nThe sample mean \\bar{X} follows a normal distribution when the underlying population is normally distributed, or when the sample size is sufficiently large due to the Central Limit Theorem. However, when you’re converting the sample mean into a standardized form, the distribution you use depends on whether or not you know the population standard deviation \\sigma.\n\nIf \\sigma is known, the sample mean \\bar{X} can be standardized using the formula:\n\n\nz = \\frac{\\bar{X} - \\mu}{\\sigma_{\\bar{X}}} = \\frac{\\bar{X} - \\mu}{\\sigma / \\sqrt{n}}\n\nIn this case, the standardized variable Z follows a standard normal distribution N(0, 1).\n\nIf \\sigma is unknown, which is often the case in practice, you need to estimate it using the sample standard deviation S. The formula for standardizing the sample mean becomes:\n\n\nt = \\frac{\\bar{X} - \\mu}{S_{\\bar{X}}} = \\frac{\\bar{X} - \\mu}{S / \\sqrt{n}}\n\nHere, t doesn’t not follow a standard normal distribution anymore, it follows a t-distribution with degrees of freedom \\nu = n - 1 (n is the sample size).\nThe t-distribution, also known as Student’s t-distribution, is used in statistics when estimating population parameters when the sample size is small and/or the population variance is unknown. It’s especially important in confidence intervals, hypothesis testing, and regression analysis.\n\n5.3.1 Visual representation\nThe t-distribution can be plotted to show how it compares with the normal distribution.\n\n\n\n\n\n\n\n\n\nThe t-distribution is similar to the standard normal distribution, but it has heavier tails. This means a higher probability of values further from the mean, especially with a smaller degree of freedom. As degrees of freedom increase, the t-distribution curve will converge toward the standard normal distribution curve.\nThe shape of the t-distribution depends on the degrees of freedom \\nu, which is typically related to the sample size (\\nu = n - 1). As the degrees of freedom increase, the t-distribution approaches the standard normal distribution. For large sample sizes (\\nu &gt; 60), the t-distribution and normal distribution are almost indistinguishable.\n\n\n5.3.2 Common uses\n\nConfidence intervals\nThe t-distribution is used to construct confidence intervals for the population mean when the population variance is unknown and the sample size is small.\nt-test: Used to compare the means of two groups when the sample size is small and the population standard deviation is unknown.\nRegression analysis: In linear regression, t-test are used to determine whether the coefficients of the independent variables are significantly different from zero.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistical inference</span>"
    ]
  },
  {
    "objectID": "statistical-inference.html#estimation-of-population-mean",
    "href": "statistical-inference.html#estimation-of-population-mean",
    "title": "5  Statistical inference",
    "section": "5.4 Estimation of population mean",
    "text": "5.4 Estimation of population mean\nParameter estimation is a concept in statistics that involves using statistic off a sample to estimate parameters of the corresponding population. The sample should be random and large enough to provide an accurate estimate. A parameter is a numerical characteristic about a population, such as a mean, proportion, or standard deviation. Here we just talk about the estimation of population mean.\n\n5.4.1 Point estimation\nThis involves using sample data to calculate a single value that serves as the best guess for a population mean. However, obtaining an exact point estimate of the population mean from just one random sample is almost unattainable.\nCalculate the sample mean \\bar{X}: The sample mean is calculated using the formula:\n\n\\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\n\nwhere n is the sample size, and x_i are the individual sample values. The sample mean (\\bar{X}) serves as an estimate of the population mean (\\mu).\nHowever, a point estimate does not consider the sampling error and cannot tell you how close the estimate is likely to be to the true population mean. Instead of relying on a single point estimate, interval estimation are often used because they provide a range of values within which the true population mean is likely to lie along with a confidence level.\n\n\n5.4.2 Confidence interval\nThe method for calculating the confidence interval of the population mean varies depending on whether the population standard deviation \\sigma is known and the size of sample n. Typically, there are two types of methods: the t-distribution and the z-distribution (also known as the u-distribution). The following will introduce the methods for calculating the confidence interval of a single population mean and the confidence interval of the difference between two population means.\nSingle population mean\nConfidence interval (CI) is associated with a confidence level, typically 95%, which indicates the degree of certainty that the true population mean falls in the range. The two-sided 1-\\alpha confidence interval of single population mean can be formulated as:\n\n\\text{CI} = \\bar{X} \\pm t_{\\nu,\\alpha/2} \\cdot \\frac{S}{\\sqrt{n}} \\quad \\text{or} \\quad \\text{CI} = \\bar{X} \\pm z_{\\alpha/2} \\cdot \\frac{S}{\\sqrt{n}}\n\nSimilarly, the one-sided 1-\\alpha confidence interval for the population mean is given by：\n\n\\mu&gt; \\bar{X} - t_{\\nu,\\alpha} \\cdot \\frac{S}{\\sqrt{n}} \\quad \\text{or} \\quad \\mu&gt; \\bar{X} - z_\\alpha \\cdot \\frac{S}{\\sqrt{n}}\n\n\n\\mu &lt; \\bar{X} + t_{\\nu,\\alpha} \\cdot \\frac{S}{\\sqrt{n}} \\quad \\text{or} \\quad \\mu &lt; \\bar{X} + z_\\alpha \\cdot \\frac{S}{\\sqrt{n}}\n\nwhere t is the critical value from the t-distribution, it depends on the degree of freedom \\nu and the level of confidence \\alpha; z is the critical value from the standard normal distribution, it depends on the level of confidence \\alpha.\n\nExample 2 \nA research group randomly selected 20 adult men in a area and measured their red blood cell count. Please calculate the 95% confidence interval for the population mean.\n\n\n  Download data \n\nSince the sample size is small, we would use the t-distribution to estimate the population mean. In practice, you can always use the t-distribution regardless of the size of samples, because as the increase of degrees of freedom , t-distribution approaches the standard normal distribution.\nIn this chapter, we write a function named mean_CI() to tackle this problem.\n\n\nmean_CI  &lt;- function(x, alternative = \"two.sided\", conf.level = 0.95){\n  x_bar  &lt;- mean(x, na.rm = T)\n  s      &lt;-  sd(x, na.rm = T)\n  n      &lt;- length(x)  \n  stderr &lt;- s / sqrt(n)\n  \n  type  &lt;- alternative\n  alpha &lt;- 1- conf.level\n  \n  case_when(\n    type == \"two.sided\" ~ {\n      t_stat &lt;- qt(1 - alpha / 2, df = n - 1)\n      tibble( \n        mean       = x_bar,\n        lower_ci   = x_bar - stderr * t_stat,\n        upper_ci   = x_bar + stderr * t_stat,\n        conf_level = conf.level\n      )\n    },\n    \n    type == \"greater\"   ~ {\n      t_stat &lt;- qt(alpha, df = n - 1)\n      tibble(\n        mean       = x_bar,\n        lower_ci   = x_bar + stderr * t_stat,\n        upper_ci   = Inf,\n        conf_level = conf.level\n      )\n    },\n      \n    type == \"less\"      ~ {\n      t_stat &lt;- qt(alpha, df = n - 1)\n      tibble( \n        mean       = x_bar,\n        lower_ci   = -Inf,\n        upper_ci   = x_bar - stderr * t_stat,\n        conf_level = conf.level\n     )\n    }\n  )\n}\n\n\n\nread_csv(\"datasets/ex05-02.csv\", show_col_types = F) |&gt; \n  pull() |&gt; \n  mean_CI() \n\n#&gt; # A tibble: 1 × 4\n#&gt;    mean lower_ci upper_ci conf_level\n#&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n#&gt; 1  5.31     5.04     5.59       0.95\n\n\nAlternatively, you can use a function MeanCI() from DescTools package.\n\nread_csv(\"datasets/ex05-02.csv\", show_col_types = F) |&gt; \n  pull() |&gt;\n  DescTools::MeanCI()\n\n#&gt;     mean   lwr.ci   upr.ci \n#&gt; 5.312500 5.035249 5.589751\n\n\nDifference between two population means\nThe confidence interval for the difference between two population means is used to estimate the range within which the true difference between the means of two populations lies, based on sample data.\nRandomly sample from two normal populations N(\\mu_1, \\sigma^2) and N(\\mu_2, \\sigma^2) with equal population variances but unequal population means. The sample sizes, means, and standard deviations of the two samples are denoted by n_1, \\bar{X}_1, S_1 and n_2, \\bar{X}_2, S_2 respectively. If the population variances are unknown and the sample sizes are small, then the two-sided (1-\\alpha) confidence interval for the difference between the two population means (\\mu_1 - \\mu_2) is given by:\n\n(\\bar{X}_1 - \\bar{X}2) \\pm t_{\\alpha/2, \\nu} \\cdot \\sqrt{S_c^2(\\frac{1}{n_1} + \\frac{1}{n_2})} \\ , \\quad S_c^2 = \\frac{(n_1 -1)S_1^2 + (n_2 -1)S_2^2}{n_1 + n_2 -2}\n\nWhere t_{\\alpha/2, \\nu} is the t-score with \\nu = n_1 + n_2 - 2 degrees of freedom, \\alpha is the level of confidence, S_c^2 is called the pooled variance.\nIf thevariances of the two populations mentiond above are unknown and unequal, then the two-sided (1-\\alpha) confidence interval for the difference between the two population means (\\mu_1 - \\mu_2) is given by:\n\n(\\bar{X}_1 - \\bar{X}2) \\pm t_{\\alpha/2, \\nu} \\cdot \\sqrt{\\frac{S_1^2}{n_1} + \\frac{S_2^2}{n_2}}\\ , \\\n\\nu \\approx \\frac{\\left(\\frac{S_1^2}{n_1} + \\frac{S_2^2}{n_2}\\right)^2}{\\frac{\\left(\\frac{S_1^2}{n_1}\\right)^2}{n_1 - 1} + \\frac{\\left(\\frac{S_2^2}{n_2}\\right)^2}{n_2 - 1}}\n\nIf the sample sizes of both samples are large (e.g., both are greater than 60)， the confidence interval for the difference between two population means \\mu_1 - \\mu_2 is given by:\n\n(\\bar{X}_1 - \\bar{X}2) \\pm z_{\\alpha/2} \\cdot \\sqrt{\\frac{S_1^2}{n_1} + \\frac{S_2^2}{n_2}}\n\nWhere \\bar{X}_1 and \\bar{X}_2 are the sample means；n_1 and n_2 are the sample sizes. S_1^2 and S_2^2are the sample variances. z_{\\alpha/2} is the z-score corresponding to the desired confidence level.\nHere is a custom function named mean_diff_CI() to resolve this issue.\n\n\nmean_diff_CI &lt;- function(x, y, alternative = \"two.sided\", conf.level = 0.95, \n                         var.equal = FALSE){\n  x &lt;- na.omit(x)\n  y &lt;- na.omit(y)\n  \n  x_bar &lt;- mean(x)\n  v1    &lt;- var(x)\n  n1    &lt;- length(x)  \n  \n  y_bar &lt;- mean(y)\n  v2    &lt;- var(y)\n  n2    &lt;- length(y)  \n  \n  type  &lt;- alternative\n  alpha &lt;- 1- conf.level\n  diff  &lt;-  x_bar - y_bar \n  \n  if(var.equal) {\n    df     &lt;- n1 + n2 - 2\n    var    &lt;- ((n1 - 1) * v1 + (n2 - 1) * v2) / df\n    stderr &lt;- sqrt(var * (1 / n1 + 1 / n2))\n  } else{\n    stderr1 &lt;- sqrt(v1/n1)\n    stderr2 &lt;- sqrt(v2/n2)\n    stderr  &lt;- sqrt(stderr1^2 + stderr2^2)\n    df      &lt;- stderr^4/(stderr1^4/(n1 - 1) + stderr2^4/(n2 - 1))\n  }\n  \n  case_when(\n    type == \"two.sided\" ~ {\n      t_stat &lt;- qt(1 - alpha / 2, df)\n      tibble( \n        mean_x     = x_bar,\n        mean_y     = y_bar,\n        mean_diff  = diff,\n        lower_ci   = diff - stderr * t_stat,\n        upper_ci   = diff + stderr * t_stat,\n        conf_level = conf.level\n      )\n    },\n    \n    type == \"greater\"   ~ {\n      t_stat &lt;- qt(alpha, df)\n      tibble( \n        mean_x     = x_bar,\n        mean_y     = y_bar,\n        mean_diff  = diff,\n        lower_ci   = diff + stderr * t_stat,\n        upper_ci   = Inf,\n        conf_level = conf.level\n      )\n    },\n      \n    type == \"less\"      ~ {\n      t_stat &lt;- qt(alpha, df)\n      tibble( \n        mean_x     = x_bar,\n        mean_y     = y_bar,\n        mean_diff  = diff,\n        lower_ci   = -Inf,\n        upper_ci   = diff - stderr * t_stat,\n        conf_level = conf.level\n     )\n    }\n  )\n}\n\n\n\nExample 3 \nTo compare the difference of hemoglobin concentration between people with alpha thalassemia trait and silent carrier, a doctor choose a random sample of 42 patients from a patient database. 22 subjects have alpha thalassemia trait, 20 are silent_carrier. Have a try to analyze the 95% confidence interval of the two populations.\n\n\n  Download data \n\n\ndf &lt;- read_csv(\"datasets/ex05-03.csv\", show_col_types = F)\nx &lt;- pull(df, 1)\ny &lt;- pull(df, -1)\nmean_diff_CI(x, y, var.equal = T)\n\n#&gt; # A tibble: 1 × 6\n#&gt;   mean_x mean_y mean_diff lower_ci upper_ci conf_level\n#&gt;    &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n#&gt; 1   11.0   11.8    -0.715    -1.67    0.245       0.95\n\n\nSamely, you can use the function MeanDiffCI() from DescTools package, but this function does not have a direct argument to control for whether the variances of the two populations are equal.\n\nDescTools::MeanDiffCI(x, y, na.rm = T, var.equal = T)\n\n#&gt;   meandiff     lwr.ci     upr.ci \n#&gt; -0.7145455 -1.6678398  0.2387489\n\n\nAs you can see, the confidence intervals printed out from DescTools::MeanDiffCI and mean_diff_CI() have a little difference.\n\n\n5.4.3 Interpreting the confidence interval\nThe confidence interval represents a range within which the true population mean is expected to fall with a certain level of confidence (usually 95%).\nAs for the difference between two population means, the truth is the same. Moreover, if the interval of the difference between two population means includes 0, this suggests that there may be no significant difference between the two population means. If the interval does not include 0, it suggests that there is a significant difference.\nConsider the 100 samples drawn from normal populations N(17.5, 1.2) (see Example 1), we can construct the 95% confidence intervals for them. Look up the results in Table 5.2 , flag = TRUE indicates the confidence interval contain the population mean, and the vise is not true.\n\npop.mean = 17.5\n\ndf &lt;- read_csv(\"datasets/ex05-01.csv\", show_col_types = F) |&gt; \n  summarise_all(\n    list(\n      s.mean = mean,\n      ci     = mean_CI\n    )\n  ) |&gt; \n  pivot_longer(\n    cols      = everything(),\n    names_to  = c(\"sample\", \".value\"),\n    names_sep = \"_\"\n  ) |&gt; \n  mutate(\n    lower.ci = ci[[2]],\n    upper.ci = ci[[3]],\n    ci       = NULL\n  ) |&gt;\n  mutate(\n    flag = lower.ci &lt;= pop.mean & upper.ci &gt;= pop.mean,\n    sample = as.factor(c(1:100)),\n    across(c(2:4), ~ round(., digits = 2))\n  )\ndf |&gt; \n  paged_table()\n\n\n\nTable 5.2: The confidence intervals for the 100 samples\n\n\n\n\n  \n\n\n\n\n\n\nTo highlight the confidence intervals that do not contain the population mean with a different color,\nTo see more clearli, we visualize the confidence intervals for the means of 100 samples as below. The blue verticale line represents the population mean, black dot denotes each sample mean, and the short horizontal lines characterize the confidence intervals. The confidence intervals that do not contain the population mean are highlighted with a red color.\n\ndf |&gt; \n  ggplot(aes(x = s.mean, y = sample)) +\n  geom_point(size = 0.4) +\n  geom_vline(xintercept = pop.mean, linetype = \"dashed\", color = \"blue\") +\n  geom_errorbarh(\n    aes(xmin = lower.ci, xmax = upper.ci, color = flag), \n    height = 0.3,\n    linewidth = 0.4) +\n  xlim(15, 20) +\n  labs(x = \"Means of sample and 95% CI\", y = \"Sample index\") +\n  theme_light() +\n  theme(\n    axis.text = element_text(size = 6.5), \n    legend.position = \"none\",\n    axis.title  = element_text(size = 10)\n  )",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistical inference</span>"
    ]
  },
  {
    "objectID": "hypothesis-test.html",
    "href": "hypothesis-test.html",
    "title": "6  Hypothesis test",
    "section": "",
    "text": "6.1 Fundamentals of hypothesis test\nHypothesis test is a statistical method used to make inferences or draw conclusions about a population based on sample data. It involves testing a statement (the hypothesis) about a population parameter, such as the population mean or proportion, by comparing the sample data to what is expected under the null hypothesis.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Hypothesis test</span>"
    ]
  },
  {
    "objectID": "hypothesis-test.html#fundamentals-of-hypothesis-test",
    "href": "hypothesis-test.html#fundamentals-of-hypothesis-test",
    "title": "6  Hypothesis test",
    "section": "",
    "text": "6.1.1 The fundamentals\nThe small probability event method or proof by contradiction using small probability event, is a reasoning approach used in hypothesis test based on the principle of probability theory. The core idea is that if an event is highly unlikely to occur under a certain assumption, and it does occur, then the assumption is likely incorrect.\nIt start from the opposite assumption (H_0) to indirectly assess whether the problem of interest (H_1) holds true. This involves assuming that H_0 is true, calculating the test statistic, and then using the resulting p-value to determine whether H_0 can be upheld.\n\n\n6.1.2 Some concepts and notations\nBefore you learn and insight into more about hypothesis test, some concepts and notations need to be noticed.\nNull hypothesis (H_0)\nThis is the default assumption that there is no effect or no difference. It is the hypothesis that we seek to test against. For example: The average height of adult men in a certain city is 175 cm. (H_0: \\mu = 175 cm)\nAlternative hypothesis (H_1)\nThis is what you want to prove. It represents an effect, a difference, or a relationship that contradicts the null hypothesis. The hypothesis is that there is an effect or difference. For example: The average height of adult men in the city is not 175 cm. (H_1: \\mu \\ne 175 cm)\nTest statistic\nA standardized value calculated from the sample data that is used to determine whether to reject H_0, such as the t-statistic, z-statistic, or chi-square statistic.\nSignificance level (\\alpha)\nThe probability threshold for rejecting the null hypothesis. Commonly used values are 0.05, 0.01, or 0.10. α = 0.05 means there’s a 5% risk of rejecting the null hypothesis when it is actually true.\nP-value\nThe probability of obtaining a test statistic at least as extreme as the one observed, assuming the null hypothesis is true. If the p-value is less than α, the null hypothesis is rejected. The smaller the p-value, the stronger the evidence against the null hypothesis. For example, if the p-value is 0.03, there is a 3% chance of observing such a result if H₀ is true.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Hypothesis test</span>"
    ]
  },
  {
    "objectID": "hypothesis-test.html#steps-of-hypothesis-test",
    "href": "hypothesis-test.html#steps-of-hypothesis-test",
    "title": "6  Hypothesis test",
    "section": "6.2 Steps of hypothesis test",
    "text": "6.2 Steps of hypothesis test\nThe process of conducting a hypothesis test involves several key steps. Here’s an outline of the typical procedure:\n\nDefine H_0 and H_1.\nChoose the significance level \\alpha, typically 0.05.\nSelect the appropriate test.\nCompute the test statistic.\nDetermine the p-value.\nMake a decision based on the p-value and significance level.\n\nReject H_0: If the p-value ≤ α.\nFail to reject H_0: If the p-value &gt; α.\n\nDraw conclusions in the context of the research.\n\nIn the subsequet chapters, some typical test method will be introduced.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Hypothesis test</span>"
    ]
  },
  {
    "objectID": "hypothesis-test.html#considerations-for-hypothesis-test",
    "href": "hypothesis-test.html#considerations-for-hypothesis-test",
    "title": "6  Hypothesis test",
    "section": "6.3 Considerations for hypothesis test",
    "text": "6.3 Considerations for hypothesis test\n\n6.3.1 Type I error and type II error\nIt is known that hypothesis test employs the principle of proof by contradiction through small probability events. The conclusions drawn based on the p-value are probabilistic essentially, meaning that the conclusions may not be entirely correct. Two types of errors can occur.\nType I error (α) (false positive)\nA type I error occurs when you reject the null hypothesis (H_0) when it is actually true. For example, imagine that you are testing a new drug to see if it lowers blood pressure. The null hypothesis states that the drug has no effect. If you conclude that the drug does lower blood pressure when in fact it does not, you have made a type I error.\nThe probability of making a type I error is denoted by the significance level α .Common choices for α are 0.05 or 0.01, meaning there is a 5% or 1% risk of rejecting H₀ when it is true.\nType II error (β) (false negative)\nA type II error occurs when you fail to reject the null hypothesis (H_0) when the alternative hypothesis (H_1) is actually true. Continuing with the drug example, if the drug actually does lower blood pressure, but you conclude that it doesn’t, you’ve made a type II error.\nThe probability of making a type II error is denoted by \\beta. The power of a test (1-\\beta) represents the probability of correctly rejecting a false null hypothesis when the alternative hypothesis is true. In other words, it’s the likelihood that the test will detect an effect or difference when one actually exists.\nFor example , in a medical study designed to detect whether a new drug is effective, if the power of the test is 0.9, there is a 90% chance of correctly rejecting H_0 (that the drug has no effect) when H_1 (that the drug is effective) is true.\nFactors that affect β include:.\n\nSample size: Larger sample sizes generally increase the power of a test.\nEffect size: Larger differences or stronger effects are easier to detect, thus increasing power.\nSignificance level (α): Increasing α can increase power, but it also increases the risk of a type I error.\nVariance: Lower variability within the data increases power.\n\nIn hypothesis test, there is often a trade-off between the risks of type I and type II errors. Lowering the significance level α reduces the risk of a type I error but increases the risk of a type II error, and vice versa.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Hypothesis test</span>"
    ]
  },
  {
    "objectID": "hypothesis-test.html#considerations-in-hypothesis-test",
    "href": "hypothesis-test.html#considerations-in-hypothesis-test",
    "title": "6  Hypothesis test",
    "section": "6.4 Considerations in hypothesis test",
    "text": "6.4 Considerations in hypothesis test\n\n6.4.1 Study design\nStudy design is the prerequisite of hypothesis test. The groups to be compared should be balanced and comparable, meaning that aside from the primary factor under investigation (such as a new drug in a clinical trial versus a control drug), other factors (such as age, gender, disease duration, and severity) that could influence the results should be identical or similar between groups. The best way to ensure balance is randomization before treatment.\n\n\n6.4.2 Different tests for different types of data\nThe appropriate test should be selected based on the purpose of the analysis, the type and distribution of the data, the study design, the sample size, and the conditions under which different statistical methods are applicable. For example, paired t-tests should be used for paired design measurement data; for completely randomized design measurement data with small sample sizes (i.e., n ≤ 60) with equal variances, the two-sample t-test should be used. If variances are unequal, an approximate t-test should be used.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Hypothesis test</span>"
    ]
  },
  {
    "objectID": "ttest.html",
    "href": "ttest.html",
    "title": "7  t-test",
    "section": "",
    "text": "7.1 Prerequisites\nA t-test is a statistical test used to compare the means of two groups to determine if they are significantly different from each other. It is commonly used in situations where the sample size is small and the population variance is unknown. There are different types of t-tests depending on the study design and the nature of the data.\nlibrary(tidyverse)",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>t-test</span>"
    ]
  },
  {
    "objectID": "ttest.html#one-sample-t-test",
    "href": "ttest.html#one-sample-t-test",
    "title": "7  t-test",
    "section": "7.2 One-sample t-test",
    "text": "7.2 One-sample t-test\nThe one-sample t-test is used to determine whether the mean of a single sample is significantly different from a known or hypothesized population mean. It is commonly used when you have a sample and want to test if its mean differs from a theoretical value or an expected value. Its test statistic t is calculated as follows:\n\nt = \\frac{\\bar{X} - \\mu_0}{S_{\\bar{X}}} = \\frac{\\bar{X} - \\mu_0}{S / \\sqrt{n}}\n\nwhere \\bar{X} is the sample mean , S is the sample standard deviation and n is the sample size, \\mu_0 is the hypothesized population mean.\nThe one-sample t-test is a straightforward but powerful tool for hypothesis testing in many research scenarios.\n\nExample 1 \nA doctor measured the hemoglobin concentration in 36 male workers involved in lead-related jobs. The data can be downloaded below. The question is whether the mean hemoglobin concentration (\\mu) of male workers involved in lead-related jobs differs from the mean of 140 (g/L) for normal adult males.\n\n\n  Download data \n\nR codes for one-sample t-test is:\n\nread_csv(\"datasets/ex07-01.csv\", show_col_types = F) |&gt; \n  t.test(mu = 140) \n\n#&gt; \n#&gt;  One Sample t-test\n#&gt; \n#&gt; data:  read_csv(\"datasets/ex07-01.csv\", show_col_types = F)\n#&gt; t = -2.1367, df = 35, p-value = 0.03969\n#&gt; alternative hypothesis: true mean is not equal to 140\n#&gt; 95 percent confidence interval:\n#&gt;  122.1238 139.5428\n#&gt; sample estimates:\n#&gt; mean of x \n#&gt;  130.8333\n\n\nThe results show t = -2.1367 , \\text{p-value} = 0.03969. At the significance level of \\alpha = 0.05 , reject H_0 and accept H_1, indicating that the difference is statistically significant. In the context of this case, it can be concluded that the average hemoglobin concentration of male workers engaged in lead work is lower than the average hemoglobin concentration of normal adult males.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>t-test</span>"
    ]
  },
  {
    "objectID": "ttest.html#paired-t-test",
    "href": "ttest.html#paired-t-test",
    "title": "7  t-test",
    "section": "7.3 Paired t-test",
    "text": "7.3 Paired t-test\nThe paired t-test is used to compare the means of two related groups. This test is typically used when the observations are paired in some meaningful way, such as measurements taken on the same subjects at two different times or under two different conditions.\n\nH_0 : The mean difference between the paired observations is zero (\\mu_d = 0).\nH_1 : The mean difference between the paired observations is not zero (\\mu_d \\neq 0).\n\nThe test statistic for the paired t-test is calculated as:\nt = \\frac{\\bar{d}}{S_d / \\sqrt{n}} Where \\bar{d} is the mean of the differences between the paired observations, S_d is the standard deviation of the differences, and n is the number of pairs.\n\nExample 2 \nTo compare whether the results of fat content measurement in lactic acid beverages differ between two methods, 10 samples of lactic acid beverages were randomly selected. The fat content was measured using both the Gerber-Gottlieb method and the fatty acid hydrolysis method. The data can be downloaded below. The question is whether the measurement results from the two methods are different?\n\n\n  Download data \n\nR codes for paired t-test is:\n\nread_csv(\"datasets/ex07-02.csv\", show_col_types = F) |&gt; \n  with(t.test(x1, x2, paired = T))\n\n#&gt; \n#&gt;  Paired t-test\n#&gt; \n#&gt; data:  x1 and x2\n#&gt; t = 7.926, df = 9, p-value = 2.384e-05\n#&gt; alternative hypothesis: true mean difference is not equal to 0\n#&gt; 95 percent confidence interval:\n#&gt;  0.1946542 0.3501458\n#&gt; sample estimates:\n#&gt; mean difference \n#&gt;          0.2724\n\n\nThe results show t = -2.1367 , \\text{p-value} &lt; 0.001. At the significance level of \\alpha = 0.05, reject H_0 and accept H_1, indicating that the difference is statistically significant. It can be concluded that the two methods yield different fat content measurements. As mean difference estimation is 0.2724 &gt; 0, the Gerber-Gottlieb method providing higher results.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>t-test</span>"
    ]
  },
  {
    "objectID": "ttest.html#two-sample-t-test",
    "href": "ttest.html#two-sample-t-test",
    "title": "7  t-test",
    "section": "7.4 Two-sample t-test",
    "text": "7.4 Two-sample t-test\nThe two-sample t-test (also known as the independent t-test) is used to determine whether there is a significant difference between the means of two independent groups. It is commonly used in experiments or studies where researchers want to compare the means of two different populations or conditions.\nIn medical study it is commonly used to compare the means of two samples in a completely randomized design. In this design, subjects are randomly assigned to two different treatment groups, and the researcher is interested in whether the means of these two samples represent different population means. Additionally, in observational studies, the two-sample t-test can be used to compare the means of two samples obtained through independent random sampling from two different populations.\nWhen both samples come from normal populations, and the sample sizes are relatively small, such as n_1 \\leq 60 or/and n_2 \\leq 60 , different testing methods should be used depending on whether the population variances are equal.\n\n7.4.1 The t-test for equal population variances\nThe t-test for equal population variances, often referred to as the pooled variance t-test, is used when comparing the means of two independent samples under the assumption that the two populations have the same variance. This assumption allows the variances of the two samples to be combined (or pooled) into a single estimate, which can then be used in the t-test formula.\nBefore conducting the combined t-test, it’s common to perform an F-test to check if the variances of the two samples are equal. If the p-value from the F-test is not significant, the pooled t-test can be justified. The pooled variance is calculated as a weighted average of the variances from the two samples:\n\nS_c^2 = \\frac{(n_1 - 1)S_1^2 + (n_2 - 1)S_2^2}{n_1 + n_2 - 2}\n\nHere, S_c^2 is the combined variance, S_1^2 and S_2^2 are the variances of the two samples, and n_1 and n_2 are the sample sizes.\nOnce the combined variance is calculated, the t-statistic for the two-sample t-test can be computed as:\n\nt = \\frac{\\bar{X}_1 - \\bar{X}_2}{\\sqrt{S_c^2 \\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}}\n\n\\bar{X}_1 and \\bar{X}_2 are the sample means, and n_1 and n_2 are the sample sizes. The degrees of freedom for the pooled variance t-test are calculated as n_1 + n_2 - 2. 5.\nH_0: There is no difference between the population means (\\mu_1 = \\mu_2).\nH_1: There is a difference between the population means (\\mu_1 \\neq \\mu_2).\nIf the variances are unequal, the Welch t-test (which does not assume equal variances) is recommended.\n\nExample 3 \nTo investigate the anti-atherosclerotic effects of tetrandrine on rabbits, an atherosclerosis rabbit model was established using a high-fat diet over 12 weeks. 16 rabbits successfully modeled after 12 weeks were randomly divided into two groups. The group A was given a high-fat diet of 100g/day, while the group B was given the same high-fat diet along with an additional 109mg/(kg·d) of tetrandrine mixed into the feed. After continuous feeding for 3 weeks, blood was drawn from the heart at the end of the experiment to measure the high-density lipoprotein (mmol/L) levels in the heart blood. The data can be downloaded below. Can it be concluded that the population means of high-density lipoprotein content in the heart blood differ between group A and B?\n\n\n  Download data \n\nR codes for two-sample t-test is:\n\nread_csv(\"datasets/ex07-03.csv\", show_col_types = F) |&gt; \n  mutate(grp = factor(grp, labels = c(\"A\", \"B\"))) |&gt; \n1  t.test(x ~ grp, data = _, var.equal = T)\n\n\n1\n\nUse the pipe operator to perform t-test with a group variable and a response variable.\n\n\n\n\n#&gt; \n#&gt;  Two Sample t-test\n#&gt; \n#&gt; data:  x by grp\n#&gt; t = 3.689, df = 14, p-value = 0.00243\n#&gt; alternative hypothesis: true difference in means between group A and group B is not equal to 0\n#&gt; 95 percent confidence interval:\n#&gt;  0.05494115 0.20755885\n#&gt; sample estimates:\n#&gt; mean in group A mean in group B \n#&gt;         0.77000         0.63875\n\n\nThe results show t = 3.689 , \\text{p-value} = 0.00243. At the \\alpha = 0.05 significance level, reject H_0 and accept H_1, indicating that the difference is statistically significant. It can be concluded that the population means of high-density lipoprotein content in the heart blood are different between group A and B. Considering the sample means in this case, it can be inferred that the population mean of high-density lipoprotein content in the heart blood is higher in group A than in the model group.\n\n\n7.4.2 Welch t-test for unequal population variances\nUnlike the combined variance t-test, the Welch t-test does not assume that the two populations have the same variance. This is particularly useful when you suspect or know that the variances are different, or when the sample sizes are quite different, which makes it more flexible than the combined variance t-test.\nThe t-statistic for the Welch t-test is calculated as:\n\nt = \\frac{\\bar{X}_1 - \\bar{X}_2}{\\sqrt{\\frac{S_1^2}{n_1} + \\frac{S_2^2}{n_2}}}\n\nHere, \\bar{X}_1 and \\bar{X}_2 are the sample means, S_1^2 and S_2^2 are the variances of the two samples, and n_1 and n_2 are the sample sizes.\nThe degrees of freedom for the Welch t-test are calculated using the Welch-Satterthwaite equation:\n\n\\nu = \\frac{\\left(\\frac{S_1^2}{n_1} + \\frac{S_2^2}{n_2}\\right)^2}{\\frac{\\left(\\frac{S_1^2}{n_1}\\right)^2}{n_1 - 1} + \\frac{\\left(\\frac{S_2^2}{n_2}\\right)^2}{n_2 - 1}}\n\nThis formula typically results in a non-integer value for the degrees of freedom, which is a characteristic of the Welch t-test. It is generally considered more robust than the combined variance t-test in situations where variances differ.\n\nExample 4 \nTo analyze the effect of blood glucose control on serum total cholesterol levels, a study was conducted on type 2 diabetes patients in a community. Hemoglobin A1c levels below 7.0% were used as the target for blood glucose control. Total cholesterol levels (mmol/L) were measured in 25 patients with poor blood glucose control (Group A) and 25 patients with good blood glucose control (Group B). The data can be downloaded below. Determine whether the mean total cholesterol levels of those with poor blood glucose control and those with good blood glucose control are equal or not.\n\n\n  Download data \n\nR codes for Welch t-test is:\n\nread_csv(\"datasets/ex07-04.csv\", show_col_types = F) |&gt; \n  t.test(x ~ grp, data = _, var.equal = F)    \n\n#&gt; \n#&gt;  Welch Two Sample t-test\n#&gt; \n#&gt; data:  x by grp\n#&gt; t = -2.8322, df = 36.672, p-value = 0.007465\n#&gt; alternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n#&gt; 95 percent confidence interval:\n#&gt;  -1.4177999 -0.2350001\n#&gt; sample estimates:\n#&gt; mean in group 1 mean in group 2 \n#&gt;          4.3176          5.1440\n\n\nThe results show t = -2.8322 , \\text{p-value} = 0.007465. At the \\alpha = 0.05 significance level, reject H_0 and accept H_1, indicating that the difference is statistically significant. Therefore, we can conclude that the mean total cholesterol levels between those with poor and good blood glucose control are different.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>t-test</span>"
    ]
  },
  {
    "objectID": "ttest.html#normality-test-and-f-test",
    "href": "ttest.html#normality-test-and-f-test",
    "title": "7  t-test",
    "section": "7.5 Normality test and F-test",
    "text": "7.5 Normality test and F-test\nWhen conducting a two-sample t-test, especially when comparing the means of two small samples, it is required that the corresponding populations follow a normal distribution and that the population variances are equal, known as homogeneity of variance. For paired t-tests, it is only necessary that the distribution of the differences between each pair of data points follows a normal distribution. Therefore, when performing a two-sample t-test with small samples, it is generally advisable to first conduct a homogeneity of variance test, particularly when there is a noticeable disparity between the sample variances. If the variances are homogeneous, a standard t-test is used; if not, an approximate t-test, such as Welch t-test, is applied. Additionally, it may be necessary to perform a normality test, though normality tests are more commonly used to establish medical reference ranges using normal distribution methods.\n\n7.5.1 Normality test\nNormality test has been discussed in Section 4.3 , for more details please move there.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>t-test</span>"
    ]
  },
  {
    "objectID": "ttest.html#sec-f-test",
    "href": "ttest.html#sec-f-test",
    "title": "7  t-test",
    "section": "7.6 F-test",
    "text": "7.6 F-test\nThe F-test was traditionally used to determine whether the variances of two populations are equal. However, this test assumes that the data follow a normal distribution, which is often not the case, especially when variances are unequal. Because of this limitation, the Levene’s test has become more popular in recent years. Levene’s test is more robust and does not rely on the specific distribution of the population. Levene’s test can be used to test the homogeneity of variances for both two populations and multiple populations. Here, only the F-test for comparing the variances of two samples is introduced.\nH_0: The variances of the two populations are equal (\\sigma_1^2 = \\sigma_2^2).\nH_1: The variances of the two populations are not equal (\\sigma_1^2 \\neq \\sigma_2^2).\nThe F-statistic is calculated as the ratio of the two sample variances:\n\nF = \\frac{S_1^2}{S_2^2}\n\nwhere S_1^2 and S_2^2 are the variances of the two samples. By convention, S_1^2 should be the larger variance, so the F-statistic is always greater than or equal to 1. The F-distribution depends on two parameters: the degrees of freedom for the numerator (\\nu_1 = n_1 - 1) and the denominator (\\nu_2 = n_2 - 1).\nIf the calculated F-statistic is is close to 1, the variances are similar. If it is much larger or smaller than 1, there may be a significant difference. If the p-value is less than the chosen significance level (e.g., 0.05), reject the null hypothesis, indicating that the variances are significantly different.\nThe F-test is sensitive to non-normality. If the data does not follow a normal distribution, the test may give misleading results. In such cases, non-parametric alternatives, such as the Levene’s test, may be more appropriate.\n\ntibble(\n  F_value   = seq(0, 5, length.out = 1000),\n  y1  = df(F_value, 8, 1),\n  y2  = df(F_value, 8, Inf)\n) |&gt; \n  pivot_longer(\n    cols = contains(\"y\"),\n    names_to = \"grp\",\n    values_to = \"Density\"\n  ) |&gt; \n  ggplot(aes(x = F_value, y = Density, linetype = grp)) + \n  geom_line(linewidth = 0.5) +\n  geom_text(\n    aes(x = 0.8, y = 0.3, hjust = 0,\n        label = paste(\"df1 =\", 8, \", df2 =\", 1))) +\n  geom_text(\n    aes(x = 1, y = 0.8, hjust = 0,\n    label = paste(\"df1 =\", 8, \", df2 =\", Inf))) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nExample 5 \nFor Example 3, use the F-test to determine whether the variances of high-density lipoprotein content in the heart blood of the two populations are unequal.\n\n\nread_csv(\"datasets/ex07-03.csv\", show_col_types = F) |&gt; \n  var.test(x ~ grp, data = _)\n\n#&gt; \n#&gt;  F test to compare two variances\n#&gt; \n#&gt; data:  x by grp\n#&gt; F = 2.299, num df = 7, denom df = 7, p-value = 0.2944\n#&gt; alternative hypothesis: true ratio of variances is not equal to 1\n#&gt; 95 percent confidence interval:\n#&gt;   0.4602708 11.4833515\n#&gt; sample estimates:\n#&gt; ratio of variances \n#&gt;           2.299011\n\n\nThe results show: F = 2.299, \\text{p-value} = 0.2944, at the significant level of \\alpha = 0.1, do not reject H_0. The difference is not statistically significant. Therefore, we cannot conclude that the variances of high-density lipoprotein content in the heart blood of the Qingfujian group and the model group are unequal. Consequently, Example 3 used a two-sample t-test under the assumption of equal variances.\n\nExample 6 \nFor Example 4, Use the F test to determine whether the population variances of serum total cholesterol levels differ between those with poor and good blood glucose control.\n\n\nread_csv(\"datasets/ex07-04.csv\", show_col_types = F) |&gt; \n  var.test(x ~ grp, data = _)\n\n#&gt; \n#&gt;  F test to compare two variances\n#&gt; \n#&gt; data:  x by grp\n#&gt; F = 3.5022, num df = 24, denom df = 24, p-value = 0.0032\n#&gt; alternative hypothesis: true ratio of variances is not equal to 1\n#&gt; 95 percent confidence interval:\n#&gt;  1.543310 7.947462\n#&gt; sample estimates:\n#&gt; ratio of variances \n#&gt;           3.502199\n\n\nThe results show: F = 3.5022, \\text{p-value} = 0.0032, at the significant level of \\alpha = 0.1, reject H_0, accept H_1. The difference is statistically significant. It can be considered that the population variances of total cholesterol levels differ between those with poor and good blood glucose control. Therefore, in Example 4, the two-sample t-test for unequal variances was used.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>t-test</span>"
    ]
  },
  {
    "objectID": "anova.html",
    "href": "anova.html",
    "title": "8  Analysis of Variance",
    "section": "",
    "text": "8.1 Prerequisite\nAnalysis of variance (ANOVA) is commonly used to determine whether there are statistically significant differences between the means of three or more groups. It is particularly useful in clinical trials, observational studies, and experiments where researchers want to compare the effects of different treatments, interventions, or conditions on a continuous outcome variable.\nlibrary(tidyverse)\nlibrary(rstatix)\nlibrary(rmarkdown)\nlibrary(agricolae)\nlibrary(DescTools)",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "anova.html#the-basic-idea",
    "href": "anova.html#the-basic-idea",
    "title": "8  Analysis of Variance",
    "section": "8.2 The basic idea",
    "text": "8.2 The basic idea\nThe basic idea behind ANOVA is to analyze the variation within the data and partition it into two components: variation between groups and variation within groups. By comparing these two sources of variation, ANOVA can determine if the differences between group means are statistically significant.\n\n8.2.1 Total variance\nTotal variance in the context of ANOVA refers to the overall variability in the data. It represents the sum of the variability due to differences between group means (explained variance) and the variability within each group (unexplained variance or residual variance). The total variance can be decomposed as follows:\nThe total variance is calculated using the total sum of squares (SST), which quantifies the total variation in the data:\n\n\\text{SST} = \\sum_{i=1}^{N} (X_i - \\bar{X})^2\n\nwhere X_i is an individual observation, \\bar{X} is the overall mean of all observations, N is the total number of observations.\n\nHigh total variance: Indicates that the data points are widely spread out from the overall mean, suggesting high variability in the dataset.\nLow total variance: Indicates that the data points are closely clustered around the overall mean, suggesting low variability.\n\n\n\n8.2.2 Between-group variance\nBetween-group variance in ANOVA is a measure of how much the group means differ from the overall mean. It captures the variability that is due to the differences between the groups, rather than within them.\nThe between-group variance is calculated using the sum of squares between-groups (SSB), which is defined as:\n\n\\text{SSB} = \\sum_{j=1}^{k} n_j (\\bar{X}_j - \\bar{X})^2\n\nWhere k is the number of groups, n_j is the number of observations in group j , \\bar{X}_j is the mean of group j, \\bar{X} is the overall mean of all observations combined.\n\nHigh between-group variance: Indicates that the group means are significantly different from the overall mean, suggesting that the groups are different in a meaningful way.\nLow between-group variance: Indicates that the group means are similar to each other and to the overall mean, suggesting that the groups do not differ much.\n\n\n\n8.2.3 Within-group variance\nWithin-group variance in ANOVA measures the variability within each group. It captures the differences between individual data points and their respective group mean, indicating how spread out the data is within each group.\nThe within-group variance is calculated using the sum of squares within-groups (SSW), which is defined as:\n\n\\text{SSW} = \\sum_{j=1}^{k} \\sum_{i=1}^{n_j} (X_{ij} - \\bar{X}_j)^2\n\nWhere k is the number of groups, n_j is the number of observations in group j, X_{ij} is the i-th observation in group j, \\bar{X}_j is the mean of group j.\n\nHigh within-group variance: Indicates that there is a lot of variability within each group, meaning that individual observations differ significantly from their group mean.\nLow within-group variance: Indicates that individual observations within each group are close to their group mean, showing less variability.\n\nThe relationship between these components is given by:\n\n\\text{SST} = \\text{SSB} + \\text{SSW}\n\nIn ANOVA, the degrees of freedom associated with the within-group variance and between-group variance are essential for calculating the F-statistic. The degrees of freedom are calculated as:\n\n\\nu_{\\text{between}} = k - 1\n\n\n\\nu_{\\text{within}} = N - k\n\nWhere N is the total number of observations across all groups, k is the number of groups.\nFor example, suppose you have 3 groups with a total of 30 observations:\n\\nu_{\\text{between}} = k - 1 = 3 - 1 = 2, \\nu_{\\text{within}} = N - k = 30 - 3 = 27.\nIn the context of ANOVA, the mean square is a measure of variance and is used to compare variances between groups and within groups. It is calculated by dividing the sum of squares (SS) by the corresponding degrees of freedom.\nThe mean square between-groups (MSB) represents the average variance between the different group means. It is calculated as:\n\n\\text{MSB} = \\frac{\\text{SSB}}{\\nu_{\\text{between}}}\n\nThe mean square within-groups (MSW) represents the average variance within each group. It is calculated as:\n\n\\text{MSW} = \\frac{\\text{SSW}}{\\nu_{\\text{within}}}\n\n\n\n8.2.4 F-Statistic\nThe F-statistic is calculated by dividing the MSB by MSW:\n\nF = \\frac{\\text{MSB}}{\\text{MSW}}\n\nThe F follows an F-distribution with \\nu_{\\text{between}} and \\nu_{\\text{within}} degrees of freedom. It is used to assess the significance of the differences between group means. A larger F-value indicates that the between-group variance is relatively larger compared to the within-group variance, suggesting significant differences between the group means.\n\n\n8.2.5 Conditions for ANOVA\n\nNormality: The data within each group should be approximately normally distributed. If the data significantly deviate from normality, data transformation or non-parametric methods might be necessary.\nHomogeneity of variance: The variances across the groups should be equal, meaning the spread or variability within each group should be similar. This is a crucial assumption of ANOVA. If the variances are unequal, alternative methods like Welch’s ANOVA may be required.\nIndependence: The observations within each group and between groups should be independent of each other. This means that the measurement of one sample should not influence another.\nFixed factors: ANOVA typically assumes that the factors are fixed, meaning the levels of the factors are deliberately chosen and not randomly selected. If the factors are random, mixed-effects models or random-effects models should be considered.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "anova.html#anova-for-completely-randomized-design",
    "href": "anova.html#anova-for-completely-randomized-design",
    "title": "8  Analysis of Variance",
    "section": "8.3 ANOVA for completely randomized design",
    "text": "8.3 ANOVA for completely randomized design\nIn a completely randomized design, subjects are randomly assigned to different groups without any restrictions or blocks. Each patient has an equal chance of being assigned to any of the groups, ensuring that the groups are comparable and that the treatment effects can be attributed to the interventions rather than other factors.\n\nExample 1 \nTo clear the efficacy of three drugs, 120 patients were selected according to a inclusion criteria and evenly divided into three groups using a completely randomized design. How to use R to arrange random group assignments?\n\nYou can use the sample() function to perform random group assignments and random drug allocation. Below is an example of how to achieve this:\n\nset.seed(100)\n\nn &lt;- 120\nk &lt;- 3\ngroup &lt;- sample(rep(1:k, each = n / k))\nblind_code &lt;- tibble(\n  id = 1:n,\n  group = factor(group, labels = c(\"A\", \"B\", \"C\"))\n) \n\npaged_table(blind_code)\n\n\n  \n\n\n\nIn the code above, set.seed(220) ensures that the randomization is reproducible. The sample() function shuffles the sequence generated by rep(1:k, each = n/k) to randomly assign patients to the three groups. The final result is stored in the blind_code data frame, which includes patient IDs and their assigned group. This way, each patient is not only assigned to a specific group.\nTo ensure that the randomization has been done correctly and that each group has the correct number of patients, you can check the distribution:\n\nblind_code |&gt; \n  group_by(group) |&gt; \n  summarise(n = n())\n\n#&gt; # A tibble: 3 × 2\n#&gt;   group     n\n#&gt;   &lt;fct&gt; &lt;int&gt;\n#&gt; 1 A        40\n#&gt; 2 B        40\n#&gt; 3 C        40\n\n\n\nExample 2 \nTo study the clinical efficacy of three lipid-lowering drugs, 120 patients were selected based on uniform inclusion criteria and randomly divided into three groups using a completely randomized design. Each group received one of the drugs in a double-blind trial. The effectiveness was evaluated based on the reduction in triglyceride levels before and after 6 weeks of treatment. To analyze whether there are any differences in the average reduction in triglyceride levels among the three groups.\n\n\n  Download data \n\nImport the data into R:\n\ndf &lt;- read_csv(\"datasets/ex08-01.csv\", show_col_types = F) |&gt; \n  mutate(group = factor(group, labels = c(\"A\", \"B\", \"C\")))\n\nBefore or after ANOVA, you need to check the assumptions of normality and homogeneity of variance. Here is a method of testing normality for multiple samples simultaneously.\n\nwith(df, tapply(diff_tri, INDEX = group, FUN = shapiro.test)) \n\n#&gt; $A\n#&gt; \n#&gt;  Shapiro-Wilk normality test\n#&gt; \n#&gt; data:  X[[i]]\n#&gt; W = 0.97245, p-value = 0.4289\n#&gt; \n#&gt; \n#&gt; $B\n#&gt; \n#&gt;  Shapiro-Wilk normality test\n#&gt; \n#&gt; data:  X[[i]]\n#&gt; W = 0.95872, p-value = 0.1514\n#&gt; \n#&gt; \n#&gt; $C\n#&gt; \n#&gt;  Shapiro-Wilk normality test\n#&gt; \n#&gt; data:  X[[i]]\n#&gt; W = 0.96547, p-value = 0.2564\n\n\nAn alternative method is written below:\n\ndf |&gt; \n  group_by(group) |&gt; \n  shapiro_test(diff_tri)\n\n#&gt; # A tibble: 3 × 4\n#&gt;   group variable statistic     p\n#&gt;   &lt;fct&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 A     diff_tri     0.972 0.429\n#&gt; 2 B     diff_tri     0.959 0.151\n#&gt; 3 C     diff_tri     0.965 0.256\n\n\nThe p-values above are all greater than 0.05, it says that the three populations meet the assumption of normal distribution.\nIn Section 7.6 we have discussed the test of homogeneity of variance using F-test. F-test is suitable for two samples, but here there are three samples. In Section 8.7 , we will talk about the comparison of multiple sample variances , and come back to this example again.\nHere we use the aov() function to perform a one-way ANOVA, where diff_tri is your dependent variable and group is the independent factor.\n\ndf |&gt; \n  aov(diff_tri ~ group, data = _) |&gt; \n  summary() \n\n#&gt;              Df Sum Sq Mean Sq F value   Pr(&gt;F)    \n#&gt; group         2  13.62   6.811   12.27 1.45e-05 ***\n#&gt; Residuals   117  64.92   0.555                     \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe results indicate: \\text{F}=12.3, p &lt; 0.01. At the significance level of \\alpha = 0.05 , reject H_0 and accept H_1 , indicating that the average reduction of triglyceride levels among patients taking different drugs is not all equal. This suggests that different drugs may have different effects on triglyceride reduction.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "anova.html#anova-for-randomized-block-design",
    "href": "anova.html#anova-for-randomized-block-design",
    "title": "8  Analysis of Variance",
    "section": "8.4 ANOVA for randomized block design",
    "text": "8.4 ANOVA for randomized block design\nA randomized block design is a type of experimental design in which subjects are divided into blocks based on certain characteristics (e.g., age, gender, baseline measurements) that are expected to influence the response to the treatments. Within each block, subjects are then randomly assigned to different treatment groups. This design helps to control for the variability within blocks, thereby increasing the precision of the experiment.\nRandomized block design is particularly useful when there are sources of variation that are not of primary interest but could affect the outcome, allowing the experimenter to control for these factors and focus on the treatment effects.\n\nExample 3 \nIn a study to compare the effectiveness of three different types of pain relief medications (A, B, and C) in postoperative patients, the patients’ baseline pain levels (low, medium, high) may influence their response to the medications. So you want to control for this variable in your study. Patients are first divided into blocks based on their baseline pain levels. Then, within each block, patients are randomly assigned to one of the three pain relief medications (A, B, or C).\n\nHere’s the R code to achieve the randomized block design:\n\nLet’s assume you have 36 patients, with 12 in each block (low, medium, high baseline pain levels).\n\n\nset.seed(100)  \n\nn &lt;- 36 # sample size\ntreat &lt;- c(\"drug A\", \"drug B\", \"drug C\")\nblock &lt;- c(\"low\", \"medium\", \"high\")\nk &lt;- length(treat)  # number of treatments\nb &lt;- length(block)  # number of blocks\n\nblind_code &lt;- data.frame(\n  id = 1:n,\n  bl_pain = rep(block, each = n / k)\n)\n\nblind_code |&gt; \n  paged_table()\n\n\n  \n\n\n\n\nNow, you can assign the patients within each block to one of the three medications (A, B, or C).\n\n\n# Randomly assign medications within each block\nblind_code  |&gt; \n  group_by(bl_pain) |&gt;   \n  mutate(\n    treatment = sample(rep(treat, each = n / (k * b)))\n  ) |&gt; \n  ungroup() |&gt; \n  paged_table()\n\n\n  \n\n\n\nThe output will show each patient’s ID, their baseline pain level, and the randomly assigned medication. The assignment will be random within each block, reflecting the randomized block design.\n\nExample 4 \nIn a trial to study the effects of three prenatal nutritional supplements on newborn weight, a randomized block design was used. Pregnant women with similar living locations, ages, and similar family economic status formed 10 blocks, each containing 3 pregnant women. Within each block, Pregnant women were randomly assigned to one of the three nutritional supplements (A, B, or C). Explore whether there is a difference in newborn birth weights among the three prenatal nutritional supplements?\n\n\n  Download data \n\n\nread_csv(\"datasets/ex08-02.csv\", show_col_types = F) |&gt; \n  mutate(\n    treat = factor(treat, labels = c(\"A\", \"B\", \"C\")),\n    block = factor(block)\n  ) |&gt; aov(weight ~ treat + block, data = _) |&gt; \n  summary()\n\n#&gt;             Df  Sum Sq Mean Sq F value  Pr(&gt;F)   \n#&gt; treat        2 1032327  516163   9.506 0.00152 **\n#&gt; block        9 1530670  170074   3.132 0.01877 * \n#&gt; Residuals   18  977340   54297                   \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "anova.html#anova-for-latin-square-design",
    "href": "anova.html#anova-for-latin-square-design",
    "title": "8  Analysis of Variance",
    "section": "8.5 ANOVA for latin square design",
    "text": "8.5 ANOVA for latin square design\nA Latin square design is used to control the effects of two extraneous variables. When there are three factors, each with the same number of levels, a Latin square design can systematically arrange experimental treatments to minimize confounding between the factors. This design is particularly useful in fields such as agriculture, education, or other research areas where it’s important to control multiple external variables.\n\n8.5.1 Key points\n\nStructure: A Latin Square Design is typically represented as an n × n square grid, where each row, column, and diagonal represent different external variables or factors. Each cell within the grid represents a combination of experimental conditions.\nTreatment allocation: Each treatment appears exactly once in each row and column of the grid, ensuring that all treatments are evenly distributed across the different levels of the two controlled variables.\nControlled variables: The design controls two primary extraneous variables (represented by rows and columns), thereby reducing their potential impact on the experimental outcomes. The third factor (usually the treatment) is randomly assigned to each cell in the grid.\n\n\n# Generate a 3x3 Latin Square Design\ntreatments &lt;- c(\"A\", \"B\", \"C\", \"D\", \"E\", \"F\")\nlatin_square &lt;- design.lsd(trt = treatments, seed = 42) \n\n# Display the Latin Square Design\nlatin_square$sketch \n\n#&gt;      [,1] [,2] [,3] [,4] [,5] [,6]\n#&gt; [1,] \"E\"  \"A\"  \"D\"  \"C\"  \"F\"  \"B\" \n#&gt; [2,] \"C\"  \"E\"  \"B\"  \"A\"  \"D\"  \"F\" \n#&gt; [3,] \"F\"  \"B\"  \"E\"  \"D\"  \"A\"  \"C\" \n#&gt; [4,] \"B\"  \"D\"  \"A\"  \"F\"  \"C\"  \"E\" \n#&gt; [5,] \"A\"  \"C\"  \"F\"  \"E\"  \"B\"  \"D\" \n#&gt; [6,] \"D\"  \"F\"  \"C\"  \"B\"  \"E\"  \"A\"\n\n\nThis code generates a 6x6 Latin dquare design where each treatment (A, B, C, D, E, F) appears exactly once in each row and column.\n\nExample 5 \nTo compare the size of skin blisters caused by six different drugs (A, B, C, D, E, F) after being injected into rabbits. A Latin square design was employed, using six rabbits, with each drug being injected into six different sites on each rabbit. The task is to perform an analysis of variance (ANOVA) to determine if there are any significant differences in the blister sizes caused by these drugs.\n\n\n  Download data \n\n\nread_csv(\"datasets/ex08-03.csv\", show_col_types = F) |&gt; \n  mutate(\n    rat_id = as.factor(rat_id),\n    part   = as.factor(part),\n    treat  = factor(treat, labels = c(\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"))\n  ) |&gt; \n  aov(herpes_size ~ treat + part + rat_id, data = _) |&gt; \n  summary()\n\n#&gt;             Df Sum Sq Mean Sq F value Pr(&gt;F)  \n#&gt; treat        5  667.1  133.43   3.906 0.0124 *\n#&gt; part         5   85.5   17.09   0.500 0.7723  \n#&gt; rat_id       5  250.5   50.09   1.466 0.2447  \n#&gt; Residuals   20  683.2   34.16                 \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "anova.html#cross-over-design",
    "href": "anova.html#cross-over-design",
    "title": "8  Analysis of Variance",
    "section": "8.6 Cross-over design",
    "text": "8.6 Cross-over design\nA cross-over design is commonly used in clinical trials where participants receive a sequence of different treatments. Each participant acts as their own control, meaning they receive all the treatments under study, but in a randomized order. This design is particularly useful for comparing the effects of treatments in situations where the response to a treatment is expected to return to baseline before the next treatment is administered.\n\n8.6.1 Key features\n\nParticipants as their own control: Since each participant receives multiple treatments, variability due to individual differences is minimized.\nRandomization: The order in which participants receive the treatments is randomized to avoid bias.\nWashout period: A period of time between treatments is often included to ensure that the effects of the previous treatment do not carry over into the next one.\nBalanced design: The design is usually balanced, meaning each treatment is administered the same number of times in each sequence.\n\nFor example, consider a clinical trial comparing the effects of two drugs, Drug A and Drug B, on blood pressure. In a cross-over design, each participant would receive both drugs in a randomized order. For example, some participants might receive Drug A first, followed by a washout period, and then receive Drug B. Others would receive Drug B first, followed by Drug A after the washout period. By comparing the effects of each drug within the same participants, the design controls for individual variability.\n\n# Number of subjects\nn &lt;- 10\n\n# Generate treatment sequences\nset.seed(200)  # For reproducibility\ntreat_seq &lt;- sample(rep(c(\"A\", \"B\"), each = 5))\n\n# Create a data frame for the study\ncross_over_design &lt;- tibble(\n  subject = 1:n,\n  phase1 = treat_seq,\n  phase2 = if_else(treat_seq == \"A\", \"B\", \"A\")\n)\n\ncross_over_design |&gt; \n  paged_table()\n\n\n  \n\n\n\nThis example generates a basic 2x2 cross-over design. For more complex designs, the package can accommodate more treatments and periods.\nCross-over designs are powerful tools in clinical research, especially when the treatment effects are short-lived and the sample size is limited.\n\nExample 6 \nA cross-over trial was conducted to measure 3H-cGMP levels in plasma using two scintillation liquids, A and B. In the first phase, samples from subjects 1, 3, 4, 7, and 9 were measured using liquid A, while samples from subjects 2, 5, 6, 8, and 10 were measured using liquid B. In the second phase, the measurement methods were switched, with subjects 1, 3, 4, 7, and 9 using liquid B and subjects 2, 5, 6, 8, and 10 using liquid A. Perform an analysis of variance (ANOVA) on the results of the cross-over trial.\n\n\n  Download data \n\n\nread_csv(\"datasets/ex08-04.csv\", show_col_types = F) |&gt; \n  mutate(\n    subject = as.factor(subject),\n    treat = factor(treat, labels = c(\"A\", \"B\")),\n    phase = factor(phase, labels = c(\"I\", \"II\")),\n    .keep = \"unused\"\n  ) |&gt; \n  aov(response ~ treat + phase + subject, data = _) |&gt; \n  summary()\n\n#&gt;             Df Sum Sq Mean Sq  F value   Pr(&gt;F)    \n#&gt; treat        1    198     198    4.019   0.0799 .  \n#&gt; phase        1    490     490    9.925   0.0136 *  \n#&gt; subject      9 551111   61235 1240.195 1.32e-11 ***\n#&gt; Residuals    8    395      49                      \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "anova.html#sec-multi-sample-comp",
    "href": "anova.html#sec-multi-sample-comp",
    "title": "8  Analysis of Variance",
    "section": "8.7 Comparison of multiple sample means",
    "text": "8.7 Comparison of multiple sample means\nAfter conducting an ANOVA and finding a significant difference between groups, you often need to pinpoint which groups differ. This is often done using post-hoc tests for multiple comparison. Multiple comparison tests control for type I error across the comparisons, meaning they adjust the significance level to prevent too many false positives.\nHere are some of the most common post-hoc tests:\n\nLeast Significant Difference (LSD) test: The LSD test is one of the oldest and simplest post-hoc tests, and it is essentially a series of t-tests between pairs of group means.\nTukey’s honest significant difference (HSD) test: Compares all possible pairs of means and is particularly effective when group sizes are equal.\nStudent-Newman-Keuls (SNK) test: It is derived from Tukey’s HSD, but is less conservative (finds more differences). Tukey controls the error for all comparisons, where SNK only controls for comparisons under consideration. This makes it more powerful but also more prone to type I error.\nDunnett’s test: Compares each treatment group to a control group, useful when you have a control group and several treatments.\nBonferroni correction: Adjusts the significance level for the number of comparisons being made. It’s conservative and controls for type I error. Useful when you have a small number of comparisons.\nScheffé’s Method: Allows for all possible contrasts, not just pairwise comparisons, making it a flexible but conservative choice.\n\n\n8.7.1 When to use which\nChoosing the right post-hoc test after an ANOVA depends on several factors, including your study design, the number of groups, your tolerance for type I error (false positive), and the nature of your data. Here’s a guide to help you decide when to select which post-hoc test:\n\nUse Tukey’s HSD if you need a balanced approach between controlling type I error and maintaining power, especially for all-pairwise comparisons.\nUse SNK if you want more power and are okay with slightly higher type I error risk.\nUse LSD if your primary goal is to detect differences, particularly in an exploratory setting, and you are less concerned about type I error.\nUse Bonferroni if you have a small number of comparisons and need strong control over type I error.\nUse Dunnett’s if your comparisons are focused on multiple treatments against a single control group.\n\n\nExample 7 \nIn Example 1, after ANOVA rejected the null hypothesis, we still want to know whihc pairs of drugs differ in the mean reduction of triglycerides?\n\nThe PostHocTest() function in DescTools package is a convenience wrapper for computing post-hoc test after having calculated an ANOVA.\n\ndf &lt;- read_csv(\"datasets/ex08-01.csv\", show_col_types = F) |&gt; \n  mutate(group = factor(group, labels = c(\"A\", \"B\", \"C\")))\n\naov_model &lt;- aov(diff_tri ~ group, data = df)\naov_model |&gt; summary()\n\n#&gt;              Df Sum Sq Mean Sq F value   Pr(&gt;F)    \n#&gt; group         2  13.62   6.811   12.27 1.45e-05 ***\n#&gt; Residuals   117  64.92   0.555                     \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\np&lt;0.05 indicates the difference between groups is significant. The multiple comparison between groups is conducted using different methods.\n\nc(\"hsd\", \"bonferroni\", \"lsd\", \"scheffe\", \"newmankeuls\") |&gt; \n  map(~ PostHocTest(aov_model, method = .)) \n\n#&gt; [[1]]\n#&gt; \n#&gt;   Posthoc multiple comparisons of means : Tukey HSD \n#&gt;     95% family-wise confidence level\n#&gt; \n#&gt; $group\n#&gt;         diff     lwr.ci     upr.ci    pval    \n#&gt; B-A -0.09200 -0.4874133  0.3034133 0.84549    \n#&gt; C-A -0.75625 -1.1516633 -0.3608367 4.1e-05 ***\n#&gt; C-B -0.66425 -1.0596633 -0.2688367 0.00034 ***\n#&gt; \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; \n#&gt; [[2]]\n#&gt; \n#&gt;   Posthoc multiple comparisons of means : Bonferroni \n#&gt;     95% family-wise confidence level\n#&gt; \n#&gt; $group\n#&gt;         diff     lwr.ci     upr.ci    pval    \n#&gt; B-A -0.09200 -0.4965707  0.3125707 1.00000    \n#&gt; C-A -0.75625 -1.1608207 -0.3516793 4.1e-05 ***\n#&gt; C-B -0.66425 -1.0688207 -0.2596793 0.00035 ***\n#&gt; \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; \n#&gt; [[3]]\n#&gt; \n#&gt;   Posthoc multiple comparisons of means : Fisher LSD \n#&gt;     95% family-wise confidence level\n#&gt; \n#&gt; $group\n#&gt;         diff     lwr.ci     upr.ci    pval    \n#&gt; B-A -0.09200 -0.4218756  0.2378756 0.58177    \n#&gt; C-A -0.75625 -1.0861256 -0.4263744 1.4e-05 ***\n#&gt; C-B -0.66425 -0.9941256 -0.3343744 0.00012 ***\n#&gt; \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; \n#&gt; [[4]]\n#&gt; \n#&gt;   Posthoc multiple comparisons of means: Scheffe Test \n#&gt;     95% family-wise confidence level\n#&gt; \n#&gt; $group\n#&gt;         diff     lwr.ci     upr.ci    pval    \n#&gt; B-A -0.09200 -0.5049876  0.3209876 0.85870    \n#&gt; C-A -0.75625 -1.1692376 -0.3432624 7.5e-05 ***\n#&gt; C-B -0.66425 -1.0772376 -0.2512624 0.00058 ***\n#&gt; \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; \n#&gt; [[5]]\n#&gt; \n#&gt;   Posthoc multiple comparisons of means : Newman-Keuls \n#&gt;     95% family-wise confidence level\n#&gt; \n#&gt; $group\n#&gt;         diff     lwr.ci     upr.ci    pval    \n#&gt; B-A -0.09200 -0.4218756  0.2378756 0.58177    \n#&gt; C-A -0.75625 -1.1516633 -0.3608367 4.1e-05 ***\n#&gt; C-B -0.66425 -0.9941256 -0.3343744 0.00012 ***\n#&gt; \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe results of different post-hoc tests are consistent. There is no difference between drug A and drug B, only the overall mean of glycerol reduction between drug C and drug A, drug C and drug B have statistical significance, and the glycerol reduction of drug C is less.\nHere is the Dunnett’s test, which can assign the control group.\n\nDunnettTest(diff_tri ~ group, data = df, control = \"B\")\n\n#&gt; \n#&gt;   Dunnett's test for comparing several treatments with a control :  \n#&gt;     95% family-wise confidence level\n#&gt; \n#&gt; $B\n#&gt;         diff    lwr.ci    upr.ci    pval    \n#&gt; A-B  0.09200 -0.280971  0.464971 0.80343    \n#&gt; C-B -0.66425 -1.037221 -0.291279 0.00023 ***\n#&gt; \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "anova.html#comparison-of-multiple-sample-variances",
    "href": "anova.html#comparison-of-multiple-sample-variances",
    "title": "8  Analysis of Variance",
    "section": "8.8 Comparison of multiple sample variances",
    "text": "8.8 Comparison of multiple sample variances\nIn statistical analysis, comparing multiple sample variances is important to determine whether the variances across different groups are equal. This is often referred to as testing for homogeneity of variances. There are several common statistical tests for this purpose:\n\nExample 8 \nFor data in Example 1, whether the reduction of triglyceride levels in three groups satisfies the assumption of homogeneity of variances?\n\n\n8.8.1 Bartlett’s Test\nBartlett’s test checks whether multiple sample variances are equal under the assumption that the data follow a normal distribution. It is sensitive to departures from normality, so it may not be appropriate for non-normal data.\nHere is the R code for Bartlett’s test:\n\nbartlett.test(diff_tri ~ group, data = df) \n\n#&gt; \n#&gt;  Bartlett test of homogeneity of variances\n#&gt; \n#&gt; data:  diff_tri by group\n#&gt; Bartlett's K-squared = 2.2184, df = 2, p-value = 0.3298\n\n\n\n\n8.8.2 Levene’s Test\nLevene’s test is a more robust alternative to Bartlett’s test. It tests for the equality of variances and is less sensitive to deviations from normality, making it suitable for non-normally distributed data.\nHere is the R code for this Levene’s test:\n\nLeveneTest(diff_tri ~ group, data = df)\n\n#&gt; Levene's Test for Homogeneity of Variance (center = median)\n#&gt;        Df F value Pr(&gt;F)\n#&gt; group   2  1.8226 0.1662\n#&gt;       117\n\n\n\n\n8.8.3 Fligner-Killeen Test\nThe Fligner-Killeen test is a non-parametric test that is robust against non-normal data. It uses ranks of the data to test for equality of variances across groups.\nHere is the R code for Fligner-Killeen test:\n\nfligner.test(diff_tri ~ group, data = df)\n\n#&gt; \n#&gt;  Fligner-Killeen test of homogeneity of variances\n#&gt; \n#&gt; data:  diff_tri by group\n#&gt; Fligner-Killeen:med chi-squared = 3.8501, df = 2, p-value = 0.1459",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "count-data-statdesc.html",
    "href": "count-data-statdesc.html",
    "title": "9  Statistical description of count data",
    "section": "",
    "text": "9.1 Prerequisite\nCount data refers to data that represent the number of occurrences of an event or characteristic, typically as non-negative integer values. It often represent the number of occurrences of specific health-related events, such as the number of hospital visits, the number of disease occurrences, or the number of adverse events. This type of data is widely used in epidemiological studies, clinical trials, and health services research.\nlibrary(tidyverse)\nAbsolute values and relative values are fundamental concepts in statistics, used to describe raw data and the relationships between data points, respectively.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Statistical description of count data</span>"
    ]
  },
  {
    "objectID": "count-data-statdesc.html#absolute-value",
    "href": "count-data-statdesc.html#absolute-value",
    "title": "9  Statistical description of count data",
    "section": "9.2 Absolute value",
    "text": "9.2 Absolute value\nAn absolute value refers to the raw, observed data that quantifies an event or phenomenon directly, it focus on the actual quantity of an event or measurement. For example:\n\nA hospital received 500 patients in one year.\nIn a clinical trial, there were 20 occurrences of heart attacks.\n\n\n9.2.1 Characteristics\n\nAbsolute values are often expressed with specific units, such as people, events, kilograms, etc.\nAbsolute values represent the measured quantity without any relation to other data points, no comparison involved.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Statistical description of count data</span>"
    ]
  },
  {
    "objectID": "count-data-statdesc.html#relative-value",
    "href": "count-data-statdesc.html#relative-value",
    "title": "9  Statistical description of count data",
    "section": "9.3 Relative Value",
    "text": "9.3 Relative Value\nA relative value expresses the relationship or proportion between two or more data points, often in the form of percentages, ratios, or multiples. For example:\n\nThe hospital treated 5% of the total population of the region in one year.\nThe relative risk of heart attacks in the drug group is 1.5 times that of the control group.\n\n\n9.3.1 Common forms\n\nPercentage: Expressed as a proportion of a part to the whole. For instance, 20% of patients in a clinical trial experienced side effects.\n\n\n\\text{Percentage} = \\left(\\frac{\\text{part}}{\\text{whole}}\\right) \\times 100%\n\n\nRate: This expresses the frequency of an event occurring in relation to a specific quantity of time, population, or other conditions. It is commonly used to measure the occurrence of phenomena over time or within a particular population in fields like epidemiology, demography, and clinical research.\n\nIncidence rate: The number of new cases of a disease occurring in a population during a specific time period, typically expressed as per 1,000 or 100,000 people.\n\n\n\\text{Incidence rate} = \\frac{\\text{Number of new cases}}{\\text{Total population}} \\times 1,000 \\text{ (or 100,000)}\n\n\nMortality Rate: The number of deaths in a given population over a specific time, often per 1,000 individuals.\n\n\n\\text{Mortality rate} = \\frac{\\text{Number of deaths}}{\\text{Total population}} \\times 1,000\n\n\nBirth Rate: The number of live births per 1,000 people in a population in a given time period.\n\n\n\\text{Birth rate} = \\frac{\\text{Number of live births}}{\\text{Total population}} \\times 1,000\n\n\nSurvival Rate: The proportion of people who survive a medical condition or treatment for a specific period of time.\n\nFor example, if a study finds that 50 new cases of diabetes occur in a population of 10,000 people during one year, the incidence rate would be:\n\\frac{50}{10,000} \\times 1,000 = 5 \\text{ new cases per 1,000 people per year}\nRatio: A comparison of two quantities, representing the relative frequency or strength of two phenomena. For example, the ratio of male to female visits to a clinic is 3:2.\n\n\n\\text{Ratio} = \\frac{\\text{value A}}{\\text{value B}}\n\n\nMultiple: Expresses how many times one quantity is greater or smaller than another. For example, the effectiveness of a new drug might be twice that of the standard treatment.\n\n\n\\text{Multiple} = \\frac{\\text{effectiveness of new drug}}{\\text{effectiveness of standard treatment}}\n\nRelative values are more useful for comparisons, such as comparing disease incidence rates, relative risks of a condition between different groups, or the effectiveness of different treatments. For example:\n\nAbsolute value: In a study, 150 patients received a specific treatment.\nRelative value: The treatment group had an 80% success rate, while the control group had a 50% success rate, giving a relative risk of 1.6.\n\n\n\n9.3.2 Rate and proportion\nRate includes a time element (or a denominator such as population size), indicating how frequently an event happens over time. Proportion is simply a part-to-whole comparison and does not necessarily involve time.\nRelative values compare events or measurements, highlighting the proportional or relative differences between groups or phenomena.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Statistical description of count data</span>"
    ]
  },
  {
    "objectID": "discrete-data-dist.html",
    "href": "discrete-data-dist.html",
    "title": "10  Distributions of discrete variables",
    "section": "",
    "text": "10.1 Binomial distribution\nDiscrete variables take on a finite or countable set of values, and their distribution can be described by the frequency or probability of each possible value. There are several common probability distributions used to model discrete variables. These distributions depend on the nature of the data and the context in which the events occur, which help to model the probability of different outcomes for events that occur in fixed trials or over time and space.\nThe binomial distribution is a discrete probability distribution that describes the number of successes in a fixed number of independent trials, where each trial has only two possible outcomes (often referred to as success or failure), and the probability of success remains constant in each trial.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Distributions of discrete variables</span>"
    ]
  },
  {
    "objectID": "discrete-data-dist.html#binomial-distribution",
    "href": "discrete-data-dist.html#binomial-distribution",
    "title": "10  Distributions of discrete variables",
    "section": "",
    "text": "10.1.1 Key Characteristics\n\nNumber of trials (n): The distribution considers a fixed number of trials, denoted by n.\nProbability of success (p): Each trial has the same probability of success, denoted by p. The probability of failure is 1 - p.\nIndependent trials: The outcome of one trial does not affect the outcome of any other trial.\n\n\n\n10.1.2 Probability mass function (PMF)\nThe probability of getting exactly k successes in n trials is given by the binomial probability formula:\n\nP(X = k) = \\binom{n}{k} p^k (1-p)^{n-k}\n\nWhere X is the random variable representing the number of successes, \\binom{n}{k} is the binomial coefficient, calculated as \\frac{n!}{k!(n-k)!}, p is the probability of success, k is the number of successes, where k = 0, 1, 2, \\ldots, n .\n\n\n10.1.3 Mean and standard deviation\nIn a binomial distribution, the mean (or expected value) and standard deviation of the number of success X is given by: mean (expected value) \\mu = np; standard deviation \\sigma = \\sqrt{n \\cdot p \\cdot (1 - p)}\nwhere n is the number of trials, p is the probability of success in each trial, 1 - p is the probability of failure in each trial.\nIn R, you can calculate the mean and standard deviation using the following code:\n\nn &lt;- 10  # Number of trials\np &lt;- 0.7  # Probability of success\n\n# Calculate mean\nmean &lt;- n * p\nprint(paste(\"Mean:\", mean))\n\n#&gt; [1] \"Mean: 7\"\n\n# Calculate standard deviation\nstd_dev &lt;- sqrt(n * p * (1 - p))\nprint(paste(\"Standard deviation:\", std_dev))\n\n#&gt; [1] \"Standard deviation: 1.44913767461894\"\n\n\n\nExample 1 \nSuppose you are conducting a clinical trial for a new medication. Each patient has a 70% chance of responding positively to the treatment. If you treat 10 patients, the binomial distribution can be used to model the number of positive responses.\n\nHere n is 10 patients, p is 0.70. You can calculate the probability of exactly 7 patients responding positively:\n\nP(X = 7) = \\binom{10}{7} (0.7)^7 (0.3)^3 = 0.2668\n\nSo, there’s a 26.68% chance that exactly 7 patients will respond positively. You can calculate binomial probabilities using the dbinom() function:\n\n# Probability of exactly 7 successes in 10 trials with p = 0.7\ndbinom(7, size = 10, prob = 0.7)\n\n#&gt; [1] 0.2668279\n\n\nFor the binomial distribution, the mean and standard deviation of the proportion of successes \\hat{p} are as follows: mean of \\hat{p} is \\mu_{\\hat{p}} = p, standard deviation of \\hat{p} (also called standard error) is:\n\\sigma_{\\hat{p}} = \\sqrt{\\frac{p(1 - p)}{n}}\nThe mean of the proportion of successes is the true success probability p. This standard deviation decreases as the number of trials n increases, which means that the proportion of successes becomes more stable with a larger sample size.\n\n\n10.1.4 Visualization\nYou can also plot the binomial distribution. The plot typically uses a bar chart to show the probability for each possible number of successes.\n\n# Define parameters\nn &lt;- 10  # Number of trials\np &lt;- 0.7  # Probability of success\n\n# Generate data\nx &lt;- 0:n\nprobabilities &lt;- dbinom(x, size = n, prob = p)\ndata &lt;- tibble(successes = x, probability = probabilities)\n\n# Create the plot\nggplot(data, aes(x = successes, y = probability)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\", color = \"black\") +\n  labs(x = \"Number of successes\", y = \"Probability\")\n\n\n\n\n\n\n\n\nWith n = 10 and p = 0.7, the generated plot will show the probability distribution for achieving different numbers of successes in 10 trials. When p = 0.7, the distribution typically peaks around 7 successes, meaning the most likely number of successes is near 9.\nThe shape of the distribution depends on the number of trials n and the probability of success p. When p = 0.5, the distribution is symmetric; when p is far from 0.5, the distribution becomes skewed. When n \\to \\infty , as long as p is not too close to 0 or 1, the binomial distribution approximates a normal distribution.\n\nn1 &lt;- 5  \nn2 &lt;- 10\np &lt;- 0.5\n\n# Generate data\nx1 &lt;- 0:n1\nx2 &lt;- 0:n2\nprob1 &lt;- dbinom(x1, size = n1, prob = p)\nprob2 &lt;- dbinom(x2, size = n2, prob = p)\ndf1 &lt;- tibble(successes = x1, probability = prob1)\ndf2 &lt;- tibble(successes = x2, probability = prob2)\n\n# Create the plot\nggplot(df1, aes(x = successes, y = probability)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\", color = \"black\") +\n  labs(x = \"Number of successes\", y = \"Probability\") +\nggplot(df2, aes(x = successes, y = probability)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\", color = \"black\") +\n  labs(x = \"Number of successes\", y = \"Probability\")\n\n\n\n\n\n\n\n\n\nn1 &lt;- 10  \nn2 &lt;- 50\np &lt;- 0.3\n\n# Generate data\nx1 &lt;- 0:n1\nx2 &lt;- 0:n2\nprob1 &lt;- dbinom(x1, size = n1, prob = p)\nprob2 &lt;- dbinom(x2, size = n2, prob = p)\ndf1 &lt;- tibble(successes = x1, probability = prob1)\ndf2 &lt;- tibble(successes = x2, probability = prob2)\n\n# Create the plot\nggplot(df1, aes(x = successes, y = probability)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\", color = \"black\") +\n  labs(x = \"Number of successes\", y = \"Probability\") +\nggplot(df2, aes(x = successes, y = probability)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\", color = \"black\") +\n  labs(x = \"Number of successes\", y = \"Probability\")\n\n\n\n\n\n\n\n\n\n\n10.1.5 Applications\nThe binomial distribution is widely applied in scenarios where there are only two possible outcomes (often referred to as “success” and “failure”) in repeated trials or experiments. Each trial is independent, and the probability of success remains constant across trials. Below are some key applications of the binomial distribution:\n\nConfidence interval (CI) estimation\nThe CI for a population proportion (or population rate) is a range of values that is likely to contain the true proportion p of a population, which can be estimated based on a sample proportion \\hat{p} = \\frac{X}{n}, where X is the number of successes and n is the sample size.\nThe Wald method is the traditional approach to estimate the confidence interval for a population proportion. The formula is:\n\\hat{p} \\pm Z_{\\alpha/2} \\times \\sqrt{\\frac{\\hat{p}(1 - \\hat{p})}{n}}\nWhere \\hat{p} is the sample proportion (i.e., the number of successes divided by the total sample size), Z_{\\alpha/2} is the critical value from the standard normal distribution for a given confidence level (e.g., 1.96 for 95% confidence), n is the sample size, \\sqrt{\\frac{\\hat{p}(1 - \\hat{p})}{n}} is the standard error of the proportion.\n\nExample 2 \nSuppose you take a sample of 200 people, and 50 people have a positive result. To calculate a 95% confidence interval for the population rate of positive.\n\nIf you want to manually calculate the Wald CI, you can write your own R code, for example:\n\n# Example: 50 successes in 200 trials\nx &lt;- 50  # Number of successes\nn &lt;- 200  # Total number of trials\np_hat &lt;- x / n  # Sample proportion\n\n# Z-value for 95% confidence\nz &lt;- 1.96\n\n# Standard error\nse &lt;- sqrt(p_hat * (1 - p_hat) / n)\n\n# Confidence interval\nlower &lt;- p_hat - z * se\nupper &lt;- p_hat + z * se\n\n# Output the result\ncat(\"95% CI: [\", lower, \",\", upper, \"]\\n\")\n\n#&gt; 95% CI: [ 0.1899875 , 0.3100125 ]\n\n\nThe binconf() function from the Hmisc package allows you to compute confidence intervals for proportions using several methods, including Wald, Wilson, and Agresti-Coull.\n\nbinconf(x = 50, n = 200, alpha = 0.05, method = \"asymptotic\")  \n\n#&gt;  PointEst     Lower     Upper\n#&gt;      0.25 0.1899886 0.3100114\n\n\nFor small samples or when \\hat{p} is close to 0 or 1, the Wald method might not perform well. Other methods, such as the Wilson score interval, Agresti-Coull interval, or Bayesian intervals, are often preferred in these cases.\nWilson Score Interval:\n\n\n\\frac{\\hat{p} + \\frac{Z^2}{2n}}{1 + \\frac{Z^2}{n}} \\pm \\frac{Z}{1 + \\frac{Z^2}{n}} \\sqrt{\\frac{\\hat{p}(1 - \\hat{p})}{n} + \\frac{Z^2}{4n^2}}\n\n\nOne-sample proportion test\nThis test determines whether the sample proportion is significantly different from the known population proportion. The test statistic for this comparison is based on the standard error of the sample proportion, which is given by:\nSE = \\sqrt{\\frac{p_0(1 - p_0)}{n}}where p_0 is the known population proportion, and n is the sample size.\nThe test statistic Z is calculated as:\n\nZ = \\frac{\\hat{p} - p_0}{SE}\n\n\nExample 3 \nSuppose each patient has a 70% chance of responding to a treatment. In a clinical trial, 13 out of a set of 20 patients taking this treatment have positive responses. Determine if there is a significant difference between the sample and population proportion.\n\nYou can perform the one-sample proportion test using the prop.test() function or binom.test() function in R:\n\n# Example: 13 successes in 20 trials, comparing to a known population proportion of 0.370\nx &lt;- 13  # Number of successes\nn &lt;- 20  # Sample size\np0 &lt;- 0.70  # Known population proportion\n\n# Perform one-sample proportion test\nprop.test(x = x, n = n, p = p0)\n\n#&gt; \n#&gt;  1-sample proportions test with continuity correction\n#&gt; \n#&gt; data:  x out of n, null probability p0\n#&gt; X-squared = 0.059524, df = 1, p-value = 0.8073\n#&gt; alternative hypothesis: true p is not equal to 0.7\n#&gt; 95 percent confidence interval:\n#&gt;  0.4094896 0.8369133\n#&gt; sample estimates:\n#&gt;    p \n#&gt; 0.65\n\n\n\nbinom.test(x = x, n = n, p = p0)\n\n#&gt; \n#&gt;  Exact binomial test\n#&gt; \n#&gt; data:  x and n\n#&gt; number of successes = 13, number of trials = 20, p-value = 0.6295\n#&gt; alternative hypothesis: true probability of success is not equal to 0.7\n#&gt; 95 percent confidence interval:\n#&gt;  0.4078115 0.8460908\n#&gt; sample estimates:\n#&gt; probability of success \n#&gt;                   0.65\n\n\nTwo-sample proportion test\nThis test determines whether there is a significant difference between the proportions of two independent samples. The test statistic is based on the difference between the sample proportions of the two groups:\n\nZ = \\frac{\\hat{p}_1 - \\hat{p}_2}{SE}\n\nWhere \\hat{p}_1 and \\hat{p}_2 are the sample proportions from the two groups, and SE is the standard error of the difference in proportions, calculated as:\n\nSE = \\sqrt{\\hat{p}(1 - \\hat{p}) \\left( \\frac{1}{n_1} + \\frac{1}{n_2} \\right)}\n\nHere, \\hat{p} is the pooled sample proportion:\n\n\\hat{p} = \\frac{X_1 + X_2}{n_1 + n_2}\n\nwhere X_1 and X_2 are the number of successes in the two groups, and n_1 and n_2 are the sample sizes of the two groups.\n\nExample 4 \nSuppose you have two independent groups: Group A 30 successes out of 150 trials; Group B 45 successes out of 200 trials. Perform the two-sample proportion test to check if the proportions in these two groups differ significantly.\n\n\nx &lt;- c(30, 45)  # Number of successes in each group\nn &lt;- c(150, 200)  # Sample sizes of each group\n\n# Perform two-sample proportion test\nprop.test(x = x, n = n)\n\n#&gt; \n#&gt;  2-sample test for equality of proportions with continuity correction\n#&gt; \n#&gt; data:  x out of n\n#&gt; X-squared = 0.18702, df = 1, p-value = 0.6654\n#&gt; alternative hypothesis: two.sided\n#&gt; 95 percent confidence interval:\n#&gt;  -0.11712834  0.06712834\n#&gt; sample estimates:\n#&gt; prop 1 prop 2 \n#&gt;  0.200  0.225",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Distributions of discrete variables</span>"
    ]
  },
  {
    "objectID": "discrete-data-dist.html#poisson-distribution",
    "href": "discrete-data-dist.html#poisson-distribution",
    "title": "10  Distributions of discrete variables",
    "section": "10.2 Poisson distribution",
    "text": "10.2 Poisson distribution\nThe Poisson distribution is a discrete probability distribution that expresses the probability of a given number of events occurring in a fixed interval of time or space, given that these events occur with a known constant rate and independently of the time since the last event. For examples, the number of phone calls received by a call center in an hour, the number of hospital admissions per day.\n\n10.2.1 Key characteristics\n\nEvents are independent: The occurrence of one event does not affect the probability of another event.\nThe rate is constant: The average number of events in a fixed interval is constant.\nRare events: The events are typically rare relative to the observation period or space.\n\n\n\n10.2.2 Probability mass function (PMF)\nThe probability of observing k events in a given interval is:\n\nP(X = k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}\n\nWhere X is the random variable representing the number of events, k is the number of events (a non-negative integer), \\lambda is the average number of events (rate parameter), e is the base of the natural logarithm.\n\n\n10.2.3 Mean and standard deviation\nIn a Poisson distribution, mean \\mu = \\lambda, variance \\sigma = \\sqrt\\lambda. This means that the mean and variance of the Poisson distribution are both equal to the rate parameter \\lambda.\n\nExample 5 \nIf the average number of patients arriving at a hospital per hour is 50, you could use a Poisson distribution with \\lambda = 50 to model the number of patients expected to arrive in any given hour.\n\nYou can use the rpois() function in R to generate random values from a Poisson distribution.\n\nset.seed(200)\n# Simulate the number of patients arriving in 8 hours from a Poisson distribution with a rate of 5\nlambda &lt;- 50\nk &lt;- 8\nrpois(n = k, lambda = lambda)\n\n#&gt; [1] 50 51 53 53 50 49 42 46\n\n\nYou can use the dpois() function to calculate the probability of a specific number of events.\n\n# Probability of observing exactly 5 events when lambda is 5\ndpois(x = k, lambda = lambda)\n\n#&gt; [1] 1.868596e-13\n\n\n\n\n10.2.4 Visualization\nYou can plot the Poisson distribution. Here’s an example where we plot the probability mass function (PMF) of the Poisson distribution for different values of k (the number of events), given a specific rate \\lambda.\n\nlambda &lt;- 5\nk_values &lt;- 0:5  # Values from 0 to 10 events\nprobabilities &lt;- dpois(k_values, lambda = lambda)\npoisson_data &lt;- tibble(k_values, probabilities)\n\nggplot(poisson_data, aes(x = k_values, y = probabilities)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\", color = \"black\") +\n  labs(x = \"Number of events (k)\", y = \"Probability\")\n\n\n\n\n\n\n\n\nThe resulting plot will show the probability mass function for the Poisson distribution with \\lambda = 5 . It will depict how likely different counts of events (0, 1, 2, 3, etc.) are.\nThe shape of the Poisson distribution depends on the parameter \\lambda, which represents the average number of events in a given time or space interval. As \\lambda changes, the shape of the distribution also changes in the following ways:\n\nFor very small values of \\lambda (e.g., close to 0), the distribution is skewed to the right. Most of the probability mass is concentrated around 0, with a sharp drop-off for higher values of k. The distribution is unimodal with its peak near k = 0 .2.\nAs \\lambda increases, the distribution becomes more symmetric. The peak of the distribution shifts to the right, centering around \\lambda, since the expected number of events increases. The skewness reduces as \\lambda increases.\nWhen \\lambda becomes large (e.g., \\lambda &gt; 10), the distribution approximates a normal distribution. The distribution becomes nearly symmetric and bell-shaped, with the mean and variance both close to \\lambda.\n\n\nlambda1 &lt;- 0.5\nlambda2 &lt;- 5\nk_values &lt;- 0:10  # Values from 0 to 10 events\nprob1 &lt;- dpois(k_values, lambda = lambda1)\nprob2 &lt;- dpois(k_values, lambda = lambda2)\ndf1 &lt;- tibble(k_values, probabilities = prob1)\ndf2 &lt;- tibble(k_values, probabilities = prob2)\n\nggplot(df1, aes(x = k_values, y = probabilities)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\", color = \"black\") +\n  labs(x = \"Number of events (k)\", y = \"Probability\") +\nggplot(df2, aes(x = k_values, y = probabilities)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\", color = \"black\") +\n  labs(x = \"Number of events (k)\", y = \"Probability\")\n\n\n\n\n\n\n\n\n\n\n10.2.5 Applications\nThe poisson.test() function is used to perform an exact test for the rate parameter \\lambda of a Poisson distribution. This function helps compare the observed count data (events) to an expected rate or two Poisson-distributed samples to check whether the observed events match a hypothesized or known rate.\n\nOne-sample comparison\nTo test whether the observed number of events follows a hypothesized Poisson rate \\lambda.\n\nExample 6 \nAn emergency room wants to determine if the average number of patients arriving per hour differs from a known average rate. Suppose the historical data shows an average of 8 patients arriving per hour. They observed 17 patients arriving in 2 hours.Test if the observed rates differ significantly from the historical average of 8 patients per hour.\n\n\n# Observed data\nx &lt;- 17  # Number of patients in the second sample\nt &lt;- 2   # Time period in hours\n# Hypothesized rate (historical average)\nlambda_0 &lt;- 8\n\npoisson.test(x, T = t, r = lambda_0) \n\n#&gt; \n#&gt;  Exact Poisson test\n#&gt; \n#&gt; data:  x time base: t\n#&gt; number of events = 17, time base = 2, p-value = 0.8016\n#&gt; alternative hypothesis: true event rate is not equal to 8\n#&gt; 95 percent confidence interval:\n#&gt;   4.951563 13.609323\n#&gt; sample estimates:\n#&gt; event rate \n#&gt;        8.5\n\n\nTwo-sample comparison\n\nTo test whether the rates from two Poisson-distributed samples are equal.\n\nExample 7 \nA researcher is studying the incidence of a rare disease in two different regions to see if the rate of new cases per year is the same. The researcher wants to compare the rates of the disease in two regions. 15 new cases of the disease were observed in 2 years in region A. 4 new cases of the disease were observed in 2 years in region B. Test if the disease incidence rates in the two regions are significantly different.\n\n\n# Observed data\nx1 &lt;- 15  # New cases in region A\nT1 &lt;- 2  # Number of years in region A\n\nx2 &lt;- 4  # New cases in region B\nT2 &lt;- 2  # Number of years in region B\n\npoisson.test(c(x1, x2), T = c(T1, T2))\n\n#&gt; \n#&gt;  Comparison of Poisson rates\n#&gt; \n#&gt; data:  c(x1, x2) time base: c(T1, T2)\n#&gt; count1 = 15, expected count1 = 9.5, p-value = 0.01921\n#&gt; alternative hypothesis: true rate ratio is not equal to 1\n#&gt; 95 percent confidence interval:\n#&gt;   1.194652 15.522225\n#&gt; sample estimates:\n#&gt; rate ratio \n#&gt;       3.75",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Distributions of discrete variables</span>"
    ]
  },
  {
    "objectID": "discrete-data-dist.html#negative-binomial-distribution",
    "href": "discrete-data-dist.html#negative-binomial-distribution",
    "title": "10  Distributions of discrete variables",
    "section": "10.3 Negative binomial distribution",
    "text": "10.3 Negative binomial distribution\nThe Negative Binomial distribution is a probability distribution used to model the number of trials required to achieve a certain number of successes in a sequence of independent and identically distributed Bernoulli trials. Unlike the binomial distribution, which counts the number of successes in a fixed number of trials, the negative binomial distribution counts the number of failures before achieving a fixed number of successes.\nFor Eexample, the number of patients treated before achieving 5 successful recoveries; The number of hospital visits until a patient recovers (achieves a fixed number of successful treatments).;The number of failures (e.g., adverse events or complications) before a certain number of successful outcomes (e.g., disease remission).\n\n10.3.1 Key characteristics\n\nParameter r: The number of successes we want to achieve.\nParameter p : The probability of success on each trial.\nSupport: The negative binomial distribution models the number of failures before the r-th success occurs.\n\n\n\n10.3.2 Probability mass function (PMF)\nThe probability that there are k failures before the r-th success is given by:\n\nP(X = k) = \\binom{k + r - 1}{k} p^r (1 - p)^k\n\nwhere X is the number of failures, r is the number of successes, and p is the probability of success.\n\n\n10.3.3 Mean and standard deviation\nFor negative binomial distribution, the mean and standard deviation are calculated using the number of successes r and the probability of success p.\nmean: \\mu = \\frac{r(1 - p)}{p} standard deviation: \\sigma = \\sqrt\\frac{r(1 - p)}{p^2}\nThe Negative binomial distribution is often used as an alternative to the Poisson distribution when the data show overdispersion, meaning the variance exceeds the mean.\n\n\n10.3.4 Applications\nThe negative binomial distribution can be used to describe the clustering of organisms and also to compare the differences in the population mean.\n\nClustering of organisms\nThe negative ninomial distribution is often used in ecological studies and public health to describe the clustering of organisms or events when the data are overdispersed (i.e., the variance is greater than the mean). This distribution is particularly useful in modeling situations where events (such as organisms) tend to cluster or aggregate rather than be randomly distributed.\n\nExample 8 \nA hospital is studying the clustering of bacterial infections in patients within a specific unit. The hospital collects data on the number of bacterial colonies detected in wound samples from 50 patients over a month. The mean number of bacterial colonies per patient is \\lambda = 10, but due to clustering, some patients have many colonies, while others have very few. This clustering leads to overdispersion, where the variance is greater than the mean. To model the number of bacterial colonies detected per patient and estimate the clustering pattern.\n\n\n# Simulate data for 50 patients using a Negative Binomial distribution\nset.seed(123)\nnum_patients &lt;- 50\nmean_colonies &lt;- 10  # Average number of bacterial colonies per patient\ndispersion_param &lt;- 3  # Dispersion parameter (greater dispersion leads to more clustering)\n\n# Generate data for the number of colonies per patient\ncolonies &lt;- rnbinom(num_patients, size = dispersion_param, mu = mean_colonies)\n\n# Summary of the simulated data\nsummary(colonies)\n\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;    1.00    5.00    9.00    9.32   12.75   22.00\n\n# Fit a Negative Binomial model to the data\nnb_model &lt;- glm.nb(colonies ~ 1)\n\n# View model results\nsummary(nb_model)\n\n#&gt; \n#&gt; Call:\n#&gt; glm.nb(formula = colonies ~ 1, init.theta = 4.368663822, link = log)\n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept)    2.232      0.082   27.22   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for Negative Binomial(4.3687) family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 53.936  on 49  degrees of freedom\n#&gt; Residual deviance: 53.936  on 49  degrees of freedom\n#&gt; AIC: 306.89\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 1\n#&gt; \n#&gt; \n#&gt;               Theta:  4.37 \n#&gt;           Std. Err.:  1.35 \n#&gt; \n#&gt;  2 x log-likelihood:  -302.885\n\n# Plot histogram of the data to visualize clustering\ncolonies |&gt; \n  as_tibble() |&gt; \n  ggplot(aes(x = value)) +\n  geom_histogram(bins = 10, fill = \"skyblue\", color = \"black\", alpha = 0.7) +\n  labs(title = \"Clustering of bacterial colonies\",\n       x = \"Number of bacterial colonies per patient\",\n       y = \"Frequency\")\n\n\n\n\n\n\n\n\nThe data are generated using a negative binomial distribution to reflect the fact that bacterial colonies cluster within certain patients. The dispersion parameter controls how much clustering there is (the smaller the parameter, the more clustering). The fitted model allows you to estimate the degree of clustering and assess whether the number of colonies per patient significantly deviates from a purely random (Poisson) distribution.\nIn practice, this kind of modeling can help hospitals or researchers understand the spread and concentration of infections in patient populations. It is also commonly used in ecological studies to model the distribution of species, such as insects or plants, in a habitat.\nOne-sample comparison\nFor a single-sample comparison, you compare the sample mean to a known or hypothesized population mean. You may use a Wald test or likelihood ratio test in the context of a negative binomial regression model to compare the sample mean with the hypothesized mean.\n\nExample 9 \nSuppose a hospital wants to check if the average number of emergency room (ER) visits due to asthma per month has increased compared to a hypothesized national mean. Historically, it is known that the average number of ER visits due to asthma per month is \\mu_0 = 30. The hospital collects data for 12 months and finds that the mean number of ER visits per month is 35. To compare whether the hospital’s average ER visits due to asthma differ significantly from the national average of 30 visits per month.\n\n\n# Simulate data for 12 months of ER visits (mean = 35, size parameter controls dispersion)\nset.seed(200)\ner_visits &lt;- rnbinom(12, size = 5, mu = 35)\n\n# Fit a Negative Binomial model and compare the mean to 30\nmean_visits &lt;- 30  # Hypothesized national mean\nnb_model &lt;- glm.nb(er_visits ~ 1)  # Null model (intercept-only)\n\n# Test if the mean differs from 30\nsummary(nb_model)\n\n#&gt; \n#&gt; Call:\n#&gt; glm.nb(formula = er_visits ~ 1, init.theta = 8.233978635, link = log)\n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept)   3.3142     0.1147    28.9   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for Negative Binomial(8.234) family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 11.962  on 11  degrees of freedom\n#&gt; Residual deviance: 11.962  on 11  degrees of freedom\n#&gt; AIC: 94.162\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 1\n#&gt; \n#&gt; \n#&gt;               Theta:  8.23 \n#&gt;           Std. Err.:  4.27 \n#&gt; \n#&gt;  2 x log-likelihood:  -90.162\n\n\nThe results will show whether the average number of ER visits significantly differs from the hypothesized mean.\nTwo-sample comparison\nFor a two-sample comparison, you compare the means of two independent samples.\nA common approach for comparing two means in the negative binomial distribution is using a negative binomial regression model. This is analogous to a two-sample t-test, but tailored for count data.\n\nExample 10 \nA clinical trial is conducted to compare the number of hospitalizations between two groups of patients: those receiving a new drug and those receiving a standard drug for chronic obstructive pulmonary disease. Over the course of a year, the hospital records the number of hospitalizations for both groups.To determine if the mean number of hospitalizations differs significantly between the two groups.\n\n\nset.seed(200)\n# Simulate data for two groups\ngroup &lt;- factor(rep(1:2, each = 100))  # Two groups\ncounts &lt;- rnbinom(200, size = 5, mu = if_else(group == 1, 10, 15))  # Negative Binomial counts\n\n# Fit a Negative Binomial regression model\nnb_model &lt;- glm.nb(counts ~ group)\n\n# View the summary of the model\nsummary(nb_model)\n\n#&gt; \n#&gt; Call:\n#&gt; glm.nb(formula = counts ~ group, init.theta = 5.66947187, link = log)\n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept)  2.26176    0.05297  42.701  &lt; 2e-16 ***\n#&gt; group2       0.35928    0.07278   4.937 7.95e-07 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for Negative Binomial(5.6695) family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 234.55  on 199  degrees of freedom\n#&gt; Residual deviance: 210.16  on 198  degrees of freedom\n#&gt; AIC: 1258.7\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 1\n#&gt; \n#&gt; \n#&gt;               Theta:  5.669 \n#&gt;           Std. Err.:  0.859 \n#&gt; \n#&gt;  2 x log-likelihood:  -1252.688\n\n\nThe coefficient for group will tell you whether the mean number of hospitalizations differs between the two groups. If the p-value for the group variable is less than 0.05, there is a statistically significant difference in hospitalizations between the new and standard drug groups.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Distributions of discrete variables</span>"
    ]
  },
  {
    "objectID": "discrete-data-dist.html#statistical-models-for-count-data",
    "href": "discrete-data-dist.html#statistical-models-for-count-data",
    "title": "10  Distributions of discrete variables",
    "section": "10.4 Statistical models for count data",
    "text": "10.4 Statistical models for count data\n\n10.4.1 Poisson regression\nA common model used to analyze count data, assuming that the mean is equal to the variance. It is appropriate when the count data are not overdispersed.\n\n\n10.4.2 Negative binomial regression\nUsed when count data exhibit overdispersion. This model is more flexible than Poisson regression, as it allows the variance to exceed the mean.\n\n\n10.4.3 Zero-inflated models\nIn some medical data, there may be an excess of zero counts (e.g., a large number of patients with no hospital visits). Zero-inflated Poisson (ZIP) or Zero-inflated Negative Binomial (ZINB) models can account for this excess of zeros.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Distributions of discrete variables</span>"
    ]
  },
  {
    "objectID": "chi-square-test.html",
    "href": "chi-square-test.html",
    "title": "11  Chi-square test",
    "section": "",
    "text": "11.1 Prerequisite\nThe Chi-square test is a statistical test commonly used to determine whether there is a significant association between categorical variables. It can be applied in several different contexts, including testing for goodness-of-fit, independence, and homogeneity.\nlibrary(tidyverse)\nlibrary(rmarkdown)\nlibrary(coin)",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chi-square test</span>"
    ]
  },
  {
    "objectID": "chi-square-test.html#chi-square-test-for-2x2-tables",
    "href": "chi-square-test.html#chi-square-test-for-2x2-tables",
    "title": "11  Chi-square test",
    "section": "11.2 Chi-square test for 2x2 tables",
    "text": "11.2 Chi-square test for 2x2 tables\n\n11.2.1 Chi-square distribution\nThe Chi-square distribution is a probability distribution commonly used in hypothesis testing and confidence interval estimation for variance and categorical data analysis, particularly in the Chi-square test for independence and goodness-of-fit.\nThe probability density function of the Chi-square distribution with k degrees of freedom is:\n\nf(x; k) = \\frac{1}{2^{k/2} \\Gamma(k/2)} x^{(k/2 - 1)} e^{-x/2}\n\nwhere x \\geq 0, k is the degrees of freedom, \\Gamma is the Gamma function.\n\n\n11.2.2 Visualization\nThe shape of the Chi-square distribution is determined by its degrees of freedom. The code below will plot Chi-square distributions with different degrees of freedom to demonstrate how the shape of the distribution changes.\n\n\n\n\n\n\n\n\n\nIt is positively skewed for low degrees of freedom but becomes more symmetrical as the degrees of freedom increase. Since it’s a distribution of squared values, the Chi-square statistic is always non-negative, meaning it ranges from 0 to infinity.\n\n\n11.2.3 2x2 contingency table\n2x2 contingency table, also known as fourfold table , is a matrix used to display the frequencies of two categorical variables with two levels each (see Table 11.1 ).\n\n\n\n\nTable 11.1: Structure of a 2x2 contingency table\n\n\n\n\n\n\n\nevent\nnon-event\ntotal\n\n\n\n\nexposed\na\nb\na + b\n\n\nnon-exoposed\nc\nd\nc + d\n\n\ntotal\na + c\nb + d\nN = a + b + c + d\n\n\n\n\n\n\n\n\nwhere a is the number of events in the exposed group, b is the number of non-events in the exposed group, c is the number of events in the non-exposed group, d is the number of non-events in the non-exposed group.\nFor example, you might use a fourfold table to study whether a new treatment (exposure) improves the survival rates (event) compared to a standard treatment.\n\n\n\n\nTable 11.2: Heart attack outcomes by two treatment groups\n\n\n\n\n\n\n\n\n\n\n\n\n\nheart attack (survived)\nheart attack (died)\ntotal\n\n\n\n\nnew treatment\n30\n20\n50\n\n\nstandard treatment\n15\n35\n50\n\n\ntotal\n45\n55\n100\n\n\n\n\n\n\n\n\nChi-square statistic\nThe Chi-square statistic for a 2x2 table is calculated using the formula:\n\n\\chi^2 = \\frac{N(ad - bc)^2}{(a + b)(c + d)(a + c)(b + d)}\n\nwhere N is the total sample size, N = a + b + c + d, ad - bc is the product difference between rows and columns.\nYates’ continuity correction\nYates’ continuity correction is a statistical adjustment applied to the Chi-square test for 2x2 contingency tables to correct for the fact that the Chi-square test is an approximation of a continuous distribution, but the data in a contingency table are discrete. This correction makes the Chi-square test more conservative, reducing the likelihood of type I errors (false positives) by slightly lowering the Chi-square statistic.\nWith Yates’ correction, the formula for the Chi-square statistic is:\n\n\\chi^2 = \\frac{(|ad - bc| - 0.5N)^2 N}{(a+b)(c+d)(a+c)(b+d)}\n\n\nWhen to use Yates’ correction\n\nIt is generally recommended for 2x2 tables with small sample sizes, particularly when any expected frequency is less than 10 but greater than 5. It is automatically applied in chisq.test() for 2x2 tables unless specified otherwise.\nWhen you want to be more conservative and reduce the chance of false positives (finding a significant result when there is none), Yates’ correction can help.\n\nWhen not to use Yates’ correction\n\nWhen the sample size is large, or the expected frequencies in each cell are well above 5, Yates’ correction is not necessary and may be too conservative.\nYates’ correction is only relevant for 2x2 tables, so it’s ignored for larger contingency tables.\n\n\nIt is generally recommended for small sample sizes but can be unnecessary or even too conservative in larger samples.\n\n\n11.2.4 Chi-square test\nThe Chi-square test for 2x2 tables is commonly used to assess whether there is an association between two categorical variables. In medical research, this test is frequently applied to analyze whether exposure (e.g., treatment, risk factor) to a risk factor is associated with a particular outcome (e.g., disease, recovery).\nIn R, you can perform a Chi-square test for the 2x2 contingency table using the chisq.test() function or fisher.test() function.\nWhen to use chisq.test()\n\nChi-square test works well when the sample size is large enough, typically when the expected frequency in each cell of the contingency table is 5 or more.\nChi-square test provides an approximate result based on the Chi-square distribution. It is not suitable for small sample sizes, but it’s efficient for larger data sets.\nChi-square test assumes the total row and column frequencies can vary. It’s used when you don’t have fixed marginal totals.\n\nWhen to use fisher.test()\n\nFisher’s exact test (discussed in Section 11.4 )is recommended for small sample sizes, particularly when any expected cell count is less than 5. It calculates the exact p-value without relying on large sample approximations.\nFisher’s exact test assumes that both the row and column totals are fixed. This makes it a more conservative test compared to the Chi-square test.\nFisher’s test is preferred when dealing with sparse data (i.e., a lot of cells with small counts), as it doesn’t rely on assumptions of normality or expected frequencies.\n\n\nExample 1 \nTo determine if a new treatment for heart attack patients improves survival rates compared to a standard treatment, a total of 100 patients who have suffered heart attacks were randomly divided into two groups: 50 patients receive new treatment, 50 patients receive standard treatment. The outcome of interest is whether the patient survived (recovered or showed significant improvement) or died (passed away due to complications from the heart attack) after the treatment. The observed data is summarized in Table 11.2 . Is there a significant difference in survival rates between patients who received the new treatment and those who received the standard treatment?\n\n\n# Create a 2x2 contingency table\nmatrix(c(30, 20, 15, 35), nrow = 2, byrow = T) |&gt; \n  chisq.test(correct = F)\n\n#&gt; \n#&gt;  Pearson's Chi-squared test\n#&gt; \n#&gt; data:  matrix(c(30, 20, 15, 35), nrow = 2, byrow = T)\n#&gt; X-squared = 9.0909, df = 1, p-value = 0.002569\n\n\nThe calculated Chi-square statistic is 9.0909, p-value is 0.002569, less than the significance level 0.05. This indicates a statistically significant difference in survival rates between the two treatments.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chi-square test</span>"
    ]
  },
  {
    "objectID": "chi-square-test.html#chi-square-test-for-paired-2x2-tables",
    "href": "chi-square-test.html#chi-square-test-for-paired-2x2-tables",
    "title": "11  Chi-square test",
    "section": "11.3 Chi-square test for paired 2x2 tables",
    "text": "11.3 Chi-square test for paired 2x2 tables\nChi-square test for paired 2x2 table is typically used for analyzing paired categorical data, such as when the same subjects are measured before and after an intervention, or when two related groups are compared. In this case, the McNemar’s test is commonly applied for paired data rather than the standard Chi-square test, as it is designed specifically to handle dependent data (paired observations).\n\n11.3.1 McNemar’s Test\nMcNemar’s test focuses only on the off-diagonal cells b and c, which represent discordant pairs (i.e., patients who switched their outcomes between pre- and post-treatment). The cells a and d (concordant pairs) are ignored in this test because they reflect no change in outcome.\nThe test statistic without continuity correction:\n\n\\chi^2 = \\frac{(b - c)^2}{b + c},\n\nThe test statistic with continuity correction:\n\n\\chi^2 = \\frac{(|b - c| - 1)^2}{b + c}\n\nwhere b is the number of individuals who succeeded in pre-treatment but failed in post-treatment, c is the number of individuals who failed in pre-treatment but succeeded in post-treatment, 1 in the formula is a continuity correction applied for small sample sizes.\nThe continuity correction is especially important when b + c is small (e.g., less than 40). It helps to prevent overestimating the test statistic. For larger sample sizes, the correction has a smaller impact, and some recommend not applying it (i.e., setting correct = FALSE), as it can be overly conservative.\n\nExample 2 \nA study is conducted to evaluate the efficacy of a new vaccine in preventing the flu. The same group of individuals is tested before and after receiving the vaccine to see if they develop flu symptoms. The observed data is summarized in Table 11.3 . Determine whether the vaccine have a significant effect on reducing flu incidence.\n\n\n\n\n\nTable 11.3: Paired flu status before and after vaccination\n\n\n\n\n\n\n\n\n\n\n\n\n\nflu post-vaccine\nno flu post-vaccine\ntotal\n\n\n\n\nflue pre-vaccine\n10 (had flu both times)\n25 (had flu pre, no flu post)\n35\n\n\nno flu pre-vaccine\n5 (no flu pre, had flu post)\n60 (no flu both times)\n65\n\n\ntotal\n15\n85\n100\n\n\n\n\n\n\n\n\n\nmatrix(c(10, 25, 5, 60), nrow = 2, byrow = T) |&gt; \n  mcnemar.test(correct = T)\n\n#&gt; \n#&gt;  McNemar's Chi-squared test with continuity correction\n#&gt; \n#&gt; data:  matrix(c(10, 25, 5, 60), nrow = 2, byrow = T)\n#&gt; McNemar's chi-squared = 12.033, df = 1, p-value = 0.0005226\n\n\nIf the p-value from the McNemar’s test is less than the significance level (typically \\alpha = 0.05), you would reject the null hypothesis and conclude that the vaccine had a significant effect on reducing flu incidence. If the p-value is greater than the significance level, the null hypothesis is not rejected, and the data do not provide sufficient evidence to conclude that the vaccine significantly changed flu outcomes. Here the p-value is less than 0.05.\nThis type of paired data analysis is common in medical research, especially in pre-post studies where the same subjects are followed over time to assess the impact of an intervention (e.g., medication, surgery, or vaccines).",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chi-square test</span>"
    ]
  },
  {
    "objectID": "chi-square-test.html#sec-fisher-test",
    "href": "chi-square-test.html#sec-fisher-test",
    "title": "11  Chi-square test",
    "section": "11.4 Fisher’s exact test",
    "text": "11.4 Fisher’s exact test\nThe Fisher’s exact test is used to determine if there are nonrandom associations between two categorical variables, typically in a 2x2 contingency table (fourfold table). It is commonly applied when sample sizes are small, making the Chi-square test unreliable due to low expected frequencies in one or more cells.\nFisher’s exact test calculates the exact probability of obtaining a table at least as extreme as the one observed, assuming the null hypothesis is true. The probability is calculated based on the hypergeometric distribution, and Fisher’s test uses this distribution to calculate the p-value.\nThe formula for the probability of observing a given 2x2 table is:\n\nP = \\frac{(a+b)!(c+d)!(a+c)!(b+d)!}{a!b!c!d!N!}\n\nwhere N = a + b + c + d is the total sample size.\n\nExample 3 \nA clinical trial is conducted to evaluate the effectiveness of a new drug for treating hypertension. Patients are divided into treatment group and control group. After treatment, the number of patients whose blood pressure returned to normal (cured) and those whose blood pressure remained high (not cured) is recorded. The data collected is in . Determine if the new drug shows a statistically significant difference in its effectiveness compared to the control.\n\n\n\n\n\nTable 11.4: Structure of a 2x2 contingency table\n\n\n\n\n\n\n\nblood pressure normal\nBlood pressure high\ntotal\n\n\n\n\ntreatment\n8\n2\n10\n\n\ncontrol\n3\n7\n10d\n\n\ntotal\n11\n9\n20\n\n\n\n\n\n\n\n\nIn R, Fisher’s exact test can be performed using the fisher.test() function.\n\nmatrix(c(8, 2, 3, 7), nrow = 2, byrow = T) |&gt; \nfisher.test()\n\n#&gt; \n#&gt;  Fisher's Exact Test for Count Data\n#&gt; \n#&gt; data:  matrix(c(8, 2, 3, 7), nrow = 2, byrow = T)\n#&gt; p-value = 0.06978\n#&gt; alternative hypothesis: true odds ratio is not equal to 1\n#&gt; 95 percent confidence interval:\n#&gt;    0.8821175 127.0558418\n#&gt; sample estimates:\n#&gt; odds ratio \n#&gt;   8.153063\n\n\nThe result of Fisher’s Exact Test will give you a p-value. Here p-value is 0.06978, above 0.05, not reject the null hypothesis. This means the data do not provide sufficient evidence to conclude that the new drug shows a better effectiveness compared to control.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chi-square test</span>"
    ]
  },
  {
    "objectID": "chi-square-test.html#chi-square-test-for-rc-tables",
    "href": "chi-square-test.html#chi-square-test-for-rc-tables",
    "title": "11  Chi-square test",
    "section": "11.5 Chi-square test for R×C tables",
    "text": "11.5 Chi-square test for R×C tables\nThe Chi-square test for R×C contingency tables is used to assess whether there is an association between two categorical variables where one variable has R categories and the other has C categories. This is also called the Chi-square test of independence.\n\n11.5.1 Comparison of multiple sample rates\nWhen comparing multiple sample rates, several statistical methods are commonly used, including the Chi-square test and Cochran’s Q test. The choice of method depends on the structure of the data and the research question.\n\nChi-Square test for rates\nThe Chi-square test is widely used to compare rates across multiple independent samples. It tests whether the observed differences between sample proportions are statistically significant.\n\nExample 4 \nSuppose you want to compare recovery rates between three hospitals for a specific disease. The data is in Table 11.5 . Determine if the recovery rates are significantly different between these hospitals.\n\n\n\n\n\nTable 11.5: The recovery results of a disease in three hospitals\n\n\n\n\n\n\n\nrecovered\nnot revovered\n\n\n\n\nhospital A\n50\n20\n\n\nhospital B\n55\n15\n\n\nhospital C\n65\n5\n\n\n\n\n\n\n\n\nHere is chisq.test() is used to conduct this kind of test. In Section 11.6 , we reanalyze this example using function prop.test(), which gives the same result.\n\nmatrix(c(50, 20, 55, 15, 65, 5), nrow = 3, byrow = T) |&gt; \n  chisq.test(correct = F)\n\n#&gt; \n#&gt;  Pearson's Chi-squared test\n#&gt; \n#&gt; data:  matrix(c(50, 20, 55, 15, 65, 5), nrow = 3, byrow = T)\n#&gt; X-squared = 10.809, df = 2, p-value = 0.004497\n\n\nCochran’s Q test\nCochran’s Q test is an extension of the Chi-Square test, designed for comparing rates across multiple samples in paired or repeated measures settings. It is typically used with binary outcomes (e.g., “yes/no” or “success/failure”).\n\nExample 5 \nA study is conducted to test the effectiveness of a new drug across three time points (1 month, 3 months, and 6 months) on the same group of patients. The outcome is whether the treatment was successful (1) or not (0), recorded for 10 patients in Table 11.6 . Determine if there is a significant difference in success rates across the three time points.\n\n\n\n\n\nTable 11.6: The recovery results of a disease in three hospitals\n\n\n\n\n\n\nid\n1 month\n3 months\n6 months\n\n\n\n\n1\n1\n0\n1\n\n\n2\n0\n0\n1\n\n\n3\n1\n1\n1\n\n\n4\n0\n1\n0\n\n\n5\n1\n1\n1\n\n\n6\n1\n0\n1\n\n\n7\n0\n1\n0\n\n\n8\n1\n1\n1\n\n\n9\n1\n0\n0\n\n\n10\n1\n1\n1\n\n\n\n\n\n\n\n\n\ndata.frame(\n  id = factor(1:10),\n  time = factor(rep(c(\"1 Month\", \"3 Months\", \"6 Months\"), each = 10)),\n  outcome = c(1,0,1,0,1,1,0,1,1,1, 0,0,1,1,1,0,1,1,0,1, 1,0,0,1,0,1,1,1,0,1)\n) |&gt; \nsymmetry_test(outcome ~ time | id, data = _, teststat = \"quadratic\")\n\n#&gt; \n#&gt;  Asymptotic General Symmetry Test\n#&gt; \n#&gt; data:  outcome by\n#&gt;   time (1 Month, 3 Months, 6 Months) \n#&gt;   stratified by id\n#&gt; chi-squared = 0.28571, df = 2, p-value = 0.8669\n\n\nCochran-Armitage trend test\nIf you want to test whether there is a linear trend in the rates across ordered groups, you can use the Cochran-Armitage test for trend. This test is useful for ordered categorical data and examines if there is a significant linear trend in proportions.\n\nExample 6 \nSuppose you have data from a study on smoking and lung cancer risk. The groups are non-smokers, light smokers, and heavy smokers, and the outcome is the presence of lung cancer (yes/no). The data is listed in Table 11.7 . Determine if there’s a linear trend in cancer risk across the ordered smoking categories.\n\n\n\n\n\nTable 11.7: The data from a study on smoking and lung cancer risk\n\n\n\n\n\n\ngroup\ncancer\nno cancer\n\n\n\n\nnoon-smokers\n5\n95\n\n\nlight smokers\n15\n85\n\n\nheavy smokers\n40\n60\n\n\n\n\n\n\n\n\n\nwith(df, prop.trend.test(cancer, cancer + `no cancer`))\n\n#&gt; \n#&gt;  Chi-squared Test for Trend in Proportions\n#&gt; \n#&gt; data:  cancer out of cancer + `no cancer` ,\n#&gt;  using scores: 1 2 3\n#&gt; X-squared = 38.281, df = 1, p-value = 6.125e-10\n\n\nIt is specifically designed to detect linear trends in rates across ordered groups. This test is particularly useful when you have binary outcomes (e.g., success/failure) across different levels of an ordinal independent variable (e.g., increasing doses of a drug). It directly tests for a linear trend in the proportions of success.\n\n\n\n11.5.2 Comparison of multiple sample proportions\n\nExample 7 \nSuppose you are studying the distribution of a gene variant related to a certain disease in two groups: a healthy control group, and a disease group. You aim to compare the frequencies of the AA, Aa, and aa genotypes between these groups. The data is in Table 11.8 . Determine whether there is a significant difference in the distribution of genotypes between the healthy and disease groups.\n\n\n\n\n\nTable 11.8: The distribution of a gene variant in two groups\n\n\n\n\n\n\ngroup\nAA\nAa\naa\n\n\n\n\nhealth group\n40\n35\n25\n\n\ndisease group\n30\n45\n25\n\n\n\n\n\n\n\n\n\n# Create the data matrix\nmatrix(c(40, 35, 25, 30, 45, 25), nrow = 2, byrow = T) |&gt; \n  chisq.test(correct = F)\n\n#&gt; \n#&gt;  Pearson's Chi-squared test\n#&gt; \n#&gt; data:  matrix(c(40, 35, 25, 30, 45, 25), nrow = 2, byrow = T)\n#&gt; X-squared = 2.6786, df = 2, p-value = 0.262\n\n\nFor smaller sample sizes, Fisher’s Exact Test can be used.\nGenotyping is often used to study the distribution of specific genes in different populations, particularly when exploring the relationship between genetic variations and disease risk. For instance, certain gene alleles may be linked to chronic diseases like cardiovascular conditions or cancer. These analyses can help identify associations between specific genotypes and disease susceptibility. For example , comparing the distribution of APOE genotypes (APOE ε2, ε3, ε4) between patients with Alzheimer’s disease and healthy controls; Investigating differences in the INS gene (insulin gene) genotypes between diabetic and non-diabetic patients. Such analysis can help researchers understand the link between genetic variations and disease risk, providing insights into personalized medicine and genetic risk factors.\n\n\n11.5.3 Association Analysis of bivariate nominal data\nAssociation analysis of bivariate nominal data involves examining the relationship between two categorical variables. This is typically done using chi-square tests or Fisher’s exact tests to determine if there is a significant association between the two variables.\nIf the association is significant, and the strength of the relationship needs to be further analyzed, the Pearson contingency coefficient C need to be calculated.\nPearson’s contingency coefficient\nPearson’s contingency coefficient is a measure used to assess the strength of association between two categorical variables in a contingency table. It quantifies how strongly two variables are related based on the chi-square statistic. It is calculated using the following formula:\n\nC = \\sqrt{\\frac{\\chi^2}{\\chi^2 + n}}\n\nwhere C is the Pearson’s contingency coefficient, \\chi^2 is the chi-square statistic obtained from a chi-square test, n is the total sample size.\nThe value of C ranges from 0 \\leq C &lt; 1. Values closer to 1 indicate a stronger association between the variables, while values close to 0 indicate weak or no association. As the size of the contingency table (degrees of freedom) increases, the contingency coefficient tends to decrease.\n\nExample 8 \nResearchers aim to evaluate the effectiveness of two different antibiotics (A and B) for two types of infections (X and Y). The effectiveness is measured by whether patients recover, categorized as “recovered” or “not recovered.” The data is summarized in Table 11.9 . The goal is to determine if the effectiveness of the antibiotics differs significantly for different infection types.\n\n\n\n\n\nTable 11.9: The results of two different antibiotics for two types of infections\n\n\n\n\n\n\nantibiotic\ninfection type\nrecovered\nnot recovered\n\n\n\n\nA\nX\n35\n15\n\n\nA\nY\n25\n25\n\n\nB\nX\n40\n10\n\n\nB\nY\n30\n20\n\n\n\n\n\n\n\n\nTo evaluate whether the effectiveness of antibiotics A and B differs significantly for different infection types (X and Y), you can perform a chi-square test of independence on the contingency table. This will assess if there is a significant association between the type of antibiotic and the recovery status for the two infection types.\n\n# Create data\ndata &lt;- matrix(c(35, 15, 25, 25, 40, 10, 30, 20), nrow = 2, byrow = T)\n\nchisq.test(data)\n\n#&gt; \n#&gt;  Pearson's Chi-squared test\n#&gt; \n#&gt; data:  data\n#&gt; X-squared = 2.3434, df = 3, p-value = 0.5043\n\n# Calculate Pearson’s Contingency Coefficient\nDescTools::ContCoef(data)\n\n#&gt; [1] 0.1076172\n\n\nThe p-value is greater than your significance level 0.05, not reject the null hypothesis, indicating that the recovery status is not associated with the antibiotic and infection type.\nA little expansion*：\nTo analyze the interaction between antibiotics (A and B) and infection types (X and Y) on recovery outcomes (recovered or not recovered), a logistic regression model is appropriate. This model will allow you to test the interaction effect between the two categorical variables (antibiotics and infection types) and their influence on recovery.\nIn this case, you can model the recovery outcome as the dependent variable and include both antibiotics and infection types as independent variables, along with their interaction term.\n\n# Convert data to long format for logistic regression\ndf_long &lt;- df |&gt; \n  janitor::clean_names() |&gt; \n1  pivot_longer(\n    cols = c(recovered, not_recovered),\n    names_to = \"response\",\n    values_to = \"freq\"\n  ) |&gt;                                 \n  mutate(response = if_else(response == \"recovered\", 1, 0))\n  \n2df_expanded &lt;- df_long |&gt;\n  uncount(freq)\n\nmodel &lt;- glm(\n  response ~ antibiotic * infection_type, \n  data = df_expanded, \n  family = binomial(link = \"logit\")\n) \n\nsummary(model)\n\n\n1\n\nReshape the data to long format suitable for logistic regression\n\n2\n\nReplicate rows based on freq to get one row per individual (useful for logistic regression)\n\n\n\n\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = response ~ antibiotic * infection_type, family = binomial(link = \"logit\"), \n#&gt;     data = df_expanded)\n#&gt; \n#&gt; Coefficients:\n#&gt;                             Estimate Std. Error z value Pr(&gt;|z|)   \n#&gt; (Intercept)                   0.8473     0.3086   2.746  0.00604 **\n#&gt; antibioticB                   0.5390     0.4693   1.149  0.25075   \n#&gt; infection_typeY              -0.8473     0.4186  -2.024  0.04296 * \n#&gt; antibioticB:infection_typeY  -0.1335     0.6193  -0.216  0.82930   \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 258.98  on 199  degrees of freedom\n#&gt; Residual deviance: 247.74  on 196  degrees of freedom\n#&gt; AIC: 255.74\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 4\n\n\nThe logistic regression model is trying to assess the relationship between antibiotic type, infection type, and their interaction on patient recovery. Here’s a breakdown of the key results:\n\n\n\n\n\n\nInterpretation of logistic regression Results\n\n\n\n\n(Intercept): Estimate: 0.8473, p-value: 0.00604 (statistically significant)\n\nThis represents the log-odds of recovery for patients treated with antibiotic A for Infection X (the baseline group).\nA positive coefficient (0.8473) suggests that the odds of recovery are higher for this group.\nTo interpret this in terms of odds: \\exp(0.8473) \\approx 2.33 , meaning patients in the baseline group are 2.33 times more likely to recover.\n\nantibioticB: Estimate: 0.5390, p-value: 0.25075 (not statistically significant)\n\nThis is the difference in log-odds of recovery between antibiotic B and antibiotic A for Infection X.\nSince the p-value is not significant, there is no strong evidence that antibiotic B differs significantly from antibiotic A for Infection X.\n\ninfection_typeY: Estimate: -0.8473, p-value: 0.04296 (statistically significant)\n\nThis is the effect of having Infection Y instead of infection X when treated with antibiotic A.\nThe negative coefficient (-0.8473) indicates that patients with Infection Y are less likely to recover compared to those with infection X when treated with antibiotic A.\nIn terms of odds: \\exp(-0.8473) \\approx 0.43 , meaning patients with infection Y have about 43% the odds of recovery compared to those with infection X when treated with antibiotic A.\n\nantibioticB:infection_typeY (Interaction Term): Estimate: -0.1335, p-value: 0.82930 (not statistically significant)\n\nThis term represents the interaction between Antibiotic B and infection Y, meaning how the effect of antibiotic B changes for Infection Y compared to infection X.aThe non-significant p-value indicates that there is no strong evidence of an interaction effect between the type of antibiotic and the type of infection on recovery.\n\n\nSummary:\n\nAntibiotic: There is no statistically significant difference in recovery between antibiotic A and antibiotic B (p = 0.25075).\nInfection Type: Patients with infection Y are significantly less likely to recover compared to those with infection X (p = 0.04296).\nInteraction: The interaction between antibiotic type and infection type is not significant, meaning the effectiveness of the antibiotics does not seem to differ significantly between infection types (p = 0.82930).\n\nConclusion:\n\nThe type of infection has a significant impact on recovery, with infection X being more favorable for recovery than infection Y.\nHowever, there is no significant difference between antibiotic A and antibiotic B in terms of effectiveness.\nThere is also no significant interaction between antibiotic type and infection type, meaning the antibiotics perform similarly across infection types.\n\n\n\nTo visualize the interaction effect:\n\n# Add predictions to the expanded dataset\ndf_expanded  &lt;- df_expanded |&gt; \n  mutate(predicted = predict(model, type = \"response\"))\n\n# Plot the interaction between antibiotic and infection type\nggplot(df_expanded, aes(x = infection_type, y = predicted, color = antibiotic)) +\n  geom_point(position = position_dodge(width = 0.2), size = 1) +\n  geom_line(aes(group = antibiotic), position = position_dodge(width = 0.2)) +\n  labs(x = \"Infection type\", y = \"Predicted probability of recovery\") \n\n\n\n\n\n\n\nFigure 11.1: Interaction of antibiotics and infection types on recovery\n\n\n\n\n\n\n\n11.5.4 Linear trend test for two-way ordinal data\nThis test evaluates whether there is a linear trend between the categories in two ordinal variables, typically one variable representing treatment or groups and the other representing ordered outcomes. In this case, both “group” and “outcome” are ordinal variables.\n\nExample 9 \nSuppose you are studying the effect of different drug dosages on pain relief in patients. Patients are divided into four dosage groups (low dose, low-medium dose, medium-high dose, and high dose), and their pain relief is categorized into four levels (no relief, partial relief, moderate relief, and complete relief). This type of data is referred to as two-way ordinal data, listed in Table 11.10 . The aim is to assess whether there is a significant linear trend in pain relief as the drug dosage increases. For example, does increasing the dosage lead to a higher likelihood of better pain relief?\n\n\n\n\n\nTable 11.10: The association between different drug dosage and pain relief\n\n\n\n\n\n\n\n\n\n\n\n\n\ndosage\nno relief\npartial relief\nmoderate relief\ncomplete relief\n\n\n\n\nlow dose\n30\n25\n20\n10\n\n\nlow-medium dose\n20\n30\n25\n15\n\n\nmedium-high dose\n15\n35\n30\n20\n\n\nhigh dose\n10\n40\n35\n30\n\n\n\n\n\n\n\n\nThe Mantel-Haenszel chi-square statistic tests the alternative hypothesis that there is a linear association between the row variable and the column variable. Both variables must lie on an ordinal scale. Here we use the MHChisqTest() function in DescTools package.\n\nc(30, 25, 20, 10, 20, 30, 25, 15, 15, 35, 30, 20, 10, 40, 35, 30) |&gt; \n  matrix(nrow = 4, byrow = T) |&gt; \n  DescTools::MHChisqTest()\n\n#&gt; \n#&gt;  Mantel-Haenszel Chi-Square\n#&gt; \n#&gt; data:  matrix(c(30, 25, 20, 10, 20, 30, 25, 15, 15, 35, 30, 20, 10,     40, 35, 30), nrow = 4, byrow = T)\n#&gt; X-squared = 19.458, df = 1, p-value = 1.029e-05\n\n\nThe result shows chi-square statistic is 19.458, p-value less than 0.05, we can conclude that there is linear relationship between drug dosage and pain relief.\nAn alternative method is the logistic regression analysis using a binomial generalized linear model (GLM) with a logit link function.\n\ndf &lt;- tb |&gt; \n  janitor::clean_names() |&gt; \n  pivot_longer(\n    cols = contains(\"relief\"),\n    names_to = \"pain\",\n    values_to = \"freq\"\n  ) |&gt; \n  mutate(\n   pain = case_when(\n     pain == \"no_relief\" ~ 1,\n     pain == \"partial_relief\" ~ 2,\n     pain == \"moderate_relief\" ~ 3,\n     pain == \"complete_relief\" ~ 4\n   ),\n   dosage = case_when(\n     dosage == \"low dose\" ~ 1,\n     dosage == \"low-medium dose\" ~ 2,\n     dosage == \"medium-high dose\" ~ 3,\n     dosage == \"high dose\" ~ 4\n   ),\n   .keep = \"unused\"\n  ) |&gt; \n  mutate(\n1    dosage = factor(dosage, ordered = T),\n    pain = factor(pain, ordered = T)\n  ) |&gt; \n  uncount(freq)\n  \nglm(pain ~ dosage, data = df, family = binomial(link='logit')) |&gt; \n  summary()\n\n\n1\n\nmake sure dosage and pain are labeled as ordered factors\n\n\n\n\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = pain ~ dosage, family = binomial(link = \"logit\"), \n#&gt;     data = df)\n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept)  1.48622    0.13779  10.786  &lt; 2e-16 ***\n#&gt; dosage.L     1.27848    0.28214   4.531 5.86e-06 ***\n#&gt; dosage.Q    -0.01493    0.27557  -0.054    0.957    \n#&gt; dosage.C     0.06702    0.26884   0.249    0.803    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 381.85  on 389  degrees of freedom\n#&gt; Residual deviance: 358.21  on 386  degrees of freedom\n#&gt; AIC: 366.21\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 5\n\n\n\n\n\n\n\n\nResults interpretation\n\n\n\nCoefficients:\ndosage.L (Linear Contrast): The linear trend (dosage.L) has a positive coefficient (1.27848) and is highly significant (p-value = 5.86e-06), suggesting that as the dosage increases, the likelihood of pain relief also increases significantly in a linear fashion.\nGoodness-of-Fit:\nResidual deviance (358.21): The deviance with the dosage predictors included. Lower residual deviance suggests a better fit. AIC (366.21): AIC is used for model comparison; lower values indicate a better model fit.\nInterpretation:\nThe significant linear relationship between dosage and pain suggests that increasing the dosage is associated with a reduced likelihood of pain, but there is no evidence of a quadratic or cubic relationship. The model fits the data relatively well as indicated by the reduction in deviance, and dosage.L is the primary driver of the response variable (pain).",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chi-square test</span>"
    ]
  },
  {
    "objectID": "chi-square-test.html#sec-comp-multi-prop",
    "href": "chi-square-test.html#sec-comp-multi-prop",
    "title": "11  Chi-square test",
    "section": "11.6 Pairwise comparisons of multiple sample proportions",
    "text": "11.6 Pairwise comparisons of multiple sample proportions\nWhen comparing proportions across multiple samples, the goal is to determine whether there are significant differences in the rates or proportions between groups. This is commonly applied in medical research, genetics, or other fields where proportions, such as success rates, are of interest across multiple groups. When conducting multiple comparisons, the risk of a type I error increases.\n\n11.6.1 Pairwise comparison\nThe Bonferroni correction is used to adjust the significance level \\alpha to control for this risk, ensuring the results are reliable.\n\nExample 10 \nConducting pairwise comparisons for the data in Example 4 to determine whether the recovery rates are different between any two hospitals.\n\nHere we use the prop.test() function for testing the null that the proportions in several groups are the same.\n\ndata &lt;- matrix(c(50, 20, 55, 15, 65, 5), nrow = 3, byrow = T)\nrownames(data) &lt;- c(\"hospitalA\", \"hospitalB\", \"hospitalC\")\n\ndata |&gt;\n  prop.test(correct = F)\n\n#&gt; \n#&gt;  3-sample test for equality of proportions without continuity\n#&gt;  correction\n#&gt; \n#&gt; data:  data\n#&gt; X-squared = 10.809, df = 2, p-value = 0.004497\n#&gt; alternative hypothesis: two.sided\n#&gt; sample estimates:\n#&gt;    prop 1    prop 2    prop 3 \n#&gt; 0.7142857 0.7857143 0.9285714\n\n\nThe p-value is less than the significant level 0.05, reject the null hypothesis, indicating the recovery rates in the three hospitals are different. Next, we will make a pairwise comparison between the hospitals.\n\ndata |&gt;\n  pairwise.prop.test(p.adjust.method = \"bonferroni\")\n\n#&gt; \n#&gt;  Pairwise comparisons using Pairwise comparison of proportions \n#&gt; \n#&gt; data:  data \n#&gt; \n#&gt;           hospitalA hospitalB\n#&gt; hospitalB 1.000     -        \n#&gt; hospitalC 0.006     0.089    \n#&gt; \n#&gt; P value adjustment method: bonferroni\n\n\nThe result shows the p-values of different group pairs. Only the p-value between hospital A and hospital C is less than 0.05 (p-value = 0.006), saying the recovery rate between them is significantly different.\n\n\n11.6.2 Comparison with a control group\nHere we set the hospital C as the control group.\n\nalpha = 0.05\nk = 3\nalpha_adjusted &lt;- alpha / (2 * ( k- 1))\n\nchisq.test(data)\n\n#&gt; \n#&gt;  Pearson's Chi-squared test\n#&gt; \n#&gt; data:  data\n#&gt; X-squared = 10.809, df = 2, p-value = 0.004497\n\ncompare_pairs &lt;- list(\n  c(\"hospitalA\", \"hospitalC\"),\n  c(\"hospitalB\", \"hospitalC\")\n)\n\nfor (pair in compare_pairs) {\n  sub_data &lt;- data[match(pair, rownames(data)), ]\n  chi_squared_result &lt;- chisq.test(sub_data)\n  print(paste(\"Comparison between\", paste(pair, collapse=\" and \"), \":\"))\n  print(chi_squared_result)\n}\n\n#&gt; [1] \"Comparison between hospitalA and hospitalC :\"\n#&gt; \n#&gt;  Pearson's Chi-squared test with Yates' continuity correction\n#&gt; \n#&gt; data:  sub_data\n#&gt; X-squared = 9.5443, df = 1, p-value = 0.002006\n#&gt; \n#&gt; [1] \"Comparison between hospitalB and hospitalC :\"\n#&gt; \n#&gt;  Pearson's Chi-squared test with Yates' continuity correction\n#&gt; \n#&gt; data:  sub_data\n#&gt; X-squared = 4.725, df = 1, p-value = 0.02973\n\nprint(paste(\"The adjusted alpha is:\", paste(alpha_adjusted)))\n\n#&gt; [1] \"The adjusted alpha is: 0.0125\"",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chi-square test</span>"
    ]
  },
  {
    "objectID": "chi-square-test.html#chi-square-goodness-of-fit-test",
    "href": "chi-square-test.html#chi-square-goodness-of-fit-test",
    "title": "11  Chi-square test",
    "section": "11.7 Chi-square goodness-of-fit test",
    "text": "11.7 Chi-square goodness-of-fit test\nChi-square goodness-of-fit test is a statistical test used to determine whether observed frequencies conform to an expected theoretical distribution. This test is commonly used to check if data follows normal, uniform, Poisson, or other theoretical distributions.\nThe test is applicable in the following scenarios:\n\nTo test if a sample follows a theoretical distribution (e.g., normal, Poisson, etc.).\nTo compare observed frequency data with expected frequencies to identify deviations.\n\nThe test statistic is calculated as:\n\n\\chi^2 = \\sum_{i=1}^{k} \\frac{(O_i - E_i)^2}{E_i}\n\nO_i is the observed frequency for category i, E_i is the expected frequency for category i, k is the number of categories. The expected frequencies should be sufficiently large, typically at least 5 for each category, to ensure the validity of the chi-square test.\n\nExample 11 \nTo observe the spatial distribution of Keshan disease patients in a certain Keshan disease area, the investigators divided the region into 279 sampling units and recorded the cumulative number of cases in each unit over the years. The data are presented in Table 11.11 . Does this data follow a Poisson distribution?\n\n\n\n\n\nTable 11.11: The cumulative number of Keshan disease patients in each unit\n\n\n\n\n\n\nn_observed\nf_observed\n\n\n\n\n0\n26\n\n\n1\n51\n\n\n2\n75\n\n\n3\n63\n\n\n4\n38\n\n\n5\n17\n\n\n6\n5\n\n\n7\n3\n\n\n8\n1\n\n\n\n\n\n\n\n\nThe chisq.test() function can be used to perform a chi-square goodness-of-fit test.\n\nn_cases &lt;- df[[1]]\nf_observed &lt;- df[[2]]\n# Calculate the weighted average\nlambda &lt;- weighted.mean(n_cases, f_observed)\n\n# Calculated expected probability\np.expected &lt;- dpois(n_cases, lambda)\n\n# Combine the last three rows \nf &lt;- c(f_observed[1:6], sum(f_observed[7:9]))\np &lt;- c(p.expected[1:6], 1 - sum(p.expected[1:6]))\n\nchisq.test(x = f, p = p)\n\n#&gt; \n#&gt;  Chi-squared test for given probabilities\n#&gt; \n#&gt; data:  f\n#&gt; X-squared = 2.0355, df = 6, p-value = 0.9164\n\n\nHere p-value is 0.9164, greater than the significant level 0.1. If the p-value is greater than the significant level, fail to reject the null hypothesis, meaning the data might follow the expected distribution. If the calculated p-value is less than the significant level, reject the null hypothesis and conclude that the data does not follow the expected distribution.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chi-square test</span>"
    ]
  },
  {
    "objectID": "nonparametric-test.html",
    "href": "nonparametric-test.html",
    "title": "12  Nonparametric Tests",
    "section": "",
    "text": "12.1 Prerequisite\nNonparametric tests are statistical methods used when the data do not meet the assumptions required for parametric tests, such as normality or equal variances. These tests do not assume a specific distribution for the data, making them suitable for ordinal data, ranks, or non-normally distributed data.\nlibrary(tidyverse)\nlibrary(DescTools)\nlibrary(PMCMRplus)",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Nonparametric Tests</span>"
    ]
  },
  {
    "objectID": "nonparametric-test.html#wilcoxon-signed-rank-test",
    "href": "nonparametric-test.html#wilcoxon-signed-rank-test",
    "title": "12  Nonparametric Tests",
    "section": "12.2 Wilcoxon signed-rank test",
    "text": "12.2 Wilcoxon signed-rank test\nWilcoxon signed-rank test is often used for comparing paired samples or a single sample to a known median. It is an alternative to the paired t-test when data are not normally distributed. This test can be applied in situations where a before-and-after comparison is needed, such as evaluating the effect of a treatment, intervention, or condition on a specific group.\n\n12.2.1 Paired samples\nComparing the median of paired sample difference to zero is typically used to determine whether there is a significant difference between two related groups. The method most commonly applied is the Wilcoxon signed-rank test.\nThis test ranks the absolute differences between the pairs, then checks whether the median of these differences deviates from zero, indicating a statistically significant change between the two paired groups.\n\nExample 1 \nIn a paired design, two types of feed were administered to 8 pairs of rats, and the vitamin A content in their liver was measured. The data can be downloaded from the button below. Is there a significant difference in the vitamin A content in the livers of rats fed with different types of feed?\n\n\n  Download data \n\n\ndf &lt;- read_csv(\"datasets/ex12-01.csv\", show_col_types = F) \nstr(df)\n\n#&gt; spc_tbl_ [8 × 2] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n#&gt;  $ feedA: num [1:8] 3.55 2 3 3.95 3.8 3.75 3.45 3.05\n#&gt;  $ feedB: num [1:8] 2.45 2.4 1.8 3.2 3.25 2.7 2.5 1.75\n#&gt;  - attr(*, \"spec\")=\n#&gt;   .. cols(\n#&gt;   ..   feedA = col_double(),\n#&gt;   ..   feedB = col_double()\n#&gt;   .. )\n#&gt;  - attr(*, \"problems\")=&lt;externalptr&gt;\n\n\nFirst, test the normality of the difference between the pairs of observations.\n\nwith(df, shapiro.test(feedA - feedB))\n\n#&gt; \n#&gt;  Shapiro-Wilk normality test\n#&gt; \n#&gt; data:  feedA - feedB\n#&gt; W = 0.81051, p-value = 0.03705\n\n\nThe p-value is 0.03705, less than 0.05, indicating the difference does not follows a normal distribution. Then the signed-rank test can be conducted using wilcox.test() function with argument paired = TRUE.\n\nwith(df, wilcox.test(x = feedA, y = feedB, paired = T))\n\n#&gt; \n#&gt;  Wilcoxon signed rank exact test\n#&gt; \n#&gt; data:  feedA and feedB\n#&gt; V = 35, p-value = 0.01563\n#&gt; alternative hypothesis: true location shift is not equal to 0\n\n\n\n\n12.2.2 One sample\nThe comparison of a single sample median with the population median is used to assess whether the median of a sample significantly differs from a known or hypothesized population median. A common method for this comparison is the one-sample Wilcoxon signed-rank test (also known as the one-sample median test).\nThe test ranks the sample data and compares the sum of positive and negative ranks relative to the hypothesized median, determining whether the sample median is significantly different from the population median.\n\nExample 2 \nIt is known that the median urine nitrogen level of healthy individuals in a area is 45.3. A group randomly selected 12 workers from a factory in that area, and measured their fluoride content in urine. The data can be downloaded from the button below. Are the urine nitrogen levels of the factory workers higher than those of the healthy individuals in the area?\n\n\n  Download data \n\n\ndf &lt;- read_csv(\"datasets/ex12-02.csv\", show_col_types = F)\nstr(df)\n\n#&gt; spc_tbl_ [12 × 2] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n#&gt;  $ urine_fluorine: num [1:12] 44.2 45.3 46.4 49.5 51 ...\n#&gt;  $ diff          : num [1:12] -1.09 0 1.09 4.17 5.75 ...\n#&gt;  - attr(*, \"spec\")=\n#&gt;   .. cols(\n#&gt;   ..   urine_fluorine = col_double(),\n#&gt;   ..   diff = col_double()\n#&gt;   .. )\n#&gt;  - attr(*, \"problems\")=&lt;externalptr&gt;\n\n\n\nwith(df, wilcox.test(x = urine_fluorine, mu = 45.3, alter = \"greater\"))\n\n#&gt; Warning in wilcox.test.default(x = urine_fluorine, mu = 45.3, alter =\n#&gt; \"greater\"): cannot compute exact p-value with zeroes\n\n\n#&gt; \n#&gt;  Wilcoxon signed rank test with continuity correction\n#&gt; \n#&gt; data:  urine_fluorine\n#&gt; V = 65, p-value = 0.00255\n#&gt; alternative hypothesis: true location is greater than 45.3\n\n\nWhen you run the code above, a warning message “cannot compute exact p-value with zeroes” is occurring. The warning you’re encountering is because the wilcox.test() function has trouble computing an exact p-value when there are zero differences in the data (or the values themselves are zero), which can occur in certain datasets. To resolve this, you can use the following strategy: Ignore the exact p-value and rely on the asymptotic approximation, which can be computed without zero-value issues. You can do this by setting exact = FALSE.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Nonparametric Tests</span>"
    ]
  },
  {
    "objectID": "nonparametric-test.html#wilcoxon-rank-sum-test",
    "href": "nonparametric-test.html#wilcoxon-rank-sum-test",
    "title": "12  Nonparametric Tests",
    "section": "12.3 Wilcoxon rank-sum test",
    "text": "12.3 Wilcoxon rank-sum test\nThe Wilcoxon rank-sum test (also known as the Mann-Whitney U test) is a nonparametric test used to compare two independent groups to assess whether their population distributions differ. It is an alternative to the two-sample t-test when the assumption of normality is not met, making it robust for non-normally distributed data. It compares the ranks of the values rather than the actual data, making it suitable for ordinal and continuous data that is not normally distributed.\n\n12.3.1 Raw data\nRaw data refers to unprocessed or untransformed data that has been collected directly from a source. The data typically consists of observations or measurements in their original form, such as survey responses, experimental results, or sensor readings.\n\nExample 3 \nThe right hilar diameter (RD) was measured using X-ray in 10 lung cancer patients and 12 stage 0 silicosis workers. The data can be downloaded from the button below. The question is whether the RDs of lung cancer patients are higher than those of stage 0 silicosis workers?\n\n\n  Download data \n\n\ndf &lt;- read_csv(\n  \"datasets/ex12-03.csv\", \n  col_types = list(group = col_factor(), rd = col_double())\n)\nstr(df)\n\n#&gt; spc_tbl_ [22 × 2] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n#&gt;  $ group: Factor w/ 2 levels \"1\",\"2\": 1 1 1 1 1 1 1 1 1 1 ...\n#&gt;  $ rd   : num [1:22] 2.78 3.23 4.2 4.87 5.12 6.21 7.18 8.05 8.56 9.6 ...\n#&gt;  - attr(*, \"spec\")=\n#&gt;   .. cols(\n#&gt;   ..   group = col_factor(levels = NULL, ordered = FALSE, include_na = FALSE),\n#&gt;   ..   rd = col_double()\n#&gt;   .. )\n#&gt;  - attr(*, \"problems\")=&lt;externalptr&gt;\n\n\n\nwilcox.test(rd ~ group, data = df, alter = \"greater\", exact = F)\n\n#&gt; \n#&gt;  Wilcoxon rank sum test with continuity correction\n#&gt; \n#&gt; data:  rd by group\n#&gt; W = 86.5, p-value = 0.04318\n#&gt; alternative hypothesis: true location shift is greater than 0\n\n\n\n\n12.3.2 Frequency table data and ordinal data\nFrequency table data refers to data that is summarized in terms of counts or frequencies for different categories. Ordinal data, on the other hand, involves variables that have a clear, ordered ranking but the intervals between ranks are not necessarily equal. Here is an example of ordinal data.\n\nExample 4 \nA hospital treated two types of pediatric pneumonia with a new drug, and the efficacy is shown in. Determine whether the efficacy of the drug differs between the two types of pneumonia patients.\n\n\n  Download data \n\n\ndf &lt;- read_csv(\n  file = \"datasets/ex12-04.csv\", \n  col_types = list(\n    group = col_factor(),\n    efficacy = col_integer(), \n    freq = col_integer()\n  )\n)\nstr(df)\n\n#&gt; spc_tbl_ [8 × 3] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n#&gt;  $ group   : Factor w/ 2 levels \"1\",\"2\": 1 1 1 1 2 2 2 2\n#&gt;  $ efficacy: int [1:8] 1 2 3 4 1 2 3 4\n#&gt;  $ freq    : int [1:8] 65 18 30 13 42 6 23 11\n#&gt;  - attr(*, \"spec\")=\n#&gt;   .. cols(\n#&gt;   ..   group = col_factor(levels = NULL, ordered = FALSE, include_na = FALSE),\n#&gt;   ..   efficacy = col_integer(),\n#&gt;   ..   freq = col_integer()\n#&gt;   .. )\n#&gt;  - attr(*, \"problems\")=&lt;externalptr&gt;\n\n\nIf you have a frequency table that includes ordinal information, you can perform a rank-sum test with weighted data. First, the data is expanded based on frequencies, and then the rank-sum test is applied. Here we first expand the data using uncount() function in the tidyr package, then perform the Wilcoxon rank-sum test with wilcox.test():\n\ndf |&gt; \n  uncount(freq) |&gt; \n  wilcox.test(efficacy ~ group, data = _)\n\n#&gt; \n#&gt;  Wilcoxon rank sum test with continuity correction\n#&gt; \n#&gt; data:  efficacy by group\n#&gt; W = 4954.5, p-value = 0.5883\n#&gt; alternative hypothesis: true location shift is not equal to 0",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Nonparametric Tests</span>"
    ]
  },
  {
    "objectID": "nonparametric-test.html#kruskal-wallis-test",
    "href": "nonparametric-test.html#kruskal-wallis-test",
    "title": "12  Nonparametric Tests",
    "section": "12.4 Kruskal-Wallis test",
    "text": "12.4 Kruskal-Wallis test\nThe Kruskal-Wallis test is a non-parametric method used to compare more than two independent samples to determine if they come from the same distribution. It is an extension of the Mann-Whitney U test (or Wilcoxon rank-sum test) to more than two groups. The Kruskal-Wallis test is useful when the assumptions of one-way ANOVA (such as normality and homogeneity of variance) are not met.\nThe Kruskal-Wallis test statistic H is calculated using the following formula:\n\nH = \\frac{12}{N(N+1)} \\sum_{i=1}^k \\frac{R_i^2}{n_i} - 3(N+1)\n\nWhere N is the total number of observations, k is the number of groups, R_i is the sum of ranks for the i-th group, n_i is the number of observations in the i-th group.\n\n12.4.1 Raw data\n\nExample 5 \nUsing three types of drugs, A, B, and C, to kill snails, 200 live snails were used for each batch. After applying the drugs, the number of dead snails in each batch was counted, and the mortality rate (%) was calculated. The data file can be downloaded from the button below. Determine if there is a significant difference in the effectiveness of the three drugs in killing snails.\n\n\n  Download data \n\n\ndf &lt;- read_csv(\n  \"datasets/ex12-05.csv\",\n  col_types = list(\n    drug = col_factor(),\n    mortality = col_double()\n  )\n)\nstr(df)\n\n#&gt; spc_tbl_ [15 × 2] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n#&gt;  $ drug     : Factor w/ 3 levels \"1\",\"2\",\"3\": 1 1 1 1 1 2 2 2 2 2 ...\n#&gt;  $ mortality: num [1:15] 32.5 35.5 40.5 46 49 16 20.5 22.5 29 36 ...\n#&gt;  - attr(*, \"spec\")=\n#&gt;   .. cols(\n#&gt;   ..   drug = col_factor(levels = NULL, ordered = FALSE, include_na = FALSE),\n#&gt;   ..   mortality = col_double()\n#&gt;   .. )\n#&gt;  - attr(*, \"problems\")=&lt;externalptr&gt;\n\n\n\nkruskal.test(mortality ~ drug, data = df)\n\n#&gt; \n#&gt;  Kruskal-Wallis rank sum test\n#&gt; \n#&gt; data:  mortality by drug\n#&gt; Kruskal-Wallis chi-squared = 9.74, df = 2, p-value = 0.007673\n\n\n\n\n12.4.2 Frequency table data and ordinal data\n\nExample 6 \nUsing three types of drugs, A, B, and C, to kill snails, 200 live snails were used for each batch. After applying the drugs, the number of dead snails in each batch was counted, and the mortality rate (%) was calculated. The data file can be downloaded from the button below. Determine if there is a significant difference in the effectiveness of the three drugs in killing snails.\n\n\n  Download data \n\n\ndf &lt;- read_csv(\n  \"datasets/ex12-06.csv\",\n  col_types = list(\n    group = col_factor(), \n    response = col_integer(),\n    freq = col_integer()\n  )\n)\nstr(df)\n\n#&gt; spc_tbl_ [12 × 3] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n#&gt;  $ group   : Factor w/ 3 levels \"1\",\"2\",\"3\": 1 1 1 1 2 2 2 2 3 3 ...\n#&gt;  $ response: int [1:12] 1 2 3 4 1 2 3 4 1 2 ...\n#&gt;  $ freq    : int [1:12] 49 31 5 15 45 9 22 4 15 28 ...\n#&gt;  - attr(*, \"spec\")=\n#&gt;   .. cols(\n#&gt;   ..   group = col_factor(levels = NULL, ordered = FALSE, include_na = FALSE),\n#&gt;   ..   response = col_integer(),\n#&gt;   ..   freq = col_integer()\n#&gt;   .. )\n#&gt;  - attr(*, \"problems\")=&lt;externalptr&gt;\n\n\n\ndf |&gt; \n  uncount(freq) |&gt; \n  kruskal.test(response ~ group, data = _)\n\n#&gt; \n#&gt;  Kruskal-Wallis rank sum test\n#&gt; \n#&gt; data:  response by group\n#&gt; Kruskal-Wallis chi-squared = 20.458, df = 2, p-value = 3.612e-05\n\n\n\n\n12.4.3 Nemenyi test\nThe Nemenyi test is a post-hoc test used to compare multiple groups after a non-parametric test, such as the Kruskal-Wallis test has shown a significant difference among groups. It is specifically designed for multiple comparisons between groups to determine which pairs of groups differ significantly from each other.\n\nExample 7 \nA study is to compare the survival days of mice after being inoculated with three different strains of typhoid bacillus. The data can be downloaded from the button below. The question is whether there is a difference in the survival days of mice inoculated with these three different strains of typhoid bacillus.\n\n\n  Download data \n\n\ndf &lt;- read_csv(\n  \"datasets/ex12-07.csv\",\n  col_types = list(\n    group = col_factor(), \n    time = col_integer()\n  )\n)\nstr(df)\n\n#&gt; spc_tbl_ [30 × 2] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n#&gt;  $ group: Factor w/ 3 levels \"1\",\"2\",\"3\": 1 1 1 1 1 1 1 1 1 1 ...\n#&gt;  $ time : int [1:30] 2 2 2 3 4 4 4 5 7 7 ...\n#&gt;  - attr(*, \"spec\")=\n#&gt;   .. cols(\n#&gt;   ..   group = col_factor(levels = NULL, ordered = FALSE, include_na = FALSE),\n#&gt;   ..   time = col_integer()\n#&gt;   .. )\n#&gt;  - attr(*, \"problems\")=&lt;externalptr&gt;\n\n\n\nkruskal.test(time ~ group, data = df)\n\n#&gt; \n#&gt;  Kruskal-Wallis rank sum test\n#&gt; \n#&gt; data:  time by group\n#&gt; Kruskal-Wallis chi-squared = 9.9405, df = 2, p-value = 0.006941\n\n\n\nNemenyiTest(time ~ group, data =df)\n\n#&gt; \n#&gt;  Nemenyi's test of multiple comparisons for independent samples (tukey)  \n#&gt; \n#&gt;     mean.rank.diff   pval    \n#&gt; 2-1     10.3777778 0.0278 *  \n#&gt; 3-1     10.8727273 0.0131 *  \n#&gt; 3-2      0.4949495 0.9914    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Nonparametric Tests</span>"
    ]
  },
  {
    "objectID": "nonparametric-test.html#friedman-test",
    "href": "nonparametric-test.html#friedman-test",
    "title": "12  Nonparametric Tests",
    "section": "12.5 Friedman test",
    "text": "12.5 Friedman test\nThe Friedman test is a non-parametric statistical test used to detect differences in treatments across multiple test attempts. It’s particularly useful for comparing three or more paired groups when the data is not normally distributed. The test is an alternative to the repeated measures ANOVA and is often applied in cases where the same subjects are used for each treatment.\n\nExample 8 \nEight subjects were exposed to four different sound frequencies under the same conditions and their response rates (%) were measured. The data can be downloaded from the button below. Is there a difference in their response rates to the four different sound frequencies?\n\n\n  Download data \n\n\ndf &lt;- read_csv(\"datasets/ex12-08.csv\", show_col_types = F)\nstr(df)\n\n#&gt; spc_tbl_ [8 × 4] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n#&gt;  $ freq_a: num [1:8] 8.4 11.6 9.4 9.8 8.3 8.6 8.9 7.8\n#&gt;  $ freq_b: num [1:8] 9.6 12.7 9.1 8.7 8 9.8 9 8.2\n#&gt;  $ freq_c: num [1:8] 9.8 11.8 10.4 9.9 8.6 9.6 10.6 8.5\n#&gt;  $ freq_d: num [1:8] 11.7 12 9.8 12 8.6 10.6 11.4 10.8\n#&gt;  - attr(*, \"spec\")=\n#&gt;   .. cols(\n#&gt;   ..   freq_a = col_double(),\n#&gt;   ..   freq_b = col_double(),\n#&gt;   ..   freq_c = col_double(),\n#&gt;   ..   freq_d = col_double()\n#&gt;   .. )\n#&gt;  - attr(*, \"problems\")=&lt;externalptr&gt;\n\n\nFirst, perform the Friedman test to determine if there are significant differences among groups using friedman.test() function.\n\ndf |&gt; \n  as.matrix() |&gt; \n  friedman.test()\n\n#&gt; \n#&gt;  Friedman rank sum test\n#&gt; \n#&gt; data:  as.matrix(df)\n#&gt; Friedman chi-squared = 15.152, df = 3, p-value = 0.001691\n\n\nHere is an alternative method based on the long format data.\n\ndf |&gt; \n  mutate(subject = c(1:8)) |&gt; \n  pivot_longer(\n    cols = contains(\"freq\"),\n    names_to = \"group\",\n    values_to = \"response\",\n    names_ptypes = list(group = factor())\n  ) |&gt; \n  friedman.test(response ~ group | subject, data = _)\n\n#&gt; \n#&gt;  Friedman rank sum test\n#&gt; \n#&gt; data:  response and group and subject\n#&gt; Friedman chi-squared = 15.152, df = 3, p-value = 0.001691\n\n\nAfter identifying significant differences with the Friedman test, you can use frdAllPairsNemenyiTest() to conduct pairwise comparisons. The test allows you to determine which pairs of groups are significantly different from each other, with multiple comparison corrections.\n\ndf |&gt; \n  mutate(subject = c(1:8)) |&gt; \n  pivot_longer(\n    cols = contains(\"freq\"),\n    names_to = \"group\",\n    values_to = \"response\",\n    names_ptypes = list(group = factor())\n  ) |&gt; \n  frdAllPairsNemenyiTest(response ~ group | subject, data = _) |&gt; \n  summary()\n#&gt; \n#&gt;  Pairwise comparisons using Nemenyi-Wilcoxon-Wilcox all-pairs test for a two-way balanced complete block design\n#&gt; data: response and group and subject\n#&gt; P value adjustment method: single-step\n#&gt; H0\n#&gt;                      q value  Pr(&gt;|q|)   \n#&gt; freq_b - freq_a == 0   1.369 0.7675424   \n#&gt; freq_c - freq_a == 0   3.423 0.0732221  .\n#&gt; freq_d - freq_a == 0   5.066 0.0019311 **\n#&gt; freq_c - freq_b == 0   2.054 0.4666088   \n#&gt; freq_d - freq_b == 0   3.697 0.0442915  *\n#&gt; freq_d - freq_c == 0   1.643 0.6509544\n#&gt; ---\n#&gt; Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Nonparametric Tests</span>"
    ]
  },
  {
    "objectID": "regression-correlation.html",
    "href": "regression-correlation.html",
    "title": "13  Bivariate regression and correlation",
    "section": "",
    "text": "13.1 Prerequisite\nRegression and correlation are two fundamental concepts in bivariate analysis, which involves examining the relationship between two variables. They provide different approaches to understanding how one variable is related to another.\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(patchwork)",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Bivariate regression and correlation</span>"
    ]
  },
  {
    "objectID": "regression-correlation.html#simple-linear-regression",
    "href": "regression-correlation.html#simple-linear-regression",
    "title": "13  Bivariate regression and correlation",
    "section": "13.2 Simple linear regression",
    "text": "13.2 Simple linear regression\n\n13.2.1 Key concepts\nSimple regression analysis involves modeling the relationship between a dependent variable and an independent variable. Generally, this relationship is formulated as a simple linear regression model:\n\nY = \\beta_0 + \\beta_1 X + \\epsilon\n\nwhere Y is the dependent (response) variable, X is the independent (predictor) variable, \\beta_0 is the intercept (value of Y when X is 0), \\beta_1 is the slope (the change in Y for each one-unit change in X), \\epsilon is the error term (difference between observed and predicted values).\nThe goal of regression analysis is to estimate the coefficients \\beta_0 and \\beta_1, fitting a linear equation to the data. It allows for predicting the value of Y based on a given value of X. Here is an example to illustrate the process of linear regression.\n\nExample 1 \nTo determine the relationship between a patient’s age and their systolic blood pressure, a researcher gathered data from a sample of patients, including their age and systolic blood pressure readings. The data can be downloaded from the button below, where age is the independent variable (X) and systolic blood pressure is the dependent variable (Y).\n\n\n  Download data \n\n\ndf &lt;- read_csv(\"datasets/ex13-01.csv\", show_col_types = F)\nstr(df)\n\n#&gt; spc_tbl_ [10 × 2] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n#&gt;  $ age: num [1:10] 25 30 45 50 60 70 80 20 65 73\n#&gt;  $ sbp: num [1:10] 120 126 130 139 145 150 160 116 142 148\n#&gt;  - attr(*, \"spec\")=\n#&gt;   .. cols(\n#&gt;   ..   age = col_double(),\n#&gt;   ..   sbp = col_double()\n#&gt;   .. )\n#&gt;  - attr(*, \"problems\")=&lt;externalptr&gt;\n\n\nA scatter plot in regression analysis visually represents the relationship between an independent variable X and a dependent variable Y. It helps in understanding whether the data fits a linear model, which is crucial for linear regression.\n\nggplot(df, aes(x = age, y =sbp)) +\n  geom_point(size = 1) + \n  geom_smooth(method = lm, formula = 'y ~ x', se = T) +\n  labs(x = \"Age\", y = \"Systolic Blood Pressure\") +\n  theme(\n    axis.title.x = element_text(size = 9),\n    axis.title.y = element_text(size = 9)\n  )\n\n\n\n\n\n\n\n\nEach point represents a data pair, showing the relationship between X and Y. The line added fits the data using least squares linear regression. The shaded area around the regression line represents the 95% confidence interval, indicating the uncertainty around the predicted regression line.\nIf the points cluster closely around the regression line, it indicates a strong linear relationship between the variables. A narrower confidence interval suggests more precise estimates for the regression line, while a wider interval implies greater uncertainty. This method provides a visual way to understand regression analysis and assess the fit of the linear model to the data.\n\n\n13.2.2 Parameter estimation\nThe slope \\beta_1 and intercept \\beta_0 are estimated using the least squares method, which minimize the sum of squared differences between observed and predicted values of Y. They can be calculated by the following derived formulas:\n\n\\beta_1 = \\frac{\\sum{(X_i - \\bar{X})(Y_i - \\bar{Y})}}{\\sum{(X_i - \\bar{X})^2}} \n\n\\beta_0 = \\bar{Y} - \\beta_1 \\bar{X}\n\nwhere \\bar{X} and \\bar{Y} are the mean values of X and Y , respectively, and X_i, Y_i are individual data points. Once you calculate the slope \\beta_1 and intercept \\beta_0 , you can plug them into the equation Y = \\beta_0 + \\beta_1 X to create your specific regression model.\nHere we use lm() function to fit a simple linear regression model to predict systolic blood pressure based on age.\n\nmodel &lt;- lm(sbp ~ age, data = df) \nsummary(model)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = sbp ~ age, data = df)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -4.2146 -2.5833  0.2063  2.4432  3.9962 \n#&gt; \n#&gt; Coefficients:\n#&gt;              Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept) 103.79438    2.65440   39.10 2.01e-10 ***\n#&gt; age           0.65262    0.04775   13.67 7.91e-07 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 3.047 on 8 degrees of freedom\n#&gt; Multiple R-squared:  0.9589, Adjusted R-squared:  0.9538 \n#&gt; F-statistic: 186.8 on 1 and 8 DF,  p-value: 7.907e-07\n\n\nThe output of summary(model) provides you the estimated coefficients for the intercept \\beta_0 and slope \\beta_1, which can be used to write the equation for your specific data. Based on the results above on this case, we can write the regression equation as:\n\nY = 103.79438 + 0.65262 \\ X\n\n\n\n13.2.3 Statistical inference\nStatistical inference in linear regression involves making conclusions about the population parameters based on the sample data. The goal is to test hypothesis and estimate confidence intervals to determine whether there is a significant relationship between the independent and dependent variables.\n\nHypothesis test\n\nTo assess whether there’s a significant linear relationship between X and Y, we perform a hypothesis test on the slope coefficient \\beta_1:\n\nH_0: \\beta_1 = 0, no relationship between X and Y.\nH_1: \\beta_1 \\neq 0, there is a relationship between X and Y.\n\nTo test the significance of the slope (\\beta_1), the t-statistic is calculated by:\n\nt = \\frac{\\hat{\\beta}_1}{\\text{SE}_{\\hat{\\beta}_1}}\n\nwhere \\hat{\\beta}_1 is point estimate of the true coefficient \\beta_1, \\text{SE}_{\\hat{\\beta}_1} is the standard error (\\text{SE}) of the slope \\hat{\\beta}_1 .\nThe F-statistic is calculated as:\n\nF = \\frac{\\text{SSR} / 1}{\\text{SSE} / (n - 2)}\n\nwhere SSR (sum of squares due to regression) is the variation in the dependent variable explained by the predictor, SSE (sum of squares of errors) is the variation in the dependent variable that is not explained by the model, n is the number of observations.\nThe output of summary(model) will give you the t-statistic and p-value of the t-test, as well as the F-statistic and its associated p-value from the F-test. For this case, the t-statistic is 13.67, and its p-value is 7.91e\\text{-}07. The F-statistic is 186.8, its p-value is 7.907e\\text{-}07. Both test methods indicate a statistically significant relationship between the independent and dependent variable.\n\nConfidence interval\n\nA confidence interval provides a range of values within which the true population parameter (\\beta_1) is likely to lie with a confidence level \\alpha:\n\n\\hat{\\beta}_1 \\pm t_{\\alpha/2} \\ \\text{SE}(\\hat{\\beta}_1)\n\nwhere t_{\\alpha/2} is the critical value from the t-distribution.\nYou can use the confint() function to calculate the confidence interval of the regression coefficient.\n\nconfint(model)\n\n#&gt;                  2.5 %      97.5 %\n#&gt; (Intercept) 97.6733240 109.9154372\n#&gt; age          0.5425101   0.7627262\n\n\nIf the confidence interval for the slope does not contain 0, it suggests that the relationship between X and Y is statistically significant. Here the confidence interval for age is [0.6328419, 0.8334338], not contain 0, the conclusion is consistent with the hypothesis test.\nStatistical inference allows us to generalize the findings from our sample to the larger population and make conclusions about the strength and significance of relationships in linear regression.\n\n\n13.2.4 Coefficient of determination\nThe coefficient of determination, commonly denoted as R^2 , is a statistical measure used in the context of regression analysis to assess the proportion of variance in the dependent variable that is predictable from the independent variable. In simple terms, it indicates the proportion of variance in Y explained by X. A value close to 1 indicates a strong linear relationship, while a value near 0 suggests a weak relationship.\nIn the case above, the R^2 value of 0.9589 indicates that 95.89% of the variance in systolic blood pressure is explained by the patient’s age.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Bivariate regression and correlation</span>"
    ]
  },
  {
    "objectID": "regression-correlation.html#linear-correlation",
    "href": "regression-correlation.html#linear-correlation",
    "title": "13  Bivariate regression and correlation",
    "section": "13.3 Linear correlation",
    "text": "13.3 Linear correlation\nLinear correlation measures the strength and direction of the linear relationship between two continuous variables. The most common measure of linear correlation is the Pearson correlation coefficient, denoted by r, which ranges from -1 to +1.\n\n13.3.1 Pearson correlation coefficient\nThe Pearson correlation coefficient is calculated as:\n\nr = \\frac{\\sum{(X_i - \\bar{X})(Y_i - \\bar{Y})}}{\\sqrt{\\sum{(X_i - \\bar{X})^2} \\sum{(Y_i - \\bar{Y})^2}}}\n\nwhere X_i and Y_i are the individual data points of the two variables, \\bar{X} and \\bar{Y} are the means of the two variables.\nLinear correlation, particularly using the Pearson correlation coefficient, helps quantify the strength and direction of a linear relationship between two continuous variables. Here are some plots with different values of r.\n\nset.seed(200)\n\n# Generating data, given sample size and r\ngenerate_data &lt;- function(n, r) {\n  x &lt;- rnorm(n)\n  y &lt;- r * x + sqrt(1 - r^2) * rnorm(n)\n  tibble(x = x, y = y)\n}\n\nn &lt;- 200\nr_values &lt;- c(1, 0.9, 0.6, -1, -0.9, -0.6)\n\nlapply(\n  r_values, \n  \\(r.value) {\n    ggplot(generate_data(n, r.value), aes(x = x, y = y)) +\n    geom_point(size = 0.2, color = if_else(r.value &gt; 0, \"blue\", \"#A039A0\")) +\n    ggtitle(paste(\"r = \", r.value))\n  }\n) |&gt; \n  wrap_plots(ncol = 3)\n\n\n\n\n\n\n\n\nBefore conducting correlation and regression analysis, it is crucial to plot a scatterplot. A scatterplot helps to visually examine the relationship between two variables, validate assumptions for analysis, determine if a linear relationship exists, and identify any outliers. The dataset of Anscombe’s quartet was created by the statistician Francis Anscombe to illustrate the importance of graphing data before analyzing it. This dataset that consists of four sets of data with nearly identical simple descriptive statistics (mean, variance, correlation coefficient, etc.), but they have very different distributions and visual characteristics, as shown in Figure 13.1 .\n\ndata(anscombe)\nanscombe_long &lt;- anscombe |&gt; \n  pivot_longer(\n    cols = everything(),\n    cols_vary = \"slowest\",\n    names_to = c(\".value\", \"data\"),\n    names_pattern = \"(.)(.)\"\n  )\n\n# Plot all four datasets\nggplot(anscombe_long, aes(x = x, y = y)) +\n  geom_point(size = 1.2) +\n  geom_smooth(method = 'lm', formula = 'y ~ x', se = F, linewidth = 0.6) +\n  facet_wrap(~ data, scales = \"free\") \n\n\n\n\n\n\n\nFigure 13.1: Anscombe’s Quartet with same r but different distributions\n\n\n\n\n\n\nExample 2 \nAssume a dataset where we have collected BMI and cholesterol readings for a group of patients. Let’s look at the correlation between BMI and cholesterol levels in a this study.\n\n\n  Download data \n\n\ndf &lt;- read_csv(\"datasets/ex13-02.csv\", show_col_types = F)\nstr(df)\n\n#&gt; spc_tbl_ [10 × 2] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n#&gt;  $ bmi : num [1:10] 22.1 27.3 30.5 25 28.2 32.1 21.7 26.9 29 33.2\n#&gt;  $ chol: num [1:10] 180 195 210 185 200 220 175 190 205 215\n#&gt;  - attr(*, \"spec\")=\n#&gt;   .. cols(\n#&gt;   ..   bmi = col_double(),\n#&gt;   ..   chol = col_double()\n#&gt;   .. )\n#&gt;  - attr(*, \"problems\")=&lt;externalptr&gt;\n\n\n\nggplot(df, aes(bmi, chol)) +\n  geom_point(size = 1) + \n  geom_smooth(method = lm, formula = 'y ~ x', se = T) +\n  labs(\n    x = \"BMI\",\n    y = \"Cholesterol\"\n  ) +\n  theme(\n    axis.title.x = element_text(size = 9),\n    axis.title.y = element_text(size = 9)\n  )\n\n\n\n\n\n\n\n\nHere we use cor() function to calculate the correlation coefficient:\n\nwith(df, cor(bmi, chol, method = \"pearson\")) \n\n#&gt; [1] 0.9769267\n\n\n\n\n13.3.2 Statistical inference\nIn linear correlation, we not only calculate the correlation coefficient but also test its significance, create confidence intervals, and interpret the result in the context of the data.\n\nHypothesis test\n\nTo make statistical inferences about the correlation, we test the following hypotheses:\n\nH_0 : There is no linear relationship between the two variables (ρ = 0).\nH_1 : There is a linear relationship between the two variables (ρ ≠ 0 for a two-tailed test, ρ &gt; 0 or ρ &lt; 0 for a one-tailed test).\n\nCalculate the t-statistic by:\n\nt = \\frac{r \\sqrt{n - 2}}{\\sqrt{1 - r^2}}\n\n\nConfidence interval\n\nThe confidence interval provides a range of values that likely contain the population correlation ρ. The formula for the confidence interval of the correlation coefficient depends on the Fisher transformation. First apply the Fisher transformation to r:\n\nz = \\frac{1}{2} \\ln \\left( \\frac{1 + r}{1 - r} \\right)\n\nThen construct the confidence interval for z:\n\nz \\pm z_{\\alpha/2} \\frac{1}{\\sqrt{n - 3}}\n\nFinally transform it back to the correlation scale:\n\nr = \\frac{e^{2z - 1}}{e^{2z + 1}}\n\nThe cor.test() function is used to test the association between two numeric variables, and when you specify method = \"pearson\", it performs a Pearson correlation test to measure the strength and direction of the linear relationship between them.\n\ncor.test(~ bmi + chol, data = df, method = \"pearson\")\n\n#&gt; \n#&gt;  Pearson's product-moment correlation\n#&gt; \n#&gt; data:  bmi and chol\n#&gt; t = 12.938, df = 8, p-value = 1.206e-06\n#&gt; alternative hypothesis: true correlation is not equal to 0\n#&gt; 95 percent confidence interval:\n#&gt;  0.9023104 0.9947088\n#&gt; sample estimates:\n#&gt;       cor \n#&gt; 0.9769267\n\n\nThis result indicates a strong positive correlation (r = 0.9769267), with a p-value of 1.206e\\text{-}06, which suggests the correlation is statistically significant. The t-statistic and 95% confidence interval is provided as well.\n\n\n13.3.3 Some considerations\nTo ensure the estimate and inference of linear regression model are valid, the following assumptions should be met:\n\nLinearity: The relationship between X and Y must be linear.\nIndependence: Observations must be independent of each other.\nHomoscedasticity: Constant variance of residuals across all levels of X.\nNormality of Residuals: The residuals (errors) should be approximately normally distributed.\n\nTo validly use the Pearson correlation coefficient and its statistical tests, the following assumptions should be met:\n\nLinearity: The relationship between the two variables must be linear.\nNormality: The two variables should be approximately normally distributed.\nHomoscedasticity: The variability in one variable should be the same at all levels of the other variable.\n\nResidual Plots in Regression\nUsing a residual plot is a key step in examining whether the data meets the assumptions required for a linear regression model. Residual plots display the residuals on the vertical axis and the predicted values or independent variable on the horizontal axis. Here is the residual plot on the basis of the data from Example 1.\n\ndf &lt;- read_csv(\"datasets/ex13-01.csv\", show_col_types = F)\n# Fit a linear regression model\nmodel &lt;- lm(sbp ~ age, data = df)\n\n# Create a residual plot using ggplot2\nggplot(df, aes(x = fitted(model), y = resid(model))) +\n  geom_point() +\n  geom_hline(yintercept = 0, color = \"red\") +\n  xlab(\"Fitted Values\") +\n  ylab(\"Residuals\")\n\n\n\n\n\n\n\n\nIf the residuals appear randomly scattered around zero with no clear patterns, the assumptions of linearity, homoscedasticity, and independence are likely satisfied. If any patterns or issues are identified, you may need to consider transforming variables or using a different model to better fit the data.\nIf these assumptions are violated, non-parametric alternatives such as Spearman’s rank correlation can be used.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Bivariate regression and correlation</span>"
    ]
  },
  {
    "objectID": "regression-correlation.html#rank-correlation",
    "href": "regression-correlation.html#rank-correlation",
    "title": "13  Bivariate regression and correlation",
    "section": "13.4 Rank correlation",
    "text": "13.4 Rank correlation\nRank correlation is a measure of the relationship between two variables based on the rank ordering of the data, rather than the raw data values. It assesses how well the relationship between two variables can be described by a monotonic function, meaning as one variable increases, the other tends to increase (positive correlation) or decrease (negative correlation). Rank correlation is especially useful when dealing with ordinal or non-normally distributed data, such as in patient ratings, clinical scores.\n\n13.4.1 Spearman’s rank correlation\nSpearman’s rank correlation coefficient r_s measures the strength and direction of the association between two ranked variables. It is a non-parametric measure, meaning it does not assume a linear relationship or normally distributed data.\n\nr_s = 1 - \\frac{6 \\sum d_i^2}{n(n^2 - 1)}\n\nwhere d_i is the difference between the ranks of corresponding values and n is the sample size.\n\n\n13.4.2 Spearman’s r_s for tied ranks\nWhen there are tied ranks, the denominator of the formula needs to be adjusted because the calculation assumes unique ranks. The correction ensures the ranks are properly weighted to reflect the presence of ties, improving the reliability of the coefficient. The correction involves calculating a factor that accounts for the groups of tied values.\nThe corrected formula for Spearman’s r_s with ties is:\n\nr_s = \\frac{n(n^2 - 1) - 6 \\sum d_i^2 - T_x - T_y}{\\sqrt{(n(n^2 - 1) - T_x)(n(n^2 - 1) - T_y)}}\n\nwhere T_X and T_Y are the tie correction factors for each variable, calculated as follows:\n\nT = \\sum (t_j^3 - t_j) / 12\n\nwhere t_j is the number of tied values in the j\\text{-}th group of ties. This sum is calculated over all groups of tied ranks.\n\nExample 3 \nConsider a study where researchers want to see the correlation between pain score and mobility score in patients with arthritis. They collected the scores from 15 patients, both are ranked from 1 to 10. The data can be accessed from the button below.\n\n\n  Download data \n\n\ndf &lt;- read_csv(\n  file = \"datasets/ex13-03.csv\",\n  col_types = list(\n    pain_score = col_integer(),\n    mobi_score = col_integer()\n  )\n)\nstr(df)\n\n#&gt; spc_tbl_ [15 × 2] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n#&gt;  $ pain_score: int [1:15] 3 5 6 8 7 9 2 4 5 6 ...\n#&gt;  $ mobi_score: int [1:15] 7 6 5 4 3 2 8 7 6 5 ...\n#&gt;  - attr(*, \"spec\")=\n#&gt;   .. cols(\n#&gt;   ..   pain_score = col_integer(),\n#&gt;   ..   mobi_score = col_integer()\n#&gt;   .. )\n#&gt;  - attr(*, \"problems\")=&lt;externalptr&gt;\n\n\nThe cor() function can be used to estimate Spearman’s rank correlation coefficient.\n\nwith(df, cor(pain_score, mobi_score, method = \"spearman\"))\n\n#&gt; [1] -0.952515\n\n\nYou can also use the cor.test() function to perform Spearman’s rank correlation test with method = \"spearman\".\n\ncor.test(~ pain_score + mobi_score, data = df, method = \"spearman\")\n\n#&gt; Warning in cor.test.default(x = mf[[1L]], y = mf[[2L]], ...): Cannot compute\n#&gt; exact p-value with ties\n\n\n#&gt; \n#&gt;  Spearman's rank correlation rho\n#&gt; \n#&gt; data:  pain_score and mobi_score\n#&gt; S = 1093.4, p-value = 4.378e-08\n#&gt; alternative hypothesis: true rho is not equal to 0\n#&gt; sample estimates:\n#&gt;       rho \n#&gt; -0.952515\n\n\nIn the output a warning occurs, noting that the data has ties. To correct for ties, you can set the argument exact = FALSE, which ensures the function handles ties and applies the necessary corrections.\n\ncor.test(~ pain_score + mobi_score, data = df, method = \"spearman\", exact = F)\n\n#&gt; \n#&gt;  Spearman's rank correlation rho\n#&gt; \n#&gt; data:  pain_score and mobi_score\n#&gt; S = 1093.4, p-value = 4.378e-08\n#&gt; alternative hypothesis: true rho is not equal to 0\n#&gt; sample estimates:\n#&gt;       rho \n#&gt; -0.952515\n\n\nThe result shows a strong monotonically negative relationship between the pain_score and mobi_score (r_s = -0.952515). The p-value is smaller than significance level 0.05, reject the null hypothesis and conclude that there is a significant monotonic relationship between the two variables. The test statistic used in determining the p-value is S = 1093.4.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Bivariate regression and correlation</span>"
    ]
  },
  {
    "objectID": "regression-correlation.html#curve-fitting",
    "href": "regression-correlation.html#curve-fitting",
    "title": "13  Bivariate regression and correlation",
    "section": "13.5 Curve fitting",
    "text": "13.5 Curve fitting\nCurve fitting is the process of finding a curve that best fits a given set of data points. It is commonly used when the relationship between the independent variable X and dependent variable Y is not linear, and a more complex model is needed to describe the data. Curve fitting can be used in many fields, including biology and the medical sciences.\n\n13.5.1 Types of curve fitting models\n\nLinear fit: A straight line that best describes the data. The equation is typically of the form y = a + bX . Example: Simple linear regression.\nPolynomial fit: A polynomial function is used to fit the data. The equation might look like Y = aX^2 + bX + c or higher-degree polynomials. Example: Quadratic, cubic, or higher-order polynomial fits.\nExponential fit: For exponential growth or decay, the model is typically of the form Y = a \\cdot e^{bx}. Example: Growth of populations or the decay of radioactive material.\nLogarithmic fit: A logarithmic function is used, of the form Y = a \\cdot \\ln(X) + b, where \\ln(X) is the natural logarithm of X.\nPower-Law fit: This follows the form y = ax^b, where the relationship between variables is a power function.\nSigmoidal fit: Sigmoid curves are often used for biological processes that show an S-shaped response, like population growth models. A common form is Y = \\frac{a}{1 + e^{-bX}}.\n\n\nExample 4 \nAssume we have a data from a study that tests different dosages of a drug and the corresponding effect on patients’ blood pressure reduction. The data can be downloaded from the button below, where dose represents the drug dose (mg) and response represents the percentage of blood pressure reduction. Let’s model the relationship between drug dosage and the body’s response.\n\n\n  Download data \n\n\n\n13.5.2 Steps for curve fitting\n\nData preparation\nEnsure you have collected data, typically with an independent variable X and a dependent variable Y.\n\ndf &lt;- read_csv(file = \"datasets/ex13-04.csv\", show_col_types = F)\nstr(df)\n\n#&gt; spc_tbl_ [10 × 2] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n#&gt;  $ dose    : num [1:10] 10 20 30 40 50 60 70 80 90 100\n#&gt;  $ response: num [1:10] 5 12 20 32 45 55 65 72 78 82\n#&gt;  - attr(*, \"spec\")=\n#&gt;   .. cols(\n#&gt;   ..   dose = col_double(),\n#&gt;   ..   response = col_double()\n#&gt;   .. )\n#&gt;  - attr(*, \"problems\")=&lt;externalptr&gt;\n\n\n\n\n\nData visualization\nUse a scatter plot to visualize the raw data and check for trends or outliers.\n\nggplot(df, aes(x = dose, y = response)) +\n  geom_point() + \n  geom_smooth(method = 'loess', formula = 'y ~ x', se = F)\n\n\n\n\n\n\n\n\nThe plot shows a S-shaped curve on the relationship between the two variables.\nChoose a model\nBased on the plot and theoretical background, the Logistic model Y = \\frac{1}{1 + e^{a + b\\ln(X)}} is adopted.\nPerform the Fitting\nUse statistical software to perform the fitting. You can use lm() for linear regression or nls() for nonlinear fitting. For nonlinear models, you usually need to provide initial parameter guesses. Parameters can be set based on data trends or prior experience. When using nonlinear fitting functions (e.g., nls()), initial parameter guesses are critical.\n\nmodel &lt;- nls(\n  response ~ (120 / (1 + exp(a + b * log(dose)))) , \n  data = df, \n  start = list(a = 8, b = -1)\n) \nsummary(model)\n\n#&gt; \n#&gt; Formula: response ~ (120/(1 + exp(a + b * log(dose))))\n#&gt; \n#&gt; Parameters:\n#&gt;   Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; a  8.12807    0.18750   43.35 8.84e-11 ***\n#&gt; b -1.94301    0.04505  -43.13 9.22e-11 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 1.262 on 8 degrees of freedom\n#&gt; \n#&gt; Number of iterations to convergence: 7 \n#&gt; Achieved convergence tolerance: 5.276e-06\n\n\nFrom the result we can write the fitted equation as:\n\nY = \\frac{120}{1 + e^{8.128-1.943\\ln(X)}}\n\nCheck the fit\nModel diagnostics: Use residual plots to check if the residuals meet assumptions such as independence and normality. Check the residuals to ensure that they are randomly scattered and do not show patterns that suggest a poor fit.\n\n# Create a residual plot using ggplot2\nggplot(df, aes(x = fitted(model), y = resid(model))) +\n  geom_point(size = 1) +\n  geom_hline(yintercept = 0, color = \"red\") +\n  xlab(\"Fitted Values\") +\n  ylab(\"Residuals\")\n\n\n\n\n\n\n\n\nGoodness of fit test: Use R-squared or adjusted R-squared to evaluate how well the linear model fits the data. For nonlinear models, you may use the sum of squared residuals or AIC/BIC values.\n\nAIC(model)\n\n#&gt; [1] 36.7957\n\n\nVisual inspection: Always plot the data with the fitted curve to visually check if the fit seems reasonable.\n\nggplot(df, aes(x = dose, y = response)) +\n  geom_point(size = 1) +\n  geom_line(aes(y = fitted(model)), color = \"#C0035490\", linewidth = 0.8) +\n  labs(title = \"NLS Model Fit\", x = \"Dose\", y = \"Response\")\n\n\n\n\n\n\n\n\nThis allows you to visualize both the raw data (points) and the fitted model (line) on the same plot.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Bivariate regression and correlation</span>"
    ]
  },
  {
    "objectID": "multifactor-anova.html",
    "href": "multifactor-anova.html",
    "title": "14  Multifactor ANOVA",
    "section": "",
    "text": "14.1 Prerequisite\nMultifactor ANOVA (also known as factorial ANOVA) is a statistical method used to examine the effects of two or more factors (independent variables) on a dependent variable. It allows for the analysis of the main effects of each factor as well as the interaction effects between factors. This method helps in understanding how different factors, together or separately, influence the response variable.\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(DoE.base)\nlibrary(nlme)",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Multifactor ANOVA</span>"
    ]
  },
  {
    "objectID": "multifactor-anova.html#factorial-design",
    "href": "multifactor-anova.html#factorial-design",
    "title": "14  Multifactor ANOVA",
    "section": "14.2 Factorial design",
    "text": "14.2 Factorial design\nFactorial design is an experimental setup that involves two or more factors, each with multiple levels, and tests all possible combinations of these factors. This design is widely used in various fields like medicine, biology, and psychology to analyze the simultaneous influence of multiple factors on an outcome.\n\n14.2.1 2x2 two-factor factorial design\nA 2x2 two-factor factorial design is a type of experimental design where two independent factors are studied, and each factor has two levels. The design tests both the main effects of each factor and the interaction effect between the two factors.\n\nExample 1 \nA experiment involved randomly dividing 20 rabbits into 4 groups, with 5 rabbits in each group, to perform a suture test after nerve injury. The treatments consisted of a combination of two factors. Factor A was the suture method, which had two levels: a1 and a2. Factor B was the time after suturing, which also had two levels: b1 and b2. The experimental results were the axon transmission rate (%) after nerve suturing for each rabbit, as shown in Table 14.1 . To compare the effects of different suture methods and post-suture times on axon transmission rates.\n\n\n  Download data \n\n\ndf &lt;- read_csv(\n  file = \"datasets/ex14-01.csv\",\n  col_types = list(\n    method = col_factor(),\n    time   = col_factor(),\n    response = col_double()\n  )\n) \n\n\n\n\n\nTable 14.1: The axon transmission rate (%) after nerve suturing for each rabbit\n\n\n\n\n\n\na1_1\na1_2\na2_1\na2_2\n\n\n\n\n10\n30\n10\n50\n\n\n10\n30\n20\n50\n\n\n40\n70\n30\n70\n\n\n50\n60\n50\n60\n\n\n10\n30\n30\n30\n\n\n\n\n\n\n\n\nYou can use aov() function to analyze this design and test the main effects and interaction. The code would look like this:\n\nmodel &lt;- aov(response ~ method * time, data = df)\nsummary(model)\n\n#&gt;             Df Sum Sq Mean Sq F value Pr(&gt;F)  \n#&gt; method       1    180     180   0.600 0.4499  \n#&gt; time         1   2420    2420   8.067 0.0118 *\n#&gt; method:time  1     20      20   0.067 0.7995  \n#&gt; Residuals   16   4800     300                 \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nHere, method * time tests both the main effects and the interaction effect.\nMain effect\nThe main effect refers to the impact of one independent variable on the dependent variable, ignoring the other variables. In a factorial design with multiple independent variables, a main effect is the effect of one factor averaged across the levels of the other factors. It examines the effect of each independent variable in isolation.\n\nmethod\nThe p-value for method is 0.4499, which is much greater than 0.05. This suggests that the different methods do not have a statistically significant effect on the response variable. The F value of 0.6 further indicates that the variation in the response explained by the method is weak, and there is no significant difference between the methods in terms of their impact on the outcome.\ntime\nThe p-value for time is 0.0118, which is less than 0.05, indicating that time has a statistically significant effect on the response. The F value of 8.067 supports this conclusion, showing that the response variable changes significantly across different time points.\n\nInteraction effect\nAn interaction occurs when the effect of one independent variable depends on the level of another independent variable. This means that the effect of one factor is not consistent across the levels of the other factor. An interaction suggests that the variables are not independent in their effects on the dependent variable.\n\nmethod : time\nThe p-value for the interaction effect is 0.7995, which is much greater than 0.05, indicating that there is no significant interaction between method and time. This means that the effect of the method on the response does not depend on the time. In other words, the impact of the methods is consistent across different times.\n\nVisualizing the difference\nA common way to visualize main and interaction effects is through interaction plots. If there’s no interaction, the lines for method 1 and method 2 in the graph of axon transmission rate against factor B will be parallel. This means the difference between the methods is the same regardless of factor B. If there is an interaction, the lines will cross or diverge, indicating that the effect of the method differs depending on the time after suturing.\n\n# Draw an interaction plot\nggplot(df, aes(x = time, y = response, color = method, group = method)) +\n  geom_line(stat = \"summary\", fun = \"mean\") +\n  geom_point(stat = \"summary\", fun = \"mean\")\n\n\n\n\n\n\n\n\n\n\n14.2.2 IxJ two-factor factorial design\nIn an IxJ two-factor factorial design, we have two factors (independent variables), each with a number of levels: Factor A has I levels (e.g., different treatments, conditions, or groups). Factor B has J levels (e.g., time points, environments, or conditions).\n\nExample 2 \nTo observe the analgesic effect of combining painkillers A and B during childbirth, painkiller A is administered in three different doses: 1.0 mg, 2.5 mg, and 5.0 mg, painkiller B is also administered in three different doses: 5 μg, 15 μg, and 30 μg. A total of 27 women in labor are randomly divided into 9 groups, with 3 women in each group. The analgesic time for each woman during labor is recorded, and the results are shown in Table 14.2 . Analyze the combined analgesic effect of drug A and drug B.\n\n\n  Download data \n\n\ndf &lt;- read_csv(\n  file = \"datasets/ex14-02.csv\",\n  col_types = list(\n    drugA = col_factor(),\n    drugB = col_factor(),\n    time = col_double()\n  )\n) \n\n\n\n\n\nTable 14.2: The analgesic time for each woman during labor\n\n\n\n\n\n\ndrugA\ndrugB5\ndrugB15\ndrugB30\n\n\n\n\n1\n105\n115\n75\n\n\n1\n80\n105\n95\n\n\n1\n65\n80\n85\n\n\n2.5\n75\n125\n135\n\n\n2.5\n115\n130\n120\n\n\n2.5\n80\n90\n150\n\n\n5\n85\n65\n180\n\n\n5\n120\n120\n190\n\n\n5\n125\n100\n160\n\n\n\n\n\n\n\n\nThe ANOVA model would look like this:\n\nmodel &lt;- aov(time ~ drugA * drugB, data = df)\nsummary(model)\n\n#&gt;             Df Sum Sq Mean Sq F value  Pr(&gt;F)   \n#&gt; drugA        2   6572    3286   8.470 0.00256 **\n#&gt; drugB        2   7022    3511   9.050 0.00190 **\n#&gt; drugA:drugB  4   7872    1968   5.073 0.00647 **\n#&gt; Residuals   18   6983     388                   \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThis output represents the results of a two-way ANOVA model analyzing the effects of two drugs (A and B) and their interaction on the response variable (in this case, time).\nMain effect of drugA\nThe F value is 8.470, p-value is 0.00256. Since this p-value is less than 0.01, the different levels of drug A significantly affect the response variable.\nMain effect of drugB\nThe F value is 9.050, p-value is 0.00190. This is also a significant result (p &lt; 0.01), meaning the different levels of drug B significantly affect the response variable.\nInteraction effect of drugA:drugB\nTHe F value is 5.073, p-value is 0.00647. This p-value is significant (p &lt; 0.01), indicating that there is a significant interaction between drug A and B. The effect of drug A on time depends on the level of drug B and vice versa. This means that the combination of the two drugs affects the outcome in a way that cannot be explained by the main effects alone.\n\n\n14.2.3 IxJxK three-factor factorial design\nFor an IxJxK design, where the factors have I , J , and K levels respectively, performing an ANOVA will help evaluate the significance of the factors and their interactions on the dependent variable.\nSuppose we want to investigate the effect of three factors on reducing blood pressure in patients. The three factors are:\n\nMedication type (A): 2 levels (drug 1, drug 2)\nExercise regimen (B): 2 levels (low intensity, high intensity)\nDiet plan (C): 3 levels (low salt, mediterranean, high fiber)\n\nThis gives us a 2 \\times 2 \\times 3 factorial design, resulting in 12 treatment combinations. The response is blood pressure reduction after 12 weeks of treatment. You can use the aov() or lm() functions to conduct a three-way ANOVA.\n\n# Create sample data\nset.seed(200)\ndf &lt;- data.frame(\n  a = factor(rep(c(\"drug 1\", \"drug 2\"), each = 18)),\n  b = factor(rep(rep(c(\"low\", \"high\"), each = 9), 2)),\n  c = factor(rep(c(\"low salt\", \"mediterranean\", \"high fiber\"), times = 6)),\n  y = c(6, 9, 7, 12, 11, 9, 12, 9, 11, 10, 12, 10, \n        13, 14, 12, 11, 12, 14, 6, 7, 5, 9, 10, 8,\n        11, 12, 10, 10, 11, 9, 12, 13, 11, 13, 12, 11)\n)\n\n# Perform the three-way ANOVA\naov(y ~  a * b * c, data = df) |&gt; \n  summary()\n\n#&gt;             Df Sum Sq Mean Sq F value   Pr(&gt;F)    \n#&gt; a            1   5.44    5.44   1.342 0.257999    \n#&gt; b            1  58.78   58.78  14.493 0.000857 ***\n#&gt; c            2   9.39    4.69   1.158 0.331193    \n#&gt; a:b          1   0.11    0.11   0.027 0.869920    \n#&gt; a:c          2   2.39    1.19   0.295 0.747542    \n#&gt; b:c          2   0.72    0.36   0.089 0.915109    \n#&gt; a:b:c        2   2.39    1.19   0.295 0.747542    \n#&gt; Residuals   24  97.33    4.06                     \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nlm(y ~  a * b * c, data = df) |&gt; \n  anova()\n\n#&gt; Analysis of Variance Table\n#&gt; \n#&gt; Response: y\n#&gt;           Df Sum Sq Mean Sq F value    Pr(&gt;F)    \n#&gt; a          1  5.444   5.444  1.3425 0.2579990    \n#&gt; b          1 58.778  58.778 14.4932 0.0008569 ***\n#&gt; c          2  9.389   4.694  1.1575 0.3311928    \n#&gt; a:b        1  0.111   0.111  0.0274 0.8699202    \n#&gt; a:c        2  2.389   1.194  0.2945 0.7475423    \n#&gt; b:c        2  0.722   0.361  0.0890 0.9151087    \n#&gt; a:b:c      2  2.389   1.194  0.2945 0.7475423    \n#&gt; Residuals 24 97.333   4.056                      \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Multifactor ANOVA</span>"
    ]
  },
  {
    "objectID": "multifactor-anova.html#orthogonal-design",
    "href": "multifactor-anova.html#orthogonal-design",
    "title": "14  Multifactor ANOVA",
    "section": "14.3 Orthogonal design",
    "text": "14.3 Orthogonal design\nFactorial design tests all possible combinations of levels across all factors. It allows for the study of the main effects and interactions between the factors. Orthogonal design is a more efficient form of experimental design that requires fewer experimental runs. It selects specific combinations of factor levels in such a way that each factor’s effect can be estimated independently (orthogonally) from the others, without testing all possible combinations. This reduces the number of experimental runs compared to a full factorial design. It is widely used in quality control, optimization experiments, and clinical research.\nOrthogonal designs are often used when:\n\nBudget or time constraints make full factorial designs impractical.\nThe focus is on estimating main effects and selected interactions, not all possible interactions.\nMultiple factors need to be studied simultaneously, but a full set of combinations is unnecessary.\n\n\n14.3.1 Orthogonal array\nOrthogonal array (OA) is a tool used in orthogonal design to arrange factors and their levels in a way that ensures the factors are orthogonal, meaning their effects can be estimated independently.\nOAs are typically represented as L_{n}(k^p), where n represents the number of experimental runs (or combinations), k is the number of levels for each factor, p is the number of factors. For example, L_8(2^7) refers to an experiment with 7 factors, each having 2 levels, and requiring 8 experimental runs.\nThe choice of an appropriate OA depends on the number of factors and their levels. For more complex experiments (i.e., many factors with multiple levels), larger orthogonal arrays like L_{16} or L_{18} might be suitable.\nIn R, you can use the DoE.base package to generate orthogonal arrays. Here’s an example of how to generate an L_8(2^7) array:\n\noa.design(\n  nruns = 8,\n  nfactors = 7, \n  nlevels = 2,\n  seed = 200\n)\n\n#&gt;   A B C D E F G\n#&gt; 1 2 1 2 1 1 2 2\n#&gt; 2 1 1 2 2 2 1 2\n#&gt; 3 1 2 2 1 2 2 1\n#&gt; 4 2 2 2 2 1 1 1\n#&gt; 5 1 1 1 1 1 1 1\n#&gt; 6 2 2 1 1 2 1 2\n#&gt; 7 1 2 1 2 1 2 2\n#&gt; 8 2 1 1 2 2 2 1\n#&gt; class=design, type= oa\n\n\nOr you can use the following code:\n\noa.design(ID = L8.2.7, seed = 200)\n\n#&gt;   A B C D E F G\n#&gt; 1 2 1 2 1 1 2 2\n#&gt; 2 1 1 2 2 2 1 2\n#&gt; 3 1 2 2 1 2 2 1\n#&gt; 4 2 2 2 2 1 1 1\n#&gt; 5 1 1 1 1 1 1 1\n#&gt; 6 2 2 1 1 2 1 2\n#&gt; 7 1 2 1 2 1 2 2\n#&gt; 8 2 1 1 2 2 2 1\n#&gt; class=design, type= oa\n\n\n\n\n14.3.2 Factor assignment\nIn an orthogonal design like the L_8(2^7) array, the assignment of factors to specific columns does matter to maintain the orthogonality of the design and ensure balanced and unbiased results.\nThe L_8(2^7) array consists of 7 columns, but for an experiment with 4 factors, you will only use 4 of these columns to represent the main effects of the factors. The remaining columns can either be used for first-order (two-factor) interactions or left unused.\nFor an L_8(2^7) orthogonal array, the factors are typically assigned to columns 1, 2, 4, and 7. These columns in the orthogonal array are mutually orthogonal to each other, meaning that they do not introduce confounding effects, which helps you estimate the main effects of each factor independently. Columns 3, 5, and 6 are usually reserved for analyzing interactions, because these columns are designed to accommodate two-factor interactions.\n\nExample 3 \nTo study the optimal conditions for egg-laying in female snails, an orthogonal experiment is designed. The experimental conditions included four factors, each with two levels. The interaction between temperature and oxygen content is considered. Ten female snails of the same age were raised in a 20 cm² mud box. The combinations of each factor and level for the 8 experiments, as well as the experimental results, are shown in Table 14.3 .\n\n\n  Download data \n\n\ndf &lt;- read_csv(\n  \"datasets/ex14-03.csv\",\n  col_types = list(\n    temp = col_factor(), oxygen = col_factor(),\n    water = col_factor(), ph = col_factor(),\n    n = col_integer())\n)\n\n\n\n\n\nTable 14.3: Orthogonal experiment for spawning conditions of snails\n\n\n\n\n\n\ntemp\noxygen\nwater\nph\nn\n\n\n\n\n1\n1\n1\n1\n86\n\n\n1\n1\n2\n2\n95\n\n\n1\n2\n1\n2\n91\n\n\n1\n2\n2\n1\n94\n\n\n2\n1\n1\n2\n91\n\n\n2\n1\n2\n1\n96\n\n\n2\n2\n1\n1\n83\n\n\n2\n2\n2\n2\n88\n\n\n\n\n\n\n\n\nUse the aov() function to perform ANOVA on the orthogonal design. You can model main effects and interaction terms if necessary.\n\naov(n ~ temp * oxygen + water + ph, data = df) |&gt; \n  summary()\n\n#&gt;             Df Sum Sq Mean Sq F value Pr(&gt;F)  \n#&gt; temp         1    8.0     8.0     3.2 0.2155  \n#&gt; oxygen       1   18.0    18.0     7.2 0.1153  \n#&gt; water        1   60.5    60.5    24.2 0.0389 *\n#&gt; ph           1    4.5     4.5     1.8 0.3118  \n#&gt; temp:oxygen  1   50.0    50.0    20.0 0.0465 *\n#&gt; Residuals    2    5.0     2.5                 \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThese results suggest that water and the interaction between temperature and oxygen have significant effects on the response variable, the other individual factors might not be as important for this particular response.",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Multifactor ANOVA</span>"
    ]
  },
  {
    "objectID": "multifactor-anova.html#nested-design",
    "href": "multifactor-anova.html#nested-design",
    "title": "14  Multifactor ANOVA",
    "section": "14.4 Nested design",
    "text": "14.4 Nested design\nA nested design is a type of experimental design where levels of one factor are nested within the levels of another factor. This is useful when the experimental units within a group are not comparable across groups but are comparable within groups. For example, in a medical study, hospitals might be a factor, and patients might be nested within hospitals because patients from different hospitals are not directly comparable.\nIn a nested design, factors are hierarchical. Below is a two-level nested design:\n\nHospitals (A) as a primary factor.\nDoctors (B) nested within each hospital.\nPatients’ satisfaction score as the response variable.\n\n\nExample 4 \nTo study the short-term effects of different protein feeds at different feeding amounts on the growth in body length of rats, a nested design was adopted due to the inconsistent feeding amounts required to achieve the same protein content for each protein feed. The type of feed is treated as the primary experimental factor (a), and the feeding amount as the secondary experimental factor (b) nested within the type of feed. Three rats are fed per trial, and the net increase in rat body length is the response variable. Perform an ANOVA analysis based on this data.\n\n\n  Download data \n\n\ndf &lt;- read_csv(\"datasets/ex14-04.csv\",\n  col_types = list(a = col_factor(), b = col_factor()))\n\nHere’s how you can handle such a nested design and analyze it using ANOVA.\n\naov(response ~ a + a:b, data = df) |&gt; \n  summary()\n\n#&gt;             Df Sum Sq Mean Sq F value   Pr(&gt;F)    \n#&gt; a            2 27.630  13.815  96.405 2.41e-10 ***\n#&gt; a:b          6  4.107   0.684   4.776  0.00445 ** \n#&gt; Residuals   18  2.579   0.143                     \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\naov(response ~ a/b, data = df) |&gt; \n  summary()\n\n#&gt;             Df Sum Sq Mean Sq F value   Pr(&gt;F)    \n#&gt; a            2 27.630  13.815  96.405 2.41e-10 ***\n#&gt; a:b          6  4.107   0.684   4.776  0.00445 ** \n#&gt; Residuals   18  2.579   0.143                     \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe results show that different proteins quality diet affects the growth of body length in rats. For the same protein feed, different feeding mice also had an effect on the length growth of rats.\nVisualizing the nested design\n\nggplot(df, aes(x = a, y = response, fill = b)) +\n  geom_boxplot() +\n  labs(x = \"Protein feeds\", y = \"Increase in body length\")",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Multifactor ANOVA</span>"
    ]
  },
  {
    "objectID": "multifactor-anova.html#split-plot-design",
    "href": "multifactor-anova.html#split-plot-design",
    "title": "14  Multifactor ANOVA",
    "section": "14.5 Split-plot design",
    "text": "14.5 Split-plot design\nA split-plot design is used when there are two levels of randomization or when some factors are easier to control than others. This is common in medical studies where some treatments are applied to larger groups (whole plots), and others are applied to subgroups (subplots or split plots) within those larger groups.\nFor example, if you want to study the effect of two whole-plot factors (e.g., different exercise regimens) on patients, but you also want to study the effect of a subplot factor (e.g., different dosages of a medication) within each exercise regimen. The design would look like:\n\nWhole-plot factor: Exercise regimen (e.g., low-intensity vs. high-intensity).\nSubplot factor: Medication dosage (e.g., 10 mg vs. 20 mg) applied within each exercise group.\n\nThis allows for efficient testing of interactions between exercise regimens and medication dosages while considering the hierarchical structure of the treatments.\n\nExample 5 \nTo study the protective effect of systemic injection of antitoxin on skin damage, 10 rabbits were randomly divided into two groups. One group was injected with antitoxin, while the other group was injected with saline as a control. After grouping, two areas on each rabbit were selected, and low and high concentrations of toxin were randomly injected into these areas. The response variable is the diameter of the skin damage (mm). Perform an ANOVA analysis based on this data.\n\n\n  Download data \n\n\ndf &lt;- read_csv(\"datasets/ex14-05.csv\", show_col_types = F)\n\nBy using a mixed-effects model, you can appropriately analyze the hierarchical structure of the data.\n\ntb &lt;- df |&gt; \n  pivot_longer(\n    cols = c(b1, b2),\n    names_to = \"b\",\n    names_prefix = \"b\",\n    values_to = \"y\"\n  ) |&gt; \n  mutate(across(c(no, drug, b), factor))\n\nlme(y ~ drug * b, random = ~ 1 | no, data = tb) |&gt;\n  anova() \n\n#&gt;             numDF denDF  F-value p-value\n#&gt; (Intercept)     1     8 3440.953  &lt;.0001\n#&gt; drug            1     8   28.006  0.0007\n#&gt; b               1     8  252.049  &lt;.0001\n#&gt; drug:b          1     8    0.450  0.5212\n\n\nAlternative method using aov():\n\n1aov(y ~ drug * b + Error(no/b), data = tb) |&gt;\n  summary()\n\n\n1\n\nhere no must be a factor.\n\n\n\n\n#&gt; \n#&gt; Error: no\n#&gt;           Df Sum Sq Mean Sq F value   Pr(&gt;F)    \n#&gt; drug       1  63.01   63.01   28.01 0.000735 ***\n#&gt; Residuals  8  18.00    2.25                     \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Error: no:b\n#&gt;           Df Sum Sq Mean Sq F value   Pr(&gt;F)    \n#&gt; b          1  63.01   63.01  252.05 2.48e-07 ***\n#&gt; drug:b     1   0.11    0.11    0.45    0.521    \n#&gt; Residuals  8   2.00    0.25                     \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Multifactor ANOVA</span>"
    ]
  },
  {
    "objectID": "repeated-measurement.html",
    "href": "repeated-measurement.html",
    "title": "15  Repeated measures ANOVA",
    "section": "",
    "text": "15.1 Prerequisite\nRepeated measures is a type of experimental design where the same subjects are observed under multiple conditions or over multiple time points. This approach is commonly used in research to analyze changes within subjects, making it more efficient and powerful compared to between-subject designs, where different subjects are used for each condition.\nRepeated measures ANOVA (RM-ANOVA) is a statistical test used to compare means when the same subjects are measured under different conditions or multiple time points. It helps to determine if there are significant differences between conditions or time points while accounting for the correlation within subjects.\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(nlme)\nlibrary(emmeans)",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Repeated measures ANOVA</span>"
    ]
  },
  {
    "objectID": "repeated-measurement.html#x2-repeated-measures",
    "href": "repeated-measurement.html#x2-repeated-measures",
    "title": "15  Repeated measures ANOVA",
    "section": "15.2 2x2 repeated measures",
    "text": "15.2 2x2 repeated measures\nA 2x2 repeated measures ANOVA is used when you want to test for the effects of two within-subjects factors, each having two levels, on a dependent variable. It allows you to assess both the main effects of each factor and any interaction effect between them, while accounting for within-subject variability.\n\nExample 1 \nTo evaluate whether a new drug significantly affects blood pressure in patients with mild hypertension, 20 patients were randomly assigned to either the treatment group or the control group. Blood pressure is measured before and after the intervention. Analyze the difference of blood pressure between treatment and control group.\n\n\n  Download data \n\n\ndf &lt;- read_csv(\"datasets/ex15-01.csv\", show_col_types = F) \n\nHere is the code conducting repeated measures ANOVA:\n\ntb &lt;- df |&gt; \n  mutate(id = factor(c(1:20))) |&gt; \n  pivot_longer(\n    cols = c(pretest, posttest),\n    names_to = \"time\",\n    values_to = \"response\"\n  ) |&gt; \n  mutate(\n    time = factor(time, , levels = c(\"pretest\", \"posttest\")),\n    group = factor(group, labels = c(\"trt\", \"ctl\"))\n  )\n\naov(response ~ group * time + Error(id/(time)), data = tb) |&gt; \n  summary()\n\n#&gt; \n#&gt; Error: id\n#&gt;           Df Sum Sq Mean Sq F value Pr(&gt;F)\n#&gt; group      1  202.5   202.5   1.574  0.226\n#&gt; Residuals 18 2315.4   128.6               \n#&gt; \n#&gt; Error: id:time\n#&gt;            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \n#&gt; time        1 1020.1  1020.1   55.01 7.08e-07 ***\n#&gt; group:time  1  348.1   348.1   18.77 0.000401 ***\n#&gt; Residuals  18  333.8    18.5                     \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAn alternative method using mixed-effect model:\n\nmodel &lt;- lme(response ~ group * time, random = ~ 1 | id, data = tb) \nanova(model)\n\n#&gt;             numDF denDF  F-value p-value\n#&gt; (Intercept)     1    18 4511.476  &lt;.0001\n#&gt; group           1    18    1.574  0.2256\n#&gt; time            1    18   55.009  &lt;.0001\n#&gt; group:time      1    18   18.771  0.0004\n\n\nThe model results indicate that time has a significant effect on the response variable (p &lt; .0001), while the main effect of the group is not significant (p = 0.2256). The interaction between group and time is significant (p = 0.0004), which means that the response to the treatment varies depending on the time point being measured.\n\n\n\n\n\n\nImportant\n\n\n\nWhen there is a significant interaction effect, the focus should shift to understanding the interaction rather than interpreting the main effects in isolation, because when an interaction is present, the main effects do not tell the whole story. The interaction suggests that the relationship between the factors is complex, and the effects of each factor depend on the other factor. Visualizing the interaction or conducting post-hoc analysis can help you better understand how the factors influence each other.\n\n\n\nVisualize the interaction effect\n\ntb |&gt; \n  ggplot(aes(x = time, y = response, color = group, group = group)) +\n  geom_line(stat = \"summary\", fun = \"mean\") +\n  geom_point(stat = \"summary\", fun = \"mean\") +\n  labs(x = \"Time\", y = \"Mean of blood pressure\")\n\n\n\n\n\n\n\n\nPost-hoc analysis (simple effects)\nThe emmeans package makes simple effect analysis easier by providing marginal means and allowing you to compare them across different levels of factors.\n\n1emmeans(model, ~ group | time) |&gt;\n  pairs()\n\n\n1\n\nEffect of group at each time point\n\n\n\n\n#&gt; time = pretest:\n#&gt;  contrast  estimate   SE df t.ratio p.value\n#&gt;  trt - ctl      1.4 3.84 18   0.365  0.7194\n#&gt; \n#&gt; time = posttest:\n#&gt;  contrast  estimate   SE df t.ratio p.value\n#&gt;  trt - ctl    -10.4 3.84 18  -2.711  0.0143\n#&gt; \n#&gt; Degrees-of-freedom method: containment\n\n\n\n1emmeans(model, ~ time | group) |&gt;\n  pairs()\n\n\n1\n\nEffect of time in each group\n\n\n\n\n#&gt; group = trt:\n#&gt;  contrast           estimate   SE df t.ratio p.value\n#&gt;  pretest - posttest     16.0 1.93 18   8.308  &lt;.0001\n#&gt; \n#&gt; group = ctl:\n#&gt;  contrast           estimate   SE df t.ratio p.value\n#&gt;  pretest - posttest      4.2 1.93 18   2.181  0.0427\n#&gt; \n#&gt; Degrees-of-freedom method: containment\n\n\n\nFurther exploration of the interaction between treatment group and time revealed that the mean blood pressure response differed significantly at specific time points. Simple effects analysis indicate that there is no statistical difference in baseline between the treatment and control groups, and the treatment group shows a better effect in blood pressure reduction compared to the control group.\nBy conducting these analysis and visualizations, you’ll gain deeper insights into how different groups respond over time, informing your conclusions and potential interventions based on your study’s outcomes.",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Repeated measures ANOVA</span>"
    ]
  },
  {
    "objectID": "repeated-measurement.html#ixj-repeated-measures",
    "href": "repeated-measurement.html#ixj-repeated-measures",
    "title": "15  Repeated measures ANOVA",
    "section": "15.3 IxJ repeated measures",
    "text": "15.3 IxJ repeated measures\nAn IxJ repeated measures is a generalization of the 2x2 design, where there are I levels of a between-subjects factor and J levels of a within-subjects factor. This design allows you to investigate how multiple levels of both a between-subjects factor and a within-subjects factor interact over time or different conditions.\n\nExample 2 \nTo evaluate whether a new drug significantly affects blood pressure in patients with mild hypertension, 20 patients were randomly assigned to either the treatment group or the control group. Blood pressure is measured before and after the intervention. Analyze the difference of blood pressure between treatment and control group.\n\n\n  Download data \n\n\ndf &lt;- read_csv(\"datasets/ex15-02.csv\", show_col_types = F) \n\nHere is an example code conducting repeated measures ANOVA:\n\ntb &lt;- df |&gt;\n  mutate(id = factor(c(1:15))) |&gt; \n  pivot_longer(\n    cols = contains(\"T\"),\n    names_to = \"time\",\n    values_to = \"response\"\n  ) \naov(response ~ drug * time + Error(id/(time)), data = tb) |&gt; \n  summary()\n\n#&gt; \n#&gt; Error: id\n#&gt;           Df Sum Sq Mean Sq F value Pr(&gt;F)  \n#&gt; drug       2  24.97  12.487   5.527 0.0199 *\n#&gt; Residuals 12  27.11   2.259                 \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Error: id:time\n#&gt;           Df Sum Sq Mean Sq F value   Pr(&gt;F)    \n#&gt; time       4 232.98   58.25   344.3  &lt; 2e-16 ***\n#&gt; drug:time  8  24.50    3.06    18.1 4.13e-12 ***\n#&gt; Residuals 48   8.12    0.17                     \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nHere is an alternative method using anova_test() function from rstatix package:\n\nrstatix::anova_test(\n  data = tb,           \n  dv = response,\n  wid = id,\n  within = time,\n  between = drug\n) \n\n#&gt; ANOVA Table (type II tests)\n#&gt; \n#&gt; $ANOVA\n#&gt;      Effect DFn DFd       F        p p&lt;.05   ges\n#&gt; 1      drug   2  12   5.527 2.00e-02     * 0.415\n#&gt; 2      time   4  48 344.299 1.10e-34     * 0.869\n#&gt; 3 drug:time   8  48  18.105 4.13e-12     * 0.410\n#&gt; \n#&gt; $`Mauchly's Test for Sphericity`\n#&gt;      Effect     W     p p&lt;.05\n#&gt; 1      time 0.511 0.643      \n#&gt; 2 drug:time 0.511 0.643      \n#&gt; \n#&gt; $`Sphericity Corrections`\n#&gt;      Effect   GGe      DF[GG]    p[GG] p[GG]&lt;.05   HFe      DF[HF]    p[HF]\n#&gt; 1      time 0.754 3.02, 36.21 1.07e-26         * 1.036 4.14, 49.74 1.10e-34\n#&gt; 2 drug:time 0.754 6.04, 36.21 1.34e-09         * 1.036 8.29, 49.74 4.13e-12\n#&gt;   p[HF]&lt;.05\n#&gt; 1         *\n#&gt; 2         *\n\n\nAn alternative method using mixed-effect model:\n\nmodel &lt;- lme(response ~ drug * time, random = ~ 1 | id, data = tb) \nanova(model)\n\n#&gt;             numDF denDF   F-value p-value\n#&gt; (Intercept)     1    48 17516.945  &lt;.0001\n#&gt; drug            2    12     5.527  0.0199\n#&gt; time            4    48   344.299  &lt;.0001\n#&gt; drug:time       8    48    18.105  &lt;.0001\n\n\nThe model results indicate that time has a significant effect on the response variable (p &lt; .0001), while the main effect of the drug is not significant (p = 0.2256). The interaction between group and time is significant (p &lt; .0001), which means that the response to the treatment varies depending on the time point being measured.\nTo further explore the interaction between groups and time points, consider the following steps:\n\nVisualize the interaction effect\n\ntb |&gt; \n  ggplot(aes(x = time, y = response, color = drug, group = drug)) +\n  geom_line(stat = \"summary\", fun = \"mean\") +\n  geom_point(stat = \"summary\", fun = \"mean\") +\n  labs(x = \"Time\", y = \"Mean of weight\")\n\n\n\n\n\n\n\n\nPost-hoc analysis (simple effects)\nEffect of drug at each time point:\n\nemmeans(model, ~ drug | time) |&gt; \n  pairs()\n\n#&gt; time = T0:\n#&gt;  contrast estimate    SE df t.ratio p.value\n#&gt;  A - B      -0.032 0.485 12  -0.066  0.9976\n#&gt;  A - C      -0.898 0.485 12  -1.853  0.1947\n#&gt;  B - C      -0.866 0.485 12  -1.787  0.2153\n#&gt; \n#&gt; time = T1:\n#&gt;  contrast estimate    SE df t.ratio p.value\n#&gt;  A - B      -1.068 0.485 12  -2.204  0.1109\n#&gt;  A - C      -1.534 0.485 12  -3.165  0.0206\n#&gt;  B - C      -0.466 0.485 12  -0.962  0.6137\n#&gt; \n#&gt; time = T2:\n#&gt;  contrast estimate    SE df t.ratio p.value\n#&gt;  A - B       0.072 0.485 12   0.149  0.9879\n#&gt;  A - C      -0.030 0.485 12  -0.062  0.9979\n#&gt;  B - C      -0.102 0.485 12  -0.210  0.9759\n#&gt; \n#&gt; time = T3:\n#&gt;  contrast estimate    SE df t.ratio p.value\n#&gt;  A - B      -0.418 0.485 12  -0.862  0.6730\n#&gt;  A - C      -2.900 0.485 12  -5.984  0.0002\n#&gt;  B - C      -2.482 0.485 12  -5.121  0.0007\n#&gt; \n#&gt; time = T4:\n#&gt;  contrast estimate    SE df t.ratio p.value\n#&gt;  A - B      -2.484 0.485 12  -5.125  0.0007\n#&gt;  A - C      -1.690 0.485 12  -3.487  0.0116\n#&gt;  B - C       0.794 0.485 12   1.638  0.2681\n#&gt; \n#&gt; Degrees-of-freedom method: containment \n#&gt; P value adjustment: tukey method for comparing a family of 3 estimates\n\n\nEffect of time in each drug:\n\nemmeans(model, ~ time | drug) |&gt; \n  contrast(method = \"dunnett\")\n\n#&gt; drug = A:\n#&gt;  contrast estimate   SE df t.ratio p.value\n#&gt;  T1 - T0    -0.256 0.26 48  -0.984  0.6967\n#&gt;  T2 - T0     1.154 0.26 48   4.436  0.0002\n#&gt;  T3 - T0     3.126 0.26 48  12.017  &lt;.0001\n#&gt;  T4 - T0     2.864 0.26 48  11.010  &lt;.0001\n#&gt; \n#&gt; drug = B:\n#&gt;  contrast estimate   SE df t.ratio p.value\n#&gt;  T1 - T0     0.780 0.26 48   2.998  0.0156\n#&gt;  T2 - T0     1.050 0.26 48   4.036  0.0007\n#&gt;  T3 - T0     3.512 0.26 48  13.501  &lt;.0001\n#&gt;  T4 - T0     5.316 0.26 48  20.436  &lt;.0001\n#&gt; \n#&gt; drug = C:\n#&gt;  contrast estimate   SE df t.ratio p.value\n#&gt;  T1 - T0     0.380 0.26 48   1.461  0.4029\n#&gt;  T2 - T0     0.286 0.26 48   1.099  0.6251\n#&gt;  T3 - T0     5.128 0.26 48  19.713  &lt;.0001\n#&gt;  T4 - T0     3.656 0.26 48  14.054  &lt;.0001\n#&gt; \n#&gt; Degrees-of-freedom method: containment \n#&gt; P value adjustment: dunnettx method for 4 tests\n\n\nComparing time trends\n\ndf |&gt;\n  mutate(id = factor(c(1:15))) |&gt; \n  pivot_longer(\n    cols = contains(\"T\"),\n    names_to = \"time\",\n    names_prefix = \"T\",\n    values_to = \"response\"\n  ) |&gt; \n  mutate(time = as.numeric(time)) |&gt; \n  lme(response ~ drug * time, random = ~ 1 | id, data = _) |&gt; \n  emtrends(specs = ~ drug, var = \"time\") |&gt; \n  pairs()\n\n#&gt;  contrast estimate    SE df t.ratio p.value\n#&gt;  A - B      -0.425 0.209 57  -2.038  0.1124\n#&gt;  A - C      -0.295 0.209 57  -1.413  0.3409\n#&gt;  B - C       0.130 0.209 57   0.625  0.8072\n#&gt; \n#&gt; Degrees-of-freedom method: containment \n#&gt; P value adjustment: tukey method for comparing a family of 3 estimates\n\n\nComparing time trends in repeated measures designs allows you to understand the dynamic response of subjects over time and how different experimental conditions (e.g., treatment vs. control) affect these trends. It’s essential for determining whether certain groups experience more significant changes over time, providing insights into the time-dependent effectiveness of interventions.",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Repeated measures ANOVA</span>"
    ]
  },
  {
    "objectID": "repeated-measurement.html#considerations",
    "href": "repeated-measurement.html#considerations",
    "title": "15  Repeated measures ANOVA",
    "section": "15.4 Considerations",
    "text": "15.4 Considerations\nRepeated measures designs are useful for analyzing within-subject changes over time or under different conditions. By reducing variability between subjects, they provide more statistical power. RM-ANOVA and mixed-effects models are commonly used to analyze repeated measures data, each offering different benefits based on the assumptions of the data.\nRM-ANOVA assumes equal variances (sphericity) for the differences between all pairs of repeated measures. If this assumption is violated, corrections like Greenhouse-Geisser or Huynh-Feldt adjustments are applied.\nMixed-effects models like those implemented with lme() are more robust to missing data than traditional RM-ANOVA.",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Repeated measures ANOVA</span>"
    ]
  },
  {
    "objectID": "general-linear-model.html",
    "href": "general-linear-model.html",
    "title": "16  General linear model",
    "section": "",
    "text": "16.1 Prerequisite\nThe general linear model (GLM) is a statistical model used to examine the relationship between one or more independent variables (predictors) and a dependent variable (response). It is a broad framework that encompasses various types of regression models, where the dependent variable is continuous, and the independent variables can be continuous, categorical, or a mix of both.\nlibrary(tidyverse)\nlibrary(emmeans)",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>General linear model</span>"
    ]
  },
  {
    "objectID": "general-linear-model.html#some-basics",
    "href": "general-linear-model.html#some-basics",
    "title": "16  General linear model",
    "section": "16.2 Some basics",
    "text": "16.2 Some basics\nThe general form of the GLM can be formulated as:\n\nY = \\beta X + \\epsilon\n\nwhere Y is the vector of dependent variables (n × 1, where n is the number of observations); X is the matrix of independent variables (called design matrix) (n × p, where p is the number of predictors); \\beta is the vector of regression coefficients (p × 1), representing the effect of each predictor on the dependent variable; \\epsilon is the error term (n × 1), assumed to be normally distributed with a mean of zero.\nBelow are some key types of general linear models:\n\nSimple linear regression: Analyzes the linear relationship between a single independent variable (predictor) and a continuous dependent variable (response). For example, examine how age affects systolic blood pressure.\nMultiple linear regression: Extends simple linear regression to include multiple independent variables. It explores the relationship between several predictors and a continuous dependent variable. For example, analyze how age, weight, and exercise frequency together influence cholesterol levels.\nAnalysis of variance (ANOVA): Compares the means of two or more groups to understand whether there are significant differences between them. For example, compare blood pressure across different treatment groups (e.g., drug A, drug B and control).\nAnalysis of covariance (ANCOVA): Combines ANOVA with regression. It includes both categorical and continuous independent variables to understand their effects on the dependent variable while controlling for the covariates. For example, study how treatment (categorical) affects blood pressure while controlling for weight (continuous).\nMultivariate analysis of variance (MANOVA): Extendes ANOVA to include multiple dependent variables. It tests whether mean differences exist between groups across more than one outcome variable. For example, investigate the effect of diet on both blood pressure and cholesterol simultaneously.\nRepeated measures ANOVA: Analyzes data where the same subjects are measured multiple times under different conditions or at different time points. For example, measure blood pressure before and after treatment in the same subjects.\nGeneralized linear model: Extends the GLM to accommodate dependent variables that are not normally distributed, such as binary, count, or categorical outcomes. This includes logistic regression and Poisson regression.\n\nIn summary, the GLM framework encompasses a variety of statistical models depending on the nature of the independent and dependent variables, making it a versatile tool for analyzing different types of data.",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>General linear model</span>"
    ]
  },
  {
    "objectID": "general-linear-model.html#glm-for-comparison-of-two-sample-means",
    "href": "general-linear-model.html#glm-for-comparison-of-two-sample-means",
    "title": "16  General linear model",
    "section": "16.3 GLM for comparison of two sample means",
    "text": "16.3 GLM for comparison of two sample means\nWhen comparing the means of two independent groups, the GLM approach is typically used. This can be conceptualized as an extension of a two-sample t-test but framed within a regression-based framework.\nModel setup\nThe goal is to compare the means of two samples to see if there is a significant difference between them. The model equation can be written as:\n\nY = \\beta_0 + \\beta_1 X + \\epsilon\n\nwhere Y is the outcome (dependent variable), X is the binary variable (independent variable) representing group membership (e.g., 1 for group 1 and 2 for group 2), \\beta_0 is the intercept representing the mean of group A, and \\beta_1 is the coefficient, representing the difference in means between group A and group B, \\epsilon is the error term, accounting for variation in Y not explained by X.\nIn this setup, the GLM compares the means of the two groups via the estimate for \\beta_1.\nHypothesis\nH_0: There is no difference in means between the two groups (\\beta_1 = 0).\nH_1: There is a significant difference in means between the two groups (\\beta_1 \\neq 0).\n\nExample 1 \nReanalyze the Example 3 in Chapter 7 using general linear model approach.\n\n\n  Download data \n\n\ndf &lt;- read_csv(\"datasets/ex16-01.csv\", col_types = list(grp = col_factor())) \n\nHere’s how you can implement this using the lm() function, which fits a linear model.\n\nlm(x ~ grp, data = df) |&gt; summary()\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = x ~ grp, data = df)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -0.11000 -0.05125  0.00000  0.05125  0.11000 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  0.77000    0.02516  30.607 3.16e-14 ***\n#&gt; grp2        -0.13125    0.03558  -3.689  0.00243 ** \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.07116 on 14 degrees of freedom\n#&gt; Multiple R-squared:  0.4929, Adjusted R-squared:  0.4567 \n#&gt; F-statistic: 13.61 on 1 and 14 DF,  p-value: 0.00243\n\n\nThe mean of group 1 (intercept) is 0.77, the difference between the means of group 1 and group 2 (group coefficient) is -0.13125. The p-value is 0.00243, suggesting that there is a statistically significant difference between the means of the two groups.\nThe GLM approach fits a linear regression model to test for differences in means. This approach can be generalized to multiple groups or covariates, allowing more complex analyses. Two-sample t-test is a simpler, direct method for testing the difference between two means. It’s equivalent to the GLM approach when no other covariates are included. Both methods will give the same results, but the GLM approach provides a framework for extending to more complex models.",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>General linear model</span>"
    ]
  },
  {
    "objectID": "general-linear-model.html#glm-for-comparison-of-multiple-sample-means",
    "href": "general-linear-model.html#glm-for-comparison-of-multiple-sample-means",
    "title": "16  General linear model",
    "section": "16.4 GLM for comparison of multiple sample means",
    "text": "16.4 GLM for comparison of multiple sample means\nANOVA is a special case of GLM used when the dependent variable is continuous and the independent variables are categorical. GLM provides a flexible framework for comparing multiple sample means. Here we only consider the comparison of three sample means.\nModel setup\nThe model equation can be written as:\n\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\epsilon\n\nwhere Y is the outcome (dependent variable), X_1 and X_2 are dummy variables (also called indicator variables) used to encode the group memberships of the samples. Typically, if there are three groups, you can create two dummy variables to compare the means: The group 1 is the reference group where both X_1 = 0 and X_2 = 0. X_1 = 1 if the observation is from group 2, and 0 otherwise. X_2 = 1 if the observation is from group 3, and 0 otherwise. \\beta_0 is the intercept representing the mean of group 1 (reference group), \\beta_1 is the coefficient representing the difference in means between group 1 and group 2, and \\beta_2 is the coefficient representing the difference in means between group 1 and group 3, \\epsilon is the error term.\nThus, the GLM compares the mean of the reference group (encoded by the intercept \\beta_0) with the other two groups through the coefficients \\beta_1 and \\beta_2.\nHypothesis\nH_0: There is no difference in means among the three groups (\\beta_1 = \\beta_2 = 0).\nH_1: There is a difference in means among the three groups (At least one of the \\beta_1 or \\beta_2 \\neq 0).\n\nExample 2 \nReanalyze the Example 1 in Chapter 8 using general linear model approach.\n\n\n  Download data \n\n\ndf &lt;- read_csv(\"datasets/ex16-02.csv\", col_types = list(group = col_factor())) \n\n\nlm(diff_tri ~ group, data = df) |&gt; summary()\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = diff_tri ~ group, data = df)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -1.4002 -0.5206 -0.1021  0.6013  1.7260 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)   2.9602     0.1178  25.134  &lt; 2e-16 ***\n#&gt; group2       -0.0920     0.1666  -0.552    0.582    \n#&gt; group3       -0.7562     0.1666  -4.540 1.37e-05 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.7449 on 117 degrees of freedom\n#&gt; Multiple R-squared:  0.1734, Adjusted R-squared:  0.1593 \n#&gt; F-statistic: 12.27 on 2 and 117 DF,  p-value: 1.449e-05\n\n\nThe model uses group 1 as the reference category (since it’s not listed), and estimates the differences in diff_tri between groups 2 and 3 compared to group 1.\n\nIntercept (group 1): The intercept represents the estimated mean of diff_tri for group 1, which is 2.9602. This is the baseline value when group = 1. The very small p-value (&lt; 2e-16) indicates that this estimate is significantly different from zero.\ngroup 2 coefficient: The estimate for group2 is -0.0920, meaning the average difference between group 2 and group 1 is -0.092. Group 2 has a slightly lower diff_tri compared to group 1. The p-value (0.582) indicates that this difference is not statistically significant, as the value is much greater than typical significance levels (e.g., 0.05).\ngroup 3 coefficient: The estimate for group3 is -0.7562, indicating that group 3 has a lower average diff_tri by 0.756 units compared to group 1. The p-value (1.37e-05) is highly significant (below 0.001), suggesting that the difference between group 3 and group 1 is statistically significant.\n\nTo compare group 2 and group 3, you can either perform pairwise comparisons with emmeans, or relevel the reference group in the model. Here are the code blocks for the two methods.\n\nlm(diff_tri ~ group, data = df) |&gt; \n  emmeans(pairwise ~ group)\n\n#&gt; $emmeans\n#&gt;  group emmean    SE  df lower.CL upper.CL\n#&gt;  1       2.96 0.118 117     2.73     3.19\n#&gt;  2       2.87 0.118 117     2.63     3.10\n#&gt;  3       2.20 0.118 117     1.97     2.44\n#&gt; \n#&gt; Confidence level used: 0.95 \n#&gt; \n#&gt; $contrasts\n#&gt;  contrast        estimate    SE  df t.ratio p.value\n#&gt;  group1 - group2    0.092 0.167 117   0.552  0.8455\n#&gt;  group1 - group3    0.756 0.167 117   4.540  &lt;.0001\n#&gt;  group2 - group3    0.664 0.167 117   3.988  0.0003\n#&gt; \n#&gt; P value adjustment: tukey method for comparing a family of 3 estimates\n\n\n\n# Relevel the factor so that group 2 is the reference group\ndf |&gt;\n  mutate(group = relevel(group, ref = \"2\")) |&gt;\n  lm(diff_tri ~ group, data = _) |&gt;\n  summary()\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = diff_tri ~ group, data = mutate(df, group = relevel(group, \n#&gt;     ref = \"2\")))\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -1.4002 -0.5206 -0.1021  0.6013  1.7260 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)   2.8683     0.1178  24.353  &lt; 2e-16 ***\n#&gt; group1        0.0920     0.1666   0.552 0.581774    \n#&gt; group3       -0.6643     0.1666  -3.988 0.000116 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.7449 on 117 degrees of freedom\n#&gt; Multiple R-squared:  0.1734, Adjusted R-squared:  0.1593 \n#&gt; F-statistic: 12.27 on 2 and 117 DF,  p-value: 1.449e-05\n\n\nThese methods will give you the direct difference between the two groups, along with p-values for significance.",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>General linear model</span>"
    ]
  },
  {
    "objectID": "general-linear-model.html#analysis-of-covariance",
    "href": "general-linear-model.html#analysis-of-covariance",
    "title": "16  General linear model",
    "section": "16.5 Analysis of covariance",
    "text": "16.5 Analysis of covariance\nAnalysis of covariance (ANCOVA) combines both analysis of variance (ANOVA) and linear regression. It is used to compare the means of dependent variables across different groups while controlling for the effects of one or more continuous covariates (independent variables that are not of primary interest but may influence the outcome). By accounting for covariates, ANCOVA reduces the error variance, thus increasing the statistical power of the test.\nThe general form of the ANCOVA model is:\n\nY = \\mu + \\beta X + \\tau G + \\epsilon\n\nwhere Y is the dependent variable (outcome of interest), X is the covariate, G represents the groups or factors (categorical independent variable), \\mu is the overall mean, \\beta is the regression coefficient for the covariate, \\tau is the effect of the groups on the outcome variable, and \\epsilon is the residual error.\n\n16.5.1 ANCOVA assumptions\nANCOVA Assumptions are critical to ensure the validity of the results. These assumptions are similar to those in ANOVA and linear regression but include additional considerations related to covariates. Here are the key assumptions:\n\nLinearity\nThe relationship between the covariate(s) and the dependent variable should be linear across all levels of the independent variable (grouping factor). In other words, the effect of the covariate should consistently increase or decrease the dependent variable in a straight-line fashion.\nHow to check: Scatterplots of the covariate against the dependent variable, stratified by group, can be used to visually check for linear relationships.\nHomogeneity of regression slopes\nThe slopes of the regression lines (representing the relationship between the covariate and the dependent variable) must be the same for all groups. This ensures that the covariate affects each group in the same way. If this assumption is violated, it indicates an interaction between the covariate and the group factor.\nHow to check: Include an interaction term between the covariate and group in the model. If the interaction is significant, the assumption is violated.\n\nmodel &lt;- aov(outcome ~ group * covariate, data = data)\nsummary(model)\n\nNormality\nThe residuals (differences between observed and predicted values) of the dependent variable should be normally distributed. This assumption ensures that the error terms follow a normal distribution across all levels of the independent variable.\nHow to check: Use a normal Q-Q plot or conduct the Shapiro-Wilk test to check the normality of residuals.\n\nshapiro.test(residuals(model))\n\nIndependence of observations\nThe observations should be independent of one another. This means that there is no correlation or dependency between the values of the dependent variable within or between groups.\nHow to check: Ensure that the data collection process was designed to avoid dependency between observations (e.g., no repeated measures or clustering without proper adjustments).\nHomogeneity of variance\nThe variances of the dependent variable should be equal across all groups. This is similar to the ANOVA assumption of homogeneity of variances.\nHow to check: Use Levene’s test or Bartlett’s test for homogeneity of variances.\n\nrstatix::levene_test(data = data, outcome ~ group)\n\nCovariate independence of treatment effects\nThe covariate should be independent of the group factor, meaning the covariate should not differ significantly across the groups. If the covariate is not balanced across groups, it can introduce bias into the results.\nHow to check: Test whether the covariate is significantly different between groups, for example using an ANOVA or t-test depending on the number of groups.\n\nsummary(aov(covariate ~ group, data = data))\n\n\nViolating these assumptions can lead to biased or invalid results, so it’s important to check each assumption before interpreting the results of ANCOVA.\n\nExample 3 \nA study aims to investigate whether there is a difference in serum cholesterol between adults with normal weight and those who are overweight. During a health examination, 13 individuals from two groups (normal weight and overweight) were randomly selected, and their cholesterol levels were measured. Additionally, cholesterol levels are also related to age. The study data can be accessed from the download button below. Compare the two sample means of the cholesterol level.\n\n\n  Download data \n\n\ndf &lt;- read_csv(\"datasets/ex16-03.csv\", col_types = list(group = col_factor())) \n\nYou can conduct ANCOVA using the aov() or lm() functions:\n\naov(chole ~ group + age, data = df) |&gt; \n  summary()\n\n#&gt;             Df Sum Sq Mean Sq F value   Pr(&gt;F)    \n#&gt; group        1  18.61  18.615   20.34 0.000158 ***\n#&gt; age          1  24.38  24.380   26.64 3.13e-05 ***\n#&gt; Residuals   23  21.05   0.915                     \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\ndf |&gt; \n  group_by(group) |&gt; \n  summarise(\n    m = mean(chole) \n  )\n\n#&gt; # A tibble: 2 × 2\n#&gt;   group     m\n#&gt;   &lt;fct&gt; &lt;dbl&gt;\n#&gt; 1 0      5.09\n#&gt; 2 1      6.78\n\nlm(chole ~ group + age, data = df) |&gt; \n  summary()\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = chole ~ group + age, data = df)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -1.7807 -0.4706  0.0270  0.6301  2.0068 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  0.76053    0.88016   0.864   0.3965    \n#&gt; group1       0.89549    0.40572   2.207   0.0376 *  \n#&gt; age          0.09417    0.01824   5.162 3.13e-05 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.9566 on 23 degrees of freedom\n#&gt; Multiple R-squared:  0.6714, Adjusted R-squared:  0.6428 \n#&gt; F-statistic: 23.49 on 2 and 23 DF,  p-value: 2.769e-06\n\n\nThe coefficients for the model show the estimated effect of each predictor on cholesterol levels:\n(Intercept): The intercept is estimated at 0.76053, which is the expected cholesterol level when both predictors (group and age) are zero. However, the intercept’s significance is not high (p = 0.3965).\ngroup1: The coefficient for group1 is 0.89549. This indicates that individuals in group 1 have, on average, a cholesterol level that is 0.89549 mmol/L higher than those in the reference group (the normal weight). The p-value (0.0376) suggests this effect is statistically significant at the 0.05 level, indicating a meaningful difference in cholesterol levels between the groups.\nage: The coefficient for age is 0.09417, suggesting that for each additional year of age, the cholesterol level increases by approximately 0.09417 mmol/L. The p-value for age (3.13e-05) is highly significant, indicating a possible relationship between age and cholesterol levels.\n\nggplot(df, aes(x = age, y = chole, shape = group, color = group)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = F) \n\n#&gt; `geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>General linear model</span>"
    ]
  },
  {
    "objectID": "multivariate-groups-diff.html",
    "href": "multivariate-groups-diff.html",
    "title": "17  Multivariate data",
    "section": "",
    "text": "17.1 Prerequisite\nMultivariate data usually refers to data involving more than one variable, but here specially refers to datasets where there are multiple outcome or response variables (also called dependent variables) that are measured simultaneously for each observation or experimental unit. This type of data is common in many fields such as psychology, medicine, biology, and social sciences, where researchers want to study how one or more predictors (independent variables) affect multiple outcomes.\nlibrary(tidyverse)\nlibrary(GGally)\nlibrary(DescTools)\nlibrary(emmeans)\nlibrary(nlme)\nlibrary(profileR)",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Multivariate data</span>"
    ]
  },
  {
    "objectID": "multivariate-groups-diff.html#descriptive-statistics",
    "href": "multivariate-groups-diff.html#descriptive-statistics",
    "title": "17  Multivariate data",
    "section": "17.2 Descriptive statistics",
    "text": "17.2 Descriptive statistics\nDescriptive statistics for multivariate data help summarize the properties of several variables simultaneously. Some key statistics include:\n\n17.2.1 Mean vector\nThe mean vector is a multivariate extension of the concept of a mean in univariate statistics. It provides the average or central value for each variable in a multivariate dataset. For multiple variables, it is represented as a vector:\n\\mu = (\\mu_1, \\mu_2, \\dots, \\mu_p)\nwhere p is the number of variables.\n\n\n17.2.2 Deviation matrix\nThe deviation matrix (also known as the mean deviation matrix or matrix of deviations) is a matrix that represents how much each observation in a dataset deviates from the mean of the corresponding variable. It is often used in multivariate data analysis as a step before calculating other statistics, like the covariance matrix.\nIf the dataset is represented by the matrix \\mathbf{X} with n rows (observations) and p columns (variables), and v is the mean vector of the variables, the deviation matrix \\mathbf{D} is calculated as:\n\n\\mathbf{D} = \\mathbf{X} - \\mathbf{1} \\mu^T\n\nwhere \\mathbf{X} is the n \\times p data matrix, \\mathbf{1} is an n \\times 1 column vector of ones, \\mu^T is the 1 \\times p mean vector transposed to match the dimensions, \\mathbf{D} is the n \\times p deviation matrix.\n\n\n17.2.3 Covariance matrix\nThe covariance matrix describes the relationships between pairs of variables in a dataset. It captures how much two variables change together (covary) and generalizes the concept of variance (which measures how much a single variable varies) to multiple variables. The covariance matrix \\mathbf{S} contains variances on the diagonal and covariances off-diagonal:\n\n\\mathbf{S} = \\begin{pmatrix}S_{11} & S_{12} & \\cdots & S_{1p} \\\\S_{21} & S_{22} & \\cdots & S_{2p} \\\\\\vdots & \\vdots & \\ddots & \\vdots \\\\S_{p1} & S_{p2} & \\cdots & S_{pp}\\end{pmatrix}\n\nWhere S_{ii} is the variance of the i-th variable, S_{ij} (for i \\neq j) is the covariance between the i-th and j-th variables.\nThe covariance between X_i and X_j is calculated as:\n\nS_{ij} = \\frac{1}{n-1} \\sum_{k=1}^{n} (X_{ik} - \\bar{X}_i)(X_{jk} - \\bar{X}_j)\n\nwhere n is the number of observations, X_{ik} and X_{jk} are the k -th observations of the variables X_i and X_j, respectively, \\bar{X}_i and \\bar{X}_j are the means of the variables X_i and X_j.\n\n\n17.2.4 Correlation matrix\nThe correlation matrix provides the pairwise correlation coefficients between variables in a dataset. It is a normalized version of the covariance matrix, giving a measure of how strongly variables are linearly related, with values ranging from -1 to 1.\nFor a dataset with p variables, the correlation matrix is a p \\times p matrix where each element r_{ij} represents the correlation coefficient between the i -th andj -th variables. The diagonal elements are always 1, as a variable is perfectly correlated with itself.\nMathematically, the correlation matrix R is:\n\nR = \\begin{pmatrix}    1 & r_{12} & r_{13} & \\dots & r_{1p} \\\\    r_{21} & 1 & r_{23} & \\dots & r_{2p} \\\\    r_{31} & r_{32} & 1 & \\dots & r_{3p} \\\\    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\    r_{p1} & r_{p2} & r_{p3} & \\dots & 1    \\end{pmatrix}\n\nwhere r_{ij} is the Pearson correlation coefficient between variable i and variable j, defined as:\n\nr_{ij} = \\frac{\\text{Cov}(X_i, X_j)}{\\sigma_{X_i} \\sigma_{X_j}}\n\n\\text{Cov}(X_i, X_j) is the covariance between X_i and X_j, \\sigma_{X_i} and\\sigma_{X_j}are the standard deviations of X_i and X_j, respectively.\n\n\n17.2.5 Scatterplot matrix\nA scatterplot matrix (also called a pairs plot) is a grid of scatterplots that visually represents the relationships between pairs of variables in a multivariate dataset. It is particularly useful for examining potential correlations or patterns among multiple variables simultaneously.\nFor a dataset with p variables, a scatterplot matrix will contain a p \\times p grid of plots, where the plot in the i -th and j -th column displays a scatterplot of the i -th variable versus the j -th variable. The diagonal often shows the distribution of each variable, typically as a histogram or a density plot. Each off-diagonal plot shows the relationship (scatterplot) between two variables, helping identify patterns like linearity, clusters, or outliers.\n\nExample 1 \nPulmonary function measurements were performed on 15 patients with acute lower respiratory tract infections admitted to the respiratory department of a hospital. The measurements included forced vital capacity (FVC), forced expiratory volume in the first second (FEV1), and peak expiratory flow (PEF). Please perform a multivariate description of these three response variables.\n\n\n  Download data \n\n\ndf &lt;- read_csv(\"datasets/ex17-01.csv\", show_col_types = F) |&gt; \n  select(FVC, FEV1, PEF)\n\nMean vector:\nYou can easily calculate a correlation matrix using the colMeans() function:\n\ncolMeans(df)\n\n#&gt;      FVC     FEV1      PEF \n#&gt; 2.339333 1.690000 3.515333\n\n\nCovariance matrix:\nYou can easily calculate a correlation matrix using the cov() function:\n\ncov(df)\n\n#&gt;              FVC        FEV1         PEF\n#&gt; FVC  0.003949524 0.001750000 0.010432381\n#&gt; FEV1 0.001750000 0.057557143 0.006414286\n#&gt; PEF  0.010432381 0.006414286 0.139926667\n\n\nCorrelation matrix:\nYou can easily calculate a correlation matrix using the cor() function:\n\ncor(df) \n\n#&gt;            FVC       FEV1        PEF\n#&gt; FVC  1.0000000 0.11606895 0.44377295\n#&gt; FEV1 0.1160690 1.00000000 0.07147406\n#&gt; PEF  0.4437729 0.07147406 1.00000000\n\n\nScatterplot matrix\nTools like pairs() in base R or ggpairs() in GGally can generate scatterplot matrices for visual exploration of data relationships.\n\npairs(df, main = \"Scatterplot matrix with pairs()\")\n\n\n\n\n\n\n\n\n\nggpairs(df, title = \"Scatterplot matrix with ggpairs()\")\n\n\n\n\n\n\n\n\n\n\n17.2.6 Multivariate normal distribution\nMultivariate normal distribution (also known as multivariate Gaussian distribution) is a generalization of the univariate normal distribution to multiple dimensions. It describes a set of p-dimensional random variables, where each variable is normally distributed and their joint distribution follows a normal pattern.\nA random vector \\mathbf{X} = (X_1, X_2, \\dots, X_p)^T is said to follow a multivariate normal distribution if every linear combination of its components is normally distributed. It is denoted as:\n\n\\mathbf{X} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\n\nwhere \\boldsymbol{\\mu} = (\\mu_1, \\mu_2, \\dots, \\mu_p)^T is the mean vector, representing the expected values of the variables, \\boldsymbol{\\Sigma} is the p \\times p covariance matrix, representing the variances of and covariances between the variables.\nThe probability density function of a multivariate normal distribution is given by:\n\nf(\\mathbf{X}) = \\frac{1}{(2\\pi)^{p/2} |\\boldsymbol{\\Sigma}|^{1/2}} \\exp\\left( -\\frac{1}{2} (\\mathbf{X} - \\boldsymbol{\\mu})^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{X} - \\boldsymbol{\\mu}) \\right)\n\nwhere \\mathbf{X} is the p-dimensional vector of variables, \\boldsymbol{\\Sigma} is the determinant of the covariance matrix, \\boldsymbol{\\Sigma}^{-1} is the inverse of the covariance matrix.",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Multivariate data</span>"
    ]
  },
  {
    "objectID": "multivariate-groups-diff.html#comparison-of-differences-between-groups",
    "href": "multivariate-groups-diff.html#comparison-of-differences-between-groups",
    "title": "17  Multivariate data",
    "section": "17.3 Comparison of differences between groups",
    "text": "17.3 Comparison of differences between groups\nWhen comparing differences between groups with multivariate data, we deal with situations where there are multiple dependent variables or outcomes that need to be analyzed simultaneously. Traditional univariate methods (such as t-tests or ANOVA) only test differences for one outcome variable at a time. For multivariate data, we use techniques that account for the interrelationships between the multiple outcomes.\nHere are the common methods for comparing group differences with multivariate data:\n\n17.3.1 Hotelling’s T² test: one-sample\nHotelling’s T² test is the multivariate extension of the univariate t-test. It can be used in two different contexts: one-sample and two-sample. Both tests are multivariate versions of t-tests, designed for situations where there are multiple dependent variables.\nFor one-sample, it tests whether the sample’s mean vector differs from a known mean vector. The hypothesis tested is:\n\nH_0: The mean vector of the sample is equal to the known mean vector.\nH_1: The mean vector of the sample is different from the known mean vector.\n\nLet \\mathbf{X} be the sample data matrix of n observations and p variables. The mean vector \\bar{\\mathbf{X}} is compared to a hypothesized mean vector \\mathbf{\\mu}_0. The test statistic is given by:\n\nT^2 = n (\\bar{\\mathbf{X}} - \\mathbf{\\mu}_0)^{\\prime} \\mathbf{S}^{-1} (\\bar{\\mathbf{X}} - \\mathbf{\\mu}_0)\n\nwhere n is the sample size, \\bar{\\mathbf{X}} is the sample mean vector, \\mathbf{S} is the sample covariance matrix, \\mathbf{\\mu}_0 is the hypothesized mean vector.\nUnder the null hypothesis, the test statistic T^2 follows an F-distribution after scaling:\n\nF = \\frac{(n - p)}{p(n - 1)} T^2 \\sim F(p, n - p)\n\nwhere p is the number of variables (dimensions) in the data.\n\nExample 2 \nIn a battery factory, 15 workers engaged in lead-related tasks were randomly selected for measurement of liver function indicators, including alanine aminotransferase (ALT), aspartate aminotransferase (AST), and gamma-glutamyl transferase (GGT). It is known that the mean values for ALT, AST, and GGT in the general population in that area are 23.9 U/L, 25.7 U/L, and 26.7 U/L, respectively. The question is whether the liver function of the lead-exposed workers differs from that of the normal population.\n\n\n  Download data \n\n\ndf &lt;- read_csv(\"datasets/ex17-02.csv\", show_col_types = F)\n\nTo perform a one-sample Hotelling’s T² test, you can use the HotellingsT2Test() function from the DescTools package.\n\nnormal_mean &lt;- matrix(c(23.9, 25.7, 26.7), nrow = 3)\ndf |&gt; \n  HotellingsT2Test(mu = normal_mean)\n\n#&gt; \n#&gt;  Hotelling's one sample T2-test\n#&gt; \n#&gt; data:  df\n#&gt; T.2 = 5.3391, df1 = 3, df2 = 12, p-value = 0.01439\n#&gt; alternative hypothesis: true location is not equal to c(23.9,25.7,26.7)\n\n\nThe results show that the Hotelling’s T² statistic is 5.3391, p-value is 0.01439, less than 0.05, reject the null hypothesis. This suggest that the liver function (ALT, AST, and GGT) of the lead-exposed workers is different from the normal population values ([23.9, 25.7, 26.7]), implying a potential impact of lead exposure on the workers’ liver function.\n\n\n17.3.2 Hotelling’s T² test: two-sample\nFor two groups, it tests whether the mean vectors of two independent samples are equal. The hypothesis tested is:\n\nH_0: The mean vectors of the two groups are equal: \\boldsymbol{\\mu}_1 = \\boldsymbol{\\mu}_2.\nH_1: The mean vectors of the two groups are not equal: \\boldsymbol{\\mu}_1 \\neq \\boldsymbol{\\mu}_2.\n\nThe test statistic for Hotelling’s T^2 is computed similarly to a multivariate version of the t-test. When comparing two independent groups with sample sizes n_1 and n_2, and sample mean vectors \\bar{\\mathbf{X}}_1 and \\bar{\\mathbf{X}}_2, the test statistic is:\n\nT^2 = \\frac{n_1 n_2}{n_1 + n_2} (\\bar{\\mathbf{X}}_1 - \\bar{\\mathbf{X}}_2)^{\\prime} \\mathbf{S}_p^{-1} (\\bar{\\mathbf{X}}_1 - \\bar{\\mathbf{X}}_2)\n\nwhere \\mathbf{S}_p is the pooled covariance matrix.\nSpecifically, the Hotelling’s T^2 statistic can be transformed into an F-statistic as:\n\nF = \\frac{(n_1 + n_2 - p - 1) T^2}{(n_1 + n_2 - 2)p}\n\nwhere p is the number of variables.\nF follows an F-distribution with p and n_1 + n_2 - p - 1 degrees of freedom under the null hypothesis.\n\nExample 3 \nA hospital conducted a study to investigate the therapeutic effect of drug A on heart failure. 20 heart failure patients were randomly divided into two groups: one group received standard treatment, while the other group received drug A in addition to the standard treatment. The post-treatment heart function indicators, left ventricular ejection fraction (LVEF) and the 6-minute walk test (6MWT) were measured. It is known that lower values of LVEF and 6MWT indicate more severe heart function impairment. The question is whether drug A is effective in treating heart failure.\n\n\n  Download data \n\n\ndf &lt;- read_csv(\"datasets/ex17-03.csv\", col_types = list(group = col_factor()))\n\nThe HotellingsT2Test() function from DescTools package is used to perform Hotelling’s T² test for two groups.\n\nHotellingsT2Test(cbind(LVEF, MWT) ~ group, data = df)\n\n#&gt; \n#&gt;  Hotelling's two sample T2-test\n#&gt; \n#&gt; data:  cbind(LVEF, MWT) by group\n#&gt; T.2 = 1.29, df1 = 2, df2 = 17, p-value = 0.3009\n#&gt; alternative hypothesis: true location difference is not equal to c(0,0)\n\n\nAn alternative method is using the manova() function to perform multivariate ANOVA, just as univariate two-sample mean difference can be tested either by t-test or ANOVA.\n\nmanova(cbind(LVEF, MWT) ~ group, data = df) |&gt; \n1  summary(test = \"Wilks\")\n\n\n1\n\nThis provides a summary of the MANOVA results using Wilks’ Lambda as the test statistic.\n\n\n\n\n#&gt;           Df   Wilks approx F num Df den Df Pr(&gt;F)\n#&gt; group      1 0.86823     1.29      2     17 0.3009\n#&gt; Residuals 18\n\n\nIn this case, the p-value is 0.3009, which is higher than the significance level of 0.05, not reject the null hypothesis. There is no significant evidence to suggest that drug A has a effect on improving heart function (as measured by LVEF and 6MWT) when compared to standard treatment alone.\n\n\n17.3.3 MANOVA for multiple groups\nMANOVA (Multivariate analysis of variance) is an extension of ANOVA that allows for comparing the mean vectors of multiple groups across several dependent variables while considering the correlation between them.\n\nExample 4 \nThe children with chronic gastritis were randomly divided into 3 groups, with group I and group II as treatment groups, and another group serving as the control. The aim is to compare the effect of the treatment drugs on T-cell immune function (percentages of peripheral blood T3, T4, and T5 cells).\n\n\n  Download data \n\n\ndf &lt;- read_csv(\"datasets/ex17-04.csv\", col_types = list(group = col_factor()))\ndf |&gt; \n  group_by(group)|&gt; \n  summarise_all(.funs = list(mean = mean))\n\n#&gt; # A tibble: 3 × 4\n#&gt;   group t3_mean t4_mean t8_mean\n#&gt;   &lt;fct&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1 1        62.3    31.8    30.8\n#&gt; 2 2        42.5    20.1    17.4\n#&gt; 3 3        73.2    38.1    33.7\n\n\nUse the manova() function to test whether the treatment drugs have a significant effect on T-cell immune function.\n\nmodel &lt;- manova(cbind(t3, t4, t8) ~ group, data = df)\nsummary(model, test = \"Wilks\")\n\n#&gt;           Df    Wilks approx F num Df den Df   Pr(&gt;F)   \n#&gt; group      2 0.088735   5.4997      6     14 0.004104 **\n#&gt; Residuals  9                                            \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe MANOVA results indicate the following:\n\nWilks’ Lambda (0.088735): This is a test statistic used in multivariate analysis of variance. A lower Wilks’ Lambda value indicates that the group differences explain a large portion of the variance in the dependent variables (T3, T4, T8). In this case, the value is relatively low, which suggests that the groups differ significantly in the combined T-cell percentages.\nApproximate F-statistic (5.4997): The F-value is used to determine whether the differences between the groups’ means are statistically significant. A larger F-value typically suggests stronger evidence against the null hypothesis (i.e., no difference between groups).\nDegrees of Freedom (num Df = 6, den Df = 14): num Df refers to the numerator degrees of freedom, associated with the number of dependent variables and groups. den Df refers to the denominator degrees of freedom, related to the residuals or error in the model.\nP-value: This is the most critical value for interpreting the test. A p-value of 0.004104 indicates that there is a statistically significant difference in the combined T-cell immune function measurements (T3, T4, and T8 cell percentages) between the groups at the 0.05 significance level.\n\nSince the p-value is significant (0.004104), we can reject the null hypothesis and conclude that the treatments have a significant impact on the T-cell immune function across the groups. To determine which specific variables (T3, T4, or T8) are driving these differences, you can run follow-up univariate ANOVAs for each dependent variable.\n\n# Univariate ANOVA\ndf |&gt; \n  select(t3, t4, t8) |&gt; \n  lapply(\\(x) aov(x ~ group, data = df)) |&gt; \n  map(summary)\n\n#&gt; $t3\n#&gt;             Df Sum Sq Mean Sq F value   Pr(&gt;F)    \n#&gt; group        2 2110.0  1055.0   32.55 7.58e-05 ***\n#&gt; Residuals    9  291.7    32.4                     \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; $t4\n#&gt;             Df Sum Sq Mean Sq F value  Pr(&gt;F)   \n#&gt; group        2  727.7   363.9    8.72 0.00783 **\n#&gt; Residuals    9  375.5    41.7                   \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; $t8\n#&gt;             Df Sum Sq Mean Sq F value  Pr(&gt;F)   \n#&gt; group        2  638.0   319.0   14.81 0.00142 **\n#&gt; Residuals    9  193.9    21.5                   \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAnother convenient tool is summary.aov(), which can extract the univariate ANOVAs directly from the model for each dependent variable.\n\nsummary.aov(model)\n\n#&gt;  Response t3 :\n#&gt;             Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \n#&gt; group        2 2110.01 1055.00  32.553 7.582e-05 ***\n#&gt; Residuals    9  291.68   32.41                      \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt;  Response t4 :\n#&gt;             Df Sum Sq Mean Sq F value   Pr(&gt;F)   \n#&gt; group        2 727.70  363.85  8.7197 0.007833 **\n#&gt; Residuals    9 375.55   41.73                    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt;  Response t8 :\n#&gt;             Df Sum Sq Mean Sq F value   Pr(&gt;F)   \n#&gt; group        2 638.04  319.02  14.807 0.001425 **\n#&gt; Residuals    9 193.91   21.55                    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThis will give you the individual ANOVA tables for each dependent variable. If any of these ANOVAs are significant, you should conduct post-hoc pairwise comparisons to see which two groups are significantly different.\n\ndf |&gt; \n  select(t3, t4, t8) |&gt; \n  lapply(\\(x) aov(x ~ group, data = df)) |&gt; \n  map(~ TukeyHSD(.)) \n\n#&gt; $t3\n#&gt;   Tukey multiple comparisons of means\n#&gt;     95% family-wise confidence level\n#&gt; \n#&gt; Fit: aov(formula = x ~ group, data = df)\n#&gt; \n#&gt; $group\n#&gt;          diff         lwr       upr     p adj\n#&gt; 2-1 -19.74167 -31.8813782 -7.601955 0.0036024\n#&gt; 3-1  10.95333  -0.6544511 22.561118 0.0638950\n#&gt; 3-2  30.69500  20.0325691 41.357431 0.0000567\n#&gt; \n#&gt; \n#&gt; $t4\n#&gt;   Tukey multiple comparisons of means\n#&gt;     95% family-wise confidence level\n#&gt; \n#&gt; Fit: aov(formula = x ~ group, data = df)\n#&gt; \n#&gt; $group\n#&gt;           diff        lwr      upr     p adj\n#&gt; 2-1 -11.708333 -25.483127  2.06646 0.0957183\n#&gt; 3-1   6.306667  -6.864555 19.47789 0.4114644\n#&gt; 3-2  18.015000   5.916460 30.11354 0.0062399\n#&gt; \n#&gt; \n#&gt; $t8\n#&gt;   Tukey multiple comparisons of means\n#&gt;     95% family-wise confidence level\n#&gt; \n#&gt; Fit: aov(formula = x ~ group, data = df)\n#&gt; \n#&gt; $group\n#&gt;           diff        lwr       upr     p adj\n#&gt; 2-1 -13.458333 -23.356374 -3.560293 0.0106483\n#&gt; 3-1   2.906667  -6.557671 12.371004 0.6787514\n#&gt; 3-2  16.365000   7.671451 25.058549 0.0013602\n\n\nThe results show that group 1 and group 3 do not show a significant difference in each response variable mean. This method performs pairwise comparisons on each response variable independently, which can be simpler but loses the multivariate aspect.\nThe Hotelling’s T² test is a multivariate generalization of the t-test for comparing mean vectors. Here we define a customized function named pairs.hotellings.T2.test() to perform pairwise comparisons of the multivariate means between groups.\n\n# Define the pairwise_hotelling function\npairs.hotellings.T2.test &lt;- function(formula, data, p.adjust.method = p.adjust.methods) {\n  vars &lt;- all.vars(formula[[2]])  # Get the response variables\n  g &lt;- all.vars(formula[[3]])  # Get the group variable \n  p.adjust.method &lt;- match.arg(p.adjust.method)\n  \n  group &lt;- unique(data[[g]])  # Get unique groups\n  n_group &lt;- length(group)\n  pairwise_table &lt;- c()\n  METHOD &lt;- \"Pairwise comparison using Hotelling's two sample T2-test\"\n  \n  # Loop through all unique pairwise combinations of groups\n  for (i in 1:(n_group - 1)) {\n    for (j in (i + 1):n_group) {\n      x &lt;- data[data[[g]] == group[i], vars]\n      y &lt;- data[data[[g]] == group[j], vars]\n      \n      # Perform Hotelling's T² test\n      htest &lt;- HotellingsT2Test(x, y)\n      tb &lt;- tibble(\n        pairs = paste0(group[i], \"~\", group[j]),\n        T.2 = htest$statistic,\n        df1 = htest$parameter[1],\n        df2 = htest$parameter[2],\n        p_value = htest$p.value\n      )\n      # Store the result in a list with descriptive names\n      pairwise_table &lt;- rbind(pairwise_table, tb)\n    }\n  }\n  pairwise_table &lt;- pairwise_table |&gt; \n    mutate(p_value = p.adjust(p_value, method = p.adjust.method))\n  \n  list(tbl = pairwise_table, method = METHOD, p.adjust.method=p.adjust.method)\n}\n\n\npairs.hotellings.T2.test(cbind(t3, t4, t8) ~ group, data = df) \n\n#&gt; $tbl\n#&gt; # A tibble: 3 × 5\n#&gt;   pairs T.2[,1]   df1   df2 p_value\n#&gt;   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1 1~2      3.46     3     3  0.168 \n#&gt; 2 1~3     13.0      3     4  0.0316\n#&gt; 3 2~3     14.0      3     5  0.0218\n#&gt; \n#&gt; $method\n#&gt; [1] \"Pairwise comparison using Hotelling's two sample T2-test\"\n#&gt; \n#&gt; $p.adjust.method\n#&gt; [1] \"holm\"\n\n\nThe results show that group 1 and group 2 do not show a significant difference in their mean vectors (p = 0.1676). Both group 1 and group 3, group 2 and group 3 show significant differences (p = 0.0316 and p = 0.0218, respectively), indicating that group 3 is notably different from both group 1 and group 2.\n\n\n\n\n\n\nCaution\n\n\n\nYou may have noticed that the conclusions made by pairs.hotellings.T2.test() (multivariate test) and by TukeyHSD() (multiple univariate tests) are not consistent.\n\n\n\n\n17.3.4 Multivariate analysis vs univariate analysis\nThe conclusions of a multivariate hypothesis test (MANOVA or Hotelling’s T² test) and performing multiple univariate hypothesis tests (ANOVA or t-test) may differ due to the following key reasons:\n\nCorrelations between variables\n\nMultivariate Tests: Multivariate hypothesis tests take into account the correlations between the m response variables. This means that these tests evaluate the joint distribution of the variables, assessing if there is a significant difference in the combined response vector.\nUnivariate Tests: Univariate tests treat each response variable independently. They ignore any relationships or correlations between the variables.\nImpact: If the response variables are correlated, a multivariate test may detect significant differences that individual univariate tests would miss, and vice versa. Ignoring the correlation structure in univariate tests can lead to inaccurate conclusions or lower statistical power.\n\nInflation of type I error\n\nMultivariate Tests: A multivariate test, such as MANOVA, tests the overall difference across multiple response variables at once, and hence controls for the overall type I error (false positive rate) at the desired significance level.\nUnivariate Tests: Performing multiple univariate tests increases the risk of type I error, especially without adjustment. If you perform multiple individual tests, the cumulative probability of making at least one false positive error increases. This problem can be addressed by adjusting the p-values using correction methods, but this reduces the power of the tests.\nImpact: Without correction, univariate tests might show more significant results due to an inflated type I error rate. After correction, the results of multiple univariate tests might be more conservative compared to a multivariate approach.\n\nPower of the test\n\nMultivariate Tests: Multivariate tests tend to have higher statistical power when the variables are correlated, meaning that they are better at detecting differences when response variables are related.\nUnivariate Tests: Separate univariate tests can lose power because they do not take advantage of the relationships between variables. Each test only considers one variable at a time, and the overall pattern of change across multiple variables may be missed.\nImpact: Multivariate tests may detect a significant overall difference even when none of the univariate tests do.\n\n\nConclusion\nIf the response variables are uncorrelated and you adjust for multiple comparisons in the univariate tests, the conclusions from the multivariate test and the separate univariate tests may be similar. If the response variables are correlated, or if no correction is applied to the univariate tests, the conclusions can differ significantly. Multivariate tests provide a more comprehensive picture by accounting for relationships between variables, whereas univariate tests focus on one variable at a time.\n\nExample 5 \nA researcher collected data of birth weight and length for two groups of newborns. Perform univariate and multivariate analysis separately.\n\n\n  Download data \n\n\ndf &lt;- read_csv(\n  \"datasets/ex17-05.csv\", \n  col_types = list(group = col_factor(), no = col_factor()))\n\n\nUnivariate analysis\n\nConduct two-sample t-test separately for birth weight and length to compare if there are significant differences between the two groups.\n\ndf |&gt; \n  select(weight, height) |&gt; \n  lapply(\\(x) t.test(x ~ group, data = df)) \n\n#&gt; $weight\n#&gt; \n#&gt;  Welch Two Sample t-test\n#&gt; \n#&gt; data:  x by group\n#&gt; t = -1.6219, df = 13.822, p-value = 0.1274\n#&gt; alternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n#&gt; 95 percent confidence interval:\n#&gt;  -0.53742421  0.07492421\n#&gt; sample estimates:\n#&gt; mean in group 1 mean in group 2 \n#&gt;         3.28750         3.51875 \n#&gt; \n#&gt; \n#&gt; $height\n#&gt; \n#&gt;  Welch Two Sample t-test\n#&gt; \n#&gt; data:  x by group\n#&gt; t = -0.036693, df = 11.938, p-value = 0.9713\n#&gt; alternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n#&gt; 95 percent confidence interval:\n#&gt;  -7.551649  7.301649\n#&gt; sample estimates:\n#&gt; mean in group 1 mean in group 2 \n#&gt;          52.500          52.625\n\n\n\nMultivariate analysis\n\nFor multivariate analysis, you can use Hotelling’s T² test to simultaneously compare the mean vectors of birth weight and length between the two groups.\n\nHotellingsT2Test(cbind(weight, height) ~ group, data = df)\n\n#&gt; \n#&gt;  Hotelling's two sample T2-test\n#&gt; \n#&gt; data:  cbind(weight, height) by group\n#&gt; T.2 = 4.5811, df1 = 2, df2 = 13, p-value = 0.0312\n#&gt; alternative hypothesis: true location difference is not equal to c(0,0)\n\n\nIt is clear that the results of univariate and multivariate analysis are not consistent. The multivariate analysis reveals the difference between groups, while the univariate analysis not. Below is a visualization of difference based on multivariate data.\n\nggplot(df, aes(height, weight, color = group)) + \n  geom_point()",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Multivariate data</span>"
    ]
  },
  {
    "objectID": "multivariate-groups-diff.html#repeated-measures-manova",
    "href": "multivariate-groups-diff.html#repeated-measures-manova",
    "title": "17  Multivariate data",
    "section": "17.4 Repeated measures MANOVA",
    "text": "17.4 Repeated measures MANOVA\nRepeated measures MANOVA is an extension of MANOVA that accounts for multiple measurements of the same subjects over time. It’s a robust technique used when there are several dependent variables and repeated measurements for each subject, considering the correlations between these repeated measurements.\n\nExample 6 \nThe weight of 10 obese patients was recorded before and during the 1st to 4th weeks of taking weight-loss medication under the guidance of a doctor, following a unified standard. Analyze the effectiveness of the weight-loss medication.\n\n\n  Download data \n\n\ndf &lt;- read_csv(\"datasets/ex17-06.csv\", col_types = list(no = col_factor()))\n\ntb &lt;- df |&gt; \n  pivot_longer(\n    contains(\"t\"),\n    names_to = \"time\",\n    names_prefix = \"t\",\n    values_to = \"weight\",\n    names_transform = as.ordered\n  ) \n\nSince there are multiple time points, a repeated measures ANOVA is suitable. This will test whether there is a significant difference in the weights across time points.\n\naov(weight ~ time + Error(no), data = tb) |&gt; summary()\n\n#&gt; \n#&gt; Error: no\n#&gt;           Df Sum Sq Mean Sq F value Pr(&gt;F)\n#&gt; Residuals  9  11871    1319               \n#&gt; \n#&gt; Error: Within\n#&gt;           Df Sum Sq Mean Sq F value Pr(&gt;F)    \n#&gt; time       4  403.8     101   101.4 &lt;2e-16 ***\n#&gt; Residuals 36   35.8       1                   \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIf you have more complex data, such as if patients drop out at different times, you can use a linear mixed-effects model to account for missing data or individual differences.\n\nlme(weight ~ time, random = ~ 1 | no, data = tb) |&gt; anova()\n\n#&gt;             numDF denDF  F-value p-value\n#&gt; (Intercept)     1    36 639.0199  &lt;.0001\n#&gt; time            4    36 101.4041  &lt;.0001\n\n\nSpecifically, you can treat the measurements at each time point as separate variables (since they represent the same outcome measured at different time points). This is common in repeated measures analysis, but in this case, we will treat the time points as distinct variables for a multivariate analysis.\n\nmean0 &lt;- rep(0, times = 4)\ndf |&gt; \n  mutate(t2 = t2 - t1, t3 = t3 - t1, t4 = t4 - t1, t5 = t5 - t1, .keep = \"none\") |&gt;\n  HotellingsT2Test(mu = mean0) \n\n#&gt; \n#&gt;  Hotelling's one sample T2-test\n#&gt; \n#&gt; data:  mutate(df, t2 = t2 - t1, t3 = t3 - t1, t4 = t4 - t1, t5 = t5 -     t1, .keep = \"none\")\n#&gt; T.2 = 41.308, df1 = 4, df2 = 6, p-value = 0.0001676\n#&gt; alternative hypothesis: true location is not equal to c(0,0,0,0)\n\n\n\nlme(weight ~ time, random = ~ 1 | no, data = tb) |&gt; \n  emmeans(specs = ~ time) |&gt; \n1  contrast(method = \"poly\")\n\n\n1\n\nThis type of contrast can be useful for examining trends over ordered factors like time.\n\n\n\n\n#&gt;  contrast  estimate    SE df t.ratio p.value\n#&gt;  linear      -20.09 0.998 36 -20.134  &lt;.0001\n#&gt;  quadratic     0.47 1.180 36   0.398  0.6929\n#&gt;  cubic         0.23 0.998 36   0.231  0.8190\n#&gt;  quartic      -0.43 2.640 36  -0.163  0.8715\n#&gt; \n#&gt; Degrees-of-freedom method: containment\n\n\nThe linear trend is significant (p &lt; 0.0001), indicating that there is a significant linear weight reduction over time.\nFinally, to visualize the weight reduction trend, you can plot the average weight over time:\n\nggplot(tb, aes(x = time, y = weight, group = 1)) + \n  geom_point(stat = \"summary\", fun = \"mean\") +\n  geom_line(stat = \"summary\", fun = \"mean\") \n\n\n\n\n\n\n\n\nIf you want to plot individual patient data, you can adjust the code to explicitly set the group, color and linetype aesthetics to no:\n\nggplot(tb, aes(time, weight, color = no, linetype = no, group = no)) +\n  geom_point(size = 0.9) +\n  geom_line() +\n  labs(color = \"\", linetype = \"\")",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Multivariate data</span>"
    ]
  },
  {
    "objectID": "multivariate-groups-diff.html#profile-analysis",
    "href": "multivariate-groups-diff.html#profile-analysis",
    "title": "17  Multivariate data",
    "section": "17.5 Profile analysis",
    "text": "17.5 Profile analysis\nProfile Analysis is a multivariate technique that examines the relationship between two or more groups across multiple related variables. It is commonly used when the same set of measurements (variables) is collected for different groups, and the goal is to assess whether these groups differ in their “profiles” across those variables.\nIn essence, profile analysis is a multivariate extension of repeated measures ANOVA where each variable is treated as a repeated measure.\nProfile analysis typically focuses on three main hypothesis:\n\nTest of parallelism (interaction between factors of group and variable/time)\nDo the profiles of the groups (mean values across the variables) have the same shape? If the test is significant, it means that the two or more groups differ in the pattern or trend across the variables.\nTest of levels (main effect of group)\nAre the overall mean levels of the variables different between groups (i.e., are the profiles at different heights, but still parallel)? A significant result indicates that there is a difference in the average levels of the variables across groups, although their trends are similar.\nTest of flatness (main effect of variables)\nIs the profile flat, meaning there is no significant difference between the variables? A significant test means that there are significant differences between the variables for each group.\n\n\nExample 7 \nA health status survey was conducted on 50 master’s students and 30 doctoral students. The questionnaire included 7 questions, and responses to each question were rated on a 4-point scale from good to poor (scored as 1, 2, 3, and 4, respectively). The question is: are the responses to each question the same between the master’s students and the doctoral students?\n\n\n  Download data \n\n\ndf &lt;- read_csv(\"datasets/ex17-07.csv\", col_types = list(group = col_factor()))\n\ntb &lt;- df |&gt; \n  pivot_longer(\n    cols = contains(\"x\"),\n    names_to = \"ques\",\n    names_prefix = \"x\",\n    values_to = \"y\"\n  )\n\nPlot profiles using ggplot2\n\ndf |&gt; \n  group_by(group) |&gt; \n  summarise(across(contains(\"x\"), mean)) |&gt; \n1  pivot_longer(\n    cols = contains(\"x\"),\n    names_to = \"ques\",\n    names_prefix = \"x\",\n    values_to = \"y\"\n  ) |&gt; \n  ggplot(aes(x = ques, y = y, group = group, linetype = group, shape = group, color = group)) + \n  geom_point(size = 2) +\n  geom_line() +\n  labs(x = \"Question\", y = \"Average score\") +\n  scale_color_manual(\n    values = c(\"blue\", \"red\"),\n    labels = c(\"Master\", \"Doctor\")\n  ) +\n  scale_shape_manual(\n    values = c(1, 5), \n    labels = c(\"Master\", \"Doctor\")\n  ) +\n  scale_linetype_manual(\n    values = c(1, 2), \n    labels = c(\"Master\", \"Doctor\")\n  ) +\n  ylim(1.5, 2.8) +\n  theme(legend.title = element_blank())\n\n\n1\n\nReshape data to long format for visualization\n\n\n\n\n\n\n\n\n\n\n\nThe pbg() function of profileR package implements the three hypothesis tests. These tests are whether the profiles are parallel, have equal levels, and are flat across groups defined by the grouping variable. If parallelism is rejected, the other two tests are not necessary. In that case, flatness may be assessed within each group, and various within- and between-group contrasts may be analyzed.\n\nwith(df, pbg(cbind(x1, x2, x3 ,x4, x5, x6, x7), group, original.names = T)) |&gt;\n  summary()\n\n#&gt; Call:\n#&gt; pbg(data = cbind(x1, x2, x3, x4, x5, x6, x7), group = group, \n#&gt;     original.names = T)\n#&gt; \n#&gt; Hypothesis Tests:\n#&gt; $`Ho: Profiles are parallel`\n#&gt;   Multivariate.Test  Statistic  Approx.F num.df den.df   p.value\n#&gt; 1             Wilks 0.96198449 0.4807999      6     73 0.8205697\n#&gt; 2            Pillai 0.03801551 0.4807999      6     73 0.8205697\n#&gt; 3  Hotelling-Lawley 0.03951780 0.4807999      6     73 0.8205697\n#&gt; 4               Roy 0.03951780 0.4807999      6     73 0.8205697\n#&gt; \n#&gt; $`Ho: Profiles have equal levels`\n#&gt;             Df Sum Sq Mean Sq F value Pr(&gt;F)\n#&gt; group        1   0.05 0.04959   0.276  0.601\n#&gt; Residuals   78  13.99 0.17936               \n#&gt; \n#&gt; $`Ho: Profiles are flat`\n#&gt;          F df1 df2      p-value\n#&gt; 1 10.83958   6  73 1.355666e-08\n\n\nThe output summarizes the results of three hypothesis tests from a profile analysis, which tests different aspects of the profiles between two groups:\n\nTest of parallelism\n\nStatistics: Four tests were conducted: Wilks’ Lambda, Pillai’s Trace, Hotelling-Lawley Trace, and Roy’s Largest Root, p-value: 0.8205697 for all four tests.\nInterpretation: Since the p-value is 0.8205697, larger than the typical significance level 0.05, fail to reject the null hypothesis. This means the profiles of the two groups are parallel, indicating that the trend in scores across the seven questions is similar between the groups.\n\nTest of equal levels\n\nStatistics: F-value is 0.276, with a p-value of 0.601.\nInterpretation: Since p-value is 0.601, greater than 0.05, fail to reject the null hypothesis. This suggests that the overall levels of the profiles are not significantly different between the two groups, indicating similar average scores.\n\nTest of flatness\n\nStatistics: F-value is 10.83958, and the p-value is 1.36e-08.\nInterpretation: The p-value is less than 0.05, so reject the null hypothesis. This means the profiles are not flat, suggesting that the scores vary significantly across the seven questions within each group.\n\n\nConclusion:\nThe profiles are parallel, meaning the trends across the variables are similar for both groups.\nThe overall levels of the profiles are not significantly different, meaning the average scores are comparable between the groups. The profiles are not flat, indicating that within each group, there is significant variation in scores across the variables.\nIn summary, while both groups show similar trends across the questions, the scores for each question vary within the groups.",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Multivariate data</span>"
    ]
  },
  {
    "objectID": "multiple-linear-regression.html",
    "href": "multiple-linear-regression.html",
    "title": "18  Multiple linear regression analysis",
    "section": "",
    "text": "18.1 Prerequisite\nMultiple linear regression is a statistical technique used to model the relationship between one dependent (response) variable and two or more independent (predictor) variables. It extends simple linear regression, which only considers one predictor variable, to situations where multiple variables contribute to predicting the outcome.\nlibrary(tidyverse)\nlibrary(lm.beta)\nlibrary(equatiomatic)\nlibrary(leaps)\nlibrary(glmnet)\nlibrary(MASS)\nlibrary(olsrr)\nlibrary(lmtest)\nlibrary(sandwich)",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Multiple linear regression analysis</span>"
    ]
  },
  {
    "objectID": "multiple-linear-regression.html#multiple-linear-regression",
    "href": "multiple-linear-regression.html#multiple-linear-regression",
    "title": "18  Multiple linear regression analysis",
    "section": "18.2 Multiple linear regression",
    "text": "18.2 Multiple linear regression\n\n18.2.1 Multiple linear regression model\nThe general form for multiple linear regression model is:\n\nY = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\cdots + \\beta_nX_n + \\epsilon\n\nwhere Y is the dependent variable, X_1, X_2, …, X_n are the independent variables, \\beta_0 is the intercept (the value of Y when all X’s are zero), \\beta_1, \\beta_2, …, \\beta_n are the coefficients for each independent variable, \\epsilon is the error term.\nMultiple minear regression relies on several key assumptions to produce valid results. If these assumptions are violated, the model’s results may be biased or misleading. Here are the primary assumptions:\n\nThe relationship between the dependent and independent variables must be linear.\nObservations should be independent of each other.\nThe variance of the residuals should be constant across all levels of the independent variables.\nThe residuals (errors) should be normally distributed.\n\n\n\n18.2.2 Estimating regression coefficients\nTo estimate the regression coefficients \\beta_0, \\beta_1, \\dots, \\beta_n, use statistical software to fit the model to your data. This process minimizes the sum of the squared differences between the observed and predicted values (ordinary least squares, or OLS). The OLS estimator for \\boldsymbol{\\beta} is:\n\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{Y}\n\nThis formula minimizes the sum of squared residuals and provides the best linear unbiased estimates of the coefficients under the assumptions of the classical linear regression model.\n\nExample 1 \nGiven the measurements of total serum cholesterol (X1), triglycerides (X2), fasting insulin (X3), glycated hemoglobin (X4), and fasting blood glucose (Y) from 27 diabetic patients, establish a multiple linear regression equation for fasting blood glucose with the other indicators.\n\n\n  Download data \n\n\ndf &lt;- read_csv(\"datasets/ex18-01.csv\", show_col_types = F)\n\nYou can use lm() function to perform the regression analysis, with Y as the dependent variable and X1, X2, X3, X4 as the independent variables.\n\nmodel &lt;- lm(Y ~ X1 + X2 + X3 + X4, data = df)\ncoef(model)\n\n#&gt; (Intercept)          X1          X2          X3          X4 \n#&gt;   5.9432678   0.1424465   0.3514655  -0.2705853   0.6382012\n\n\nThe code can also be written as:\n\nmodel &lt;- lm(Y ~ ., data = df)\ncoef(model)\n\n#&gt; (Intercept)          X1          X2          X3          X4 \n#&gt;   5.9432678   0.1424465   0.3514655  -0.2705853   0.6382012\n\n\nYou can use the coef() function to extract regression coefficients from the model. The extract_eq() function from equatiomatic package can extract regression equation from the fitted model:\n\nextract_eq(model, use_coefs = T, coef_digits = 4, font_size = \"small\")     \n\n\n\\small\n\\begin{aligned}\n\\operatorname{\\widehat{Y}} &= 5.9433 + 0.1424(\\operatorname{X1}) + 0.3515(\\operatorname{X2}) - 0.2706(\\operatorname{X3}) + 0.6382(\\operatorname{X4})\n\\end{aligned}\n\n\n\n\n\n18.2.3 Evaluating the model\nIn multiple linear regression, hypothesis tests help evaluate the significance of the model and individual predictors, while assumption tests ensure that the model is reliable. The F-test determines if the model is significant overall, t-tests assess the impact of individual predictors, and R² and adjusted R² indicate the model’s explanatory power. Residual analysis further ensures the validity of the model’s assumptions.\n\nF-test\n\n\nH_0: All regression coefficients are zero, meaning none of the independent variables have a significant effect on the dependent variable.\nH_1: At least one regression coefficient is not zero, indicating that at least one independent variable has a significant effect.\n\nThe F-test evaluates the overall significance of the model. The formula for the F-statistic is:\n\nF = \\frac{MSR}{MSE} = \\frac{\\text{Mean Square Regression}}{\\text{Mean Square Error}}\n\nIf the F-value is large and the p-value is small (typically &lt; 0.05), reject the null hypothesis, suggesting the model is statistically significant.\n\nt-test\n\n\nH_0: A particular regression coefficient is zero, indicating that the corresponding independent variable has no significant effect on the dependent variable.\nH_1: The regression coefficient is not zero, implying the independent variable has a significant effect.\n\nThe t-test for each coefficient is used to assess whether each predictor significantly contributes to the model:\n\nt = \\frac{\\hat{\\beta}_i}{SE(\\hat{\\beta}_i)}\n\nwhere \\hat{\\beta}_i is the estimated coefficient, and SE(\\hat{\\beta}_i) is the standard error.\nIf the t-value is large and the p-value is small (typically &lt; 0.05), reject the null hypothesis, indicating that the variable has a significant effect on the dependent variable.\n\nGoodness of fit\n\n\nR^2 (Coefficient of determination)\nThis indicates the proportion of variance in the dependent variable explained by the independent variables. It ranges from 0 to 1, with higher values suggesting a better fit. The R^2 is defined as:\n\nR^2 = 1 - \\frac{SS_{\\text{residuals}}}{SS_{\\text{total}}}\n\nwhere SS_{\\text{residuals}} denotes the sum of squared residuals (errors), i.e., the unexplained variance, SS_{\\text{total}} is the total sum of squares, i.e., the total variance of the dependent variable.\nAdjusted R^2\nIt is formulated as:\n\n\\text{adjusted } R^2 = 1 - \\left( \\frac{(1 - R^2)(n - 1)}{n - p - 1} \\right)\n\nwhere n is the number of observations, p is the number of predictors (independent variables). This adjustment penalizes models that include too many predictors that don’t contribute to a better fit. A higher adjusted R² indicates a model with strong explanatory power and parsimony.\nR (Multiple correlation coefficient)\nThis measures the strength of the linear relationship between a set of independent variables (predictors) and a dependent variable in a multiple linear regression model. The R is calculated as:\n\nR = \\sqrt{R^2}\n\nwhere R^2 is the coefficient of determination.\n\nHere summary() is used to produce result summaries of the results from the model fitting functions.\n\nsummary(model)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = Y ~ ., data = df)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -3.6268 -1.2004 -0.2276  1.5389  4.4467 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)  \n#&gt; (Intercept)   5.9433     2.8286   2.101   0.0473 *\n#&gt; X1            0.1424     0.3657   0.390   0.7006  \n#&gt; X2            0.3515     0.2042   1.721   0.0993 .\n#&gt; X3           -0.2706     0.1214  -2.229   0.0363 *\n#&gt; X4            0.6382     0.2433   2.623   0.0155 *\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 2.01 on 22 degrees of freedom\n#&gt; Multiple R-squared:  0.6008, Adjusted R-squared:  0.5282 \n#&gt; F-statistic: 8.278 on 4 and 22 DF,  p-value: 0.0003121\n\n\nFrom the result summaries, we can find that the overall model is statistically significant (F-statistic is 8.278, p &lt; 0.001), indicating that at least one of the predictors is related to the fasting blood glucose.\nX1 (t=0.390, p=0.7006) and X2 (t=1.721, p=0.0993) are not statistically significant predictors, X3 (t=0-2.229, p=0.0363) and X4 (t=2.623, p=0.0155) are significant in this model.\nThe R^2 = 0.6008, this indicates that approximately 60.08% of the variance in fasting blood glucose is explained by the model, which suggests a moderate fit.\nThe adjusted R^2 = 0.5282, this indicates after adjusting for the number of predictors, the model explains around 52.82% of the variance.\n\n\n18.2.4 Standardized partial regression\nIn multiple linear regression, standardized partial regression coefficients (also known as standardized beta coefficients) are used to compare the relative importance of each predictor variable in the model, as they are measured on the same scale (standard deviations). They are obtained after standardizing both the independent and dependent variables, so that each variable has a mean of zero and a standard deviation of one.\nThe standardized regression coefficient \\beta_i^* is computed as:\n\n\\beta_i^* = \\beta_i \\times \\frac{\\sigma_{X_i}}{\\sigma_Y}\n\nwhere \\beta_iis the unstandardized regression coefficient for the predictorX_i,\\sigma_{X_i} is the standard deviation of predictor X_i, \\sigma_Y is the standard deviation of the dependent variable Y.\nYou can obtain standardized coefficients by using the scale() function to standardize the variables before performing the regression. Standardizing ensures that each variable has a mean of 0 and a standard deviation of 1, allowing you to directly interpret the coefficients as standardized coefficients.\n\nscale(df) |&gt; \n  as_tibble() |&gt; \n  lm(Y ~ X1 + X2 + X3 + X4, data = _) |&gt; \n  coefficients()\n\n#&gt;   (Intercept)            X1            X2            X3            X4 \n#&gt;  6.525869e-16  7.757862e-02  3.093081e-01 -3.394808e-01  3.977418e-01\n\n\nAlternatively, the lm.beta package provides a convenient lm.beta() function that takes an ordinary linear model and returns the standardized coefficients directly.\n\nlm.beta(model) |&gt; coef()\n\n#&gt; (Intercept)          X1          X2          X3          X4 \n#&gt;          NA  0.07757862  0.30930814 -0.33948078  0.39774184\n\n\nIn a standardized regression model, the intercept is typically not interpreted because it is not directly comparable to the other variables after standardization. Therefore, it is shown as NA in the output of lm.beta(). Among the predictors, X4 has the strongest positive effect on Y, followed by X2. X3 has a significant negative impact on Y, while X1 has a relatively small effect.",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Multiple linear regression analysis</span>"
    ]
  },
  {
    "objectID": "multiple-linear-regression.html#independent-variable-selection",
    "href": "multiple-linear-regression.html#independent-variable-selection",
    "title": "18  Multiple linear regression analysis",
    "section": "18.3 Independent variable selection",
    "text": "18.3 Independent variable selection\nThe selection of independent variables refers to the process of choosing the most significant predictors (independent variables) to construct a model in multiple regression analysis or other predictive models. By selecting independent variables, you can improve the model’s prediction performance, simplify the model, and reduce the risk of overfitting. Common methods for independent variable selection include:\n\n18.3.1 All-subsets regression\nAll-subsets regression (also called best subset regression) is a method that evaluates all possible subsets of the predictor variables in order to identify the best model or models. This approach is exhaustive, meaning it fits regression models for every possible combination of predictors and selects the one(s) based on specific criteria like adjusted R^2, AIC, BIC, or Mallow’s C_p.\n\nAdjusted R^2\nAdjusted R^2 is a modified version of the R^2 that adjusts for the number of predictors (independent variables) in a regression model. Unlike the regular R^2, which always increases when new predictors are added, Adjusted R^2 compensates for the model complexity, ensuring that only significant predictors improve the model’s fit.\nAkaike information criterion (AIC)\nAIC is based on information theory and estimates the relative quality of statistical models for a given dataset. It provides a measure of the goodness of fit while penalizing for the number of parameters in the model.\nThe formula for AIC is:\n\nAIC = 2k - 2\\log(L)\n\nwhere k is the number of parameters in the model, L is the likelihood of the model (how well the model fits the data). Among competing models, the one with the lowest AIC is preferred.\nBayesian information criterion (BIC)\nBIC is similar to AIC but includes a stronger penalty for the number of parameters, especially as the sample size increases. BIC is derived from a Bayesian perspective and is more conservative in model selection compared to AIC.\nThe formula for BIC is:\n\nBIC = k\\log(n) - 2\\log(L)\n\nwhere k is the number of parameters in the model, n is the sample size, L is the likelihood of the model. Like AIC, a lower BIC value indicates a better model.\nMallow’s C_p\nMallows’ C_p is a statistical criterion used for model selection in regression analysis. It helps identify models that offer a good balance between complexity (number of predictors) and goodness of fit. Specifically, it is used to compare regression models with different numbers of predictors to select the one that is neither underfitting nor overfitting the data.\nMallows’ C_p is defined as:\n\nC_p = \\frac{\\text{RSS}_p}{\\hat{\\sigma}^2} + 2p - n\n\nwhere \\text{RSS}_p is the residual sum of squares for the model, \\hat{\\sigma}^2 is an estimate of the true error variance, p is the number of parameters in the model (including the intercept), n is the number of observations.\n\nIf C_p \\approx p: The model is considered to have an appropriate number of predictors. The C_p value should be close to the number of predictors in the model.\nIf C_p &gt; p: This suggests that the model is overfitting, meaning it’s too complex and is fitting the noise in the data.\nIf C_p &lt; p: This suggests that the model is underfitting, meaning it’s too simple and is missing important predictors.\n\nIn practice, the goal is to find the model with the lowest C_p that is close to p.\n\n\nExample 2 \nUse the all-subsets regression method to select the independent variables of the data in Example 1.\n\nThe regsubsets() function from leaps package is a tool primarily used for performing best subset selection in regression analysis. It helps identify the best-fitting model with a given number of predictors by comparing all possible subsets of predictors based on specific criteria like Adjusted R-squared, AIC, BIC, etc.\n\nmodel &lt;- regsubsets(Y ~ ., data = df, nbest = 1, method = \"exhaustive\") \nsummary(model)\n\n#&gt; Subset selection object\n#&gt; Call: regsubsets.formula(Y ~ ., data = df, nbest = 1, method = \"exhaustive\")\n#&gt; 4 Variables  (and intercept)\n#&gt;    Forced in Forced out\n#&gt; X1     FALSE      FALSE\n#&gt; X2     FALSE      FALSE\n#&gt; X3     FALSE      FALSE\n#&gt; X4     FALSE      FALSE\n#&gt; 1 subsets of each size up to 4\n#&gt; Selection Algorithm: exhaustive\n#&gt;          X1  X2  X3  X4 \n#&gt; 1  ( 1 ) \" \" \" \" \" \" \"*\"\n#&gt; 2  ( 1 ) \"*\" \" \" \" \" \"*\"\n#&gt; 3  ( 1 ) \" \" \"*\" \"*\" \"*\"\n#&gt; 4  ( 1 ) \"*\" \"*\" \"*\" \"*\"\n\n\nUse the plot() function to visualize different scale values for the model.\n\nplot(model, scale = \"adjr2\")   # adjusted R²\nplot(model, scale = \"Cp\")      # Mallow's Cp\nplot(model, scale = \"bic\")     # BIC\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFind the best model based on a specific criteria:\n\nwhich.max(summary(model)$adjr2)\n\n#&gt; [1] 3\n\nwhich.min(summary(model)$cp)\n\n#&gt; [1] 3\n\nwhich.min(summary(model)$bic)\n\n#&gt; [1] 3\n\n\nPrint the selected variables:\n\n# Find the model with the largest adjustment R²\nmax_adjr2_index &lt;- which.max(summary(model)$adjr2)\ncat(\"The model with the maximum adjusted R² is at index:\", max_adjr2_index, \"\\n\")\n\n#&gt; The model with the maximum adjusted R² is at index: 3\n\n# Gets the variables selected in the model\nselected_vars &lt;- summary(model)$which[max_adjr2_index, ]\nselected_vars\n\n#&gt; (Intercept)          X1          X2          X3          X4 \n#&gt;        TRUE       FALSE        TRUE        TRUE        TRUE\n\n# Print the selected variables\nselected_vars_names &lt;- names(selected_vars)[selected_vars]\ncat(\"The selected variables in the best model (with maximum adjusted R²):\", paste(selected_vars_names, collapse = \", \"), \"\\n\")\n\n#&gt; The selected variables in the best model (with maximum adjusted R²): (Intercept), X2, X3, X4\n\n\nAlthough this method is likely to find the best-fitting model, its computational complexity increases sharply with the increase of the number of independent variables, so it is usually only suitable for the case of a small number of independent variables in practical application.\n\n\n18.3.2 Stepwise regression\nThe stepwise regression is a method for selecting the most significant variables in a regression model by adding or removing predictor variables based on specific criteria (e.g., AIC, BIC, p-values). There are three common types of stepwise regression:\n\nForward selection: Starts with no predictors and adds the most significant predictor at each step.\nBackward elimination: Starts with all predictors and removes the least significant predictor at each step.\nStepwise selection: A combination of forward selection and backward elimination where predictors can be added or removed at each step based on their significance.\n\nThe step() function in base R is the most commonly used function for performing stepwise regression based on AIC.\n\n# Fit a full model\nfull_model &lt;- lm(Y ~ X1 + X2 + X3 + X4, data = df)\n\n# Perform stepwise regression using AIC (default)\nstepwise_model &lt;- step(full_model, direction = \"both\")\n\n#&gt; Start:  AIC=42.16\n#&gt; Y ~ X1 + X2 + X3 + X4\n#&gt; \n#&gt;        Df Sum of Sq     RSS    AIC\n#&gt; - X1    1    0.6129  89.454 40.343\n#&gt; &lt;none&gt;               88.841 42.157\n#&gt; - X2    1   11.9627 100.804 43.568\n#&gt; - X3    1   20.0635 108.905 45.655\n#&gt; - X4    1   27.7939 116.635 47.507\n#&gt; \n#&gt; Step:  AIC=40.34\n#&gt; Y ~ X2 + X3 + X4\n#&gt; \n#&gt;        Df Sum of Sq     RSS    AIC\n#&gt; &lt;none&gt;               89.454 40.343\n#&gt; + X1    1     0.613  88.841 42.157\n#&gt; - X3    1    25.690 115.144 45.159\n#&gt; - X2    1    26.530 115.984 45.356\n#&gt; - X4    1    32.269 121.723 46.660\n\n# View the summary of the final model\nstepwise_model\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = Y ~ X2 + X3 + X4, data = df)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)           X2           X3           X4  \n#&gt;      6.4996       0.4023      -0.2870       0.6632\n\n\n\nextract_eq(stepwise_model, use_coefs = T, coef_digits = 4, font_size = \"small\")  \n\n\n\\small\n\\begin{aligned}\n\\operatorname{\\widehat{Y}} &= 6.4996 + 0.4023(\\operatorname{X2}) - 0.287(\\operatorname{X3}) + 0.6632(\\operatorname{X4})\n\\end{aligned}\n\n\n\nYou can use the stepAIC() function from the MASS package, which works similarly to step() but is more flexible.\n\nstepwise_model &lt;- stepAIC(full_model, direction = \"both\")\n\n#&gt; Start:  AIC=42.16\n#&gt; Y ~ X1 + X2 + X3 + X4\n#&gt; \n#&gt;        Df Sum of Sq     RSS    AIC\n#&gt; - X1    1    0.6129  89.454 40.343\n#&gt; &lt;none&gt;               88.841 42.157\n#&gt; - X2    1   11.9627 100.804 43.568\n#&gt; - X3    1   20.0635 108.905 45.655\n#&gt; - X4    1   27.7939 116.635 47.507\n#&gt; \n#&gt; Step:  AIC=40.34\n#&gt; Y ~ X2 + X3 + X4\n#&gt; \n#&gt;        Df Sum of Sq     RSS    AIC\n#&gt; &lt;none&gt;               89.454 40.343\n#&gt; + X1    1     0.613  88.841 42.157\n#&gt; - X3    1    25.690 115.144 45.159\n#&gt; - X2    1    26.530 115.984 45.356\n#&gt; - X4    1    32.269 121.723 46.660\n\nstepwise_model\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = Y ~ X2 + X3 + X4, data = df)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)           X2           X3           X4  \n#&gt;      6.4996       0.4023      -0.2870       0.6632\n\n\n\nextract_eq(stepwise_model, use_coefs = T, coef_digits = 4, font_size = \"small\")  \n\n\n\\small\n\\begin{aligned}\n\\operatorname{\\widehat{Y}} &= 6.4996 + 0.4023(\\operatorname{X2}) - 0.287(\\operatorname{X3}) + 0.6632(\\operatorname{X4})\n\\end{aligned}\n\n\n\nIf you want to use other model selection criteria, the olsrr package provides a more intuitive way to perform stepwise regression, with options to perform forward, backward, and stepwise regression.\n\nstepwise_model &lt;- ols_step_both_adj_r2(full_model)\nstepwise_model |&gt; plot()\nstepwise_model\n\n#&gt; \n#&gt; \n#&gt;                              Stepwise Summary                             \n#&gt; ------------------------------------------------------------------------\n#&gt; Step    Variable        AIC        SBC       SBIC       R2       Adj. R2 \n#&gt; ------------------------------------------------------------------------\n#&gt;  0      Base Model    137.574    140.166    59.411    0.00000    0.00000 \n#&gt;  1      X4 (+)        127.028    130.916    49.427    0.37166    0.34653 \n#&gt;  2      X1 (+)        123.692    128.876    46.765    0.48434    0.44137 \n#&gt;  3      X3 (+)        122.191    128.670    46.208    0.54705    0.48797 \n#&gt;  4      X2 (+)        120.780    128.555    46.327    0.60081    0.52823 \n#&gt;  5      X1 (-)        118.966    125.445    43.998    0.59805    0.54563 \n#&gt; ------------------------------------------------------------------------\n#&gt; \n#&gt; Final Model Output \n#&gt; ------------------\n#&gt; \n#&gt;                          Model Summary                          \n#&gt; ---------------------------------------------------------------\n#&gt; R                       0.773       RMSE                 1.820 \n#&gt; R-Squared               0.598       MSE                  3.313 \n#&gt; Adj. R-Squared          0.546       Coef. Var           16.537 \n#&gt; Pred R-Squared          0.365       AIC                118.966 \n#&gt; MAE                     1.460       SBC                125.445 \n#&gt; ---------------------------------------------------------------\n#&gt;  RMSE: Root Mean Square Error \n#&gt;  MSE: Mean Square Error \n#&gt;  MAE: Mean Absolute Error \n#&gt;  AIC: Akaike Information Criteria \n#&gt;  SBC: Schwarz Bayesian Criteria \n#&gt; \n#&gt;                                ANOVA                                \n#&gt; -------------------------------------------------------------------\n#&gt;                Sum of                                              \n#&gt;               Squares        DF    Mean Square      F         Sig. \n#&gt; -------------------------------------------------------------------\n#&gt; Regression    133.098         3         44.366    11.407     1e-04 \n#&gt; Residual       89.454        23          3.889                     \n#&gt; Total         222.552        26                                    \n#&gt; -------------------------------------------------------------------\n#&gt; \n#&gt;                                   Parameter Estimates                                    \n#&gt; ----------------------------------------------------------------------------------------\n#&gt;       model      Beta    Std. Error    Std. Beta      t        Sig      lower     upper \n#&gt; ----------------------------------------------------------------------------------------\n#&gt; (Intercept)     6.500         2.396                  2.713    0.012     1.543    11.456 \n#&gt;          X4     0.663         0.230        0.413     2.880    0.008     0.187     1.140 \n#&gt;          X3    -0.287         0.112       -0.360    -2.570    0.017    -0.518    -0.056 \n#&gt;          X2     0.402         0.154        0.354     2.612    0.016     0.084     0.721 \n#&gt; ----------------------------------------------------------------------------------------\n\n\n\n\n\n\n\n\n\n\nstepwise_model[[\"model\"]] |&gt; \nextract_eq(use_coefs = T, coef_digits = 4, font_size = \"small\")  \n\n\n\\small\n\\begin{aligned}\n\\operatorname{\\widehat{Y}} &= 6.4996 + 0.6632(\\operatorname{X4}) - 0.287(\\operatorname{X3}) + 0.4023(\\operatorname{X2})\n\\end{aligned}",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Multiple linear regression analysis</span>"
    ]
  },
  {
    "objectID": "multiple-linear-regression.html#application-of-multiple-linear-regression-and-some-considerations",
    "href": "multiple-linear-regression.html#application-of-multiple-linear-regression-and-some-considerations",
    "title": "18  Multiple linear regression analysis",
    "section": "18.4 Application of multiple linear regression and some considerations",
    "text": "18.4 Application of multiple linear regression and some considerations\n\n18.4.1 Application of multiple linear regression\nMultiple linear regression is widely used in medical research for predicting patient outcomes, analyzing the effects of various biological markers on health results, and controlling for confounding variables. Below are several scenarios where MLR is applied in the medical field:\n\nDisease risk prediction\nMultiple linear regression can be used to predict disease risk by incorporating multiple biomarkers or clinical variables. For example, factors such as age, weight, cholesterol level, and blood pressure can be used to predict the risk of cardiovascular disease.\n\nlm(heart_disease_risk ~ age + cholesterol + blood_pressure + smoking_status, data = patients_data)\n\nDrug efficacy analysis\nIn clinical trials, multiple linear regression is useful for evaluating the combined effects of multiple factors on drug efficacy. For example, when assessing blood sugar control in diabetic patients, variables such as drug dosage, age, weight, and lifestyle habits can be included to understand their effects on glucose levels.\n\nlm(fasting_glucose ~ drug_dose + age + weight + exercise, data = diabetes_data)\n\nControlling for confounding variables\nConfounding factors such as age, gender, or lifestyle can affect the relationships between variables. Multiple linear regression helps control for these confounders by incorporating multiple variables, allowing a more accurate estimation of the primary exposure variable’s effect (e.g., treatment method) on health outcomes.\n\nlm(treatment_effect ~ age + gender + treatment_group, data = clinical_data)\n\nPrognostic model construction\nPrognostic models are used to predict survival rates or disease outcomes in medical research. Multiple linear regression helps identify significant factors influencing disease progression and uses these factors to build a prognostic model.\n\nlm(survival_time ~ tumor_size + age + treatment_type + physical_activity, data = cancer_data)\n\nCorrelation analysis of physiological indicators\nMultiple linear regression can analyze the relationship between different physiological indicators. For example, it can examine how weight, cholesterol, and blood pressure affect a health outcome, such as kidney or heart function.\n\nlm(kidney_function ~ weight + cholesterol + blood_pressure, data = health_data)\n\n\n\n\n18.4.2 Considerations for multiple linear regression\n\nMulticollinearity: High correlation between independent variables can make the model’s coefficients unstable and inflate standard errors, making it difficult to assess the individual effect of each predictor. Multicollinearity can be assessed using the variance inflation factor (VIF) by vif() function from car package.\n\nmodel &lt;-lm(Y ~ X1 + X2 + X3 + X4, data = df)\ncar::vif(model)\n\n#&gt;       X1       X2       X3       X4 \n#&gt; 2.185539 1.779862 1.278364 1.266730\n\n\nNormality and independence of residuals: The model assumes that the residuals are independent and normally distributed. Residual plots and tests like the Shapiro-Wilk test can be used to assess this assumption.\n\nresiduals(model) |&gt; plot()\nresiduals(model) |&gt; shapiro.test()\n\n#&gt; \n#&gt;  Shapiro-Wilk normality test\n#&gt; \n#&gt; data:  residuals(model)\n#&gt; W = 0.97835, p-value = 0.8233\n\n\n\n\n\n\n\n\n\nHomoscedasticity: The model assumes that the variance of the residuals is constant. If the residual variance is not constant (heteroscedasticity), it can impact the efficiency of the estimates. Residual plots or tests like White’s test can detect heteroscedasticity.\nYou can use the bptest() function from the lmtest package to conduct a Breusch-Pagan test, a similar test for heteroscedasticity.\n\nlmtest::bptest(model)  # Breusch-Pagan test for heteroscedasticity\n\n#&gt; \n#&gt;  studentized Breusch-Pagan test\n#&gt; \n#&gt; data:  model\n#&gt; BP = 6.4508, df = 4, p-value = 0.1679\n\n\nAs for White’s test specifically, we defined a custom function named whites.test():\n\nwhites.test &lt;- function(model) {\n  # Extract the independent variables in the model\n  terms &lt;- attr(terms(model), \"term.labels\")\n  # Extract the data used in the model\n  data &lt;- model$model\n\n  # Create square and interaction terms\n  squared_terms &lt;- sapply(terms, function(term) paste0(\"I(\", term, \"^2)\"))\n  interaction_terms &lt;- combn(terms, 2, function(x) paste(x, collapse = \":\"))\n\n  # Construct the formula of auxiliary regression model\n  auxiliary_formula &lt;- as.formula(\n    paste(\n      \"e2 ~\", \n      paste(c(terms, squared_terms, interaction_terms), collapse = \" + \")))\n\n  # Calculate the square of the residual\n  e2 &lt;- resid(model)^2\n\n  # Construct auxiliary regression model\n  auxiliary_model &lt;- lm(auxiliary_formula, data = data)\n\n  # F-test\n  f_statistic &lt;- summary(auxiliary_model)$fstatistic\n  p_value &lt;- pf(f_statistic[1], f_statistic[2], f_statistic[3], lower.tail = FALSE)\n\n  result &lt;- list(\n    LM_test_statistic = f_statistic[1],\n    df1 = f_statistic[2],\n    df2 = f_statistic[3],\n    p_value = p_value,\n    auxiliary_model = auxiliary_model\n  )\n\n  class(result) &lt;- \"whites.test\"\n  result\n}\n\n# Printing results\nprint.whites.test &lt;- function(x, ...) {\n  cat(\"White's Test for Heteroskedasticity\\n\")\n  cat(\"LM test statistic: \", x$LM_test_statistic, \"\\n\")\n  cat(\"df1: \", x$df1, \"\\n\")\n  cat(\"df2: \", x$df2, \"\\n\")\n  cat(\"p-value: \", x$p_value, \"\\n\")\n  if (x$p_value &lt; 0.05) {\n    cat(\"Heteroskedasticity is present.\\n\")\n  } else {\n    cat(\"No significant heteroskedasticity detected.\\n\")\n  }\n}\n\n\nmodel &lt;- lm(Y ~ X1 + X2 + X3 + X4, data = df)\nwhites.test(model) \n\n#&gt; White's Test for Heteroskedasticity\n#&gt; LM test statistic:  1.838336 \n#&gt; df1:  14 \n#&gt; df2:  12 \n#&gt; p-value:  0.1484346 \n#&gt; No significant heteroskedasticity detected.\n\n\nIf the p-value is small (typically less than 0.05), reject the null hypothesis of homoscedasticity, which means heteroscedasticity is likely present. If White’s test indicates heteroscedasticity, you can use robust standard errors (e.g., vcovHC() function from the sandwich package) to correct for it:\n\n# Use robust standard errors\ncoeftest(model, vcov = vcovHC(model, type = \"HC3\"))\n\nSelection of independent variables: Including too many predictors can lead to overfitting, while excluding important variables can lead to underfitting. Methods like stepwise regression or information criteria (AIC, BIC) can be used to optimize the model.\nOutliers and leverage points: The model is sensitive to outliers and high-leverage points, which can disproportionately influence the results. Use diagnostic tools such as standardized residuals and Cook’s distance to identify and handle these points.\nLinear relationship assumption: The model assumes a linear relationship between independent variables and the dependent variable. If the relationship is non-linear, transformations or polynomial terms may be needed.\n\nLimitations of Multiple Linear Regression\n\nCausality: Regression analysis only shows association, not causation.\nModel Interpretability: When there are many predictors, the model may become difficult to interpret, requiring careful variable selection and result explanation.\nExtrapolation: The regression model is generally reliable within the range of the data, but predictions outside of this range may be inaccurate.",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Multiple linear regression analysis</span>"
    ]
  },
  {
    "objectID": "logistic-regression.html",
    "href": "logistic-regression.html",
    "title": "19  Logistic regression analysis",
    "section": "",
    "text": "19.1 Prerequisite\nLogistic regression is a type of regression analysis used for prediction of outcome based on one or more predictor variables. It is particularly useful when the dependent variable is binary (i.e., it takes on only two values, such as success/failure or yes/no). Its application is crucial in epidemiology, clinical research, and public health studies.\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(glmnet)\nlibrary(survival)\nlibrary(equatiomatic)\nlibrary(rms)\nlibrary(VGAM)",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Logistic regression analysis</span>"
    ]
  },
  {
    "objectID": "logistic-regression.html#logistic-regression",
    "href": "logistic-regression.html#logistic-regression",
    "title": "19  Logistic regression analysis",
    "section": "19.2 Logistic regression",
    "text": "19.2 Logistic regression\n\n19.2.1 The regression model\nThe logistic regression model can be represented by the following formula:\n\nP = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_n X_n)}}\n\nwhere P is the probability of the event occurring (e.g., the probability of success) given the independent features X, \\beta_0 is the intercept, and \\beta_1, \\dots, \\beta_n are the regression coefficients for the independent X_1, X_2, \\dots, X_n. This formula indicates the probability of an event occurring based on the input features X.\nHere is a plot of the logistic function:\n\n\n\n\n\n\n\n\n\nThe plot of the logistic function has an S-shaped or sigmoid curve, the output probabilities range between 0 and 1.\nThe model can be transformed to a linear form as follows:\n\n\\text{logit}(P) = \\ln\\left(\\frac{P}{1-P}\\right) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_k X_k\n\nwhere \\frac{P}{1-P} is the odds of the event occurring, \\text{logit}(P) is the natural logarithm of the odds.\n\n\n19.2.2 Model parameters\nIn logistic regression, the parameters mainly signify how the independent variables relate to the dependent variable (outcome) in terms of probability. Below are the primary meanings and interpretations of the parameters in a logistic regression model:\nThe intercept (β_0) represents the predicted log odds when all independent variables are equal to zero. It provides the baseline probability of the outcome. If the values of independent variables are zero, the intercept determines the initial output probability.\nEach coefficient represents the effect of the corresponding independent variable on the log odds of the dependent variable. Positive coefficient indicates that an increase in this variable raises the log odds of the event occurring, thereby increasing its probability. Negative coefficient suggests that an increase in this variable lowers the log odds of the event occurring, thereby decreasing its probability. The larger the absolute value of the coefficient, the more significant the independent variable’s effect on the outcome.\nParameter estimation\nIn logistic regression, parameter estimation is used to determine the relationship between independent variables (features) and dependent variables (usually binary outcomes). Maximum likelihood estimation (MLE) is the standard method for estimating parameters in logistic regression. MLE estimates model parameters by maximizing the likelihood function, which makes the observed data most probable under the model.\nFor each observation i, if Y_i is the binary response variable, the likelihood function can be written as:\n\nL(\\beta) = \\prod_{i=1}^{n} P_i^{Y_i} (1 - P_i)^{(1 - Y_i)}\n\nwhere P_i = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 X_{1i} + … + \\beta_k X_{ki})}}. To simplify the calculations, the log-likelihood function is often used:\n\n\\log L(\\beta) = \\sum_{i=1}^{n} \\left( Y_i \\log(P_i) + (1 - Y_i) \\log(1 - P_i) \\right)\n\nOdds ratio estimation\nIn logistic regression, odds ratio (OR) is a key measure that quantifies the strength of association between a independent variable and the outcome of interest. The OR compares the odds of the event occurring in one group to the odds of the event occurring in another group. To get the OR, we exponentiate the coefficient:\n\n\\text{OR} = e^{\\beta_j}\n\nThe OR representing how the odds of the event change with a one-unit increase in the independent variable X_j.\n\nOR &gt; 1: Suggests that increasing the variable by one unit increases the odds of the event.\nOR &lt; 1: Indicates that increasing the variable by one unit decreases the odds of the event.\nOR = 1: Implies no effect of the variable on the odds of the event.\n\nIt is common to calculate confidence intervals for the OR to assess the precision of the estimate. A 95% confidence interval for the OR that does not include 1 suggests a statistically significant association.\n\n\n19.2.3 Hypothesis test\nEach parameter typically undergoes hypothesis testing to determine its significance in the model. The Wald test and the likelihood ratio test are two commonly used statistical tests for assessing the significance of parameters in regression models, including logistic regression.\nWald Test\nThe Wald test evaluates the significance of individual coefficients in a regression model. It tests the null hypothesis that a specific parameter (coefficient) is equal to zero (no effect). For a coefficient \\beta_j:\n\nW = \\frac{\\hat{\\beta_j}^2}{S_{\\hat{\\beta_j}}}\n\nwhere \\hat{\\beta} is the estimated coefficient, S_{\\hat{\\beta_j}} is the estimated variance of the coefficient.\nThe Wald statistic follows a chi-squared distribution with 1 degree of freedom under the null hypothesis. A large Wald statistic indicates that the coefficient is significantly different from zero, leading to rejection of the null hypothesis.\nThe Wald test can be unreliable in small samples or when the estimates are near the boundary of the parameter space, as it may lead to inflated type I error rates.\nLikelihood ratio test\nThe likelihood ratio test compares the fit of two models: a full model (with the parameter of interest) and a reduced model (without the parameter). It tests whether the additional parameters improve the model fit significantly. The statistic is calculated as:\n\nG = 2 \\cdot \\left(\\log L_{\\text{full}} - \\log L_{\\text{reduced}} \\right)\n\nwhere L_{\\text{full}} is the likelihood of the reduced model, L_{\\text{reduced}} is the likelihood of the full model. The G statistic follows a chi-squared distribution, with degrees of freedom equal to the difference in the number of parameters between the full and reduced models. A large G statistic suggests that the full model significantly improves the fit compared to the reduced model, leading to rejection of the null hypothesis that the additional parameters do not improve the model. The likelihood ratio test is more reliable than the Wald test in small samples and can be used for more complex models where multiple parameters are tested simultaneously.\nP-value are used to assess the significance of parameters. If the P-value is below a significance level (e.g., 0.05), the independent variable is considered to have a significant effect on the dependent variable.\n\nExample 1 \nHere is the data on a case-control study investigating the relationship between smoking, alcohol consumption, and esophageal cancer. Conduct a logistic regression analysis.\n\n\n  Download data \n\n\ndf &lt;- read_csv(\"datasets/ex19-01.csv\", show_col_types = F) |&gt; \n  mutate(\n    x1 = factor(x1),\n    x2 = factor(x2),\n    y  = factor(case_ctr),\n    freq  = as.integer(freq),\n    .keep = \"none\"\n  )\n\nLogistic regression can be performed using the glm() function. Here’s an example code:\n\nmodel &lt;- glm(\n  formula = y ~ x1 + x2, \n  family  = binomial(link = \"logit\"),\n  data    = df,\n  weights = freq\n)\n\nUse summary(model) to display a detailed summary of the fitted model:\n\nsummary(model) \n\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = y ~ x1 + x2, family = binomial(link = \"logit\"), \n#&gt;     data = df, weights = freq)\n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept)  -0.9099     0.1358  -6.699 2.10e-11 ***\n#&gt; x11           0.8856     0.1500   5.904 3.54e-09 ***\n#&gt; x21           0.5261     0.1572   3.348 0.000815 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 1228.0  on 7  degrees of freedom\n#&gt; Residual deviance: 1159.4  on 5  degrees of freedom\n#&gt; AIC: 1165.4\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 4\n\n\nIn the summary output of glm(), the columns labeled “z value” and “Pr(&gt;|z|)” represent the results of the Wald test. The results show that both independent variables (X1 and X2) are significant in the logistic regression model, showing a strong relationship with the outcome variable. The model fits the data well, as indicated by the reduction in deviance from the null model.\nThe regression equation can be extracted from the model by extract_eq() function from equatiomatic package.\n\nextract_eq(\n  model,\n  swap_var_names = c(\"y\" = \"Y\", \"x1\" = \"X1\", \"x2\" = \"X2\"),\n  use_coefs = T, \n  coef_digits = 4,\n  font_size = \"small\"\n)\n\n\n\\small\n\\begin{aligned}\n\\log\\left[ \\frac { \\widehat{P( \\operatorname{Y} = \\operatorname{1} )} }{ 1 - \\widehat{P( \\operatorname{Y} = \\operatorname{1} )} } \\right] &= -0.9099 + 0.8856(\\operatorname{X1}_{\\operatorname{1}}) + 0.5261(\\operatorname{X2}_{\\operatorname{1}})\n\\end{aligned}\n\n\n\nThe odds ratios are the exponentiated coefficients from the logistic regression model, since logistic regression models the log-odds (logarithm of the odds). To get the odds ratios, you can use the exp() function on the coefficients.\n\ncoef(model) |&gt; exp()\n\n#&gt; (Intercept)         x11         x21 \n#&gt;   0.4025457   2.4243991   1.6923589\n\n\nYou can use the confint() function to compute confidence intervals for the model coefficients. To get confidence intervals for the odds ratios, you need to exponentiate the confidence intervals of the log-odds.\n\nconfint(model) |&gt; exp() \n\n#&gt;                 2.5 %    97.5 %\n#&gt; (Intercept) 0.3071678 0.5234695\n#&gt; x11         1.8090525 3.2580064\n#&gt; x21         1.2441606 2.3048818\n\n\n\n\n19.2.4 Variable selection\nIn logistic regression, like any regression model, variable selection is the process of choosing the most relevant independent variables for the model. This step is crucial because including too many irrelevant variables can lead to overfitting, making the model less generalizable to new data. On the other hand, excluding important variables can result in biased estimates.\n\nExample 2 \nTo investigate the risk factors for coronary heart disease, a case-control study was conducted with 26 coronary heart disease patients and 28 controls. The description of each factor are shown in Table 19.1 . Logistic stepwise regression analysis was performed to screen for risk factors.\n\n\n  Download data \n\n\n\n\n\nTable 19.1: Possible risk factors for coronary heart disease and their values\n\n\n\n\n\n\n\n\n\n\n\nFactor\nVariable\nAssignment specification\n\n\n\n\nAge (year)\nX1\n&lt;45=1，45～=2，55～=3，65～=4\n\n\nHypertension history\nX2\nNo=0，Yes=1\n\n\nFamily history of hypertension\nX3\nNo=0，Yes=1\n\n\nSmoking\nX4\nNo=0，Yes=1\n\n\nHyperlipidemia history\nX5\nNo=0，Yes=1\n\n\nAnimal fat intake\nX6\nLow=0，High=1\n\n\nBMI\nX7\n&lt;24=1，24～=2，26～=3\n\n\nType A personality\nX8\nNo=0，Yes=1\n\n\ncoronary heart disease\nY\nControl=0，Case=1\n\n\n\n\n\n\n\n\n\ndf &lt;- read_csv(\"datasets/ex19-02.csv\", col_types = list(y = col_factor()))\n\nHere are common methods used for variable selection in logistic regression:\nStepwise selection\nStepwise selection involves adding or removing variables based on specific criteria, usually involving p-values or AIC. This can be done using the step() function:\n\nmodel_full &lt;- glm( \n  y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8,\n  binomial(link = \"logit\"),\n  data = df\n)\n\nstep(model_full, direction = \"both\", trace = 0, k = 2)\n\n#&gt; \n#&gt; Call:  glm(formula = y ~ x1 + x2 + x5 + x6 + x8, family = binomial(link = \"logit\"), \n#&gt;     data = df)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)           x1           x2           x5           x6  \n#&gt;      -4.760        0.746        1.145        1.293        3.370  \n#&gt;          x8  \n#&gt;       2.000  \n#&gt; \n#&gt; Degrees of Freedom: 53 Total (i.e. Null);  48 Residual\n#&gt; Null Deviance:       74.79 \n#&gt; Residual Deviance: 44.16     AIC: 56.16\n\n\n\nmodel_reduced &lt;- step(model_full, direction = \"both\", trace = 0, k = log(nrow(df)))\nmodel_reduced\n\n#&gt; \n#&gt; Call:  glm(formula = y ~ x1 + x5 + x6 + x8, family = binomial(link = \"logit\"), \n#&gt;     data = df)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)           x1           x5           x6           x8  \n#&gt;     -4.7050       0.9239       1.4959       3.1355       1.9471  \n#&gt; \n#&gt; Degrees of Freedom: 53 Total (i.e. Null);  49 Residual\n#&gt; Null Deviance:       74.79 \n#&gt; Residual Deviance: 46.22     AIC: 56.22\n\n\nThe k argument in the step() function is used to specify the penalty per parameter that is added to the model when performing model selection. This penalty influences the criteria for including or excluding variables during the stepwise regression process. When k = 2, this corresponds to the AIC selection, which is common for model selection. When k = log(n), it corresponds to the BIC selection, which imposes a stronger penalty for additional parameters, often leading to simpler models.\n\n\n\n\\small\n\\begin{aligned}\n\\log\\left[ \\frac { \\widehat{P( \\operatorname{Y} = \\operatorname{1} )} }{ 1 - \\widehat{P( \\operatorname{Y} = \\operatorname{1} )} } \\right] &= -4.705 + 0.9239(\\operatorname{X1}) + 1.4959(\\operatorname{X5}) + 3.1355(\\operatorname{X6}) + 1.9471(\\operatorname{X8})\n\\end{aligned}\n\n\n\nAlternatively, stepwise() function from StepReg package is another useful and convenient tool to select a optimal model using various stepwise regression strategies.\n\n# StepReg::stepwise(\n#   y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8,\n#   df,\n#   type = \"logit\",\n#   strategy = \"bidirection\",\n#   metric = \"SBC\",\n#   num_digits = 5\n# )\n\nStepwise methods can sometimes lead to overfitting, especially if the dataset is small. It may be slower for larger models or datasets compared to penalized regression methods like Lasso.\nLASSO\nLASSO (least absolute shrinkage and selection operator) is a regularization technique that performs both variable selection and regularization by adding a penalty term to the logistic regression model. It forces some of the coefficients to be exactly zero. Variables with coefficients set to zero are effectively excluded from the model, while variables with non-zero coefficients are selected as important.\nYou can use the glmnet package for LASSO. Here is an example code block:\n\nx = select(df, contains(\"x\")) |&gt; as.matrix()\ny = select(df, y) |&gt; pull()\n\n# Select the optimal lambda using cross-validation\ncv_model &lt;- cv.glmnet(x, y, family = \"binomial\", alpha = 1)\nplot(cv_model)\nbest_lambda &lt;- cv_model$lambda.min\n\n\n\n\n\n\n\n\n\nThe elasticnet mixing parameter, with 0 ≤ \\alpha ≤ 1. \\alpha=1 is the lasso penalty, and \\alpha=0 the ridge penalty. If \\alpha is between 0 and 1, it would be Elastic Net, a combination of Lasso and Ridge regression.\n\nSince Lasso tends to set some coefficients exactly to zero, using optimal lambda ensures that only the most relevant features are kept in the model. The final model will have the lowest cross-validation error, making it more reliable for prediction and interpretation.\n\nlasso_model &lt;- glmnet(x, y, family = \"binomial\", alpha = 1, lambda = best_lambda)\n\nAfter fitting the final model with the optimal lambda, you can check which variables have non-zero coefficients.\n\ncoef(lasso_model)\n\n#&gt; 9 x 1 sparse Matrix of class \"dgCMatrix\"\n#&gt;                      s0\n#&gt; (Intercept) -2.81593546\n#&gt; x1           0.36588508\n#&gt; x2           0.47145719\n#&gt; x3           0.36146901\n#&gt; x4           0.20506780\n#&gt; x5           0.61619725\n#&gt; x6           1.79944497\n#&gt; x7           0.07481632\n#&gt; x8           0.98715786\n\n\nWhen the regularization parameter (lambda) is very small, the Lasso penalty is not strong enough to shrink coefficients to zero. This could result in all variables being retained in the model. To address the reason and potentially shrink more coefficients to zero, you can try the following:\n\n1lambda_strong &lt;- cv_model$lambda.1se\nlasso_model &lt;- glmnet(x, y, family = \"binomial\", alpha = 1, lambda = lambda_strong)\n\n\n1\n\nThe largest lambda within 1 standard error of the best.\n\n\n\n\nThe lambda.min value minimizes the cross-validation error. The lambda.1se is more conservative and applies a larger penalty, often shrinking more coefficients to zero. After refitting the model with a higher lambda, you can check the coefficients to see if any are zero:\n\ncoef(lasso_model)\n\n#&gt; 9 x 1 sparse Matrix of class \"dgCMatrix\"\n#&gt;                      s0\n#&gt; (Intercept) -0.63708463\n#&gt; x1           0.04213502\n#&gt; x2           0.09792626\n#&gt; x3           .         \n#&gt; x4           .         \n#&gt; x5           0.30243773\n#&gt; x6           0.75451858\n#&gt; x7           .         \n#&gt; x8           0.24106914\n\n\nYou can try a larger value for lambda (manually increasing the penalty):\n\nstronger_lambda &lt;- best_lambda * 3.1  # Increase lambda by a factor of 3.1\nlasso_model &lt;- glmnet(x, y, family = \"binomial\", alpha = 1, lambda = stronger_lambda)\ncoef(lasso_model)\n\n#&gt; 9 x 1 sparse Matrix of class \"dgCMatrix\"\n#&gt;                     s0\n#&gt; (Intercept) -0.3825940\n#&gt; x1           .        \n#&gt; x2           0.0255965\n#&gt; x3           .        \n#&gt; x4           .        \n#&gt; x5           0.2267783\n#&gt; x6           0.6189254\n#&gt; x7           .        \n#&gt; x8           0.1337567\n\n\n\n\n\n\n\n\nTip\n\n\n\nLasso or Ridge regression both can constrain the regression coefficients to prevent overfitting. Moreover, Ridge regression does not perform variable selection, but rather shrinks all features to some degree. Although it retains all the variables, but it can help deal with multicollinearity problems.",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Logistic regression analysis</span>"
    ]
  },
  {
    "objectID": "logistic-regression.html#conditional-logistic-regression",
    "href": "logistic-regression.html#conditional-logistic-regression",
    "title": "19  Logistic regression analysis",
    "section": "19.3 Conditional logistic regression",
    "text": "19.3 Conditional logistic regression\nConditional logistic regression is used when dealing with matched case-control studies where subjects (cases and controls) are matched based on certain characteristics, like age, sex, or other variables. Unlike standard logistic regression, conditional logistic regression takes into account the matching by estimating the relationship between predictors and outcomes within matched sets.\nIn conditional logistic regression, the model is stratified by the matched sets (e.g., matched pairs or groups), meaning the analysis is performed within these groups.\nThe conditional logistic regression model takes the following form:\n\n\\log\\left( \\frac{P(Y_{ij} = 1 | X_{ij})}{P(Y_{ij} = 0 | X_{ij})} \\right) = \\beta_1 X_{ij_1} + \\beta_2 X_{ij_2} + \\cdots + \\beta_p X_{ij_p}\n\nwhere Y_{ij} is the response variable for the j-th individual in the i-th group, X_{ij_1}, X_{ij_2}, \\dots, X_{ij_p} are the predictors, and \\beta_1, \\beta_2, \\dots, \\beta_p are the corresponding coefficients. The intercept is often omitted because the matching or stratification controls for differences between groups, or say it is conditioned out as part of the model’s structure when using the conditional likelihood function.\nIn conditional logistic regression, the parameter estimation focuses on the effects of the explanatory variables on the odds of the outcome, while controlling for the matching or stratification. It uses a conditional likelihood approach to estimate parameters, which removes the effect of unmeasured confounders that are constant within strata.\nConsider n paired observations, where each paired observation i includes two individuals i_1 and i_2, with i_1 representing the observed success and i_2 representing the observed failure. The conditional likelihood function can be expressed as:\n\nL(\\beta) = \\prod_{i=1}^{n} \\frac{\\exp(X_{i_1}^\\top \\beta)}{\\exp(X_{i_1}^\\top \\beta) + \\exp(X_{i_2}^\\top \\beta)}\n\nwhere X_{i_1} and X_{i_2} are the vectors of independent variables for the two individuals in the matched pair, \\beta is the vector of coefficients you aim to estimate, \\exp(X_{i}^\\top \\beta) represents the linear predictor component in the logistic regression model.\n\nExample 3 \nTo investigate the risk factors associated with laryngeal cancer in a northern city, a 1:2 matched case-control study was conducted. Six potential risk factors were selected, and data from 25 matched pairs were extracted. The descriptions for the variable assignments are provided in Table 19.2 . Please perform a conditional logistic stepwise regression analysis.\n\n\n  Download data \n\n\n\n\n\nTable 19.2: Risk factors and assignment of laryngeal cancer\n\n\n\n\n\n\n\n\n\n\n\nFactor\nVariable\nAssignment specification\n\n\n\n\nPharyngitis\nX1\nNo = 1, Occasionally = 2, Frequently = 3\n\n\nSmoking count (cigarettes per day)\nX2\n0=1，1～=2，5～=3，10～=4，20～=5\n\n\nHoarseness history\nX3\nNo = 1, Occasionally = 2, Frequently = 3\n\n\nFresh vegetables eating\nX4\nRarely = 1, Often = 2, Daily = 3\n\n\nFruit eating\nX5\nRarely = 1, Small amount = 2, Often = 3\n\n\nFamily history of cancer\nX6\nNo=0，Yes=1\n\n\nThroat cancer\nY\nControl=0，Case=1\n\n\n\n\n\n\n\n\n\ndf &lt;- read_csv(\"datasets/ex19-03.csv\", show_col_types = F)\n\nTo perform a conditional logistic stepwise regression analysis, you can use the survival package, which provides functions for conditional logistic regression. Here is an example code block:\n\nfull_model &lt;- clogit(\n  Y ~ X1 + X2 + X3 + X4 + X5 + X6 + strata(id),\n  data = df,\n  method = \"exact\"\n) \nsummary(full_model)\n\n#&gt; Call:\n#&gt; coxph(formula = Surv(rep(1, 75L), Y) ~ X1 + X2 + X3 + X4 + X5 + \n#&gt;     X6 + strata(id), data = df, method = \"exact\")\n#&gt; \n#&gt;   n= 75, number of events= 25 \n#&gt; \n#&gt;        coef exp(coef) se(coef)      z Pr(&gt;|z|)  \n#&gt; X1  2.58880  13.31380  2.50172  1.035   0.3008  \n#&gt; X2  1.68796   5.40843  0.68545  2.463   0.0138 *\n#&gt; X3  2.31944  10.16995  1.26096  1.839   0.0659 .\n#&gt; X4 -3.88886   0.02047  1.90656 -2.040   0.0414 *\n#&gt; X5 -0.49102   0.61200  1.19020 -0.413   0.6799  \n#&gt; X6  3.50899  33.41447  2.13723  1.642   0.1006  \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt;    exp(coef) exp(-coef) lower .95 upper .95\n#&gt; X1  13.31380    0.07511 0.0988170 1793.7921\n#&gt; X2   5.40843    0.18490 1.4112830   20.7266\n#&gt; X3  10.16995    0.09833 0.8589963  120.4056\n#&gt; X4   0.02047   48.85506 0.0004878    0.8589\n#&gt; X5   0.61200    1.63399 0.0593818    6.3074\n#&gt; X6  33.41447    0.02993 0.5066653 2203.6771\n#&gt; \n#&gt; Concordance= 0.91  (se = 0.064 )\n#&gt; Likelihood ratio test= 42.21  on 6 df,   p=2e-07\n#&gt; Wald test            = 7.71  on 6 df,   p=0.3\n#&gt; Score (logrank) test = 29.13  on 6 df,   p=6e-05\n\n\n\nstep(full_model, direction = \"both\", trace = 0) |&gt; summary()\n\n#&gt; Call:\n#&gt; coxph(formula = Surv(rep(1, 75L), Y) ~ X1 + X2 + X3 + X4 + X5 + \n#&gt;     X6, data = df, method = \"exact\")\n#&gt; \n#&gt;   n= 75, number of events= 25 \n#&gt; \n#&gt;        coef exp(coef) se(coef)      z Pr(&gt;|z|)   \n#&gt; X1  1.19334   3.29809  0.70711  1.688  0.09148 . \n#&gt; X2  1.10080   3.00658  0.36318  3.031  0.00244 **\n#&gt; X3  1.90056   6.68967  0.76039  2.499  0.01244 * \n#&gt; X4 -3.44781   0.03182  1.38920 -2.482  0.01307 * \n#&gt; X5 -0.29708   0.74299  0.59809 -0.497  0.61939   \n#&gt; X6  3.60814  36.89742  1.12328  3.212  0.00132 **\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt;    exp(coef) exp(-coef) lower .95 upper .95\n#&gt; X1   3.29809     0.3032   0.82483   13.1874\n#&gt; X2   3.00658     0.3326   1.47548    6.1265\n#&gt; X3   6.68967     0.1495   1.50716   29.6927\n#&gt; X4   0.03182    31.4316   0.00209    0.4843\n#&gt; X5   0.74299     1.3459   0.23008    2.3992\n#&gt; X6  36.89742     0.0271   4.08188  333.5274\n#&gt; \n#&gt; Concordance= 0.896  (se = 0.039 )\n#&gt; Likelihood ratio test= 46.4  on 6 df,   p=2e-08\n#&gt; Wald test            = 15.06  on 6 df,   p=0.02\n#&gt; Score (logrank) test = 35.6  on 6 df,   p=3e-06\n\n\nSignificant variables in the final model include X_2, X_3, X_4, and X_6, while X_1 and X_5 is not significant.",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Logistic regression analysis</span>"
    ]
  },
  {
    "objectID": "logistic-regression.html#ordinal-logistic-regression",
    "href": "logistic-regression.html#ordinal-logistic-regression",
    "title": "19  Logistic regression analysis",
    "section": "19.4 Ordinal logistic regression",
    "text": "19.4 Ordinal logistic regression\nOrdinal logistic regression is used when the dependent variable is ordinal, meaning it represents categories with a meaningful order, but the exact distance between the categories is unknown or not equal. It’s commonly used in situations where the outcome variable has more than two categories, and these categories have a natural order (e.g., from low to high).\nIn ordinal logistic regression, the probability of an observation falling into a given category or lower is modeled as a function of the independent variables. The most commonly used model is the proportional odds model, which assumes that the relationship between each independent variable and the outcome is the same across all category boundaries.\nThe ordinal logistic regression model estimates the cumulative probabilities of the ordinal outcome. For an outcome Y with K ordered categories, the model is:\n\nP(Y \\leq k | X) = \\frac{1}{1 + e^{-(\\beta_0^* + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_p X_p)}} \\quad \\text{for } k = 1, 2, \\ldots, K-1  \\ \\ \\ \\text{or}\n\n\n\\log\\left(\\frac{P(Y \\leq k | X)}{P(Y &gt; k | X)}\\right) = \\beta_0^* + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_p X_p \\quad \\text{for } k = 1, 2, \\ldots, K-1\n\nwhere P(Y \\leq k|X) is the cumulative probability of the outcome being in category k or lower given independent variable X, P(Y &gt; k|X) is the probability of the outcome being in a category higher than k, \\log \\left( \\frac{P(Y \\leq j|X)}{P(Y &gt; j|X)} \\right) is the log-odds of being in a category k or lower, \\beta_0^* is the intercept for the model that separate the different categories, \\beta_p are the regression coefficients associated with the independent variables.\nIn other words, this model estimates the odds of the outcome falling in category j or lower, compared to falling in a higher category, as a function of the variables.\n\nExample 4 \nIn order to study the changes in nucleolar organization related to gastric cancer and precancerous lesions, we analyzed the variation in the number and size of silver-staining nucleolar proteins (AgNOR) granules in cases of gastritis, atypical hyperplasia, and gastric cancer, as well as their clinical diagnostic significance. A total of 129 patients were tested. Perform a ordinal logistic regression analysis.\n\n\n  Download data \n\n\ndf &lt;- read_csv(\"datasets/ex19-04.csv\", show_col_types = F)\n\nYou can fit an ordinal logistic regression model using the polr() function from the MASS package.\n\nordered_df &lt;- df |&gt; mutate(Y = as.ordered(Y))\nordinal_model &lt;- MASS::polr(Y ~ X1 + X2, ordered_df) \nordinal_model\n\n#&gt; Call:\n#&gt; MASS::polr(formula = Y ~ X1 + X2, data = ordered_df)\n#&gt; \n#&gt; Coefficients:\n#&gt;       X1       X2 \n#&gt; 5.042316 1.833791 \n#&gt; \n#&gt; Intercepts:\n#&gt;      1|2      2|3 \n#&gt; 11.18435 16.71586 \n#&gt; \n#&gt; Residual Deviance: 100.756 \n#&gt; AIC: 108.756\n\n\n\nextract_eq(model, use_coefs = T, coef_digits = 3, font_size = \"small\")\n\n\n\\small\n\\begin{aligned}\n\\log\\left[ \\frac { \\widehat{P( \\operatorname{y} = \\operatorname{1} )} }{ 1 - \\widehat{P( \\operatorname{y} = \\operatorname{1} )} } \\right] &= -0.91 + 0.886(\\operatorname{x1}_{\\operatorname{1}}) + 0.526(\\operatorname{x2}_{\\operatorname{1}})\n\\end{aligned}",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Logistic regression analysis</span>"
    ]
  },
  {
    "objectID": "logistic-regression.html#multinomial-logistic-regression",
    "href": "logistic-regression.html#multinomial-logistic-regression",
    "title": "19  Logistic regression analysis",
    "section": "19.5 Multinomial logistic regression",
    "text": "19.5 Multinomial logistic regression\nMultinomial logistic regression is used for modeling the relationship between a categorical dependent variable with more than two levels and one or more independent variables. It is an extension of binary logistic regression, which is used when the outcome variable has only two categories.\nIn the case of K categories (where K ),\nFor a response variable Y with categories k = 1, 2, \\ldots, K, and a set of independent variables X = (X_1, X_2, \\ldots, X_p), we typically choose one category as the reference category. The probability of Y being in category k is given by:\n\nP(Y = k | X) = \\frac{e^{\\beta_{0k} + \\beta_{1k} X_1 + \\beta_{2k} X_2 + \\ldots + \\beta_{pk} X_p}}{\\sum_{j=1}^{K} e^{\\beta_{0j} + \\beta_{1j} X_1 + \\beta_{2j} X_2 + \\ldots + \\beta_{pj} X_p}}  \\ \\ \\ \\ \\   \\text{or}\n\n\n\\text{logit}(P(Y = k | X)) = \\log\\left(\\frac{P(Y = k | X)}{P(Y = K | X)}\\right) = \\beta_{0k} + \\beta_{1k} X_1 + \\beta_{2k} X_2 + \\ldots + \\beta_{pk} X_p\n\nwhere P(Y = k | X) is the probability of the dependent variable being in category k, P(Y = K | X) is the probability of the response variable Y being in the reference category K, \\beta_{jk} are the coefficients for the j-{\\text{th}} independent variable associated with the k-\\text{th} category.\nEach coefficient in the model represents the change in the log odds of the dependent variable being in a particular category compared to a reference category for a one-unit change in the predictor variable. Odds ratio can be calculated by exponentiating the coefficients, providing a multiplicative effect on the odds of being in one category versus the reference category.\n\nExample 5 \nUse the data in Example 4 to perform multinomial logistic regression analysis.\n\nYou can perform multinomial logistic regression using the nnet package with the multinom() function:\n\nunordered_df &lt;- df\nmultinomial_model &lt;- nnet::multinom(Y ~ X1 + X2, unordered_df) \n\n#&gt; # weights:  12 (6 variable)\n#&gt; initial  value 141.720985 \n#&gt; iter  10 value 50.383848\n#&gt; iter  20 value 50.168939\n#&gt; final  value 50.168755 \n#&gt; converged\n\nmultinomial_model |&gt; coef()\n\n#&gt;   (Intercept)        X1       X2\n#&gt; 2   -11.35736  5.290469 1.776146\n#&gt; 3   -27.56254 10.011614 3.713867",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Logistic regression analysis</span>"
    ]
  },
  {
    "objectID": "logistic-regression.html#matters-need-attention",
    "href": "logistic-regression.html#matters-need-attention",
    "title": "19  Logistic regression analysis",
    "section": "19.6 Matters need attention",
    "text": "19.6 Matters need attention\n\n19.6.1 Parallel lines assumption\nOrdinal logistic regression (also known as proportional odds logistic regression) assumes that the effect of each predictor variable on the cumulative probabilities (i.e., the odds ratios) is constant across all levels of the response variable. This assumption is known as the proportional odds assumption or parallel lines assumption. Violation of this assumption means that the effect of the predictor variables is not constant across the levels of the response variable, which can affect the validity and interpretability of the model.\nTo test whether this assumption holds, you can perform a parallel lines test. Here are some methods to test the parallel lines assumption in R for an ordinal logistic regression model:\n\nBrant test The Brant test is one of the most commonly used tests to check the proportional odds assumption. The brant package provides the brant() function for this test.\n\nlibrary(brant)\nbrant(ordinal_model) |&gt; print()\n\n#&gt; -------------------------------------------- \n#&gt; Test for X2  df  probability \n#&gt; -------------------------------------------- \n#&gt; Omnibus      0.41    2   0.82\n#&gt; X1       0.16    1   0.69\n#&gt; X2       0.03    1   0.87\n#&gt; -------------------------------------------- \n#&gt; \n#&gt; H0: Parallel Regression Assumption holds\n#&gt;                 X2 df probability\n#&gt; Omnibus 0.40627948  2   0.8161642\n#&gt; X1      0.15502241  1   0.6937812\n#&gt; X2      0.02667615  1   0.8702599\n\n\nIf the results show that some predictor variables have p-values less than the significance level, it indicates that these variables violate the parallel lines assumption.\nLikelihood ratio test Another approach is to compare the ordinal logistic regression model (which assumes parallel lines) to a multinomial logistic regression model (which does not make this assumption). This comparison can be done using a likelihood ratio test. Here we define a function named likelihood_ratio_test() to perform this test:\n\nlikelihood_ratio_test &lt;- function(x, y) {\n  # Extract log-likelihoods\n  logLik_x &lt;- logLik(x) \n  logLik_y &lt;- logLik(y)\n\n  lr_statistic &lt;- 2 * (logLik_y - logLik_x)\n  df_diff &lt;- attr(logLik_y, \"df\") - attr(logLik_x, \"df\")\n  p_value &lt;- pchisq(lr_statistic, df = df_diff, lower.tail = F)\n\n  result &lt;- list(\n    lr_statistic = lr_statistic,\n    df_diff = df_diff,\n    p_value = p_value\n  )\n  class(result) &lt;- \"likelihood_ratio_test\"\n  result\n}\n\nprint.likelihood_ratio_test &lt;- function(x, ...) {\n  cat(\"Likelihood ratio test for parallel lines assumption:\\n\")\n  cat(\"statistic: \", x$lr_statistic, \"\\n\")\n  cat(\"df: \", x$df_diff, \"\\n\")\n  cat(\"p-value: \", x$p_value, \"\\n\")\n  if (x$p_value &lt; 0.05) {\n    cat(\"The parallel lines assumption is not held.\\n\")\n  } else {\n    cat(\"The parallel lines assumption is held.\\n\")\n  }\n}\n\n\nlikelihood_ratio_test(ordinal_model, multinomial_model) \n\n#&gt; Likelihood ratio test for parallel lines assumption:\n#&gt; statistic:  0.4184909 \n#&gt; df:  2 \n#&gt; p-value:  0.8111961 \n#&gt; The parallel lines assumption is held.\n\n\nIf the p-value is small (e.g., less than 0.05), it indicates that the multinomial logistic regression model provides a significantly better fit to the data, and the parallel lines assumption of the ordinal logistic regression model might not hold. If the p-value is large (e.g., greater than 0.05), it suggests that the ordinal logistic regression model is sufficient, and the extra complexity of the multinomial model is not warranted.\nIf you prefer to use the VGAM package to test the parallel lines assumption, you can follow these steps:\n\nlibrary(VGAM)\n\nmodel_parallel &lt;- vglm(Y ~ X1 + X2, family = cumulative(parallel = T), data = ordered_df)\nmodel_relaxed &lt;- vglm(Y ~ X1 + X2, family = cumulative(parallel = F, reverse = T), data = ordered_df) \n\nlr_statistic &lt;- -2 * (logLik(model_parallel) - logLik(model_relaxed))\ndf_diff &lt;- df.residual(model_parallel) - df.residual(model_relaxed)\np_value &lt;- pchisq(lr_statistic, df = df_diff, lower.tail = F)\n\ntibble(\"statistic\" = lr_statistic, \"df\" = df_diff, \"p-value:\" = p_value)\n\n#&gt; # A tibble: 1 × 3\n#&gt;   statistic    df `p-value:`\n#&gt;       &lt;dbl&gt; &lt;int&gt;      &lt;dbl&gt;\n#&gt; 1     0.366     2      0.833\n\n\nThe likelihood ratio test helps determine if the additional flexibility of the multinomial model significantly improves the fit over the ordinal model.\n\n\n\n19.6.2 Sample size\nThe sample size for logistic regression is crucial to ensure the reliability of the model and the statistical significance of its coefficients. Several factors affect the sample size in logistic regression, including the number of variables, expected effect sizes, prevalence of the outcome (for binary logistic regression), and the desired statistical power.\nHere’s a guide to understanding how to determine the sample size:\nGeneral rule of thumb\nA commonly used guideline suggests that for binary logistic regression, you need at least 10 events (outcome occurrences) per independent variable. An “event” refers to cases where the binary outcome of interest occurs (e.g., “1” for success). For example, if you have 5 independent variables and the outcome (e.g., disease occurrence) happens in 30% of the sample, you would need:\n\n\\text{Sample size} \\geq \\frac{10 \\times \\text{number of predictors}}{\\text{proportion of events}} = \\frac{10 \\times 5}{0.3} \\approx 167 \\text{ subjects}\n\nThis ensures that the model can reliably estimate the coefficients for each predictor.\nSample size for binary logistic regression\nThe formula to calculate the sample size for binary logistic regression can be complex, and software tools like G*Power or the pwr package in R are often used. The following parameters are typically needed:\nSignificance level (α): Commonly set at 0.05.\nStatistical power (1-β): Typically set at 0.80, meaning you have an 80% chance of detecting an effect if one exists. Effect size: This refers to the strength of the association between predictors and the outcome. Proportion of events (P): The proportion of positive outcomes in the population.\nAn approximate formula for determining sample size for binary logistic regression is:\n\nn = \\frac{(Z_{\\alpha/2} + Z_\\beta)^2 \\times (p_1(1 - p_1) + p_2(1 - p_2))}{(p_1 - p_2)^2}\n\nwhere p_1 and p_2 are the proportions of events in the two groups. Z_{\\alpha/2} and Z_\\beta are the z-scores corresponding to the desired significance level and power.\nSample size for ordinal and multinomial logistic regression\nFor ordinal logistic regression, the sample size determination is similar to binary logistic regression, but since there are more than two categories, the complexity increases. A general rule is to aim for at least 10 observations per predictor per outcome category.\nFor multinomial logistic regression, the sample size depends on the number of categories in the outcome variable. A general rule is to aim for 10-20 cases per category per predictor.\nPractical considerations\nOutcome Imbalance: If the outcome is rare (e.g., a disease with very low prevalence), you may need a larger sample size to detect associations. More complex models with many interaction terms or non-linearities will require a larger sample size. Insufficient sample size can lead to overfitting, where the model fits the data too closely, leading to poor generalization to new data.",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Logistic regression analysis</span>"
    ]
  },
  {
    "objectID": "generalized-linear-model.html",
    "href": "generalized-linear-model.html",
    "title": "20  Generalized linear model",
    "section": "",
    "text": "20.1 Prerequisite\nA generalized linear model (GLM) is a flexible generalization of ordinary linear regression that allows for response variables that have error distribution models other than a normal distribution. GLMs are widely used in various fields, including medical statistics, economics, and social sciences, as they provide a robust framework to model different types of data and relationships.\nlibrary(tidyverse)\nlibrary(ggplot2)",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Generalized linear model</span>"
    ]
  },
  {
    "objectID": "generalized-linear-model.html#components-of-glm",
    "href": "generalized-linear-model.html#components-of-glm",
    "title": "20  Generalized linear model",
    "section": "20.2 Components of GLM",
    "text": "20.2 Components of GLM\nA generalized linear model consists of three main components:\n\nRandom component\nSpecifies the probability distribution of the response variable Y. Unlike linear regression, where Y is assumed to follow a normal distribution, GLMs allow for different distributions from the exponential family, such as normal distribution, binomial distribution, Poisson distribution, Gamma distribution.\nSystematic component\nThis refers to the linear predictor, which is the linear combination of the explanatory (independent) variables. It is similar to the equation used in linear regression:\n\n\\eta = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p\nwhere \\eta is the linear predictor, \\beta_0, \\beta_1, \\dots, \\beta_p are the regression coefficients, X_1, X_2, \\dots, X_p are the independent variables.\nLink function\nThe link function connects the linear predictor \\eta to the mean of the response variable. This is important because, in many cases, the response variable’s distribution and scale are not appropriate for direct use in a linear equation. The most commonly used link functions are:\n\nIdentity link (for normal distribution): \\eta = \\mu\nLogit link (for binomial distribution): \\eta = \\ln \\left( \\frac{\\mu}{1 - \\mu} \\right)\nLog link (for Poisson distribution): \\eta = \\ln(\\mu)\nInverse link (for gamma distribution): \\eta = \\mu^{-1}, \\ \\text{where} \\ \\mu is the expected value of Y.\n\nwhere \\mu is the expected value of Y.\n\nThe general form of a GLM is:\n\n\\text{g}(\\mu_i) = \\beta_0 + \\beta_1 X_1 + \\dots + \\beta_p X_p\n\nwhere g(\\mu_i) is the link function that relates the expected value \\mu_i of the response variable Y_i to the linear predictor.\nBelow are some common distributions and their link functions:\n\n\n\n\n\n\n\n\n\n\n\nDistribution\nDependent variable\nLink function\nCommon model\n\n\n\n\nNormal distribution\nContinuous data\nIdentity g(\\mu) = \\mu\nLinear regression\n\n\nBinomial distribution\nBinary data\nLogit g(\\mu) = ln(\\mu / (1 - \\mu))\nLogistic regression\n\n\nPoisson distribution\nCount data\nLog g(\\mu) = ln(\\mu)\nPoisson regression\n\n\nGamma distribution\nPositive continuous data\nInverse g(\\mu) = 1 / \\mu\nGamma regression\n\n\nInverse Gaussian distribution\nPositive skewed continuous data\nInverse squared g(\\mu) = 1 / \\mu^2\nInverse Gaussian regression\n\n\nMultinomial distribution\nCategorical data\nMultinomial Logit g(\\mu) = ln(\\mu_j / \\mu_K)\nMultinomial logistic regression\n\n\n\n\n\nThe GLMs are used to model relationships between clinical predictors and outcomes, where the response variable can follow different distributions (e.g., binomial, Poisson). The correct specification of the link function and distribution is critical for drawing valid inferences.",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Generalized linear model</span>"
    ]
  },
  {
    "objectID": "generalized-linear-model.html#log-linear-model",
    "href": "generalized-linear-model.html#log-linear-model",
    "title": "20  Generalized linear model",
    "section": "20.3 Log-linear model",
    "text": "20.3 Log-linear model\nLog-linear model is often used to describe the relationship between categorical variables in a contingency table by modeling the natural logarithm of the expected cell frequencies as a linear combination of parameters. This model is often applied to multidimensional categorical data and is particularly useful for exploring interactions between categorical variables.\nThe general form for a 3-way table with variables X_1 , X_2, and X_3 is:\n\n\\ln(\\mu_{ijk}) = \\lambda + \\lambda_{i}^{(X_1)} + \\lambda_{j}^{(X_2)} + \\lambda_{k}^{(X_3)} + \\lambda_{ij}^{(X_1 X_2)} + \\lambda_{ik}^{(X_1 X_3)} + \\lambda_{jk}^{(X_2 X_3)} + \\lambda_{ijk}^{(X_1 X_2 X_3)}\n\nwhere \\mu_{ijk} is the expected frequency for the combination of category i, j, and k, \\lambda is an intercept term (overall mean), \\lambda_{i}^{(X_1)}, \\lambda_{j}^{(X_2)}, and \\lambda_{k}^{(X_3)}are the main effects for each variable,\\lambda_{ij}^{(X_1 X_2)}, \\lambda_{ik}^{(X_1 X_3)}, and \\lambda_{jk}^{(X_2 X_3)} are the interaction effects between pairs of variables, \\lambda_{ijk}^{(X_1 X_2 X_3)} is the three-way interaction term among the three variables.\nThere are several types of log-linear models, depending on the complexity of the relationships they model. These include:\n\nIndependence model\nThis assumes that all variables are independent of each other, meaning no interaction terms are included. For a three-way table, the model looks like:\n\n\\ln(\\mu_{ijk}) = \\lambda + \\lambda_{i}^{(X_1)} + \\lambda_{j}^{(X_2)} + \\lambda_{k}^{(X_3)}\n\nThis model assumes no association between variables X_1 , X_2 , and X_3 .\nInteraction model\nThis model includes interactions between two or more variables. For example, a model that includes two-way interactions but excludes higher-order interactions is:\n\n\\ln(\\mu_{ijk}) = \\lambda + \\lambda_{i}^{(X_1)} + \\lambda_{j}^{(X_2)} + \\lambda_{k}^{(X_3)} + \\lambda_{ij}^{(X_1 X_2)} + \\lambda_{ik}^{(X_1 X_3)} + \\lambda_{jk}^{(X_2 X_3)}\nThis allows for interaction between pairs of variables but not all three variables together.\nSaturated model\nThis includes all possible main effects and interactions (up to the highest order), fully describing the relationships between the variables:\n\n\\ln(\\mu_{ijk}) = \\lambda + \\lambda_{i}^{(X_1)} + \\lambda_{j}^{(X_2)} + \\lambda_{k}^{(X_3)} + \\lambda_{ij}^{(X_1 X_2)} + \\lambda_{ik}^{(X_1 X_3)} + \\lambda_{jk}^{(X_2 X_3)} + \\lambda_{ijk}^{(X_1 X_2 X_3)}\n\nThis model perfectly fits the observed data but might overfit if the data set is small.\n\nThese types of log-linear models allow flexibility in modeling categorical data, from simple independence models to complex interaction models depending on the structure and hypotheses in the data.\nLog-linear models have several key applications, particularly in the analysis of categorical data in contingency tables.\n\nContingency table analysis: Log-linear models are commonly used to analyze contingency tables, where they provide a way to model and test for associations between multiple categorical variables.\nInteraction analysis: Log-linear models are ideal for identifying interaction effects between multiple categorical variables. For example, they can help understand if a response variable depends on combinations of two or more predictors.\nAssociation testing: Log-linear models allow testing hypothesis about the independence or association between categorical variables in multi-way tables.\n\nResearchers often fit both saturated (fully interactive) and reduced models to identify which interactions and main effects are statistically significant. The interpretation is based on the estimated parameters, which represent the log of the odds ratios. Understanding these parameters is crucial in drawing meaningful conclusions about the relationships between variables.\nLog-linear models are powerful tools for exploring multi-way relationships between categorical variables, making them widely applicable across various fields of research.\n\n20.3.1 Goodness-of-fit test\nThe goodness-of-fit test in log-linear model is used to assess how well the model fits the observed data. Two main tests are typically used for this purpose:\n\nLikelihood ratio test\nThe likelihood ratio test (also called the deviance test) compares the deviance of a fitted log-linear model to the deviance of a saturated model (a model with as many parameters as data points). The deviance D is calculated as:\n\nD = 2 \\sum O \\cdot \\ln \\left( \\frac{O}{E} \\right)\n\nwhere O are the observed cell frequencies, and E are the expected cell frequencies under the model.\nThe deviance follows a chi-square distribution with degrees of freedom equal to the difference in the number of parameters between the two models. If the p-value is small (e.g., less than 0.05), the null hypothesis is rejected, indicating that the model does not fit the data well.\nPearson chi-square test\nThe Pearson chi-square test is an alternative goodness-of-fit test for log-linear models, which measures the discrepancy between observed and expected frequencies. The test statistic is:\n\n\\chi^2 = \\sum \\frac{(O - E)^2}{E}\n\nLike the deviance test, the Pearson chi-square statistic follows a chi-square distribution with degrees of freedom based on the number of cells and the number of parameters. If the p-value from the chi-square test is significant, the model does not provide a good fit.\n\n\n\n20.3.2 Parameter estimation\nThe parameters are usually estimated using maximum likelihood estimation (MLE). This method finds the parameter values that maximize the likelihood function L(\\beta), which represents the probability of observing the given data under the model. The log-likelihood function is often maximized instead of the likelihood because it is easier to work with sums of logs rather than products. In practice, MLE is often solved using iterative algorithms like iteratively reweighted least squares or Newton-Raphson.\n\nInterpretation of parameters\nThe coefficients \\lambda reflect the log odds ratios for the associations between the variables. If the interaction terms are significant, it indicates that there are interactions between the variables.\nA positive parameter estimate for an interaction term (e.g., \\lambda^{XY}) suggests that the odds of the joint occurrence of X and Y increase relative to the marginal effects. A negative estimate indicates a negative association between the variables.\nTesting parameter significance\nThe significance of individual parameters can be tested using Wald tests or likelihood ratio tests. The Wald test assesses whether each estimated coefficient is significantly different from zero:\n\nz = \\frac{\\hat{\\beta}}{\\text{SE}(\\hat{\\beta})}\n\nThe test statistic follows a standard normal distribution, and a significant test indicates that the parameter contributes significantly to the model.\n\n\nExample 1 \nA study employed a case-control design to investigate the impact of maternal education level on growth and developmental delays in children under the age of 5. A total of 173 cases (children with developmental delays) and 173 controls (children with normal development) were surveyed, including their mothers’ education levels.\n\n\n  Download data \n\n\ntb &lt;- read_csv(\"datasets/ex20-01.csv\", show_col_types = F) |&gt; \n  mutate(\n    edu = factor(edu),\n    grow = factor(grow),\n    edu = relevel(edu, ref = 3),\n    grow = relevel(grow, ref = 2)\n  )\n\nYou can fit log-linear models using the loglm() function from the MASS package or the glm() function with a Poisson family specification.\n\nfit &lt;- MASS::loglm(f ~ edu + grow, data = tb)\nfit1 &lt;- update(fit, .~.^2)\n1#fit2 &lt;- step(fit1, direction = \"backward\")\nanova(fit, fit1)\n\n\n1\n\nThe step() may not work properly with loglm() object when there is only when independent variable left in the model. However, glm() object does.\n\n\n\n\n#&gt; LR tests for hierarchical log-linear models\n#&gt; \n#&gt; Model 1:\n#&gt;  f ~ edu + grow \n#&gt; Model 2:\n#&gt;  f ~ edu + grow + edu:grow \n#&gt; \n#&gt;            Deviance df Delta(Dev) Delta(df) P(&gt; Delta(Dev)\n#&gt; Model 1   0.4585613  2                                    \n#&gt; Model 2   0.0000000  0  0.4585613         2        0.79511\n#&gt; Saturated 0.0000000  0  0.0000000         0        1.00000\n\n\nThe results show that the p-value for the likelihood ratio test is 0.79511, which is much larger than the typical significance level (0.05). This shows there is no significant improvement in fit by adding the interaction term (edu:grow) to the model. Therefore, the simpler Model 1 without the interaction is sufficient, and there is no strong evidence that the interaction between maternal education and child growth is important for explaining the data.\nThus, the simpler model (main effects only) is preferred because the interaction term does not significantly improve the model fit.\n\nglm_fit &lt;- glm(f ~ edu + grow, family = poisson(link = \"log\"), data = tb)\nglm_fit1 &lt;- update(glm_fit, .~.^2)\nglm_fit2 &lt;- step(glm_fit1, direction = \"backward\", trace = 1)\n\n#&gt; Start:  AIC=47.08\n#&gt; f ~ edu + grow + edu:grow\n#&gt; \n#&gt;            Df Deviance    AIC\n#&gt; - edu:grow  2  0.45856 43.536\n#&gt; &lt;none&gt;         0.00000 47.078\n#&gt; \n#&gt; Step:  AIC=43.54\n#&gt; f ~ edu + grow\n#&gt; \n#&gt;        Df Deviance    AIC\n#&gt; - grow  1    0.459 41.536\n#&gt; &lt;none&gt;       0.459 43.536\n#&gt; - edu   2   34.927 74.005\n#&gt; \n#&gt; Step:  AIC=41.54\n#&gt; f ~ edu\n#&gt; \n#&gt;        Df Deviance    AIC\n#&gt; &lt;none&gt;       0.459 41.536\n#&gt; - edu   2   34.927 72.005\n\nanova(glm_fit2, glm_fit, glm_fit1)\n\n#&gt; Analysis of Deviance Table\n#&gt; \n#&gt; Model 1: f ~ edu\n#&gt; Model 2: f ~ edu + grow\n#&gt; Model 3: f ~ edu + grow + edu:grow\n#&gt;   Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)\n#&gt; 1         3    0.45856                     \n#&gt; 2         2    0.45856  1  0.00000         \n#&gt; 3         0    0.00000  2  0.45856   0.7951\n\n\n\n\n\n\n\n\nResults\n\n\n\nThere is no significant improvement in model fit when adding grow (Model 2) or the interaction term edu:grow (Model 3). Model 1 (with only edu) may be sufficient, as adding more terms does not significantly enhance the fit.\n\n\nIn this context, the p-value is 0.7951, same to that from the likelihood ratio test made above.\nThe parameters of the final fitting model can be retrieved using the code below:\n\nglm_fit2 |&gt; coef()  \n\n#&gt; (Intercept)        edu1        edu2 \n#&gt;   3.8918203   0.5330263  -0.1905183\n\n\nThe regression equation is as follows:\n\nequatiomatic::extract_eq(glm_fit2, use_coefs = T, coef_digits = 5)\n\n\n\\log ({ \\widehat{E( \\operatorname{f} )} })  = 3.89182 + 0.53303(\\operatorname{edu}_{\\operatorname{1}}) - 0.19052(\\operatorname{edu}_{\\operatorname{2}})\n\n\n\n\nExample 2 \nA case-control study was conducted to investigate the role of contraceptives and the allele of clotting factor V-Leiden in the development of venous thrombosis. A total of 324 people were investigated, including 155 cases and 169 controls. Analyze the interaction between contraceptive use and the gene.\n\n\n  Download data \n\n\ntb &lt;- read_csv(\"datasets/ex20-02.csv\", show_col_types = F) |&gt; \n  mutate(\n    grp = factor(grp),\n    expo = factor(expo),\n    gtype = factor(gtype),\n    grp = relevel(grp, ref = 2),\n    expo = relevel(expo, ref = 2),\n    gtype = relevel(gtype, ref = 2)\n  )\n\n\nfit &lt;- MASS::loglm(f ~ grp + expo + gtype, data = tb)\nfit1 &lt;- update(fit, .~.^2)\nfit2 &lt;- update(fit, .~.^3)\nfit3 &lt;- step(fit2, direction = \"backward\", trace = 1)\n\n#&gt; Start:  AIC=16\n#&gt; f ~ grp + expo + gtype + grp:expo + grp:gtype + expo:gtype + \n#&gt;     grp:expo:gtype\n#&gt; \n#&gt;                  Df    AIC\n#&gt; - grp:expo:gtype  1 14.096\n#&gt; &lt;none&gt;              16.000\n#&gt; \n#&gt; Step:  AIC=14.1\n#&gt; f ~ grp + expo + gtype + grp:expo + grp:gtype + expo:gtype\n#&gt; \n#&gt;              Df    AIC\n#&gt; - expo:gtype  1 12.097\n#&gt; &lt;none&gt;          14.096\n#&gt; - grp:gtype   1 37.909\n#&gt; - grp:expo    1 42.920\n#&gt; \n#&gt; Step:  AIC=12.1\n#&gt; f ~ grp + expo + gtype + grp:expo + grp:gtype\n#&gt; \n#&gt;             Df    AIC\n#&gt; &lt;none&gt;         12.097\n#&gt; - grp:gtype  1 38.751\n#&gt; - grp:expo   1 43.762\n\nanova(fit, fit1, fit2, fit3)\n\n#&gt; LR tests for hierarchical log-linear models\n#&gt; \n#&gt; Model 1:\n#&gt;  f ~ grp + expo + gtype \n#&gt; Model 2:\n#&gt;  f ~ grp + expo + gtype + grp:expo + grp:gtype \n#&gt; Model 3:\n#&gt;  f ~ grp + expo + gtype + grp:expo + grp:gtype + expo:gtype \n#&gt; Model 4:\n#&gt;  f ~ grp + expo + gtype + grp:expo + grp:gtype + expo:gtype + grp:expo:gtype \n#&gt; \n#&gt;              Deviance df   Delta(Dev) Delta(df) P(&gt; Delta(Dev)\n#&gt; Model 1   62.41639390  4                                      \n#&gt; Model 2    0.09702652  2 6.231937e+01         2        0.00000\n#&gt; Model 3    0.09613097  1 8.955578e-04         1        0.97613\n#&gt; Model 4    0.00000000  0 9.613097e-02         1        0.75652\n#&gt; Saturated  0.00000000  0 0.000000e+00         0        1.00000\n\n\n\nglm_fit &lt;- glm(f ~ grp + expo + gtype, family = poisson(link = \"log\"), data = tb)\nglm_fit1 &lt;- update(glm_fit, .~.^2)\nglm_fit2 &lt;- update(glm_fit, .~.^3)\nglm_fit3 &lt;- step(glm_fit2, direction = \"backward\", trace = 1)\n\n#&gt; Start:  AIC=55.23\n#&gt; f ~ grp + expo + gtype + grp:expo + grp:gtype + expo:gtype + \n#&gt;     grp:expo:gtype\n#&gt; \n#&gt;                  Df Deviance    AIC\n#&gt; - grp:expo:gtype  1 0.096129 53.321\n#&gt; &lt;none&gt;              0.000000 55.225\n#&gt; \n#&gt; Step:  AIC=53.32\n#&gt; f ~ grp + expo + gtype + grp:expo + grp:gtype + expo:gtype\n#&gt; \n#&gt;              Df Deviance    AIC\n#&gt; - expo:gtype  1   0.0970 51.322\n#&gt; &lt;none&gt;            0.0961 53.321\n#&gt; - grp:gtype   1  25.9093 77.134\n#&gt; - grp:expo    1  30.9197 82.145\n#&gt; \n#&gt; Step:  AIC=51.32\n#&gt; f ~ grp + expo + gtype + grp:expo + grp:gtype\n#&gt; \n#&gt;             Df Deviance    AIC\n#&gt; &lt;none&gt;            0.097 51.322\n#&gt; - grp:gtype  1   28.751 77.977\n#&gt; - grp:expo   1   33.762 82.987\n\nanova(glm_fit, glm_fit3, glm_fit1, glm_fit2)\n\n#&gt; Analysis of Deviance Table\n#&gt; \n#&gt; Model 1: f ~ grp + expo + gtype\n#&gt; Model 2: f ~ grp + expo + gtype + grp:expo + grp:gtype\n#&gt; Model 3: f ~ grp + expo + gtype + grp:expo + grp:gtype + expo:gtype\n#&gt; Model 4: f ~ grp + expo + gtype + grp:expo + grp:gtype + expo:gtype + \n#&gt;     grp:expo:gtype\n#&gt;   Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    \n#&gt; 1         4     62.416                          \n#&gt; 2         2      0.097  2   62.319 2.934e-14 ***\n#&gt; 3         1      0.096  1    0.001    0.9761    \n#&gt; 4         0      0.000  1    0.096    0.7565    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\nResults\n\n\n\nModel 2 shows a significant improvement over Model 1, suggesting that the interaction terms grp:expo and grp:gtype significantly improve the model’s fit. Model 3 and Model 4 show no significant improvement over Model 2, with p-values above 0.75, meaning that adding the interaction term expo:gtype or the three-way interaction does not significantly improve the fit.\nThus, Model 2 is the most appropriate model, as it includes the important two-way interaction terms and significantly improves the fit without overcomplicating the model.\n\n\nThe regression coefficients of the final fitting model is:\n\nopt_model &lt;- glm(\n  f ~ grp * expo * gtype, \n  family = poisson(link = \"log\"), \n  data = tb) |&gt; \n  step(direction = \"backward\", trace = 0)\ncoef(opt_model)\n\n#&gt; (Intercept)        grp1       expo1      gtype1  grp1:expo1 grp1:gtype1 \n#&gt;   4.6082424  -1.0355344  -0.4700036  -3.3019907   1.3327101   2.0698471\n\n\nThe regression equation is as follows:\n\nequatiomatic::extract_eq(opt_model, use_coefs = T, coef_digits = 5)\n\n\n\\log ({ \\widehat{E( \\operatorname{f} )} })  = 4.60824 - 1.03553(\\operatorname{grp}_{\\operatorname{1}}) - 0.47(\\operatorname{\\expo}_{\\operatorname{1}}) - 3.30199(\\operatorname{gtype}_{\\operatorname{1}}) + 1.33271(\\operatorname{grp}_{\\operatorname{1}} \\times \\operatorname{\\expo}_{\\operatorname{1}}) + 2.06985(\\operatorname{grp}_{\\operatorname{1}} \\times \\operatorname{gtype}_{\\operatorname{1}})\n\n\n\nResidual analysis helps in assessing whether your model fits the data well and whether the assumptions of the model are met. Here’s how you can conduct residual analysis for your generalized linear model:\n\ndf &lt;- tibble(\n  dev_resid = residuals(opt_model, type = \"deviance\"),\n  fit_value = fitted.values(opt_model))\n\n# Plot residuals vs fitted values\nggplot(df, aes(fit_value, dev_resid)) +\n  geom_point(shape = 1, size = 2) +\n  geom_hline(yintercept = 0, col = \"red\")+\n  labs(\n    x = \"Fitted values\", \n    y = \"Deviance residuals\",\n    title = \"Residuals plot\"\n  )\n\n\n\n\n\n\n\n\n\n# Q-Q plot\nggplot(df, aes(sample = dev_resid)) +\n  geom_qq(shape = 1, size = 2) +\n  geom_qq_line() +\n  labs(x = \"Theoretical quantiles\", y = \"Sample quantiles\", title = \"Q-Q plot\")\n\n# Leverage plot\ntibble(\n  hat_value = hatvalues(opt_model),\n  index = c(1:length(hat_value))) |&gt; \n  ggplot(aes(index, hat_value)) +\n  geom_point(shape = 1, size = 2) +\n  labs(x = \"Index\", y = \"Hat_values\", title = \"Leverage plot\")\n\n# Cook's distance\ntibble(\n  cooks_dis = cooks.distance(opt_model),\n  index = c(1:length(cooks_dis))) |&gt; \n  ggplot(aes(index, cooks_dis)) +\n  geom_point(shape = 1, size = 2) +\n  geom_hline(yintercept = 4/(nrow(df)-length(coef(opt_model))), col = \"red\") +\n  labs(x = \"Index\", y = \"Cook's distance\", title = \"Cook's distance\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation\n\n\n\nIf the residuals are randomly scattered around zero in the Residuals plot, and the Q-Q plot follows a straight line, your model fits well.\nObservations with high leverage or high Cook’s distance might indicate influential points. You may want to examine these data points closely.\nThis analysis helps assess the final model’s fit and identify any potential issues such as outliers or non-linearity.",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Generalized linear model</span>"
    ]
  },
  {
    "objectID": "generalized-linear-model.html#poisson-regression",
    "href": "generalized-linear-model.html#poisson-regression",
    "title": "20  Generalized linear model",
    "section": "20.4 Poisson regression",
    "text": "20.4 Poisson regression\nPoisson regression is a type of generalized linear model used for count data. It is particularly useful when the dependent variable represents counts of events that occur independently over a fixed period of time or space. The model assumes that the count data follows a Poisson distribution.\nThe natural logarithm is used as the link function, connecting the linear predictor to the expected value of the response variable. This means that the model predicts the logarithm of the expected count. It assumes that the events are independent, and the mean and variance of the counts are equal.\nThe general form of a Poisson regression model can be expressed as:\n\n\\ln(\\lambda_i) = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\ldots + \\beta_k X_{ik}\n\nwhere \\lambda is the expected count for the i-th observation, X_{i1}, X_{i2}, \\ldots, X_{ik} are the predictor variables, \\beta_0, \\beta_1, \\ldots, \\beta_k are the coefficients to be estimated.\nA Poisson regression model with an offset is used when the rate of occurrence of events is being modeled, rather than the raw counts. The offset allows the model to account for different exposure times or different sizes of population at risk. In this case, the model becomes:\n\n\\ln(\\lambda_i) = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\ldots + \\beta_p X_{ik} + \\log(t_i)\n\nwhere t_i is the offset (often the exposure time or population size for each observation). The offset is added to the linear predictor as \\log(t_i), and it is treated as a known value rather than a variable for which a coefficient is estimated.\nThis adjustment is essential when the time or population at risk differs across observations.\n\n20.4.1 Goodness-of-fit test\nGoodness-of-fit tests are used to assess how well the model fits the data. Here are some common methods to conduct goodness-of-fit tests for Poisson regression.\nDeviance and Pearson tests\nDeviance is the most commonly used goodness-of-fit statistic in Poisson regression. It compares the model’s deviance to its degrees of freedom to assess the fit.\nPearson test involves calculating the sum of squared Pearson residuals and comparing it to its degrees of freedom.\nLikelihood ratio test\nThis test compares the likelihood functions of the full model and a reduced model to determine whether adding significant variables improves the model fit.\n\n\n20.4.2 Parameters estimation\nThe parameters are estimated by maximizing the likelihood function, which for the Poisson distribution is given by:\n\nL(\\beta) = \\prod_{i=1}^n \\frac{e^{-\\lambda_i} \\lambda_i^{Y_i}}{Y_i!}\n\nTo simplify the computation, the log-likelihood function is typically used. It is written as:\n\n\\log L(\\beta) = \\sum_{i=1}^{n} \\left( Y_i \\log(\\lambda_i) - \\lambda_i - \\log(Y_i!) \\right)\n\nSubstituting \\lambda_i = e^{X_i \\beta} into the equation, we get:\n\n\\log L(\\beta) = \\sum_{i=1}^{n} \\left( Y_i (X_i \\beta) - e^{X_i \\beta} - \\log(Y_i!) \\right)\n\nwhere \\lambda_i = e^{X_i \\beta}, representing the expected count for observation i, determined by the independent variables X_i and regression coefficients \\beta, Y_i is the observed count for the i-th observation.\nBy maximizing this log-likelihood function, the regression coefficients \\beta can be estimated for the Poisson regression model.\nTo assess whether a predictor is statistically significant, calculate the Z statistic:\n\nZ = \\frac{\\hat{\\beta}}{\\text{SE}(\\hat{\\beta})}\n\nwhere \\hat{\\beta} is the estimated coefficient and \\text{SE}(\\hat{\\beta}) is the standard error. The p-value is obtained from the standard normal distribution based on the calculated z-value. If the p-value is less than the significance level (commonly 0.05), reject the null hypothesis, indicating that the predictor is statistically significant.\n\nExample 3 \nA researcher conducted a retrospective cohort study to investigate the relationship between arsenic exposure in a smelting plant and deaths due to respiratory diseases among the plant’s employees from 1978 to 2009. The data can be download below. Please analyze this data.\n\n\n  Download data \n\n\ntb &lt;- read_csv(\"datasets/ex20-03.csv\", show_col_types = F) |&gt; \n  mutate(\n    AsExpo = factor(AsExpo, labels = c(\"As.No\", \"As.Yes\")),\n    ageGroup = factor(ageGroup, labels = c(\"40~49\", \"50~59\", \"60~69\", \"&gt;=70\")),\n    nDeath = as.integer(nDeath),\n    nTotal = as.integer(round(nTotal))\n  ) \n\n\ntb |&gt; \n  pivot_wider(\n    names_from  = AsExpo,\n    values_from = c(nDeath, nTotal)\n  ) |&gt; \n  knitr::kable(align = \"c\") \n\n\n\n\n\n\n\n\n\n\n\nageGroup\nnDeath_As.Yes\nnDeath_As.No\nnTotal_As.Yes\nnTotal_As.No\n\n\n\n\n40~49\n7\n14\n11026\n38337\n\n\n50~59\n42\n38\n10792\n31019\n\n\n60~69\n59\n58\n6898\n17496\n\n\n&gt;=70\n17\n41\n2581\n6842\n\n\n\n\n\nYou can use the glm() function to fit a Poisson regression model.\n\nmodel &lt;- glm(\n  nDeath ~ AsExpo + ageGroup, \n  family = poisson(link = \"log\"),\n  data = tb, \n  offset = log(nTotal)\n) \n\n\n\n\n\n\n\nOffset\n\n\n\nThe inclusion of the log(nTotal) as an offset allows us to model the mortality rate per person-year rather than just the raw number of deaths.\nWithout the offset, the model would compare the total number of deaths, which could lead to misleading conclusions if one age group simply had more number of deaths or more years than another. By using person-years as an offset, we adjust for this variability and focus on the mortality rate, making the comparison more meaningful.\nThis method is commonly used in epidemiology to model incidence rates for diseases, infections, or other health outcomes while adjusting for varying follow-up times or population sizes.\n\n\nThe coef() function can print the regression coefficients:\n\ncoef(model)\n\n#&gt;   (Intercept)  AsExpoAs.Yes ageGroup50~59 ageGroup60~69  ageGroup&gt;=70 \n#&gt;    -8.0086462     0.8108583     1.4701581     2.3660912     2.6237690\n\n\nThe regression equation is:\n\nequatiomatic::extract_eq(model, use_coefs = T, coef_digits = 5)\n\n\n\\log ({ \\widehat{E( \\operatorname{nDeath} )} })  = -8.00865 + 0.81086(\\operatorname{AsExpo}_{\\operatorname{As.Yes}}) + 1.47016(\\operatorname{ageGroup}_{\\operatorname{50\\char`\\~59}}) + 2.36609(\\operatorname{ageGroup}_{\\operatorname{60\\char`\\~69}}) + 2.62377(\\operatorname{ageGroup}_{\\operatorname{&gt;=70}}) + \\operatorname{(offset)}\n\n\n\nUse the summary() function to evaluate the parameter estimates, significance of predictors, and goodness-of-fit.\n\nsummary(model)\n\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = nDeath ~ AsExpo + ageGroup, family = poisson(link = \"log\"), \n#&gt;     data = tb, offset = log(nTotal))\n#&gt; \n#&gt; Coefficients:\n#&gt;               Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept)    -8.0086     0.2233 -35.859  &lt; 2e-16 ***\n#&gt; AsExpoAs.Yes    0.8109     0.1210   6.699 2.09e-11 ***\n#&gt; ageGroup50~59   1.4702     0.2453   5.994 2.04e-09 ***\n#&gt; ageGroup60~69   2.3661     0.2372   9.976  &lt; 2e-16 ***\n#&gt; ageGroup&gt;=70    2.6238     0.2548  10.297  &lt; 2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for poisson family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 260.9311  on 7  degrees of freedom\n#&gt; Residual deviance:   9.9323  on 3  degrees of freedom\n#&gt; AIC: 61.344\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 4\n\n\nThis Poisson regression model analyzes the relationship between arsenic exposure (AsExpo), age groups (ageGroup), and the number of deaths from respiratory diseases (nDeath). The model includes an offset (nTotal), which represents the total number of individuals at risk or the total exposure time, to adjust for varying group sizes or follow-up durations.\nHere’s a breakdown of the results:\n\nCoefficients\n\n(Intercept): -8.0086: The intercept represents the baseline log mortality rate for the reference group, which in this case is those not exposed to arsenic (AsExpo = “No”) and in the &lt;50 age group. The estimate of -8.0086 suggests a very low baseline mortality rate when exponentiated.\nAsExpoYes: 0.8109: The coefficient for arsenic exposure (Yes) indicates that the log mortality rate is increased by 0.8109 units for those exposed to arsenic compared to those not exposed. Exponentiating this value (\\exp(0.8109) = 2.25) suggests that arsenic exposure increases the mortality rate by about 125% compared to those unexposed.\nageGroup50~59: 1.4702: For individuals aged 50-59, the log mortality rate increases by 1.4702 units compared to those under 50. Exponentiating this value (\\exp(1.4702) = 4.35) shows that the mortality rate is approximately 4.35 times higher for this age group compared to those under 50.\nageGroup60~69: 2.3661: For individuals aged 60-69, the log mortality rate increases by 2.3661 units. Exponentiating this value (\\exp(2.3661) = 10.65) indicates that the mortality rate is about 10.65 times higher for this age group compared to the &lt;50 group.\nageGroup&gt;=70: 2.6238: For individuals aged 70 or older, the log mortality rate increases by 2.6238 units. Exponentiating this value (\\exp(2.6238) = 13.78) indicates that the mortality rate is 13.78 times higher for this age group compared to the reference (&lt;50) group.\n\nStatistical significance\n\nThe p-values for all variables are very small (&lt;0.001), indicating that arsenic exposure and all age groups are significantly associated with increased mortality rates.\n\nModel fit\n\nResidual Deviance: 9.9323 on 3 degrees of freedom. This indicates how well the model fits the data. The small residual deviance relative to the degrees of freedom suggests a good fit, meaning the model explains most of the variability in the data.\nNull Deviance: 260.9311 on 7 degrees of freedom. The null deviance represents the deviance of a model with no predictors, just an intercept. A large difference between null and residual deviance indicates that the predictors significantly improve the model fit.\nThe AIC (61.344) is a measure of model quality, with lower values indicating a better fit. It’s useful when comparing models to select the best one.\n\n\n\n\n\n\n\nKey findings\n\n\n\n\nArsenic exposure significantly increases the mortality rate (about 125% increase).\nAge is also a strong predictor, with older age groups showing exponentially higher mortality rates.\nThe model fits the data well, as evidenced by the low residual deviance and significant predictors.\n\n\n\n\nIf the residual deviance is significantly larger than the degrees of freedom, it may indicate overdispersion, suggesting that the Poisson model may not be appropriate. In such cases, consider using a negative binomial regression or adjusting the model accordingly.",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Generalized linear model</span>"
    ]
  },
  {
    "objectID": "generalized-linear-model.html#negative-binomial-regression",
    "href": "generalized-linear-model.html#negative-binomial-regression",
    "title": "20  Generalized linear model",
    "section": "20.5 Negative binomial regression",
    "text": "20.5 Negative binomial regression\nIn Poisson regression, the assumption is that the mean (\\lambda) and variance of the dependent variable are equal. When this assumption is violated (i.e., the data is overdispersed), the model may underestimate the standard errors, leading to misleading p-values and confidence intervals.\nNegative binomial regression is an extension of Poisson regression that is used when the data exhibits overdispersion—meaning the variance is larger than the mean. The negative binomial regression introduces an extra parameter, called the dispersion parameter or overdispersion parameter, to account for the excess variability.\n\nPoisson regression: \\text{var}(Y) = \\mu\nNegative binomial regression: \\text{var}(Y) = \\mu + \\frac{\\mu^2}{\\theta},\n\nwhere \\theta is the dispersion parameter (overdispersion parameter). When \\theta \\to \\infty, the negative binomial model reduces to a Poisson model.\nThe mean of the response variable Y_i is related to the predictors X_{i1}, X_{i2}, \\dots, X_{ik} through the following log-linear relationship:\n\n\\log(\\mu_i) = \\beta_0 + \\beta_1 X_{i1} + \\beta_2 X_{i2} + \\dots + \\beta_k X_{ik}\n\nwhere \\mu_i is the expected count (the mean of Y_i) for the i-th observation, X_{i1}, X_{i2}, \\dots, X_{ik} are the predictor variables, \\beta_0, \\beta_1, \\beta_2, \\dots, \\beta_k are the coefficients estimated from the data.\n\nExample 4 \nA researcher conducted a study to investigate the relationship between types of residence and the breeding of mosquito larvae by surveying 299 households from different types of residences. The results are shown in Table 20.1 . Please use an appropriate statistical method to analyze this data.\n\n\n  Download data \n\n\ntb &lt;- read_csv(\"datasets/ex20-04.csv\", col_types = list(resType = col_factor())) |&gt; \n  mutate(\n    resType = factor(resType, labels = c(\"rural\", \"barriada\", \"urban\"))\n  )\n\n\n\n\n\nTable 20.1: Breeding situation of mosquito larvae in different resident areas\n\n\n\n\n\n\n\n\n\n\n\n\n\nnContainer\nnFamily.rural\nnFamily.barriada\nnFamily.urban\nnFamily.total\n\n\n\n\n0\n136\n38\n67\n241\n\n\n1\n23\n8\n5\n36\n\n\n2\n10\n2\n0\n12\n\n\n3\n5\n0\n0\n5\n\n\n4\n2\n0\n0\n2\n\n\n5\n1\n0\n0\n1\n\n\n6\n1\n0\n0\n1\n\n\n11\n1\n0\n0\n1\n\n\nTotal\n179\n48\n72\n299\n\n\n\n\n\n\n\n\nYou can fit a negative binomial regression using the glm.nb() function from the MASS package:\n\nneg.bin_fit &lt;- MASS::glm.nb(nContainer ~ resType, data = tb, weights = nFamily)\npoisson_fit &lt;- glm(nContainer ~ resType, family = poisson, data = tb, weights = nFamily)\nAIC(neg.bin_fit, poisson_fit)\n\n#&gt;             df      AIC\n#&gt; neg.bin_fit  4 426.2283\n#&gt; poisson_fit  3 505.9154\n\n\nAIC is a measure used for model selection. A lower AIC value indicates a better-fitting model while penalizing for model complexity. In this case, the AIC for neg.bin_fit is 426.2283, which is lower than the AIC for poisson_fit, which is 505.9154. Since a lower AIC value is preferred, the negative binomial regression model fits the data better than the Poisson regression model.\nThe regression coefficients is:\n\ncoef(neg.bin_fit)\n\n#&gt;     (Intercept) resTypebarriada    resTypeurban \n#&gt;      -0.7100490      -0.6762454      -1.9571792\n\n\nThe regression equation is:\n\nequatiomatic::extract_eq(neg.bin_fit, use_coefs = T, coef_digits = 4,font_size = \"small\")\n\n\n\\small\n\\begin{aligned}\n\\log ({ \\widehat{E( \\operatorname{nContainer} )} })  &= -0.71 - 0.6762(\\operatorname{resType}_{\\operatorname{barriada}}) - 1.9572(\\operatorname{resType}_{\\operatorname{urban}})\n\\end{aligned}\n\n\n\n\nsummary(neg.bin_fit)\n\n#&gt; \n#&gt; Call:\n#&gt; MASS::glm.nb(formula = nContainer ~ resType, data = tb, weights = nFamily, \n#&gt;     init.theta = 0.3002652205, link = log)\n#&gt; \n#&gt; Coefficients:\n#&gt;                 Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept)      -0.7100     0.1731  -4.102  4.1e-05 ***\n#&gt; resTypebarriada  -0.6762     0.4274  -1.582 0.113612    \n#&gt; resTypeurban     -1.9572     0.5256  -3.724 0.000196 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for Negative Binomial(0.3003) family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 174.95  on 12  degrees of freedom\n#&gt; Residual deviance: 156.37  on 10  degrees of freedom\n#&gt; AIC: 426.23\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 1\n#&gt; \n#&gt; \n#&gt;               Theta:  0.3003 \n#&gt;           Std. Err.:  0.0764 \n#&gt; \n#&gt;  2 x log-likelihood:  -418.2280\n\n\nHere is the interpretation of the results from the negative binomial regression output:\n\nCoefficients\n\nIntercept: -0.7100, which corresponds to the log count of nContainer for the baseline group. p-value: 4.1e-05 (very small), which means the intercept is highly significant.\nresTypebarriada (Effect of “barriada” residences compared to the baseline): -0.6762, indicating that the log count of containers is lower in barriada residences compared to the baseline by about 0.68. p-value = 0.113612, suggesting that the effect is not statistically significant.\nresTypeurban (Effect of “urban” residences compared to the baseline): -1.9572, indicating that the log count of containers is significantly lower in urban residences compared to the baseline by about 1.96. p-value = 0.000196, suggesting this effect is statistically significant.\n\nModel fit\n\nNull deviance: 174.95 on 12 degrees of freedom, indicating how well the null model (no predictors) fits the data.\nResidual deviance: 156.37 on 10 degrees of freedom, indicating how well the model with predictors fits the data. The reduction in deviance suggests that the predictors improve the model fit.\nAIC: 426.23, a measure of model quality. Lower values suggest a better model fit, but should only be compared with other models fitted to the same data.\n\nDispersion parameter\n\nTheta: 0.3003, with a standard error of 0.0764. Theta is the overdispersion parameter in the negative binomial model, and this small value indicates overdispersion, meaning the variance is larger than the mean, which justifies the use of negative binomial over Poisson regression.\n\n\n\n\n\n\n\n\nConclusion\n\n\n\n\nThe baseline category (likely a reference category like rural) has a significant effect on the number of containers.\nUrban residences have significantly fewer containers compared to the baseline category.\nBarriada residences do not significantly differ from the baseline in terms of the number of containers.\nThe significant overdispersion in the data justifies using the negative binomial model over the Poisson model.",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Generalized linear model</span>"
    ]
  },
  {
    "objectID": "survival-analysis.html",
    "href": "survival-analysis.html",
    "title": "21  Survival analysis",
    "section": "",
    "text": "21.1 Prerequisite\nIn the medical field, survival analysis is widely used to study the time from a starting point to the occurrence of an important event, such as death, disease recurrence, or remission. It plays a critical role in analyzing time-to-event data, understanding the relationship between time and event occurrence, especially when some subjects do not experience the event during the study period, resulting in censored data. The core of survival analysis lies in properly handling censored data and time-related event occurrences, in order to effectively estimate survival probabilities and analyze risk factors.\nlibrary(tidyverse)\nlibrary(survival)\nlibrary(survminer)\nlibrary(ggplot2)",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Survival analysis</span>"
    ]
  },
  {
    "objectID": "survival-analysis.html#basic-concepts",
    "href": "survival-analysis.html#basic-concepts",
    "title": "21  Survival analysis",
    "section": "21.2 Basic concepts",
    "text": "21.2 Basic concepts\nSurvival data has several distinctive characteristics that differentiate it from other types of data, which make specialized methods necessary for analysis. The key characteristics of survival data include time-to-event data, censored data, non-normal distribution, and multiple covariates that influence survival time. Here is an example used to illustrate these features and related concepts.\n\nExample 1 \nTo estimate the survival time of patients who are HIV-positive, a researcher conducted a clinical follow-up study. The subjects were individuals diagnosed with HIV in a certain city between January 1, 2002, and December 31, 2004. These subjects were followed until they died of AIDS or its complications (with status = 1 for death, and 0 for censored cases). The study cutoff date was December 31, 2008. Additionally, the gender (sex = 1 for male, 0 for female), age (in years), and medication status (drug = 1 for on medication, 0 for not on medication) of each subject were recorded. The data format are shown in Table 21.1 .\n\n\n  Download data \n\n\ndf &lt;- read_csv(\"datasets/ex21-01.csv\", show_col_types = F)\n\n\n\n\n\nTable 21.1: Survival time and influencing factors of 100 HIV-positive patients\n\n\n\n\n\n\nID\nstartpoint\nendpoint\ntime\ngender\nage\ntreat\nstatus\n\n\n\n\n1\n2004-10-07\n2005-08-07\n10\n0\n27\n1\n1\n\n\n2\n2002-06-29\n2002-07-29\n1\n0\n47\n1\n0\n\n\n3\n2004-08-02\n2005-01-01\n5\n1\n40\n1\n1\n\n\n4\n2004-04-05\n2007-02-03\n34\n1\n37\n0\n1\n\n\n5\n2004-10-01\n2004-10-31\n1\n0\n33\n1\n1\n\n\n6\n2003-12-12\n2004-01-11\n1\n0\n42\n1\n1\n\n\n\n\n\n\n\n\n\n21.2.1 Time-to-event\nTime-to-event, also known as survival time, refers to the duration from a specified starting point (such as the onset of a condition, diagnosis, or treatment initiation) to the occurrence of a specific event of interest (such as death, disease progression, or recovery).\n\nIn Example 1, survival time represents the time from when the patient was diagnosed as HIV-positive (startpoint) until death or censoring (endpoint). The time column in Table 21.1 indicates the survival time in months for each participant.\nFor example: patient with ID=1 has a survival time of 10 months, from October 7, 2004, to August 7, 2005.\n\nThe survival times are often non-normally distributed and typically show a skewed distribution. Standard statistical techniques that assume normality are not appropriate for survival data, so specialized models like the Cox proportional hazards model or accelerated failure time models are used instead. Here is the normality test on the variable of survival time in Example 1 using shapiro.test() function:\n\nselect(df, time) |&gt; \n  pull() |&gt; \n  shapiro.test()\n\n#&gt; \n#&gt;  Shapiro-Wilk normality test\n#&gt; \n#&gt; data:  pull(select(df, time))\n#&gt; W = 0.65927, p-value = 6.682e-14\n\n\nMoreover, survival data typically exhibits heterogeneity, meaning that the time to the event is influenced by multiple factors, such as age, gender, disease stage, or treatment. Covariates need to be incorporated into survival models (e.g., Cox models) to analyze these factors’ impact on survival time.\nSurvival analysis revolves around time-to-event data, and focuses on estimating the probability that an event has not yet occurred at a certain point in time and identifying factors influencing this time.\n\n\n21.2.2 Censoring\nThe analysis of time-to-event data often involves censoring, where the exact time of the event is unknown for some individuals. Censoring occurs when the event of interest has not happened for some individuals by the end of the study or they are lost to follow-up. This means we only know that the individual survived up to a certain point, but not the exact time of the event. The common types of censoring includes:\nRight censoring: The most common type. It happens when an individual has not experienced the event by the end of the study. For example, a patient participates in a five-year study but is still alive at the end.\nLeft censoring: When the event has occurred before the observation, but the exact time is unknown. For example, a patient is diagnosed with a disease but the precise time of infection is unknown.\nInterval censoring: The event occurs within a known time interval, but the exact time is unknown. For example, a patient is diagnosed with a disease between two follow-up visits.\n\nIn Table 21.1 , the status column indicates whether the event (death) occurred: status = 1 means the patient died during the study (i.e., the event occurred), status = 0 indicates that the patient’s death was not observed, meaning they were censored.\nFor example, patient with ID=2 has status = 0, meaning the patient did not die by the end of the follow-up period, making this case censored data.\n\n\n\n21.2.3 Survival rate\nThe survival rate refers to the proportion of individuals who survive for a specific period of time. It is typically expressed as the percentage of individuals who are still alive at a given time interval. For example, a 5-year survival rate of 80% means that 80% of patients are still alive 5 years after diagnosis or treatment.\nSurvival rate can be derived from the survival function. The survival function, denoted as S(t), is a continuous function that gives the probability of an individual surviving beyond a specific time t. It is the key concept in survival analysis and shows how the probability of survival decreases over time. Mathematically, S(t) is defined as:\n\nS(t) = P(T &gt; t)\n\nwhere T is the time to the event (e.g., death), and t is a specific time point. For example, if S(5) = 0.80, this means that 80% of the individuals are expected to survive beyond 5 years. The survival function is typically visualized using a Kaplan-Meier curve, which plots the probability of survival against time. The curve generally starts at 1 (100% survival) and declines as time increases.\n\n\n21.2.4 Hazard rate\nThe hazard rate, also known as the instantaneous failure rate, describes the likelihood (or risk) that an event (e.g., death or failure) will occur at a particular instant, given that the individual has survived up to that point. It measures the “risk” at a specific time.\nFor instance, if you have survived 5 years, the hazard rate at that moment tells you how likely it is that you will experience the event (e.g., death) in the next moment of time. The hazard rate has the units of “events per unit time.” It can be interpreted as the instantaneous rate of event occurrence at a certain time.\nThe hazard function, denoted as h(t), is a continuous function that gives the hazard rate at any given time t. It provides the instantaneous rate at which events happen at time t, conditional on the fact that the individual has survived until that time. Mathematically, the hazard function is defined as:\n\nh(t) = \\lim_{\\Delta t \\to 0} \\frac{P(t \\leq T &lt; t + \\Delta t \\mid T \\geq t)}{\\Delta t}\n\nwhere T is the time to the event.\nFor example, if the hazard function h(t) = 0.02 at time t = 5 , this means the instantaneous risk of the event occurring at year 5 is 2\\% per unit of time, given that the subject has survived up to that point.\nThe survival function S(t) is related to the hazard function. In fact, the hazard function can be used to derive the survival function:\nS(t) = \\exp\\left(-\\int_0^t h(u) du \\right)\nThis shows that the survival function decreases as the hazard function increases. A higher hazard implies a greater risk of event occurrence, leading to a lower probability of survival.\nThe cumulative hazard function H(t) is the integral of the hazard function over time:\n\nH(t) = \\int_0^t h(u) du\n\nIt represents the total risk accumulated up to time t.",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Survival analysis</span>"
    ]
  },
  {
    "objectID": "survival-analysis.html#statistical-methods-for-survival-analysis",
    "href": "survival-analysis.html#statistical-methods-for-survival-analysis",
    "title": "21  Survival analysis",
    "section": "21.3 Statistical methods for survival analysis",
    "text": "21.3 Statistical methods for survival analysis\nStatistical methods for survival analysis are primarily used to process time-to-event data and analyze factors related to survival time. This type of data is characterized by the possibility of censoring, in which events for some individuals did not occur at the end of the study.\n\n21.3.1 Descriptive analysis\nDescriptive analysis of survival data aims to summarize and present the basic characteristics of survival time using median survival time, survival rates, event rates, and censored data proportions.\n\nKaplan-Meier curve\nThe Kaplan-Meier curve is a widely used non-parametric method to estimate and plot the survival function S(t), representing the probability of survival over time. It handles censored data and calculates survival probabilities at each event time, showing survival rates at different time points.\nMedian survival time\nMedian survival time is the time at which 50% of the individuals have experienced the event (e.g., death), and the other 50% are still surviving. It is more representative than the mean for skewed data and is not affected by extreme values. For example, if the median survival time for HIV-positive patients is 24 months, it means half of the patients die within 24 months of follow-up.\nEvent rate\nThe event rate reports how many individuals experience the event of interest (e.g., death) during the study and provides an overall summary of the event occurrences. For instance, in a study with 120 patients, if 30 patients die during follow-up, the event rate is 25%.\nDescription of censored data\nDescriptive analysis of censored data includes reporting the proportion of censored individuals and the distribution of censoring times. This helps understand how many individuals did not experience the event within the study period. In a clinical study with a 5-year follow-up, if 40% of patients are still alive at the end of the study, their survival data are considered censored.\nDistribution of censoring time\nDescribing the distribution of censoring times helps researchers understand when censoring occurs in the study. This can reveal whether censoring happens early, mid, or late in the study period. If there is heavy censoring early in the study, it may indicate inadequate follow-up or incomplete participation by individuals.\n\n\n\n21.3.2 Comparative analysis\nComparative analysis methods are used to assess the impact of different groups or variables on survival time. Here are some commonly used methods for comparative analysis of survival data:\n\nKaplan-Meier method\nThe Kaplan-Meier method is a widely used approach for comparing the survival outcomes of different groups by estimating and plotting survival curves.\nIt is commonly used to compare survival rates among patients with different treatments or characteristics (e.g., gender) by generating and displaying survival curves.\nLog-rank test\nThe Log-rank test is a non-parametric test used to compare the survival curves of two or more groups. It compares the number of events and the number of individuals at risk at each time point, assessing whether there are significant differences between the survival distributions of different groups.\nIt is often used in conjunction with Kaplan-Meier survival curves to determine if there are statistically significant differences in survival rates between treatment groups.\nHazard ratios comparison\nHazard ratios (HR) provide a measure for comparing the survival risks between different groups. HR can be estimated using the Cox model. For example, if HR = 1.5, it indicates that the mortality risk in group A is 1.5 times that of group B, offering a quantitative comparison of survival risk between groups.\nSurvival function comparison\nThis method involves directly comparing the survival functions of different groups (e.g., using Kullback-Leibler divergence) to assess differences in survival distributions. Comparing survival functions helps identify which groups have higher survival probabilities at specific time points.\n\n\n\n21.3.3 Affecting factors analysis\nAnalyzing the factors affecting survival data is crucial for assessing how various covariates influence survival time. Here are some commonly used methods for this type of analysis:\n\nCox proportional hazards model\nThe Cox proportional hazards model is a semi-parametric model used to analyze the relationship between survival data and multiple covariates. This model assumes that the effect of covariates on the hazard ratio is constant over time, allowing for the estimation of hazard ratios to compare survival times across different groups.\nIt is used to assess the impact of various factors (e.g., age, gender, treatment) on survival time and to perform multivariate analyses.\nKaplan-Meier analysis with covariates\nIncorporating covariates in Kaplan-Meier analysis allows for exploring survival differences among groups. Survival curves are generated for different groups based on covariates (e.g., treatment groups, age) and compared visually.\nThis approach helps to illustrate survival rates across different groups and can be complemented by the Log-rank test for statistical comparison.\nMultivariate analysis\nMultivariate analysis allows for the simultaneous consideration of multiple covariates affecting survival time. This can include using the Cox model or weighted survival regression to control for several covariates.\nIt assesses how multiple factors collectively influence survival time, such as evaluating the impact of age, gender, and treatment on patient survival.\nSurvival trees and random forests\nSurvival trees and random forests are tree-based non-parametric methods capable of handling high-dimensional data and nonlinear relationships. These models construct tree-like structures to partition the data, identifying key variables affecting survival time. These methods are useful for feature selection and modeling complex interactions.\nBayesian survival analysis\nBayesian survival analysis employs Bayesian methods to handle survival data, allowing for the incorporation of prior knowledge into the analysis. This method is beneficial for conducting uncertainty analysis and posterior predictions, particularly in small sample settings or complex models.",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Survival analysis</span>"
    ]
  },
  {
    "objectID": "survival-analysis.html#estimation-of-survival-rate-and-survival-curve",
    "href": "survival-analysis.html#estimation-of-survival-rate-and-survival-curve",
    "title": "21  Survival analysis",
    "section": "21.4 Estimation of survival rate and survival curve",
    "text": "21.4 Estimation of survival rate and survival curve\nEstimating survival rates is essential for determining the proportion of individuals who survive over a specified period. Here are some commonly used methods for estimating survival rates:\n\n21.4.1 Kaplan-Meier method\nThe Kaplan-Meier method, also known as the product-limit method, is a non-parametric method used to estimate the survival function from survival data. It accounts for censored data and adjusts the survival probability at each event occurrence. The survival probability is computed as:\n\n\\hat{S}(t) = \\prod_{i: t_i \\leq t} \\left( 1 - \\frac{d_i}{n_i} \\right)\n\nwhere t_i = the event time (time at which an event occurs), d_i = the number of events (e.g., deaths) at time t_i, n_i = the number of individuals at risk just before time t_i.\n\nExample 2 \nTo compare the efficacy of different surgical methods for treating adrenal tumors, a researcher randomly divided 43 patients into two groups: 23 in group A and 20 in group B. The survival times (in months) for each group are shown in the table below. Calculate the survival rate.\n\n\n  Download data \n\n\ntb &lt;- read_csv(\"datasets/ex21-02.csv\", show_col_types = F)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA\n1\n3\n5\n5\n5\n6\n6\n6\n7\n8\n10\n10\n14+\n17\n19+\n20+\n22+\n26+\n31+\n34\n34+\n44\n59\n\n\nB\n1\n1\n2\n3\n3\n4\n4\n4\n6\n6\n8\n9\n9\n10\n11\n12\n13\n15\n17\n18\n\n\n\n\n\n\n\n\nThe survfit() function from the survival package is commonly used to estimate survival rates. The Surv() function creates a survival object, usually used as a response variable in a model formula. Here’s an example of basic usage:\n\n1fit &lt;- survfit(Surv(time, event = status) ~ group, data = tb)\n2fit |&gt; summary(censored = T)\n\n\n1\n\nInstead of specifying ~ group in the model formula, which would compare survival between groups, you can use ~ 1 for a single group.\n\n2\n\nBy default, the summary() function for survfit objects only outputs a summary at distinct time points where events occur. If you want to show times for censored events as well, set censored = T.\n\n\n\n\n#&gt; Call: survfit(formula = Surv(time, event = status) ~ group, data = tb)\n#&gt; \n#&gt;                 group=1 \n#&gt;  time n.risk n.event survival std.err lower 95% CI upper 95% CI\n#&gt;     1     23       1    0.957  0.0425       0.8767        1.000\n#&gt;     3     22       1    0.913  0.0588       0.8049        1.000\n#&gt;     5     21       3    0.783  0.0860       0.6310        0.971\n#&gt;     6     18       3    0.652  0.0993       0.4839        0.879\n#&gt;     7     15       1    0.609  0.1018       0.4386        0.845\n#&gt;     8     14       1    0.565  0.1034       0.3950        0.809\n#&gt;    10     13       2    0.478  0.1042       0.3121        0.733\n#&gt;    14     11       0    0.478  0.1042       0.3121        0.733\n#&gt;    17     10       1    0.430  0.1041       0.2679        0.692\n#&gt;    19      9       0    0.430  0.1041       0.2679        0.692\n#&gt;    20      8       0    0.430  0.1041       0.2679        0.692\n#&gt;    22      7       0    0.430  0.1041       0.2679        0.692\n#&gt;    26      6       0    0.430  0.1041       0.2679        0.692\n#&gt;    31      5       0    0.430  0.1041       0.2679        0.692\n#&gt;    34      4       1    0.323  0.1216       0.1543        0.675\n#&gt;    44      2       1    0.161  0.1293       0.0336        0.776\n#&gt;    59      1       1    0.000     NaN           NA           NA\n#&gt; \n#&gt;                 group=2 \n#&gt;  time n.risk n.event survival std.err lower 95% CI upper 95% CI\n#&gt;     1     20       2     0.90  0.0671       0.7777        1.000\n#&gt;     2     18       1     0.85  0.0798       0.7071        1.000\n#&gt;     3     17       2     0.75  0.0968       0.5823        0.966\n#&gt;     4     15       3     0.60  0.1095       0.4195        0.858\n#&gt;     6     12       2     0.50  0.1118       0.3226        0.775\n#&gt;     8     10       1     0.45  0.1112       0.2772        0.731\n#&gt;     9      9       2     0.35  0.1067       0.1926        0.636\n#&gt;    10      7       1     0.30  0.1025       0.1536        0.586\n#&gt;    11      6       1     0.25  0.0968       0.1170        0.534\n#&gt;    12      5       1     0.20  0.0894       0.0832        0.481\n#&gt;    13      4       1     0.15  0.0798       0.0528        0.426\n#&gt;    15      3       1     0.10  0.0671       0.0269        0.372\n#&gt;    17      2       1     0.05  0.0487       0.0074        0.338\n#&gt;    18      1       1     0.00     NaN           NA           NA\n\n\nThe summary() output returns a list containing the survival rate, confidence limits for the rates, and other information.\nYou can use the code below to get the median survival time:\n\nfit |&gt; median() |&gt; as_tibble(rownames = \"term\") |&gt; \n  setNames(c(\"term\", \"median\"))\n\n#&gt; # A tibble: 2 × 2\n#&gt;   term    median\n#&gt;   &lt;chr&gt;    &lt;dbl&gt;\n#&gt; 1 group=1     10\n#&gt; 2 group=2      7\n\n\nThe ggsurvplot() function from survminer package is designed to directly plot survival curves. It’s a convenient function that creates publication-ready survival curves.\n\nggsurvplot(\n  fit,\n  xlab = \"Time(month)\",\n  ylab = \"Survival Probability\",\n  pval = T,\n  pval.size = 3.8,\n  censor.shape = 1,\n  censor.size = 2, \n  legend = c(0.9, 0.8),\n  legend.title = \"\",\n  legend.labs = c(\"A\", \"B\"),\n  palette = c(\"#E7B800\", \"#2E9FDF\"),\n  size = 0.5,\n  font.x = 9,\n  font.y = 9,\n  font.tickslab = 9,\n  font.legend = 9\n)\n\n\n\n\n\n\n\n\n\n\n21.4.2 Life table method\nLife tables are a classic method for estimating survival rates by dividing individuals into different age groups or time intervals. This method calculates survival rates based on the number of deaths and survivors in each age group. It is commonly used in public health and epidemiological studies to estimate the survival probability of specific populations.\n\nExample 3 \nA researcher followed up and collected data on 2,418 male angina patients in a certain region. The organized data is presented in Table 18-6. Please calculate the survival rate for male angina patients in this region.\n\n\n  Download data \n\n\ntb &lt;- read_csv(\"datasets/ex21-03.csv\", show_col_types = F)\n\n\nlife_df &lt;- tb |&gt; \n  mutate(censor = factor(censor, labels = c(\"event\", \"censor\"))) |&gt; \n  pivot_wider(\n    names_from = c(censor),\n    names_prefix = \"n.\",\n    values_from = freq\n  ) \n\nTo calculate the survival rates using life tables, you can use the lifetab() function from KMsurv package:\n\nsurv_rlt &lt;- life_df |&gt; \n  with(KMsurv::lifetab(\n1    tis = c(time, 16),\n    ninit = sum(n.event + n.censor),\n    nlost = n.censor,\n    nevent = n.event\n  )\n) \nsurv_rlt |&gt; knitr::kable(align = \"c\", digits = 5)\n\n\n1\n\nLength of tis is 1 greater than nlost and nevent.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnsubs\nnlost\nnrisk\nnevent\nsurv\npdf\nhazard\nse.surv\nse.pdf\nse.hazard\n\n\n\n\n0-1\n2418\n0\n2418.0\n456\n1.00000\n0.18859\n0.20822\n0.00000\n0.00796\n0.00970\n\n\n1-2\n1962\n39\n1942.5\n226\n0.81141\n0.09440\n0.12353\n0.00796\n0.00598\n0.00820\n\n\n2-3\n1697\n22\n1686.0\n152\n0.71701\n0.06464\n0.09441\n0.00918\n0.00507\n0.00765\n\n\n3-4\n1523\n23\n1511.5\n171\n0.65237\n0.07380\n0.11992\n0.00973\n0.00543\n0.00915\n\n\n4-5\n1329\n24\n1317.0\n135\n0.57856\n0.05931\n0.10804\n0.01014\n0.00495\n0.00929\n\n\n5-6\n1170\n107\n1116.5\n125\n0.51926\n0.05813\n0.11860\n0.01030\n0.00503\n0.01059\n\n\n6-7\n938\n133\n871.5\n83\n0.46112\n0.04392\n0.10000\n0.01038\n0.00469\n0.01096\n\n\n7-8\n722\n102\n671.0\n74\n0.41721\n0.04601\n0.11672\n0.01045\n0.00518\n0.01355\n\n\n8-9\n546\n68\n512.0\n51\n0.37120\n0.03697\n0.10483\n0.01058\n0.00502\n0.01466\n\n\n9-10\n427\n64\n395.0\n42\n0.33422\n0.03554\n0.11230\n0.01072\n0.00531\n0.01730\n\n\n10-11\n321\n45\n298.5\n43\n0.29868\n0.04303\n0.15523\n0.01089\n0.00627\n0.02360\n\n\n11-12\n233\n53\n206.5\n34\n0.25566\n0.04209\n0.17942\n0.01112\n0.00685\n0.03065\n\n\n12-13\n146\n33\n129.5\n18\n0.21356\n0.02968\n0.14938\n0.01140\n0.00668\n0.03511\n\n\n13-14\n95\n27\n81.5\n9\n0.18388\n0.02031\n0.11688\n0.01177\n0.00651\n0.03889\n\n\n14-15\n59\n33\n42.5\n6\n0.16357\n0.02309\n0.15190\n0.01226\n0.00891\n0.06183\n\n\n15-16\n20\n20\n10.0\n0\n0.14048\nNA\nNA\n0.01368\nNA\nNA\n\n\n\n\n\nYou can visualize the survival curve using ggplot2:\n\ntibble(\n  time = life_df$time,\n  surv.rate = surv_rlt$surv\n) |&gt; \n  ggplot(aes(time, surv.rate)) +\n  geom_point(shape = 8) +\n  geom_line() +\n  labs(x = \"Time\", y = \"Survival Probability\", font = 9) +\n  theme(axis.title = element_text(size = 9))",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Survival analysis</span>"
    ]
  },
  {
    "objectID": "survival-analysis.html#comparison-of-survival-rates",
    "href": "survival-analysis.html#comparison-of-survival-rates",
    "title": "21  Survival analysis",
    "section": "21.5 Comparison of survival rates",
    "text": "21.5 Comparison of survival rates\nThe comparison of survival rates in survival analysis is often done to evaluate whether the survival experiences of two or more groups differ significantly. There are various statistical methods available to compare survival rates across groups. Here’s an outline of common methods used for comparing survival rates:\n\nExample 4 \nBased on the data from Example 2, compare the survival rates between the two surgical methods, A and B, to determine if there is a significant difference.\n\n\ntb &lt;- read_csv(\"datasets/ex21-02.csv\", show_col_types = F)\n\n\n21.5.1 Log-rank test\nLog-rank test, also known as Mantel-Cox test, is the most commonly used method to compare survival rates between two or more groups. It compares the observed events with the expected number of events under the assumption that the survival experiences are the same for all groups. The log-rank test is used when you want to test whether two or more groups have the same survival distribution.\nYou can run th log-rank test using the survdiff() function:\n\nsurvdiff(Surv(time, status) ~ group, data = tb) \n\n#&gt; Call:\n#&gt; survdiff(formula = Surv(time, status) ~ group, data = tb)\n#&gt; \n#&gt;          N Observed Expected (O-E)^2/E (O-E)^2/V\n#&gt; group=1 23       16     23.8      2.56      8.75\n#&gt; group=2 20       20     12.2      5.00      8.75\n#&gt; \n#&gt;  Chisq= 8.8  on 1 degrees of freedom, p= 0.003\n\n\n\n\n21.5.2 Breslow test\nBreslow test, also known as generalized Wilcoxon test or Gehan-Wilcoxon test, is a non-parametric test used to compare the survival distributions of two or more groups in survival analysis. It is similar to the log-rank test, but it gives more weight to events that occur at earlier time points. This makes the Breslow test particularly useful when you expect survival differences to be more pronounced early in the study period.\nYou can run the Breslow test using the survdiff() function from the survival package by specifying the rho argument as 1.\n\nsurvdiff(Surv(time, status) ~ group, data = tb, rho = 1)\n\n#&gt; Call:\n#&gt; survdiff(formula = Surv(time, status) ~ group, data = tb, rho = 1)\n#&gt; \n#&gt;          N Observed Expected (O-E)^2/E (O-E)^2/V\n#&gt; group=1 23     9.15    13.41      1.35       5.2\n#&gt; group=2 20    12.69     8.43      2.15       5.2\n#&gt; \n#&gt;  Chisq= 5.2  on 1 degrees of freedom, p= 0.02",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Survival analysis</span>"
    ]
  },
  {
    "objectID": "survival-analysis.html#cox-proportional-hazards-regression-model",
    "href": "survival-analysis.html#cox-proportional-hazards-regression-model",
    "title": "21  Survival analysis",
    "section": "21.6 Cox proportional hazards regression model",
    "text": "21.6 Cox proportional hazards regression model\nThe Cox proportional hazards regression model, often referred to as the Cox model, is a commonly used method in survival analysis to assess the effect of several variables on the survival time. It models the relationship between covariates and the hazard function, which describes the risk of an event (such as death or failure) occurring at a given time.\nThe Cox model does not require the survival times to follow a specific probability distribution. The ratio of hazards between groups remains constant over time. This model can effectively account for right-censored data.\n\n21.6.1 Equation of Cox model\nThe Cox model expresses the hazard for an subject at time t, given a set of covariates X, as:\n\nh(t|X) = h_0(t) \\cdot \\exp(\\beta_1 X_1 + \\beta_2 X_2 + … + \\beta_p X_p)\n\nwhere h_0(t) is the baseline hazard at time t, \\beta_1, \\beta_2, …, \\beta_p are the regression coefficients corresponding to covariates X_1, X_2, …, X_p.\n\n\n21.6.2 Parameter estimation and hypothesis test\nIn the Cox model, estimating parameters and conducting hypothesis tests are critical steps for understanding the relationships between covariates and survival outcomes.\nParameter estimation\nThe Cox model estimates the regression coefficients (denoted by \\beta) using partial likelihood estimation. Unlike traditional regression models that use maximum likelihood, the Cox model’s partial likelihood method focuses only on the ordering of event times and does not require specifying the baseline hazard function h_0(t).\nFor a set of n individuals, suppose that at time t_i one individual experiences an event, while the others are either still alive or censored. The partial likelihood for the Cox model is based on the probability that the event occurs to individual i , given that it occurs to one of the individuals still at risk at time t_i .\nThe partial likelihood function L(\\beta) for the Cox model is given by:\n\nL(\\beta) = \\prod_{i \\in \\text{events}} \\frac{\\exp(\\beta^T X_i)}{\\sum_{j \\in \\text{risk set at time } t_i} \\exp(\\beta^T X_j)}\n\nwhere X_i is the covariate vector for individual i. The denominator is the sum of the covariate effects of all individuals still at risk at time t_i. The parameters \\beta are estimated by maximizing the partial likelihood L(\\beta), which provides estimates of the log-hazard ratios for each covariate.\nHypothesis test\nOnce the model is fit and the coefficients \\beta are estimated, hypothesis test is used to assess the significance of each covariate in the model. Similar to logistic regression, commonly used regression coefficient hypothesis test methods include likelihood ratio test, Wald test and score test, as discussed in Chapter 19 .\n\n\n21.6.3 Screening of factors\nIn a Cox model, selecting influential factors and establishing the best model are crucial steps to identify variables significantly associated with survival time and to construct a simplified model with strong predictive capability. Below is an overview of the methods and steps involved:\nInitial screening of factors\nWhen building a Cox model, common methods for preliminary variable screening include univariate analysis and correlation analysis.\n\nUnivariate analysis\nStart by using univariate Cox regression models to assess the individual association between each variable and survival time. By fitting univariate Cox models, you can determine which variables may have a significant effect on survival time. The log-rank test can also be used for analysis of categorical covariates.\n\ncoxph(Surv(time, status) ~ covariate, data = tb)\n\nFocus on the p-values (typically, p &lt; 0.05 is considered significant) to identify variables related to survival time.\n\ncor(var1, var2)\n\nCorrelation analysis\nFor cases where there may be multicollinearity between variables, correlation analysis (Pearson or Spearman correlation) can be used to assess relationships between variables, avoiding the inclusion of highly correlated variables in the model to prevent overfitting.\n\nStepwise regression\nAfter the initial screening, stepwise regression can be used to automatically select the best model. This method adds or removes variables step-by-step, ultimately selecting those that have a significant effect on survival time, leading to a simplified and accurate model.\nThe step() function is used for stepwise regression:\n\n# Fit the full model\nfull_model &lt;- coxph(Surv(time, status) ~ var1 + var2 + var3, data = tb)\n\n# Use AIC for stepwise regression\nbest_model &lt;- step(full_model, direction = \"both\")\n\nStepwise regression selects the model with the lowest AIC, as a lower AIC indicates a better fit.\nLASSO and RIDGE regression\nFor cases with many variables, LASSO or RIDGE regression can be used for variable selection. LASSO regression applies an L_1 norm penalty to shrink some coefficients to zero, effectively selecting variables. RIDGE regression applies an L_2 norm penalty to prevent overfitting, though it doesn’t shrink coefficients to zero.\nThe glmnet package is used for LASSO and Ridge regression:\n\nlibrary(glmnet)\n\n# Prepare data\nX &lt;- model.matrix(Surv(time, status) ~ var1 + var2 + var3, data = tb)[,-1]\ny &lt;- Surv(tb$time, tb$status)\n\n# LASSO regression (alpha=1)\nlasso_model &lt;- cv.glmnet(X, y, family = \"cox\", alpha = 1)\n\n# Check the best model's coefficients\ncoef(lasso_model, s = \"lambda.min\")\n\nModel evaluation\nAfter selecting the best model, it’s important to evaluate the model to ensure it fits well and has good predictive ability. Common evaluation methods include:\n\nC-Statistic (C-index)\nThe C-statistic evaluates the model’s ability to distinguish between different survival times. Values range between 0.5 and 1, with values closer to 1 indicating better predictive ability.\n\nsurvival::concordance()\n\nResidual analysis\nSchoenfeld residuals can be used to test whether the proportional hazards assumption holds for the Cox model. If the assumption is violated, time-dependent covariates may be needed to correct the model.\n\ncox_test &lt;- cox.zph(best_model)\ncox_test\n\nThe output of cox.zph() provides both statistical tests and diagnostic plots for each covariate in the Cox model.\nGlobal test: This tests the overall proportional hazards assumption for the entire model. If this test is significant (p &lt; 0.05), it suggests that the assumption might be violated in the model.\nIndividual tests: These are separate tests for each covariate in the model. If the p-value for a covariate is not significant (p &gt; 0.05), it indicates that the assumption hold for that covariate.\n\nggcoxzph(cox_test)\n\nIf the lines in the diagnostic plot are roughly horizontal, it suggests that the proportional hazards assumption holds.\n\nScreening for influential factors and building the best model are key steps in Cox regression analysis. Methods such as univariate analysis, stepwise regression, and LASSO regression can effectively select variables. Evaluation metrics like AIC and C-index help assess model performance, ultimately leading to a model that is both interpretable and accurate in predicting survival outcomes.\nOutput interpretation\nWhen fitting a Cox model using the coxph() function, the output provides estimated coefficients, hazard ratios, and results for hypothesis tests.\n\nCoefficients (coef)\nThese represent the estimated coefficients for the covariates in the model. A positive coefficient indicates that the covariate is associated with an increased hazard (risk of the event), and a negative coefficient indicates a decreased hazard (protective factor).\nHazard ratios (exp(coef))\nThe hazard ratio is the exponentiated coefficient (e^{\\text{coef}}) and indicates the multiplicative effect of the covariate on the hazard. A hazard ratio of 1 indicates no effect, greater than 1 indicates an increased hazard, and less than 1 indicates a decreased hazard.\np-value\nThe p-value tests the null hypothesis that the coefficient is zero (no effect). A small p-value (usually &lt; 0.05) indicates that the covariate has a statistically significant effect on the hazard.\n95% CI for exp(coef)\nThese provide a range of plausible values for the hazard ratio. If the confidence interval does not include 1, the effect of the covariate is considered statistically significant. A 95% CI that lies entirely above 1 suggests a positive association (increased hazard), while a CI below 1 suggests a negative association (decreased hazard).\nLikelihood ratio test, Wald test, and Score (logrank) test\nThese tests assess the overall significance of the Cox model. Likelihood ratio test compares the likelihood of the model with covariates to the likelihood without covariates. Wald test assesses whether the coefficients are significantly different from zero. Score test (Log-rank test) is similar to the likelihood ratio test, but uses a different method for assessment.\nConcordance (c-index)\nThe concordance index (or c-index) measures the predictive accuracy of the model, indicating how well the model predicts the ordering of survival times. A c-index of 0.5 indicates random predictions, while a value closer to 1 indicates better predictive power.\n\n\nExample 5 \nTo investigate the prognosis of a certain malignant tumor, a researcher collected survival times, survival outcomes, and influencing factors for 63 patients. The influencing factors include patient age, gender, histological type, treatment method, lymph node metastasis, and tumor infiltration depth. The survival time, denoted as t , is measured in months. The variable coding is shown in Table 21.2 . Perform an analysis on the data using Cox regression model.\n\n\n  Download data \n\n\n\n\n\nTable 21.2: Influencing factors and assignment of a malignant tumor\n\n\n\n\n\n\n\n\n\n\n\nFactor\nVariable\nAssignment specification\n\n\n\n\nAge\nX1\n(Year)\n\n\nGender\nX2\nFemale＝0，Male＝1\n\n\nHistology type\nX3\nPoorly differentiated＝0，Well differentiated＝1\n\n\nTherapeutic method\nX4\nTraditional＝0，New＝1\n\n\nLymphatic metastasis\nX5\nNo＝0，Yes＝1\n\n\nDegree of tumor invasion\nX6\nNot penetrated the serosal layer＝0，Penetrated the serosal layer＝1\n\n\nSurvival time\nt\n(Month)\n\n\nSurvival status\nY\nCensored＝0，Dead＝1\n\n\n\n\n\n\n\n\n\ntb &lt;- read_csv(\"datasets/ex21-04.csv\", show_col_types = F) \n\nYou can use the coxph() function from the survival package to fit a Cox model.\n\ncox_model &lt;- coxph(Surv(t, y) ~ ., data = tb)   \nggforest(cox_model, data = tb, main = \"\", fontsize = 0.9, noDigits = 3)\n\n\n\n\n\n\n\n\nThe function ggforest() in the survminer package is used to generate a forest plot for a Cox proportional hazards model. The forest plot is a graphical representation of the hazard ratios (HR) for each covariate included in the Cox model, along with their confidence intervals.\nFor each covariate, the plot shows how much it increases or decreases the hazard. THe HR &gt; 1 suggest an increased hazard, while HR &lt; 1 suggest a reduced hazard. If the confidence interval crosses 1, it indicates that the variable might not be statistically significant at the chosen significance level.\nA forest plot makes it easier to visually compare the relative effect sizes of different variables in the Cox model. It’s commonly used in research papers to succinctly summarize Cox regression results. It helps researchers understand which factors have a large or small impact on survival in their study.\n\noptimal_model &lt;- step(cox_model, direction = \"both\", trace = 0) \nsummary(optimal_model)\n\n#&gt; Call:\n#&gt; coxph(formula = Surv(t, y) ~ X4 + X5, data = tb)\n#&gt; \n#&gt;   n= 63, number of events= 26 \n#&gt; \n#&gt;       coef exp(coef) se(coef)      z Pr(&gt;|z|)   \n#&gt; X4 -1.7830    0.1681   0.5479 -3.254  0.00114 **\n#&gt; X5  0.9395    2.5587   0.4446  2.113  0.03460 * \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt;    exp(coef) exp(-coef) lower .95 upper .95\n#&gt; X4    0.1681     5.9477   0.05745    0.4921\n#&gt; X5    2.5587     0.3908   1.07044    6.1163\n#&gt; \n#&gt; Concordance= 0.752  (se = 0.043 )\n#&gt; Likelihood ratio test= 19.65  on 2 df,   p=5e-05\n#&gt; Wald test            = 14.87  on 2 df,   p=6e-04\n#&gt; Score (logrank) test = 18.02  on 2 df,   p=1e-04\n\n\nThe Cox model output provides insights into the relationship between the covariates and the survival time (or event occurrence).\n\n\nModel fit summary\n\nn = 63: There were 63 individuals in the dataset.\nnumber of events = 26: Out of these 63 individuals, 26 experienced the event of interest (e.g., death, relapse).\n\nCoefficients and hazard ratios\n\nX_4: Coefficient is -1.7830: This indicates that X_4 is negatively associated with the hazard of the event. A negative coefficient means that an increase in X_4 decreases the hazard (or risk). The HR = 0.1681, since the hazard ratio is less than 1, it suggests that X_4 is protective (reduces the risk). Specifically, a one-unit increase in X_4 decreases the hazard by 83.19% (calculated as 1 - 0.1681 = 0.8319). The p-value is 0.00114, this is highly significant, meaning there is strong evidence that X_4 has an effect on survival.\nX_5: Coefficient is 0.9395: This indicates that X_5 is positively associated with the hazard of the event. A positive coefficient means that an increase in X_5 increases the hazard (or risk). The HR = 2.5587, a hazard ratio greater than 1 means that X_5 increases the hazard. Specifically, a one-unit increase in X_5 multiplies the hazard by approximately 2.56, meaning a 156% increase in the risk of the event. The p-value is 0.03460, this is also statistically significant, indicating that X_5 has a meaningful effect on survival.\n\nCI for HR\n\nX_4: The 95% CI for the hazard ratio of X_4 is (0.05745, 0.4921). This means we are 95% confident that the true hazard ratio lies between these values. Since this interval does not include 1, X_4 has a statistically significant protective effect.\nX_5: The 95% CI for the hazard ratio of X_5 is (1.07044, 6.1163). Since this interval is entirely above 1, it confirms that X_5 significantly increases the hazard.\n\nModel Performance Metrics\n\nConcordance = 0.752: This measures the model’s predictive accuracy. A concordance of 0.752 means that in 75.2% of the pairs of subjects, the model correctly predicts which one experienced the event first.\nLikelihood ratio test: A test of the overall significance of the model. The p-value (p = 5e-05) is highly significant, indicating that the model as a whole explains a significant portion of the variation in the hazard.\nWald test: This tests the significance of the coefficients collectively. The p-value (p = 6e-04) is also significant, suggesting that the covariates in the model are jointly significant.\nScore (logrank) test: Another test of the model’s significance, with a highly significant p-value (p = 1e-04).\n\n\n\nIt’s important to check if the proportional hazards assumption holds for your data. You can use the cox.zph() function to test this:\n\n# Check proportional hazards assumption\ncox_test &lt;- cox.zph(optimal_model, transform = \"km\")\ncox_test\n\n#&gt;        chisq df     p\n#&gt; X4     2.835  1 0.092\n#&gt; X5     0.149  1 0.700\n#&gt; GLOBAL 2.990  2 0.224\n\n\nThis is the output from the cox.zph() test, which checks the proportional hazards assumption for each covariate in a Cox model, as well as a global test that examines the assumption across all covariates in the model.\n\n\nX_4: The p-value is 0.092, which is greater than the significance level of 0.05. This indicates that there is no strong evidence to reject the null hypothesis that the proportional hazards assumption holds for X_4. However, the p-value is somewhat close to 0.05, suggesting that the assumption might be slightly violated for X_4. You should inspect the plot for X_4 to see if there are any obvious trends.\nX_5: The p-value is 0.700, which is well above 0.05. This suggests that there is no evidence of violation of the proportional hazards assumption for X_5. The assumption holds well for this covariate.\nGlobal: The global p-value is 0.224, which is also greater than 0.05. This means that, overall, there is no significant evidence to suggest that the proportional hazards assumption is violated for the model as a whole.\n\n\nThe plots display the scaled Schoenfeld residuals for the covariates X_4 and X_5. These residuals help in assessing the proportional hazards assumption in the Cox proportional hazards model.\n\nggcoxzph(\n  cox_test, point.col = \"blue\", point.size = 1.3, point.shape = 1, \n  font.title = 9, font.x = 9, font.y = 9, font.tickslab = 9)\n\n\n\n\n\n\n\n\n\nThe lines in the diagnostic plot are roughly horizontal, it suggests that the proportional hazards assumption holds.\n\nIf the proportional hazards assumption does not hold, you could consider:\n\nStratified cox models: If the non-proportionality is due to specific covariates, you can stratify by these variables.\nTime-dependent covariates: If the hazard ratio changes over time, including time-dependent covariates in the model might address this issue.\n\nYou can also visualize the survival curves for each group using the ggsurvplot() function:\n\nggsurvplot(\n  survfit(optimal_model),\n  data = tb,\n  xlab = \"Time(month)\",\n  ylab = \"Survival Probability\",\n  pval.size = 3.8,\n  censor.shape = 1,\n  censor.size = 2, \n  legend = \"none\",\n  conf.int = F,\n  size = 0.5,\n  font.x = 9,\n  font.y = 9,\n  font.tickslab = 9,\n  font.legend = 9\n)\n\n\n\n\n\n\n\n\nThis will generate Kaplan-Meier survival curves based on the fitted Cox model, allowing you to visually compare the survival distributions between groups.\nThe Cox model is a flexible and powerful tool for assessing the impact of various covariates on survival times without making assumptions about the underlying survival distribution. It is especially useful for analyzing clinical and observational data where the event of interest is subject to right-censoring.",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Survival analysis</span>"
    ]
  },
  {
    "objectID": "survival-analysis.html#life-tabes",
    "href": "survival-analysis.html#life-tabes",
    "title": "21  Survival analysis",
    "section": "21.7 Life tabes",
    "text": "21.7 Life tabes",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Survival analysis</span>"
    ]
  },
  {
    "objectID": "cluster-analysis.html",
    "href": "cluster-analysis.html",
    "title": "22  Cluster analysis",
    "section": "",
    "text": "22.1 Prerequisite\nCluster analysis is a statistical method used to group objects, observations, or data points into clusters or groups based on their similarity or proximity to one another. It is a form of unsupervised learning, meaning there are no predefined labels or categories for the data.\nlibrary(tidyverse)\nlibrary(dendextend)\nlibrary(factoextra)\nlibrary(clustMixType)\nlibrary(cluster)\nlibrary(clustMD)\nlibrary(FactoMineR)",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Cluster analysis</span>"
    ]
  },
  {
    "objectID": "cluster-analysis.html#overview",
    "href": "cluster-analysis.html#overview",
    "title": "22  Cluster analysis",
    "section": "22.2 Overview",
    "text": "22.2 Overview\nThe goal of cluster analysis is to maximize the similarity of objects within the same group (intra-group similarity) while maximizing the differences between groups (inter-group dissimilarity).\n\n22.2.1 Similarity metrics\nSimilarity metrics are crucial for cluster analysis, as they help determine how similar or different patients, diseases, or clinical observations are from one another. These metrics are essential when grouping patients with similar characteristics, symptoms, or responses to treatments. Different types of data (continuous, categorical, binary) require different similarity metrics.\nEuclidean distance\n\nd(x, y) = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2}\n\nIt measures the straight-line distance between two points in a multidimensional space. It is one of the simplest and most commonly used metrics for continuous data, such as blood pressure, heart rate, or lab test values.\nManhattan distance\n\nd(x, y) = \\sum_{i=1}^{n} |x_i - y_i|\n\nItmeasures the absolute distance between two points along each dimension (or feature). It sums the absolute differences between corresponding coordinates.\nMinkowski distance\n\nd(x, y) = \\left( \\sum_{i=1}^{n} |x_i - y_i|^r \\right)^{1/r}\n\nThis is a generalization of both Euclidean and Manhattan distances, allowing for flexible comparison based on the value of r. For r = 1, it is Manhattan distance; for r = 2, it becomes Euclidean distance. It is useful in clinical clustering where different levels of sensitivity to distance are required.\nCosine similarity\n\\text{similarity} = \\text{cos}\\theta_{ij} = \\frac{\\sum_{i=1}^{n} x_i y_i}{\\sqrt{\\sum_{i=1}^{n} x_i^2} \\sqrt{\\sum_{i=1}^{n} y_i^2}}\n\nIt measures the cosine of the angle between two vectors, often used in text data of medical records or high-dimensional gene expression data, where the magnitude (scale) of variables is less important than their direction (pattern)\nMahalanobis distance\n\nd(x, y) = \\sqrt{(x_i - y_i)^T S^{-1} (x_i - y_i)}\n\nwhere S is the covariance matrix. It takes into account the correlation between variables.It is often used in multivariate data such as laboratory results or clinical trial data where correlations between clinical measures exist (e.g., blood pressure, heart rate).\nGower’s distance\n\nD_{ij} = \\frac{1}{n} \\sum_{k=1}^{n} d_{ijk}\n\nwhere d_{ijk} is the normalized distance for the k-th variable. It is specifically designed for datasets with mixed data types. It calculates a dissimilarity measure that can combine continuous, categorical, and binary variables.\nJaccard similarity\n\nJ(x, y) = \\frac{|x \\cap y|}{|x \\cup y|}\n\nIt is commonly used for binary or categorical data, such as the presence or absence of symptoms, genetic markers, or disease diagnoses.\nHamming distance\n\nd(x, y) = \\sum_{i=1}^{n} \\delta(x_i, y_i)\n\nwhere \\delta(x_i, y_i) is 1 if x_i \\neq y_i, and 0 otherwise. Hamming distance counts the number of positions at which the corresponding elements of two strings are different. It is used in clustering genetic sequences or binary data such as mutation profiles, or yes/no responses in questionnaires. It is effective for measuring differences in binary or categorical data.\nPearson correlation\n\nd(x, y) = 1 - \\frac{(x - \\bar{x}) \\cdot (y - \\bar{y})}{\\sqrt{(x - \\bar{x})^2} \\sqrt{(y - \\bar{y})^2}}\n\nIt is used for continuous data where the correlation between variables is of interest. A possible use is clustering patients based on correlated time-series data (e.g., progression of disease markers over time).\n\n\n22.2.2 Types of clustering methods\nHierarchical clustering\nHierarchical clustering is a method of cluster analysis that seeks to build a hierarchy of clusters by successively merging (agglomerative approach) or splitting (divisive approach) data points. In hierarchical clustering, clusters are organized into a tree-like structure called a dendrogram, which visually represents the data’s hierarchy of nested clusters. There are two approaches:\n\nAgglomerative clustering (bottom-up)\nEach observation starts as its own cluster, and clusters are merged iteratively based on their similarity.\nDivisive clustering (top-down):\nAll observations start in one cluster, and the cluster is split iteratively until each observation is in its own cluster.\n\nPartitioning clustering\nPartitioning clustering divides data points into a predefined number of clusters, where each data point belongs to exactly one cluster. The goal is to partition the dataset into clusters such that data points within a cluster are more similar to each other than to those in other clusters. Common methods include:\n\nK-means clustering\nThis method partitions the data into k clusters, where k is specified by the user. Each data point is assigned to the nearest cluster center.\nK-medoids clustering\nIti is similar to K-means but uses actual data points (medoids) as cluster centers, making it more robust to outliers.\nCLARA (clustering large applications)\nAn extension of K-medoids that works for large datasets by using samples of the data to reduce computation time.",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Cluster analysis</span>"
    ]
  },
  {
    "objectID": "cluster-analysis.html#hierarchical-clustering",
    "href": "cluster-analysis.html#hierarchical-clustering",
    "title": "22  Cluster analysis",
    "section": "22.3 Hierarchical clustering",
    "text": "22.3 Hierarchical clustering\nHierarchical clustering (also referred to as systematic clustering) is a method used to build a hierarchy of clusters, which can be visualized as a tree. The aim is to merge similar objects in a stepwise manner, creating nested clusters of increasing size.\n\n22.3.1 Linkage criterion\nThe method of merging clusters depends on the linkage criterion, which defines how the distance between clusters is calculated. The commonly used link criterions are as follows:\n\nSingle linkage: Distance between the closest points of two clusters (minimum distance).\nComplete linkage: Distance between the farthest points of two clusters (maximum distance).\nAverage linkage: Average distance between all points in one cluster to all points in another.\nCentroid linkage: Distance between the centroids (mean points) of two clusters.\nWard’s method: Commonly applied in agglomerative hierarchical clustering to minimize the total within-cluster variance.\n\n\n\n22.3.2 Key steps\n\nCalculate similarity matrix: A matrix of pairwise distances between all objects is computed based on the chosen distance metric (e.g., Euclidean distance).\nMerge clusters: At each iteration, the two clusters with the smallest distance are merged based on the selected linkage criterion (e.g., single linkage).\nRepeat: The merging process is repeated until all objects are in a single cluster or until the desired number of clusters is obtained.\nVisualize with a Dendrogram: The clustering process can be represented by a dendrogram, where the height of the branches represents the distance at which clusters are merged.\n\n\nExample 1 \nA institution collected the data of 7 indicators for medical and health services in 31 regions of China in 2007. Please use hierarchical cluster method to analyze both the 31 regions and the 7 indicators separately.\n\n\n  Download data \n\n\ndf &lt;- read_csv(\"datasets/ex22-01.csv\", show_col_types = F) |&gt; \n  column_to_rownames(\"Region\")\n\nSince the indicators might have different units, it’s a good practice to normalize them so that all variables contribute equally to the clustering process.\n\ndf_norm &lt;- scale(df)\n\nYou can perform hierarchical clustering using the hclust() function, along with dist() to compute similarities.\n\nhc_regions &lt;- df_norm |&gt; \n1  dist(method = \"euclidean\") |&gt;\n2  hclust(method = \"ward.D2\")\n\nhc_indicators &lt;- df_norm |&gt; \n  t() |&gt; \n3  dist(method = \"euclidean\") |&gt;\n4  hclust(method = \"ward.D2\")\n\n\n1\n\nCompute similarity matrix for regions\n\n2\n\nHierarchical clustering for regions\n\n3\n\nCompute similarity matrix for indicators (transpose data)\n\n4\n\nHierarchical clustering for indicators\n\n\n\n\nYou can visualize the hierarchical clustering using dendrograms to observe how the regions or indicators are clustered.\n\nhc_regions |&gt;   \n  as.dendrogram() |&gt; \n1  set(\"branches_k_color\") |&gt;\n  set(\"branches_lty\", c(1, 1, 3, 1, 1, 2)) |&gt; \n  set(\"labels_colors\", c(\"red\", \"orange\", \"black\", \"blue\", \"purple\")) |&gt; \n  set(\"labels_cex\", c(.75, .75)) |&gt; \n  plot(axes = F) \n\n2rect.hclust(hc_regions, k = 6)\n\nhc_indicators |&gt;   \n  as.dendrogram() |&gt; \n  set(\"branches_k_color\", k = 4) |&gt;             \n  set(\"branches_lty\", c(1, 2, 3, 4, 5)) |&gt; \n  set(\"labels_colors\", k = 4) |&gt; \n  set(\"labels_cex\", c(.75, .75)) |&gt; \n  plot(axes = F)\n\nrect.hclust(hc_indicators, k = 3)\n\n\n1\n\nThe set() function is from the dendextend package.\n\n2\n\nDraws rectangles around the branches of a dendrogram highlighting the corresponding clusters.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf you want to cut the dendrogram at a certain height to define clusters, use cutree().\n\ncutree(hc_indicators, k = 3)\n\n#&gt; X1 X2 X3 X4 X5 X6 X7 \n#&gt;  1  2  3  3  2  3  1",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Cluster analysis</span>"
    ]
  },
  {
    "objectID": "cluster-analysis.html#k-means-clustering",
    "href": "cluster-analysis.html#k-means-clustering",
    "title": "22  Cluster analysis",
    "section": "22.4 K-means clustering",
    "text": "22.4 K-means clustering\nK-means clustering aims to partition the dataset into k distinct, non-overlapping groups (clusters), where each data point belongs to the cluster with the nearest mean. The algorithm iteratively refines the positions of cluster centroids until convergence.\n\n22.4.1 Key steps\n\nChoose the number of clusters k.\nRandomly initialize k centroids.\nAssign each data point to the nearest centroid.\nRecalculate the centroids based on the assigned points.\nRepeat the steps until the centroids stabilize or the change in centroids is minimal.\n\nK-means clustering is efficient for large datasets when k is known, and can provide interpretable results with clear clusters. But the limitation is that you need to define the number of clusters k beforehand, which may not always be intuitive. This method is best for numeric data and may not work well with categorical or mixed-type data without modifications.\n\nExample 2 \nA team measured the scores of 5 cognitive and behavioral scales in 219 elderly individuals aged 65 and above. Use the K-means clustering method to classify the 219 elderly individuals.\n\n\n  Download data \n\n\ndf &lt;- read_csv(\"datasets/ex22-02.csv\", show_col_types = F) |&gt; \n  column_to_rownames(\"ID\")\n\nIt’s a good practice to standardize the data before clustering, especially if the variables have different units. You can use the scale() function to do this:\n\ndf_norm &lt;- scale(df)\n\nUse the kmeans() function to apply the K-means clustering algorithm. For example, you can set the number of clusters to 4:\n\nset.seed(200)  # Set seed for reproducibility\nkmeans_rlt &lt;- kmeans(\n  df_norm, centers = 4, nstart = 20, algorithm = \"MacQueen\", iter.max = 100)\n\nYou can visualize the clustering result using the fviz_cluster() function from the factoextra package:\n\nfviz_cluster(\n  kmeans_rlt, data = df_norm, geom = \"point\", pointsize = 1.2,\n  main = \"K-means clustering\", xlab = F, ylab = F, legend.title = \"\") + \n  theme(\n    plot.title = element_text(size = 10),\n    axis.text = element_text(size = 9),\n    legend.text = element_text(size = 9), \n    legend.key.size = unit(0.35, \"cm\")\n  )\n\n\n\n\n\n\n\n\nYou can use the elbow method to determine the optimal number of clusters. The fviz_nbclust() function helps visualize this:\n\nfviz_nbclust(df_norm, kmeans, method = \"silhouette\", k.max = 15) + \n  theme(\n    axis.title = element_text(size = 10), # Axis titles\n    axis.text = element_text(size = 9),  # Axis text\n    plot.title = element_text(size = 10)  # Plot title size\n  )\n\n\n\n\n\n\n\n\nThis will generate a plot showing the within-cluster sum of squares (WSS) for different numbers of clusters, with the “elbow point” often indicating the best choice.\nNbClust() is another useful function in NbClust package for determining the optimal number of clusters for a dataset. It provides multiple methods for cluster validation and automatically suggests the best number of clusters by using different criteria.\n\nset.seed(200)  \nnbclust &lt;- NbClust::NbClust(df_norm, distance = \"euclidean\", method = \"kmeans\")\n\n#&gt; *** : The Hubert index is a graphical method of determining the number of clusters.\n#&gt;                 In the plot of Hubert index, we seek a significant knee that corresponds to a \n#&gt;                 significant increase of the value of the measure i.e the significant peak in Hubert\n#&gt;                 index second differences plot. \n#&gt; \n\n\n#&gt; *** : The D index is a graphical method of determining the number of clusters. \n#&gt;                 In the plot of D index, we seek a significant knee (the significant peak in Dindex\n#&gt;                 second differences plot) that corresponds to a significant increase of the value of\n#&gt;                 the measure. \n#&gt;  \n#&gt; ******************************************************************* \n#&gt; * Among all indices:                                                \n#&gt; * 10 proposed 2 as the best number of clusters \n#&gt; * 5 proposed 3 as the best number of clusters \n#&gt; * 3 proposed 4 as the best number of clusters \n#&gt; * 1 proposed 5 as the best number of clusters \n#&gt; * 1 proposed 8 as the best number of clusters \n#&gt; * 1 proposed 11 as the best number of clusters \n#&gt; * 1 proposed 12 as the best number of clusters \n#&gt; * 2 proposed 15 as the best number of clusters \n#&gt; \n#&gt;                    ***** Conclusion *****                            \n#&gt;  \n#&gt; * According to the majority rule, the best number of clusters is  2 \n#&gt;  \n#&gt;  \n#&gt; *******************************************************************",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Cluster analysis</span>"
    ]
  },
  {
    "objectID": "cluster-analysis.html#mixed-type-data",
    "href": "cluster-analysis.html#mixed-type-data",
    "title": "22  Cluster analysis",
    "section": "22.5 Mixed-type data",
    "text": "22.5 Mixed-type data\nMixed-type data clustering refers to the clustering of datasets that contain both numerical (continuous) and categorical (discrete) variables. These datasets are common in real-world applications. Standard clustering methods like k-means are designed for numerical data, so when dealing with mixed-type data, special techniques are required. Here are common approaches and methods for clustering mixed-type data:\n\n22.5.1 K-prototypes clustering\nThe K-prototypes algorithm is a direct extension of the K-means algorithm, but it can handle mixed-type data by treating numerical and categorical data separately. For numerical data, it minimizes Euclidean distances, and for categorical data, it minimizes the number of mismatches.\nYou can use the kproto() function from the clustMixType package to perform this kind of clustering. Here’s an example:\n\n# Sample data with mixed types\nx &lt;- data.frame(\n  age = c(23, 45, 34, 28, 36, 52),\n  gender = as.factor(c(\"M\", \"F\", \"M\", \"F\", \"F\", \"M\")),\n  cholesterol = c(200, 220, 210, 190, 250, 240)\n)\n\nset.seed(123)  # Set seed for reproducibility\nlambda &lt;- lambdaest(x)\n\n#&gt; Numeric variances:\n#&gt;         age cholesterol \n#&gt;    114.6667    536.6667 \n#&gt; Average numeric variance: 325.6667 \n#&gt; \n#&gt; Heuristic for categorical variables: (method = 1) \n#&gt; gender \n#&gt;    0.5 \n#&gt; Average categorical variation: 0.5 \n#&gt; \n#&gt; Estimated lambda: 651.3333\n\nkp &lt;- kproto(x, k = 2, lambda = lambda)  \n\n#&gt; # NAs in variables:\n#&gt;         age      gender cholesterol \n#&gt;           0           0           0 \n#&gt; 0 observation(s) with NAs.\n\nsummary(kp)\n\n#&gt; age \n#&gt;   Min. 1st Qu. Median Mean 3rd Qu. Max.\n#&gt; 1   23   26.75     31 32.5   36.75   45\n#&gt; 2   36   40.00     44 44.0   48.00   52\n#&gt; \n#&gt; -----------------------------------------------------------------\n#&gt; gender \n#&gt;        \n#&gt; cluster   F   M\n#&gt;       1 0.5 0.5\n#&gt;       2 0.5 0.5\n#&gt; \n#&gt; -----------------------------------------------------------------\n#&gt; cholesterol \n#&gt;   Min. 1st Qu. Median Mean 3rd Qu. Max.\n#&gt; 1  190   197.5    205  205   212.5  220\n#&gt; 2  240   242.5    245  245   247.5  250\n#&gt; \n#&gt; -----------------------------------------------------------------\n\nclprofiles(kp, x)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis function combines K-means clustering for numerical data with K-modes clustering for categorical data. You can also use the silhouette method or other approaches to determine the optimal number of clusters.\n\n\n22.5.2 Gower’s distance + hierarchical clustering\nGower’s distance is a similarity measure that can handle mixed data types (numeric, ordinal, and categorical). After calculating the distance matrix, hierarchical clustering can be applied.\n\ndata(Byar)\nx &lt;- Byar |&gt; \n  rename(\n    Size.tumor = Size.of.primary.tumour,\n    Serum.pacp= Serum.prostatic.acid.phosphatase,\n    Ecg.code = Electrocardiogram.code\n  ) |&gt; \n  mutate(\n    Serum.pacp = if_else(Serum.pacp == 0, mean(Serum.pacp), Serum.pacp),\n    Size.tumor = sqrt(Size.tumor),                        \n    Serum.pacp = log(Serum.pacp)                           \n  ) |&gt;  \n  select(1, 2, 5, 6, 8, 9, 10, 11, 3, 4, 12, 7) |&gt;\n  mutate(\n    Ecg.code = case_when(                                  \n      Ecg.code == 0 | Ecg.code == 1 ~ 1,\n      Ecg.code == 2 | Ecg.code == 3 | Ecg.code == 4 ~ 2,\n      Ecg.code == 5 | Ecg.code == 6 ~ 3\n    )\n  ) \n\n\n# Compute Gower's distance for mixed data\ngower_dist &lt;- daisy(\n  x, metric = \"gower\", stand = T, \n  type = list(numeric = c(1:8), ordered = 9, symm = c(10,11), factor = 12)\n)\n\n# Perform hierarchical clustering\nhclust(gower_dist, method = \"ward.D2\") |&gt; \n  as.dendrogram() |&gt; \n  set(\"branches_k_color\", k = 60) |&gt; \n  set(\"branches_k_lty\", k = 2) |&gt; \n  plot(axes = F)\n\n\n\n\n\n\n\n\n\n\n22.5.3 Two-step clustering\nTwo-step clustering is a method designed to handle large datasets that contain both continuous and categorical variables. It is called “two-step” because the clustering process is divided into two main stages:\n\nPre-clustering step: In the first step, the algorithm pre-clusters the data to reduce the computational complexity. It uses a distance measure (typically log-likelihood for mixed data types) to determine preliminary clusters. This is usually done using a hierarchical clustering method.\nClustering step: The second step refines the initial clusters using a partitioning algorithm, such as K-means or another method suited for mixed data. This step ensures that the clusters are better separated.\n\nUnfortunately, there isn’t a direct two-step clustering function in R, here is a customized function named two_step_clustering() to approximate it.\n\ntwo_step_clustering &lt;- function(data, n_pre_clust = 5, n_final_clust = 3, dist_method = \"euclidean\", seed = 123) {\n  set.seed(seed)\n  \n  # STEP 1: Pre-Clustering using Hierarchical Clustering\n  # Compute the distance matrix (for mixed data, replace with Gower's distance)\n  distance_matrix &lt;- dist(data, method = dist_method)\n  \n  # Perform hierarchical clustering\n  hclust_result &lt;- hclust(distance_matrix, method = \"ward.D2\")\n  \n  # Create pre-clusters (cut the dendrogram into n_pre_clusters groups)\n  pre_clusters &lt;- cutree(hclust_result, k = n_pre_clust)\n  \n  # Add the pre-cluster information to the dataset\n  data_with_clusters &lt;- data\n  data_with_clusters$pre_cluster &lt;- as.factor(pre_clusters)\n  \n  # STEP 2: Final Clustering using K-means\n  # Now, apply k-means clustering on the pre-clustered data\n  kmeans_result &lt;- kmeans(data_with_clusters, centers = n_final_clust)\n  \n  # Add final cluster labels to the original data\n  data_with_clusters$final_cluster &lt;- as.factor(kmeans_result$cluster)\n  \n  # Visualization: Final clusters (using the first two principal components)\nfviz_plot &lt;- fviz_cluster(kmeans_result, data = data, geom = \"point\", show.clust.cent = T, main = \"Two-step clustering\", xlab = F, ylab = F, legend.title = \"\") +\n  theme(\n    plot.title = element_text(size = 10),\n    axis.text = element_text(size = 9),\n    legend.text = element_text(size = 9), \n    legend.key.size = unit(0.35, \"cm\")\n  )\n  \n data_with_clusters\n fviz_plot\n}\n\n# Example usage with the mtcars dataset\ndf &lt;- mtcars\ntwo_step_clustering(df)\n\n\n\n\n\n\n\n\n\n\n22.5.4 FAMD + clustering\nFactor analysis of mixed data (FAMD) is used for dimensionality reduction of mixed-type datasets. It reduces the mixed-type data to a smaller number of dimensions that represent both numeric and categorical information. After reducing the dimensions, standard clustering techniques like K-means or hierarchical clustering can then be applied.\n\ndata(geomorphology)\nx &lt;- geomorphology\n\nres &lt;- FAMD(x)\n\n#&gt; Warning: ggrepel: 43 unlabeled data points (too many overlaps). Consider\n#&gt; increasing max.overlaps\n\n# Plot FAMD result\nfviz_famd_ind(res, geom = \"point\", xlab = F, ylab = F) +\n  theme(\n    plot.title = element_text(size = 10),\n    axis.text = element_text(size = 9),\n    legend.text = element_text(size = 9), \n    legend.key.size = unit(0.35, \"cm\")\n  )\n# Perform k-means clustering on the FAMD results\nkmeans_res &lt;- kmeans(res$ind$coord, centers = 3)\nfviz_cluster(kmeans_res, res$ind$coord, geom = \"point\", , xlab = F, ylab = F) +\n  theme(\n    plot.title = element_text(size = 10),\n    axis.text = element_text(size = 9),\n    legend.text = element_text(size = 9), \n    legend.key.size = unit(0.35, \"cm\")\n  )\n\nclustMD package\n\nX &lt;- Byar |&gt; \n  rename(\n    Size.tumor = Size.of.primary.tumour,\n    Serum.pacp= Serum.prostatic.acid.phosphatase,\n    Ecg.code = Electrocardiogram.code\n  ) |&gt; \n  mutate(\n    Serum.pacp = if_else(Serum.pacp == 0, mean(Serum.pacp), Serum.pacp),\n1    Size.tumor = sqrt(Size.tumor),\n    Serum.pacp = log(Serum.pacp)\n  ) |&gt; \n2  select(1, 2, 5, 6, 8, 9, 10, 11, 3, 4, 12, 7) |&gt;\n  mutate(\n3    across(c(9:12), \\(x) x + 1),\n4    Ecg.code = case_when(\n      Ecg.code == 1 | Ecg.code == 2 ~ 1,\n      Ecg.code == 3 | Ecg.code == 4 | Ecg.code == 5 ~ 2,\n      Ecg.code == 6 | Ecg.code == 7 ~ 3\n    )\n  )  \n\n\n1\n\nTransformation skewed variables\n\n2\n\nOrder variables (Continuous, ordinal, nominal)\n\n3\n\nStart categorical variables at 1 rather than 0\n\n4\n\nMerge categories of EKG variable for efficiency\n\n\n\n\n\nclustMD(\n  X = as.matrix(X), \n  G = 2,  \n  CnsIndx = 8,  \n  OrdIndx = 11,  \n  Nnorms = 20000,  \n  MaxIter = 500, \n  model = \"BD\", \n  store.params = F, \n  scale = T,  \n  startCL = \"kmeans\",  \n  autoStop = T,  \n  ma.band = 30,  \n  stop.tol = 0.0001  \n) |&gt; plot()\n\n#&gt; \n  |                                                                         \n  |                                                                   |   0%\n  |                                                                         \n  |=======                                                            |  10%\n  |                                                                         \n  |===================================================================| 100%",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Cluster analysis</span>"
    ]
  },
  {
    "objectID": "discriminant-analysis.html",
    "href": "discriminant-analysis.html",
    "title": "23  Discriminant analysis",
    "section": "",
    "text": "23.1 Prerequisite\nDiscriminant analysis is widely used for classifying subjects into predefined groups based on observed variables. Its main goal is to build a model that can predict group membership (such as disease vs. no disease) based on a set of independent variables, which could be clinical measurements, lab results, or survey responses. The method assumes that different groups have distinct characteristics, and it helps identify which variables contribute most to the classification.\nlibrary(tidyverse)\nlibrary(class)",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Discriminant analysis</span>"
    ]
  },
  {
    "objectID": "discriminant-analysis.html#distance-discriminant-analysis",
    "href": "discriminant-analysis.html#distance-discriminant-analysis",
    "title": "23  Discriminant analysis",
    "section": "23.2 Distance discriminant analysis",
    "text": "23.2 Distance discriminant analysis\nDistance discriminant analysis is a method of discriminant analysis that classifies samples based on their distance from the center of each class. It is commonly used for classification problems where the goal is to differentiate between two or more groups. The method assumes that samples in each class are clustered around a central point, and the closer a sample is to a class center, the more likely it belongs to that class.\n\n23.2.1 Distance measure\nThe core of distance discriminant analysis is calculating the distance between each sample and the center of every class, assigning the sample to the class with the nearest center. Common distance measures include:\nEuclidean distance\nThis is a standard distance measure between a sample and class centers. The sample is assigned to the class with the smallest Euclidean distance.\nMahalanobis distance\nThis distance measure takes into account the variance and correlations within the data by using the covariance matrix, making it a more robust metric for classification than Euclidean distance.\nFor more details about distance measures, please refer to Section 22.2.1 .\n\n\n23.2.2 Discriminant function and criterion\nDiscriminant function\nFor a given observation \\mathbf{x} and group k with mean vector \\boldsymbol{\\mu}_k, the Mahalanobis distance (accounts for the covariance structure of the data) can be represented as:\n\nd_k(\\mathbf{x}) = (\\mathbf{x} - \\boldsymbol{\\mu}_k)^\\top \\mathbf{\\Sigma}_k^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_k)\n\nWhere \\mathbf{\\Sigma}_k is the covariance matrix of group k.\nWhen the covariance matrices are equal across groups, the Mahalanobis distance is given by:\n\nd_k(\\mathbf{x}) = (\\mathbf{x} - \\boldsymbol{\\mu}_k)^\\top \\mathbf{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu}_k)\n\nWhere \\mathbf{\\Sigma} is the common covariance matrix.\nSince the covariance matrices are assumed to be the same across all groups, estimate the pooled covariance matrix \\mathbf{\\Sigma} , which is a weighted average of the covariance matrices of the individual groups:\n\n\\mathbf{\\Sigma} = \\frac{1}{n - g} \\sum_{j=1}^{g} \\sum_{i \\in j} (\\mathbf{x}_i - \\boldsymbol{\\mu}_j)(\\mathbf{x}_i - \\boldsymbol{\\mu}_j)^\\top\n\nwhere n is the total number of observations, g is the number of groups, \\mathbf{x}_i is a observation i in group j, \\boldsymbol{\\mu}_j is the mean vector for group j.\nDiscriminant criterion\nThe observation \\mathbf{x} belongs to group k if:\n\nk = \\arg \\min_j \\{ d_j(\\mathbf{x}) \\}\n\nwhere d_j(\\mathbf{x}) is the distance between \\mathbf{x} and the centroid \\boldsymbol{\\mu}_j of group j.\nIn other words, the object is assigned to the group with the smallest discriminant function value, or in practical terms, the closest group centroid in terms of the chosen distance metric.\nFor instance, if there are two groups with centroids \\boldsymbol{\\mu}_1and \\boldsymbol{\\mu}_2, for a new observation \\mathbf{x}, if d_1(\\mathbf{x}) &lt; d_2(\\mathbf{x}), the observation is classified as belonging to group 1, otherwise it is classified as belonging to group 2.\n\nExample 1 \nIn order to predict whether a patient will develop an infection after surgery, 54 surgical patients were recruited and their extracorporeal circulation time (X1, in minutes), occlusion time (X2, in minutes), and drainage volume on the day of surgery (X3, in milliliters) were recorded. The surgical patients include 18 cases of infection and 36 cases without infection. Perform a discriminant analysis on this data.\n\n\n  Download data \n\n\ndf &lt;- read_csv(\"datasets/ex23-01.csv\", show_col_types = F)\n\nAt present there is no specialized function for this method. Thus, we write a pair of custom functions based on Mahalanobis distance to perform discriminant analysis. The accuracy of the dda() function is assessed on the training data, while the dda_cv() function uses the leave-one-out cross-validation.\n\ndda()dda_cv()\n\n\n\ndda &lt;- function(formula, data, cov.equal = TRUE) {\n  m &lt;- model.frame(formula, data)\n  group &lt;- model.response(m)  # Grouping variable\n  x_vars &lt;- m[-group]  # feature variable\n  \n  group &lt;- as.factor(group)\n  \n  # Split the data by group\n  group_name &lt;- levels(group)\n  group_data &lt;- split(x_vars, group)\n  \n  # Calculate the mean and covariance matrix for each group\n  group_means &lt;- lapply(group_data, colMeans)\n  group_covs &lt;- lapply(group_data, cov)\n  \n  # If covariance matrix is equal, the pooled covariance matrix is used\n  if (cov.equal) {\n    pooled_cov &lt;- Reduce(\n      \"+\", \n      lapply(\n        1:length(group_name), \n        \\(i) {\n          (nrow(group_data[[i]]) - 1) * group_covs[[i]]\n        }\n      )\n    ) / (nrow(data) - length(group_name))\n  }\n  \n  # The Mahalanobis distance from each sample to each group\n  mahalanobis_to_groups &lt;- sapply(1:nrow(x_vars), function(i) {\n    x_i &lt;- x_vars[i, , drop = FALSE]\n    sapply(1:length(group_name), function(j) {\n      mean_j &lt;- group_means[[j]]\n      cov_j &lt;- if (cov.equal) pooled_cov else group_covs[[j]]\n      mahalanobis(x_i, mean_j, cov_j)\n    })\n  })\n  \n  # Classify according to the group with the smallest distance\n  predicted &lt;- apply(mahalanobis_to_groups, 2, function(d) {\n    group_name[which.min(d)]\n  })\n  \n  # Generate confusion matrix and calculate accuracy\n  confusion_matrix &lt;- table(Predicted = predicted, Actual = group)\n  accuracy &lt;- mean(predicted == group)\n  \n  # Return the result\n  list(\n    Predicted = predicted,\n    Confusion_matrix = confusion_matrix,\n    Accuracy = accuracy\n  )\n}\n\n\n\n\ndda_cv &lt;- function(formula, data, cov.equal = TRUE) {\n  m &lt;- model.frame(formula, data)\n  group &lt;- model.response(m)  # Grouping variable\n  x_vars &lt;- m[-group]  # feature variable\n  \n  group &lt;- as.factor(group)\n  group_name &lt;- levels(group)\n  n &lt;- nrow(data)\n  \n  # Used to save the predicted results of each test\n  predicted &lt;- rep(NA, n)\n  \n  # Leave one out method\n  for (i in 1:n) {\n    # Training data: Sample i is removed\n    train_data &lt;- data[-i, ]\n    train_m &lt;- model.frame(formula, train_data)\n    train_group &lt;- model.response(train_m)\n    train_x_vars &lt;- train_m[-train_group]\n    \n    # Split the data by group\n    train_group_data &lt;- split(train_x_vars, train_group)\n    \n    # Calculate the mean and covariance matrix for each group\n    group_means &lt;- lapply(train_group_data, colMeans)\n    group_covs &lt;- lapply(train_group_data, cov)\n  \n    # If covariance matrices are equal, use weighted mean covariance matrix\n    if (cov.equal) {\n      pooled_cov &lt;- Reduce(\n        \"+\", \n        lapply(\n          1:length(group_name), \n          \\(j) {\n            (nrow(train_group_data[[j]]) - 1) * group_covs[[j]]\n          }\n        )\n      ) / (nrow(train_data) - length(group_name))\n    }\n    # Obtain the i th sample for testing\n    test_sample &lt;- x_vars[i, , drop = FALSE]\n  \n    # Calculate the Mahalanobis distance\n    distances &lt;- sapply(1:length(group_name), function(j) {\n      mean_j &lt;- group_means[[j]]\n      cov_j &lt;- if (cov.equal) pooled_cov else group_covs[[j]]\n      mahalanobis(test_sample, mean_j, cov_j)\n    })\n    \n    # Classify according to the group with the smallest distance\n    predicted[i] &lt;- group_name[which.min(distances)]\n }\n\n  # Generate confusion matrix and calculate accuracy\n  confusion_matrix &lt;- table(Predicted = predicted, Actual = group)\n  accuracy &lt;- mean(predicted == group)\n  \n  # Return the result\n  list(\n    Predicted = predicted,\n    Confusion_matrix = confusion_matrix,\n    Accuracy = accuracy\n  )\n}\n\n\n\n\n\ndda(group ~ x1 + x2 + x3, df, cov.equal = T)\n\n#&gt; $Predicted\n#&gt;  [1] \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"1\" \"1\" \"0\" \"0\" \"0\" \"0\" \"1\"\n#&gt; [19] \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\"\n#&gt; [37] \"1\" \"1\" \"1\" \"1\" \"1\" \"1\" \"1\" \"1\" \"1\" \"0\" \"1\" \"1\" \"1\" \"0\" \"1\" \"1\" \"1\" \"1\"\n#&gt; \n#&gt; $Confusion_matrix\n#&gt;          Actual\n#&gt; Predicted  0  1\n#&gt;         0 33  2\n#&gt;         1  3 16\n#&gt; \n#&gt; $Accuracy\n#&gt; [1] 0.9074074\n\ndda_cv(group ~ x1 + x2 + x3, df, cov.equal = T)\n\n#&gt; $Predicted\n#&gt;  [1] \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"1\" \"0\" \"0\" \"0\" \"0\" \"1\" \"1\" \"0\" \"0\" \"0\" \"0\" \"1\"\n#&gt; [19] \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\" \"0\"\n#&gt; [37] \"1\" \"1\" \"1\" \"1\" \"1\" \"1\" \"1\" \"1\" \"1\" \"0\" \"1\" \"1\" \"1\" \"0\" \"1\" \"1\" \"1\" \"1\"\n#&gt; \n#&gt; $Confusion_matrix\n#&gt;          Actual\n#&gt; Predicted  0  1\n#&gt;         0 32  2\n#&gt;         1  4 16\n#&gt; \n#&gt; $Accuracy\n#&gt; [1] 0.8888889",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Discriminant analysis</span>"
    ]
  },
  {
    "objectID": "discriminant-analysis.html#fisher-discriminant-analysis",
    "href": "discriminant-analysis.html#fisher-discriminant-analysis",
    "title": "23  Discriminant analysis",
    "section": "23.3 Fisher discriminant analysis",
    "text": "23.3 Fisher discriminant analysis\nFisher discriminant analysis is a classical statistical method used for classification and dimensionality reduction. The primary goal aims to find a linear decision boundary that maximizes the separation between classes by maximizing the between-class variance while minimizing the within-class variance. The goal is to project the data onto a lower-dimensional space where the classes are well-separated.\nSuppose we have two classes, \\text G_1 and \\text G_2, with corresponding mean vectors \\boldsymbol{\\mu}_1 and \\boldsymbol{\\mu}_2, and their covariance matrices \\boldsymbol {\\Sigma}_1 and \\boldsymbol {\\Sigma}_2. The Fisher discriminant function is expressed as:\n\nf(\\mathbf {x}) =  \\mathbf w^\\top \\mathbf x + \\mathbf b\n\nwhere \\mathbf x is a observation, \\mathbf w is the optimal projection direction found by Fisher discriminant analysis, \\mathbf b is a bias term, typically used to define the decision boundary.\nThe discriminant direction \\mathbf w is computed as:\n\n\\mathbf w = \\mathbf S_\\text{w}^{-1} (\\boldsymbol {\\mu}_1 - \\boldsymbol {\\mu}_2)\n\nwhere \\mathbf S_\\text{w} is the within-class scatter matrix, computed as \\mathbf S_\\text{w} = \\mathbf S_1 + \\mathbf S_2, with \\mathbf S_1 and \\mathbf S_2 being the within-class scatter matrix of the two classes, \\boldsymbol \\mu_1 - \\boldsymbol \\mu_2 is the difference between the mean vectors of the two classes.\nFor a new observation \\mathbf x, the Fisher discriminant function projects the observation onto the discriminant direction. If the projection value is greater than a certain threshold, the sample is classified into one class; otherwise, it is classified into the other class. The general classification rule is:\nf(\\mathbf {x}) = \\mathbf w^\\top \\mathbf x + \\mathbf b &gt; 0, \\quad \\mathbf {x} \\in \\text G_1\nf(\\mathbf {x}) = \\mathbf w^\\top \\mathbf x + \\mathbf b &lt; 0, \\quad \\mathbf x \\in \\text G_2\n\nExample 2 \nTry performing Fisher discriminant analysis on the data from Example 1.\n\n\ndf &lt;- read_csv(\"datasets/ex23-01.csv\", col_types = list(group = col_factor()))\n\nFisher discriminant analysis is performed using the lda() function from the MASS package:\n\nmodel &lt;- df |&gt; \n  mutate(across(contains(\"x\"), scale)) |&gt; \n  MASS::lda(group ~ x1 + x2 + x3, data = _, method = \"mle\")\ncf &lt;- coef(model)\n\nThe linear discriminant function is:\nZ= 1.723X_1 - 0.231X_2 - 0.184X_3\nTo assess the performance of discriminant model, you can use the confusionMatrix() from the caret package:\n\nmodel_cv &lt;- mutate(df, across(contains(\"x\"), scale)) |&gt; \n  MASS::lda(group ~ x1 + x2 + x3, data = _, method = \"mle\", CV = T) \n  \npred &lt;- pluck(model_cv, \"class\")\nrefer &lt;- pull(df, group)\n\ncaret::confusionMatrix(pred, refer)\n\n#&gt; Confusion Matrix and Statistics\n#&gt; \n#&gt;           Reference\n#&gt; Prediction  0  1\n#&gt;          0 35  5\n#&gt;          1  1 13\n#&gt;                                           \n#&gt;                Accuracy : 0.8889          \n#&gt;                  95% CI : (0.7737, 0.9581)\n#&gt;     No Information Rate : 0.6667          \n#&gt;     P-Value [Acc &gt; NIR] : 0.0001628       \n#&gt;                                           \n#&gt;                   Kappa : 0.7353          \n#&gt;                                           \n#&gt;  Mcnemar's Test P-Value : 0.2206714       \n#&gt;                                           \n#&gt;             Sensitivity : 0.9722          \n#&gt;             Specificity : 0.7222          \n#&gt;          Pos Pred Value : 0.8750          \n#&gt;          Neg Pred Value : 0.9286          \n#&gt;              Prevalence : 0.6667          \n#&gt;          Detection Rate : 0.6481          \n#&gt;    Detection Prevalence : 0.7407          \n#&gt;       Balanced Accuracy : 0.8472          \n#&gt;                                           \n#&gt;        'Positive' Class : 0               \n#&gt;",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Discriminant analysis</span>"
    ]
  },
  {
    "objectID": "discriminant-analysis.html#bayes-discriminant-analysis",
    "href": "discriminant-analysis.html#bayes-discriminant-analysis",
    "title": "23  Discriminant analysis",
    "section": "23.4 Bayes discriminant analysis",
    "text": "23.4 Bayes discriminant analysis\nBayes discriminant analysis is a classification method based on Bayesian decision theory, where the goal is to assign a given observation to the most probable class based on the posterior probabilities. This approach relies on Bayes theorem, which uses prior probabilities of classes, likelihoods (based on the distribution of data), and evidence from the data to make decisions.\n\n23.4.1 Bayes theorem\nThe fundamental principle behind Bayes discriminant analysis is Bayes theorem, which calculates the posterior probability of a class given some observed \\mathbf x.\n\nP(C_k | \\mathbf x) = \\frac{P(\\mathbf x | C_k) P(C_k)}{P(\\mathbf x)}\n\nwhere P(C_k | \\mathbf x) is the posterior probability of class C_k given the data X, P(\\mathbf x | C_k) is the likelihood of the data given class C_k, P(C_k) is the prior probability of class C_k (the probability of class C_k without any observation). P(\\mathbf x) is the evidence, a normalization factor that ensures the posterior probabilities sum to 1.\n\n\n23.4.2 Discriminant function\nIn Bayes discriminant analysis, we use a discriminant function for each class to decide which class a new observation belongs to. The discriminant function can be written as:\n\nG_k(\\mathbf x) = \\log P(C_k | \\mathbf x) = \\log P(\\mathbf x | C_k) + \\log P(C_k)\n\nThe observation \\mathbf x is assigned to the class with the largest discriminant function G_k(\\mathbf x) .\nTypes of Bayes Discriminant Analysis\n\n\n23.4.3 Gaussian discriminant analysis\nIn Gaussian Discriminant Analysis, it is assumed that the features for each class follow a multivariate normal (Gaussian) distribution. The class-conditional probability for class C_k is modeled as:\n\nP(\\mathbf{x} | C_k) = \\frac{1}{(2 \\pi)^{d/2} |\\boldsymbol {\\Sigma}_k|^{1/2}} \\exp \\left( -\\frac{1}{2} (\\mathbf{x} - \\boldsymbol \\mu_k)^T \\Sigma_k^{-1} (\\mathbf{x} - \\boldsymbol \\mu_k) \\right)\n\nWhere \\boldsymbol \\mu_k is the mean vector of class C_k, \\boldsymbol \\Sigma_k is the covariance matrix of class C_k, d is the dimensionality of the feature space.\nIf each class has the same covariance matrix \\boldsymbol \\Sigma_1 = \\boldsymbol \\Sigma_2 = \\dots = \\boldsymbol \\Sigma, the method reduces to linear discriminant analysis, and the decision boundary is linear.\nIf the covariance matrices are different for each class \\boldsymbol \\Sigma_1 \\neq \\boldsymbol \\Sigma_2, it results in quadratic discriminant analysis, and the decision boundary is quadratic.\n\n\n23.4.4 Naive Bayes classifier\nThe Naive Bayes classifier is a simplified version of Bayes discriminant analysis that assumes that the features are conditionally independent given the class. This assumption dramatically simplifies the computation of P(\\mathbf x | C_k) as a product of individual probabilities for each feature:\n\nP(\\mathbf x| C_k) = \\prod_{i=1}^d P(x_i | C_k)\n\nDespite the strong independence assumption, Naive Bayes often performs surprisingly well, particularly in high-dimensional spaces.\n\nExample 3 \nTo differentiate between vocal cord polyps and vocal cord nodules through a non-invasive voice test, a researcher randomly selected 30 patients diagnosed with vocal cord polyps and 30 patients diagnosed with vocal cord nodules. At the same time, 40 healthy individuals, matched in age, gender, and education level, were selected as a control group. The longest phonation time X1, fundamental frequency X2, minimum sound pressure level X3, and voice handicap index X4 were measured for each individual through voice tests and voice evaluation scales. Based on this data, can the researcher distinguish between individuals with normal vocal cords, vocal cord nodules, and vocal cord polyps? From the 100 subjects, 85 were randomly selected as the training sample and 15 as the testing sample.\n\n\n  Download data \n\n\ndf &lt;- read_csv(\"datasets/ex23-02.csv\", col_types = list(group = col_factor()))\n\ntrain &lt;- df |&gt; \n  filter(flag == \"1\") |&gt; \n  select(group, starts_with(\"x\"))\n\ntest &lt;- df |&gt; \n  filter(flag == \"0\")|&gt; \n  select(group, starts_with(\"x\"))\n\nYou can use the NaiveBayes() function from the klaR package to implement Naive Bayes classification:\n\nmodel &lt;- klaR::NaiveBayes(group ~ x1 + x2 + x3 + x4, data = train)\n\n\n(result &lt;- predict(model, test))\n\n#&gt; $class\n#&gt;  [1] 2 2 3 3 3 3 3 1 3 1 3 1 1 1 1\n#&gt; Levels: 1 2 3\n#&gt; \n#&gt; $posterior\n#&gt;                  1            2            3\n#&gt;  [1,] 3.342009e-47 0.8501752801 0.1498247199\n#&gt;  [2,] 4.638867e-07 0.7639742737 0.2360252624\n#&gt;  [3,] 3.060593e-42 0.4650734824 0.5349265176\n#&gt;  [4,] 2.900379e-30 0.2745014985 0.7254985015\n#&gt;  [5,] 7.845572e-19 0.4095866534 0.5904133466\n#&gt;  [6,] 4.507985e-09 0.3009847672 0.6990152283\n#&gt;  [7,] 6.113464e-12 0.1353446250 0.8646553750\n#&gt;  [8,] 9.992382e-01 0.0003526619 0.0004091246\n#&gt;  [9,] 3.924560e-21 0.2150790727 0.7849209273\n#&gt; [10,] 8.036732e-01 0.1715804667 0.0247463251\n#&gt; [11,] 3.437613e-85 0.2067930501 0.7932069499\n#&gt; [12,] 9.976201e-01 0.0010093590 0.0013705641\n#&gt; [13,] 9.994834e-01 0.0002807466 0.0002358710\n#&gt; [14,] 9.986672e-01 0.0005074507 0.0008253521\n#&gt; [15,] 9.992247e-01 0.0003056115 0.0004697038\n\n\n\npred &lt;- pluck(result, \"class\")\nrefer &lt;- pull(test, group)\nprint(paste0(\"Accuracy: \", mean(pred == refer) * 100, \"%\"))\n\n#&gt; [1] \"Accuracy: 80%\"",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Discriminant analysis</span>"
    ]
  },
  {
    "objectID": "pca-factor-analysis.html",
    "href": "pca-factor-analysis.html",
    "title": "24  Principal components and factor analysis",
    "section": "",
    "text": "24.1 Prerequisite\nPrincipal components analysis (PCA) and factor analysis are both statistical techniques used for dimensionality reduction and exploratory data analysis, but they differ in their objectives, assumptions, and interpretation.\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(gridExtra)\nlibrary(factoextra)\nlibrary(FactoMineR)\nlibrary(pls)\nlibrary(psych)",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Principal components and factor analysis</span>"
    ]
  },
  {
    "objectID": "pca-factor-analysis.html#principal-components-analysis",
    "href": "pca-factor-analysis.html#principal-components-analysis",
    "title": "24  Principal components and factor analysis",
    "section": "24.2 Principal components analysis",
    "text": "24.2 Principal components analysis\n\n24.2.1 Principal component\nPCA is a technique used to reduce the dimensionality of a dataset by transforming the original variables into a smaller set of new variables called principal components. These principal components are linear combination of the original variables in a dataset, derived in such a way that it captures the maximum amount of variance in the data with fewer components while reducing noise and redundancy.\nSuppose you have a dataset with p variables. The i-th principal component Z_i can be represented as:\n\nZ_i = w_{i1} X_1 + w_{i2} X_2 + \\cdots + w_{ip} X_p\n\nwhere w_{ij} are the weights (coefficients) corresponding to the original variables X_1, X_2, \\ldots, X_p. These weights are derived from the eigenvectors of the covariance matrix of the data.\nPrincipal components are new variables created from linear combinations of the original variables. Each principal component is a weighted sum of the original variables. Principal components are mutually orthogonal (uncorrelated) to each other. Each subsequent principal component captures variance that is not explained by the previous ones.\n\n\n24.2.2 Steps to calculate\n\nStandardize the data\n\nTo avoid biases due to different scales of the variables, it’s common to standardize each variable so that they have a mean of 0 and a standard deviation of 1. Given a data matrix \\mathbf X (with n rows representing samples and p columns representing variables), the standardized data can be computed as:\n\n\\mathbf X' = \\frac{\\mathbf X - \\mu_\\mathbf X}{\\sigma_\\mathbf X}\nwhere \\mu_\\mathbf X and \\sigma_\\mathbf X represent the mean and standard deviation of each variable in \\mathbf X.\n\nCompute the covariance matrix\n\nAfter standardizing, calculate the covariance matrix \\mathbf S:\n\n\\mathbf S = \\frac{1}{n-1} \\mathbf {X}'^\\top \\mathbf X'\n\nThe covariance matrix \\mathbf S is a p \\times p matrix, where p is the number of variables, and it describes the covariance relationships between variables.\n\nCompute eigenvalues and eigenvectors\n\nTo find the directions of the principal components, solve the eigenvalue problem of the covariance matrix. The eigenvalue equation is:\n\n\\mathbf S \\mathbf{v}_i = \\lambda_i \\mathbf{v}_i\nHere, \\mathbf{v}_i represents the i-th eigenvector (principal component), and \\lambda_i is the corresponding eigenvalue, which measures the variance along that eigenvector direction. The eigenvectors are ordered based on the descending order of their eigenvalues.\n\nSelect principal components\n\nThe eigenvectors corresponding to the largest eigenvalues are selected as the principal components. The number of components is often determined by examining the cumulative variance explained by the first k principal components:\n\n\\text{CVE} = \\frac{\\sum_{i=1}^k \\lambda_i}{\\sum_{i=1}^p \\lambda_i}\n\nTypically, the number of principal components is chosen to capture 85% to 90% of the total variance.\n\nCompute principal component scores\n\nThe principal component scores represent the original data projected onto the new principal component directions. They are calculated as:\n\nZ = \\mathbf X' \\mathbf{V}_k\n\nwhere \\mathbf{V}_k is the matrix of the first k principal component vectors.\n\n\n24.2.3 Geometric explanation\nSuppose we have a two-dimensional dataset where each data point has two features, \\text X_1 and \\text X_2. These data points can be represented on a 2D plane, with \\text X_1 and \\text X_2 ​as the coordinate axes. The covariance matrix describes the spread of the data points in different directions. By performing eigenvalue decomposition on the covariance matrix, we can find the main directions of the data. Eigenvectors represent the directions of the new principal components. Eigenvalues represent the amount of variance explained by each principal component.\nThe first principal component (Z_1) is the direction in which the data has the maximum variance. The second principal component (Z_2) is the direction orthogonal to Z_1and has the second-largest variance.\nTo better understand the geometric meaning of PCA, let’s use a simple 2D example and illustrate it with a graphical representation.\n\n\n\n\n\n\n\n\n\nIn the first plot, we visualize the original data points, which are scattered in a 2D plane. In the second plot, we add two lines, the red line represents the first principal component Z_1, and the blue line represents the second principal component Z_2. The red line captures the direction of maximum variance in the data. The blue line is orthogonal to the red line and the variance is small. Z_1 captures the direction in which the data has the maximum spread. This means that the data points are most dispersed along this line. Z_2 is orthogonal to Z_1, ensures that the new principal components are uncorrelated. If we project the data onto Z_1, we get a new one-dimensional representation that retains the maximum variance. By finding the main directions of the data, PCA simplifies the data structure while retaining as much information as possible. This geometric interpretation can help us better understand how PCA works and its applications in data analysis.\n\n\n24.2.4 Properties\n\nUncorrelated to each other\n\nPrincipal components are orthogonal to each other, meaning that they are uncorrelated. PCA involves finding new axes (principal components) in the data such that each axis maximizes the variance in the data. Each new axis (principal component) is orthogonal to the previous ones, ensuring no overlap in the information they capture.\nThe orthogonality of principal components mathematically guarantees that the components are uncorrelated. Orthogonal vectors in space imply zero covariance between them.\n\nEquality of the sum of variance\n\nThe sum of variances of the original variables is equal to the sum of variances of the principal components. This ensures that no information is lost when transforming the data into its principal components.\nIn PCA, explained variance is an important concept that helps us understand how much information (variance) each principal component captures from the original data. The contribution rate and cumulative contribution rate are used to assess how much of the total variance is explained by each principal component and how much is explained cumulatively as more components are considered.\nThe contribution rate of a principal component is the proportion of the total variance in the data that is explained by that component. It is calculated by dividing the variance explained by a principal component by the total variance:\n\n\\frac{\\lambda_i}{\\sum_{j=1}^p \\lambda_j}\n\nwhere \\lambda_i is the eigenvalue corresponding to the i-th principal component, which represents the amount of variance explained by that component, \\sum_{j=1}^p \\lambda_j is the sum of the eigenvalues for all components, representing the total variance in the data. A higher contribution rate means that the component captures a larger portion of the data’s total variance.\nThe cumulative contribution rate is the sum of the contribution rates of the first k principal components. It tells us how much of the total variance is explained by the first k components.\n\n\\sum_{i=1}^{k} \\frac{\\lambda_i}{\\sum_{j=1}^p \\lambda_j}\n\nThis value helps in determining how many principal components are necessary to explain a significant portion of the variance in the data. In practice, researchers often aim for a cumulative contribution rate of around 85% to 95%, meaning they retain enough components to capture most of the variability in the data without losing too much information.\n\nComponent loading\n\nIn PCA, the term component loading or loading indicates how much each original variable contributes to a particular principal component. These loadings help explain how the data is projected onto the new axes (the principal components).\nThe loadings are the elements of the eigenvectors corresponding to the covariance or correlation matrix of the data. If \\mathbf{V} is the matrix of eigenvectors, then the loadings for principal component Z_i are given by the corresponding eigenvector components v_{ij} (the weight of variable j in principal component i):\n\nZ_i = v_{i1}X_1 + v_{i2}X_2 + \\dots + v_{im}X_m\n\nHere, X_1, X_2, \\dots, X_m are the original variables, and v_{ij} is the loading of variable X_j on principal component Z_i.\nThe loadings represent the weight or contribution of each original variable to the corresponding principal component. They are often interpreted as follows:\n\nLarge loadings (positive or negative): The variable contributes significantly to that principal component.\nSmall loadings (close to zero): The variable contributes little to that principal component.\n\nA principal component is essentially a new variable formed as a weighted linear combination of the original variables, and the loadings tell you which original variables have the most influence in this combination.\nThe loadings are often organized into a loading matrix, where each column represents a principal component and each row represents a variable. If there are m variables and k components, the loading matrix \\mathbf{L} looks like this: \\mathbf{L} = \\begin{pmatrix}l_{11} & l_{12} & \\dots & l_{1k} \\\\l_{21} & l_{22} & \\dots & l_{2k} \\\\\\vdots & \\vdots & & \\vdots \\\\l_{m1} & l_{m2} & \\dots & l_{mk}\\end{pmatrix}\nWhere l_{ij} is the loading of variable X_j on principal component Z_i.\n\nExample 1 \nTo study the quality characteristics of deer antler, a researcher measured the content of nine amino acids—Aspartic acid (Asp), Glutamic acid (Glu), Serine (Ser), Arginine (Arg), Glycine (Gly), Threonine (Thr), Proline (Pro), Alanine (Ala), and Valine (Val)—in 39 batches of deer antler products. Perform a PCA to identify a few independent principal components.\n\n\n  Download data \n\n\ndf &lt;- read_csv(\"datasets/ex24-01.csv\", show_col_types = F)\n\nYou can use the prcomp() function to perform PCA. This function computes the principal components by singular value decomposition of the correlation matrix (if scale. = T) or covariance matrix (if scale. = F).\n\npca &lt;- prcomp(df, scale. = F)\npca |&gt; summary()\n\n#&gt; Importance of components:\n#&gt;                           PC1     PC2     PC3     PC4     PC5     PC6\n#&gt; Standard deviation     6.1803 1.41074 0.31616 0.16608 0.12823 0.11184\n#&gt; Proportion of Variance 0.9465 0.04932 0.00248 0.00068 0.00041 0.00031\n#&gt; Cumulative Proportion  0.9465 0.99586 0.99834 0.99902 0.99943 0.99974\n#&gt;                            PC7     PC8     PC9\n#&gt; Standard deviation     0.07733 0.06436 0.02299\n#&gt; Proportion of Variance 0.00015 0.00010 0.00001\n#&gt; Cumulative Proportion  0.99988 0.99999 1.00000\n\n\nThe summary shows the proportion of variance explained by each component. The first few components that explain the largest proportion of variance are typically selected for further analysis. Typically, you select components that explain a cumulative variance of 80–90%.\nYou can generate the scree plot using plot() on the PCA object. The scree plot is a graphical representation of eigenvalues of the principal components in descending order. It helps determine how many principal components should be retained by displaying the amount of variance explained by each component.\n\npar(cex.axis = 0.7)\npca |&gt; plot(\n  type = \"lines\", main = \"Scree Plot\", cex.main = 0.9, cex.lab = 0.8\n)\n\n\n\n\n\n\n\n\nThe loadings indicate how much each amino acid contributes to the principal components. You can access the loadings matrix through:\n\nloadings &lt;- pluck(pca, \"rotation\")\nloadings\n\n#&gt;           PC1         PC2         PC3         PC4         PC5         PC6\n#&gt; x1 0.11007422  0.42075325  0.34430923  0.10962164 -0.02669335 -0.35112144\n#&gt; x2 0.24231528  0.62060456 -0.65068046 -0.23859574  0.11872685 -0.17265168\n#&gt; x3 0.08006905  0.26779696  0.03658930 -0.17586521 -0.40168173  0.81244634\n#&gt; x4 0.23456949  0.19282539 -0.05182612  0.50694464 -0.33551710  0.03650842\n#&gt; x5 0.80500995 -0.33127809  0.02862815 -0.06593596 -0.31604758 -0.18825369\n#&gt; x6 0.01258536  0.29787343  0.26825382  0.56014479  0.01294990  0.02163293\n#&gt; x7 0.39261823 -0.06991218 -0.04156936  0.26599423  0.75635264  0.38304008\n#&gt; x8 0.25459880  0.07376985  0.42542938 -0.43818772  0.15477934  0.03430612\n#&gt; x9 0.02374975  0.34657811  0.44558222 -0.24952472  0.12325475 -0.04122099\n#&gt;            PC7         PC8          PC9\n#&gt; x1 -0.13284749  0.66992022  0.299740325\n#&gt; x2  0.13302184 -0.11899665  0.007786362\n#&gt; x3  0.09441518  0.24065131  0.036903121\n#&gt; x4 -0.64025177 -0.35115752  0.031275875\n#&gt; x5  0.23433292  0.09421837 -0.194761783\n#&gt; x6  0.66662631 -0.28288543  0.018445845\n#&gt; x7 -0.13507737  0.17477637 -0.030009709\n#&gt; x8 -0.05990310 -0.47058074  0.553688232\n#&gt; x9 -0.15710935 -0.12217923 -0.749674227\n\n\nYou can create biplot to visualize how the different batches of deer antler products are projected onto the principal components. This visualization shows both the variables and the observations in terms of their relationship to the principal components.\n\npca |&gt; fviz_pca_biplot(\n  geom = \"point\", pointshape = 1, col.var = \"contrib\", \n  gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n  pointsize = 1.7, ggtheme = theme_gray()) +\n  theme(text = element_text(size = 9)) \n\n\n\n\n\n\n\n\n\npca |&gt; fviz_pca_var(\n  geom = \"arrow\", col.var = \"contrib\", \n  gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n  ggtheme = theme_gray()) +\n  theme(text = element_text(size = 9)) \n\n\n\n\n\n\n\n\nYou can also plot a scatter plot of the first two principal components to see how the data clusters.\n\npca |&gt; fviz_pca_ind(\n  geom = \"point\", pointshape = 1, \n  ggtheme = theme_gray()) +\n  theme(text = element_text(size = 9)) \n\n\n\n\n\n\n\n\n\n# Create a dataframe of the PCA scores\nscores &lt;- pca |&gt; \n  pluck(\"x\") |&gt; \n  as_tibble()\n\n# Plot the first two principal components\nggplot(scores, aes(x = PC1, y = PC2)) +\n  geom_point(shape = 1) +\n  labs(title = \"PCA - Individuals\", x = \"PC1\", y = \"PC2\") +\n  theme(text = element_text(size = 9))\n\n\n\n\n\n\n\n\n\npca |&gt; fviz_contrib(choice = \"var\", axes = 7, ggtheme = theme_gray()) +\n  theme(text = element_text(size = 9))\n\npca |&gt; fviz_contrib(choice = \"ind\", axes = 7, ggtheme = theme_gray()) +\n  theme(text = element_text(size = 9))\n\npca |&gt; fviz_eig(geom = \"line\", linecolor = \"steelblue\", ggtheme = theme_gray()) +\n  theme(text = element_text(size = 9))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlso you can perform PCA using the PCA() function from FactoMineR package:\n\nres.pca &lt;- PCA(df, scale.unit = F, graph = F)\nres.pca |&gt; summary()\n\n#&gt; \n#&gt; Call:\n#&gt; PCA(X = df, scale.unit = F, graph = F) \n#&gt; \n#&gt; \n#&gt; Eigenvalues\n#&gt;                        Dim.1   Dim.2   Dim.3   Dim.4   Dim.5   Dim.6   Dim.7\n#&gt; Variance              37.217   1.939   0.097   0.027   0.016   0.012   0.006\n#&gt; % of var.             94.654   4.932   0.248   0.068   0.041   0.031   0.015\n#&gt; Cumulative % of var.  94.654  99.586  99.834  99.902  99.943  99.974  99.988\n#&gt;                        Dim.8   Dim.9\n#&gt; Variance               0.004   0.001\n#&gt; % of var.              0.010   0.001\n#&gt; Cumulative % of var.  99.999 100.000\n#&gt; \n#&gt; Individuals (the 10 first)\n#&gt;        Dist    Dim.1    ctr   cos2    Dim.2    ctr   cos2    Dim.3    ctr\n#&gt; 1  |  3.711 | -3.425  0.808  0.852 | -1.301  2.237  0.123 |  0.500  6.591\n#&gt; 2  |  0.813 | -0.471  0.015  0.336 |  0.013  0.000  0.000 |  0.491  6.353\n#&gt; 3  |  2.507 | -2.207  0.336  0.775 |  0.927  1.135  0.137 |  0.733 14.143\n#&gt; 4  |  3.177 | -3.136  0.678  0.974 |  0.268  0.095  0.007 | -0.024  0.015\n#&gt; 5  | 15.585 | 15.583 16.730  1.000 | -0.099  0.013  0.000 |  0.073  0.141\n#&gt; 6  |  2.314 | -2.142  0.316  0.857 | -0.817  0.882  0.125 |  0.293  2.267\n#&gt; 7  |  1.599 | -1.421  0.139  0.790 | -0.626  0.518  0.153 |  0.290  2.208\n#&gt; 8  |  3.054 | -2.629  0.476  0.741 | -1.544  3.151  0.255 | -0.118  0.368\n#&gt; 9  | 15.070 | 15.063 15.632  0.999 | -0.359  0.170  0.001 |  0.252  1.678\n#&gt; 10 |  5.252 | -3.953  1.077  0.566 | -3.438 15.630  0.429 | -0.312  2.558\n#&gt;      cos2  \n#&gt; 1   0.018 |\n#&gt; 2   0.365 |\n#&gt; 3   0.085 |\n#&gt; 4   0.000 |\n#&gt; 5   0.000 |\n#&gt; 6   0.016 |\n#&gt; 7   0.033 |\n#&gt; 8   0.001 |\n#&gt; 9   0.000 |\n#&gt; 10  0.004 |\n#&gt; \n#&gt; Variables\n#&gt;       Dim.1    ctr   cos2    Dim.2    ctr   cos2    Dim.3    ctr   cos2  \n#&gt; x1 |  0.672  1.212  0.557 |  0.586 17.703  0.424 |  0.107 11.855  0.014 |\n#&gt; x2 |  1.478  5.872  0.734 |  0.864 38.515  0.251 | -0.203 42.339  0.014 |\n#&gt; x3 |  0.488  0.641  0.613 |  0.373  7.172  0.357 |  0.011  0.134  0.000 |\n#&gt; x4 |  1.431  5.502  0.961 |  0.269  3.718  0.034 | -0.016  0.269  0.000 |\n#&gt; x5 |  4.911 64.804  0.991 | -0.461 10.975  0.009 |  0.009  0.082  0.000 |\n#&gt; x6 |  0.077  0.016  0.030 |  0.415  8.873  0.876 |  0.084  7.196  0.036 |\n#&gt; x7 |  2.395 15.415  0.996 | -0.097  0.489  0.002 | -0.013  0.173  0.000 |\n#&gt; x8 |  1.553  6.482  0.986 |  0.103  0.544  0.004 |  0.133 18.099  0.007 |\n#&gt; x9 |  0.145  0.056  0.076 |  0.483 12.012  0.845 |  0.139 19.854  0.070 |\n\n\nThe results include the explained variance, the coordinates of observations on principal components, and the contributions of variables to each component.\nThe fviz_pca_ind() function visualizes the distribution of individuals on the first two principal components.\n\nfviz_pca_ind(\n  res.pca, title = \"PCA - Individuals\", geom = \"point\", \n  ggtheme = theme_gray(), col.ind = \"cos2\", \n  gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"), \n  repel = T, pointshape = 1)  +   \n  theme(text = element_text(size = 9))\n\n\n\n\n\n\n\n\nThe fviz_pca_var() function visualizes the contribution of variables to the first two principal components.\n\nfviz_pca_var(\n  res.pca, title = \"PCA - Variables\", ggtheme = theme_gray(),\n  col.var = \"contrib\", \n  gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n  repel = T) +\n  theme(text = element_text(size = 9))\n\n\n\n\n\n\n\n\nThe biplot combines the information about both individuals and variables on the same plot.\n\nfviz_pca_biplot(\n  res.pca, geom.ind = \"point\", ggtheme = theme_grey(),\n  pointshape = 21, pointsize = 1.5, col.var = \"contrib\", \n  gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n  col.ind = \"cos2\", repel = TRUE) +\n  theme(text = element_text(size = 9))\n\n\n\n\n\n\n\n\n\n\n24.2.5 Principal component regression\nIn multivariate regression analysis, multicollinearity arises when independent variables are highly correlated. PCA addresses this issue by transforming the original variables into uncorrelated principal components, making the model more stable and improving regression results.\nPrincipal component regression combines PCA with multiple linear regression. It is particularly useful in situations where multicollinearity poses a problem in ordinary least squares regression.\nSteps in principal component regression\n\nPerform PCA: First, PCA is applied to the independent variables in the dataset. PCA transforms the original variables into new, uncorrelated principal components that are linear combinations of the original variables.\nSelect principal components: Not all principal components are used in principal component regression. Typically, only the first few principal components, which explain most of the variance in the data, are selected for the regression. This step helps in reducing dimensionality and eliminating noise.\nRun linear regression: After selecting the principal components, a linear regression model is fit using these components as the independent variables rather than the original variables. The dependent variable remains unchanged.\nPredict and evaluate: The regression model is used to make predictions, and its performance is evaluated using metrics such as R-squared, mean squared error (MSE), or cross-validation.\n\n\nExample 2 \nThe data of age, height, and weight for 50 infants and young children in a region were measured. Try to establish the regression relationship between age (month) , height (cm) and weight (jin).\n\n\n  Download data \n\n\ndf &lt;- read_csv(\"datasets/ex24-02.csv\", show_col_types = F)\n\nHere is a example of how to perform principal component regression using the pcr() function from the pls package.\n\n# Fit PCR model\nmodel &lt;- pcr(weight ~ ., data = df, scale = F, validation = \"LOO\")\nmodel |&gt; summary()\n\n#&gt; Data:    X dimension: 50 2 \n#&gt;  Y dimension: 50 1\n#&gt; Fit method: svdpc\n#&gt; Number of components considered: 2\n#&gt; \n#&gt; VALIDATION: RMSEP\n#&gt; Cross-validated using 50 leave-one-out segments.\n#&gt;        (Intercept)  1 comps  2 comps\n#&gt; CV           2.325    1.298    1.009\n#&gt; adjCV        2.325    1.297    1.008\n#&gt; \n#&gt; TRAINING: % variance explained\n#&gt;         1 comps  2 comps\n#&gt; X         94.09   100.00\n#&gt; weight    71.00    83.78\n\n\nAfter fitting the model, use cross-validation to determine the optimal number of components.\n\nvalidationplot(model, val.type = \"R2\")  \n\n\n\n\n\n\n\n\nYou can now use the selected number of principal components to make predictions and analyze the relationship between the dependent variable (weight) and the independent variables (age and height).\n\nres &lt;- predict(model, ncomp = 2, newdata = df)\n\nHere we write a function named pcr_extract_eq() that takes the PCR model and the number of principal components as inputs and outputs the regression equation directly.\n\npcr_extract_eq &lt;- function(model, ncomp = NULL, digits = 4) {\n  if (is.null(ncomp)) {\n    ncomp &lt;- model$ncomp\n  }\n  \n1  coef &lt;- coef(model, ncomp = ncomp, intercept = T)\n  Y_name &lt;- colnames(coef)\n  X_name &lt;- rownames(coef) |&gt; \n    gsub(\"^\", \"(\", x= _) |&gt; \n    gsub(\"$\", \")\", x= _) |&gt; \n    gsub(\"((Intercept))\", \" \", x = _, fixed = T)\n  \n  # Initialize the coefficients for the original variables\n  beta &lt;- rep(0, nrow(coef))\n  names(beta) &lt;- X_name\n  eq_parts &lt;- c(Y_name, paste0(\"=\"))\n  \n  # Loop through each principal component\n  for (i in 1:nrow(coef)) {\n    # Coefficient for the current principal component\n    beta[i] &lt;- coef[i]\n  }\n  \n  # Now build the equation using the coefficients\n  for (var_name in names(beta)) {\n    if (beta[var_name] &gt;= 0) {\n      eq_parts &lt;- c(eq_parts, paste0(\"+ \", round(beta[var_name], digits), \"\", var_name))\n    } else {\n      eq_parts &lt;- c(eq_parts, paste0(\"- \", round(abs(beta[var_name]), digits), \"\", var_name))\n    }\n  }\n  \n  # Print the final equation\n  equation &lt;- paste(eq_parts, collapse = \" \")\n  return(equation)\n}\n\n\n1\n\nGet the regression coefficients of the original variables\n\n\n\n\n\npcr_extract_eq(model, ncomp = 1)\n\n#&gt; [1] \"weight = - 3.7216  + 0.1097(age) + 0.163(height)\"\n\n\n\npcr_extract_eq(model, ncomp = 2)\n\n#&gt; [1] \"weight = - 14.2159  - 0.1661(age) + 0.3486(height)\"\n\n\nWhen all principal components are taken for regression, the regression equation is the same as that obtained by using the original independent variables.\n\nlm(weight ~ age + height, data = df) |&gt; \n  equatiomatic::extract_eq(use_coefs = T, coef_digits = 4)\n\n\n\\operatorname{\\widehat{weight}} = -14.2159 - 0.1661(\\operatorname{age}) + 0.3486(\\operatorname{height})\n\n\n\nIn a word, this principle components regression procedure helps to handle issues like multicollinearity and reduces the dimensionality of the predictors while retaining most of the information.",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Principal components and factor analysis</span>"
    ]
  },
  {
    "objectID": "pca-factor-analysis.html#factor-analysis",
    "href": "pca-factor-analysis.html#factor-analysis",
    "title": "24  Principal components and factor analysis",
    "section": "24.3 Factor analysis",
    "text": "24.3 Factor analysis\nFactor analysis (FA) is used to identify underlying relationships between observed variables. The goal is to model the data using fewer unobserved variables, called latent variables or factors, which explain the correlations or covariances among the observed variables. It’s widely used in fields such as psychology, social sciences, finance, and biology to reduce data dimensionality and uncover latent structures.\n\n24.3.1 Types of factor analysis\n\nExploratory factor analysis\n\nExploratory factor analysis (EFA) is used when you don’t have preconceived notions about the factor structure. It helps in identifying the number of factors and their relationships with the observed variables.\n\nConfirmatory factor analysis\n\nConfirmatory factor analysis (CFA) is used when you have a hypothesis or prior knowledge about the number of factors and their relationships. CFA tests how well the hypothesized model fits the data.\nThe factor analysis discussed in this chapter primarily refers to EFA, CFA will be discussed in details in Chapter 26 .\n\n\n24.3.2 Mathematical model\nIn factor analysis, each observed variable X_1, X_2, \\dots, X_p is represented as a linear combination of the latent factors F_1, F_2, \\dots, F_m and an error term:\n\nX_i = \\lambda_{i1}F_1 + \\lambda_{i2}F_2 + \\dots + \\lambda_{im}F_m + \\epsilon_i\n\nwhere X_i is the i-th observed variable, \\lambda_{ij} is the loading of variable i on factor j, F_j is the j-th factor, \\epsilon_i is the error term representing the variance in X_i not explained by the factors.\nMathematically, the factor analysis model can be written as a matrix form:\n\n\\mathbf X = \\boldsymbol \\mu + \\mathbf L \\cdot \\mathbf F + \\boldsymbol \\epsilon\n\nwhere \\mathbf X is the matrix of observed variables, \\boldsymbol \\mu is a vector of means for the observed variables (it is often assumed to be \\mathbf 0 if the data is centered). \\mathbf F is a vector of factors (unobserved variables), typically assumed to have a mean of \\mathbf 0 and a standard deviation of \\mathbf 1. \\mathbf L is a factor loading matrix, where each entry \\lambda_{ij} represents the contribution of the j-th factor to the i-th observed variable. This matrix explains the relationship between the factors and the observed variables. \\boldsymbol \\epsilon is a vector of unique factors or errors (residuals), assumed to be uncorrelated across variables and factors.\nThe model meets the following assumptions:\n\nThe observed variables are linear combinations of the factors plus error terms.\nThe unique factors (error terms) are uncorrelated with the common factors and with each other.\nThe factors are often assumed to be uncorrelated with each other (this assumption can be relaxed in some types of factor analysis, such as oblique rotations).\nBoth the factors and the observed variables are assumed to follow a multivariate normal distribution (although this assumption is not always strictly required).\n\nThe covariance structure of the observed variables can be decomposed into the contribution from the common factors and the specific variances:\n\n\\boldsymbol \\Sigma_\\mathbf X = \\mathbf L \\cdot \\mathbf{\\Psi} \\cdot \\mathbf L^\\top + \\mathbf{\\Theta}\n\nwhere \\boldsymbol \\Sigma_\\mathbf X is the covariance matrix of the observed variables, \\mathbf L \\cdot \\mathbf{\\Psi} \\cdot \\mathbf L^\\top represents the part of the covariance explained by the common factors, where \\mathbf{\\Psi} is the covariance matrix of the factors (often diagonal if factors are assumed to be uncorrelated), \\mathbf{\\Theta} is a diagonal matrix containing the specific variances (unique factor variances) of each observed variable.\n\n\n24.3.3 Factor loading matrix\nThe calculation of factor loading matrix is a key step in factor analysis, which reveals the relationship between observed variables and factors. Here’s the process:\n\nData standardization\n\nBefore performing factor analysis, the data is standardized to ensure each variable has a mean of 0 and a standard deviation of 1. This is crucial because factor analysis is based on the covariance or correlation matrix, and standardization allows for comparison across different units.\n\nConstruct correlation or covariance matrix\n\nFactor analysis begins with the construction of a correlation matrix (or covariance matrix), which measures the linear relationships between the variables. For p variables X_1, X_2, \\dots, X_p, the correlation matrix \\mathbf R is:\n\n\\mathbf R = \\begin{bmatrix}1 & r_{12} & \\cdots & r_{1p} \\\\r_{21} & 1 & \\cdots & r_{2p} \\\\\\vdots & \\vdots & \\ddots & \\vdots \\\\r_{p1} & r_{p2} & \\cdots & 1\\end{bmatrix}\n\n\nSelect the number of factors\n\nSeveral methods, such as Kaiser’s criterion (eigenvalues greater than 1), scree plot, or cumulative variance can help determine the number of factors to extract. Typically, factors with eigenvalues greater than 1 are retained, or factors that explain a substantial portion of the variance are chosen.\n\nEigenvalue and eigenvector decomposition\n\nNext, the correlation (or covariance) matrix undergoes eigenvalue decomposition. For a p \\times p correlation matrix \\mathbf R, p eigenvalues \\lambda_1, \\lambda_2, \\dots, \\lambda_p and their corresponding eigenvectors are computed. If m factors are chosen, the largest m eigenvalues and their corresponding eigenvectors are retained.\n\nFactor loading matrix calculation\n\nThe factor loading matrix \\mathbf L shows the correlation between each observed variable and the extracted factors. It is computed by combining the eigenvectors with the eigenvalues. If m factors are retained, the factor loading matrix will be a p \\times m matrix:\n\n\\mathbf L = \\begin{bmatrix}\\lambda_{11} & \\lambda_{12} & \\cdots & \\lambda_{1m} \\\\\\lambda_{21} & \\lambda_{22} & \\cdots & \\lambda_{2m} \\\\\\vdots & \\vdots & \\ddots & \\vdots \\\\\\lambda_{p1} & \\lambda_{p2} & \\cdots & \\lambda_{pm}\\end{bmatrix}\n\nwhere \\lambda_{ij} is the factor loading of variable i on factor j,\nThe loading values indicate how much of the variance in each observed variable is explained by the factor. The formula for calculating the loading matrix is:\n\n\\mathbf L = \\mathbf V \\cdot \\sqrt{\\boldsymbol \\Lambda}\n\nWhere \\mathbf V is the matrix of eigenvectors, and \\sqrt{\\boldsymbol \\Lambda} is the diagonal matrix with the square roots of the largest eigenvalues on the diagonal.\nThe factor loading matrix is derived by decomposing the correlation matrix, selecting eigenvalues, and calculating how much each factor explains the variance of observed variables. Factor rotation can be applied to make the interpretation of the matrix more straightforward.\n\n\n24.3.4 Factor rotation\nOnce the factors are extracted, they are often rotated to improve interpretability. There are two main types of factor rotation:\n\nOrthogonal rotation\n\nOrthogonal rotationassumes factors are uncorrelated. Varimax rotation is the most commonly used orthogonal rotation technique. The goal is to maximize the variance of squared loadings within factors, so each factor has a few large loadings and many near-zero loadings. This makes it easier to interpret which variables are most strongly associated with which factors. After varimax rotation, each factor typically has a clearer and more distinct set of variables loading on it.\nThere are other orthogonal rotation techniques, such as quartimax and equamax.\n\nOblique rotation\n\nOblique rotation allows factors to be correlated, meaning the factors are not constrained to be orthogonal. Oblique rotations are useful when there is reason to believe the underlying factors might be related to one another. The interpretation becomes more complex, but it can provide a more realistic solution in cases where the factors are expected to overlap.\nOblimin and promax are two common methods of oblique factor rotation. Unlike orthogonal rotations, these methods are suitable when the underlying factors are expected to be related. For example, in psychological tests, factors like “anxiety” and “depression” might correlate, so an oblique rotation could yield a better representation of the data.\nThe rotated factor loading matrix reveals the contribution of each variable to the factors. Loadings with absolute values greater than 0.4 or 0.5 are considered significant, indicating that the factor explains a substantial portion of that variable’s variance.\n\nExample 3 \nIn order to reasonably assess the quality of its medical work over different months, a hospital, collected data over three years on 9 indicators: outpatient visits, number of discharged patients, bed occupancy rate, bed turnover rate, average length of hospital stay, cure and improvement rate, mortality rate, diagnostic accuracy rate, and rescue success rate. The factor analysis is used to explore the comprehensive indicator system.\n\n\n  Download data \n\n\ndf &lt;- read_csv(\"datasets/ex24-03.csv\", show_col_types = F) \n\nDetermining the number of factors to retain in a factor analysis is a crucial step, and there are several methods commonly used to help decide how many factors are appropriate for your data:\nKaiser’s Criterion: This rule suggests that only factors with eigenvalues greater than 1 should be retained. The idea is that each factor should account for at least as much variance as a single observed variable.\n\n1cor(df) |&gt;\n  eigen() |&gt; \n  pluck(\"values\")\n\ncov(scale(df)) |&gt;\n  eigen() |&gt; \n  pluck(\"values\")\n\ncor(scale(df)) |&gt;\n  eigen() |&gt; \n  pluck(\"values\")\n\n\n1\n\nThe correlation matrix of the original data is the same as the covariance matrix of the standardized data. The correlation matrix and the covariance matrix of the standardized data are the same and exchangeable.\n\n\n\n\n#&gt;  [1] 3.46982505 2.11482958 1.45147819 0.88193039 0.68357362 0.54160851\n#&gt;  [7] 0.45380144 0.20567066 0.12673582 0.07054673\n#&gt;  [1] 3.46982505 2.11482958 1.45147819 0.88193039 0.68357362 0.54160851\n#&gt;  [7] 0.45380144 0.20567066 0.12673582 0.07054673\n#&gt;  [1] 3.46982505 2.11482958 1.45147819 0.88193039 0.68357362 0.54160851\n#&gt;  [7] 0.45380144 0.20567066 0.12673582 0.07054673\n\n\nParallel analysis: Parallel analysis compares the eigenvalues of the actual data to eigenvalues obtained from randomly generated data sets with the same number of variables and observations. The number of factors retained is equal to the number of eigenvalues that are greater than the eigenvalues from the random data.\n\nfa.parallel(cor(df), n.obs = 36, fm = \"ml\", fa = \"fa\") \n\n#&gt; Parallel analysis suggests that the number of factors =  3  and the number of components =  NA\n\n\n\n\n\n\n\n\n\nThe primary function used for factor analysis is fa() from the psych package. Additionally, the factanal() function from the base stats package can also be used for factor analysis.\n\nmodel &lt;- fa(df, nfactors = 4, rotate = \"varimax\", fm = \"ml\") \nmodel |&gt; pluck(\"loadings\")\n\n#&gt; \n#&gt; Loadings:\n#&gt;    ML2    ML4    ML3    ML1   \n#&gt; x0  0.971 -0.192              \n#&gt; x1 -0.269  0.222         0.934\n#&gt; x2  0.788  0.246  0.209  0.223\n#&gt; x3 -0.182  0.812              \n#&gt; x4  0.371  0.902         0.202\n#&gt; x5 -0.545 -0.312  0.271  0.275\n#&gt; x6                0.989       \n#&gt; x7 -0.239 -0.189 -0.416       \n#&gt; x8 -0.608                0.198\n#&gt; x9  0.687               -0.265\n#&gt; \n#&gt;                  ML2   ML4   ML3   ML1\n#&gt; SS loadings    3.006 1.761 1.288 1.169\n#&gt; Proportion Var 0.301 0.176 0.129 0.117\n#&gt; Cumulative Var 0.301 0.477 0.606 0.722\n\n\n\nmodel &lt;- factanal(df, factors = 4, rotation =\"varimax\")\nmodel |&gt; pluck(\"loadings\")\n\n#&gt; \n#&gt; Loadings:\n#&gt;    Factor1 Factor2 Factor3 Factor4\n#&gt; x0  0.971  -0.192                 \n#&gt; x1 -0.269   0.222           0.934 \n#&gt; x2  0.788   0.246   0.209   0.223 \n#&gt; x3 -0.182   0.812                 \n#&gt; x4  0.371   0.902           0.202 \n#&gt; x5 -0.545  -0.312   0.271   0.275 \n#&gt; x6                  0.989         \n#&gt; x7 -0.239  -0.189  -0.416         \n#&gt; x8 -0.608                   0.198 \n#&gt; x9  0.687                  -0.265 \n#&gt; \n#&gt;                Factor1 Factor2 Factor3 Factor4\n#&gt; SS loadings      3.006   1.761   1.288   1.169\n#&gt; Proportion Var   0.301   0.176   0.129   0.117\n#&gt; Cumulative Var   0.301   0.477   0.606   0.722\n\n\nThe output shows the results of a factor analysis with four factors. Here’s how to interpret the results:\n\nFactor1: Variable x0 has a high positive loading (0.971), meaning it is strongly related to Factor1. Variables x2 and x9 also have moderate positive loadings on Factor1 (0.788 and 0.687).\nFactor2: Variables x3 (0.812) and x4 (0.902) are strongly related to Factor2.\nFactor3: Variable x6 has a very high loading on Factor3 (0.989), suggesting it primarily defines this factor.\nFactor4: Variable x1 has a strong loading on Factor4 (0.934), suggesting it strongly contributes to this factor.\n\n\n\n24.3.5 Matters need consideration\n\nThe solution to a factor analysis is not unique. There are several reasons for this:\n\nRotation ambiguity: After extracting the initial factors, different rotations can be applied to simplify or clarify the structure of the factor loadings. The rotation affects how the factors are interpreted, but there are many possible rotations, leading to different sets of factor loadings and thus different interpretations.\nSign ambiguity: Factor analysis can produce factor loadings with positive or negative signs, and both can explain the same amount of variance. Swapping the sign of all loadings for a factor does not change the interpretation but can lead to different representations of the solution.\nFactor score indeterminacy: The factor scores (the values that represent how much each individual case scores on each factor) are indeterminate. Different methods (e.g., regression, Bartlett, Thurstone’s) can be used to estimate factor scores, and each method can yield slightly different scores. Factor analysis provides factor loadings, but the scores themselves can vary depending on the estimation method used.\nNumber of factors chosen: The number of factors to retain is subjective and based on criteria such as the Kaiser criterion (eigenvalues &gt; 1), scree plot, or parallel analysis. The choice of the number of factors can lead to different factor structures, so even when the same data is used, analysts might arrive at different conclusions.\n\nDistinctions between PCA and FA\n\nDifferent objectives: PCA is primarily used for dimensionality reduction. The goal is to simplify a dataset by extracting the most important principal components while retaining as much of the original information as possible. It focuses on combining the original variables into uncorrelated principal components that explain the maximum variance in the data. FA is primarily used to identify underlying structures or patterns. The goal is to uncover the latent relationships between variables and determine which variables can be grouped into the same factor. FA emphasizes the understanding of underlying constructs.\nMathematical Foundations: PCA is based on the eigenvalue decomposition of the covariance or correlation matrix, producing new variables (principal components) that are linear combinations of the original variables. FA is Based on a model assumption where observed variables are generated by factors and error terms. Estimation is usually done through least squares or maximum likelihood methods. Factor loadings indicate the relationships between variables and latent factors.\nApplication contexts: PCA is more suitable for data preprocessing and feature extraction, especially when the number of variables exceeds the number of observations. FA is more commonly applied in fields like psychology and sociology to identify the influence of latent variables.\n\nAppropriate sample size\nFor reliable factor analysis, it is generally recommended to have at least 100 to 200 cases, with a preferred observation-to-variable ratio of 10:1 or higher. However, specific requirements will vary depending on the complexity of the model, communalities of the variables, and the number of factors.",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>24</span>  <span class='chapter-title'>Principal components and factor analysis</span>"
    ]
  },
  {
    "objectID": "canonical-correlation.html",
    "href": "canonical-correlation.html",
    "title": "25  Canonical correlation analysis",
    "section": "",
    "text": "25.1 Prerequisite\nCanonical correlation analysis (CCA) is a multivariate statistical technique used to explore the relationship between two sets of variables. It examines the linear relationships between two datasets and finds pairs of canonical variables—linear combinations of the original variables—that have maximum correlation with each other.\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(CCP)",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Canonical correlation analysis</span>"
    ]
  },
  {
    "objectID": "canonical-correlation.html#key-concepts",
    "href": "canonical-correlation.html#key-concepts",
    "title": "25  Canonical correlation analysis",
    "section": "25.2 Key concepts",
    "text": "25.2 Key concepts\n\n25.2.1 Canonical variables\nLet \\mathbf X be a n \\times p matrix representing the first set of variables X_1, X_2, \\dots, X_p, and \\mathbf Y be a n \\times q matrix representing the second set of variables Y_1, Y_2, \\dots, Y_q. The canonical variables are linear combinations of the original variables in both sets. For example:\n\nU_1 = \\mathbf a_1 X_1 + \\mathbf a_2 X_2 + \\dots + \\mathbf a_p X_p\n\nV_1 = \\mathbf b_1 Y_1 + \\mathbf b_2 Y_2 + \\dots + \\mathbf b_q Y_q\nThe coefficients \\mathbf a_1, \\mathbf a_2, \\dots, \\mathbf a_p and \\mathbf b_1, \\mathbf b_2, \\dots, \\mathbf b_q are chosen to maximize the correlation between U_1 and V_1 , the first pair of canonical variables. The first pair of U_1 and V_1 explains the most variance, followed by subsequent pairs U_2 and V_2, and so on, which are orthogonal to the first pair.\n\n\n25.2.2 Canonical coefficients\nCanonical coefficients are the weights used to linearly combine the original variables \\mathbf X and \\mathbf Y into the canonical variables \\mathbf U = \\mathbf X \\cdot \\mathbf A and \\mathbf V = \\mathbf Y \\cdot \\mathbf B. They are obtained by solving an optimization problem that maximizes the correlation between \\mathbf U and \\mathbf V.\n\n\n25.2.3 Canonical loadings\nCanonical loadings (also known as structure coefficients) are the correlations between the original variables and their corresponding canonical variables. They show how strongly each original variable contributes to the canonical variables. In other words, canonical loadings help us understand which original variables are most strongly related to the canonical variates .\nEach set of variables (say, \\mathbf X and \\mathbf Y) has its own canonical loadings for each canonical variable. Canonical loadings reveal how much weight each original variable carries in the overall relationship between the two sets of variables.\nMathematically, the canonical loading of a variable X_i is \\text{cor}(X_i, U) , and for a variable Y_j, it is \\text{cor}(Y_j, V).\n\n\n25.2.4 Canonical correlation\nThe correlation between the canonical variables U_1 and V_1 is called the canonical correlation. CCA will provide several canonical correlations, corresponding to the number of canonical variable pairs extracted. The first canonical correlation represents the strongest relationship between the two sets of variables. The subsequent canonical correlations show relationships that are independent of the first and represent additional layers of shared variation between the two datasets.\n\n\n25.2.5 Canonical correlation coefficients\nCanonical correlation coefficient measure the correlation between a pair of canonical variables U_i and V_i. Each canonical correlation coefficient corresponds to a pair of canonical variables, with the first canonical correlation being the strongest, followed by the second, and so on. These coefficients are similar to regular correlation coefficients, but they represent the correlation between combinations of variables rather than individual variables.",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Canonical correlation analysis</span>"
    ]
  },
  {
    "objectID": "canonical-correlation.html#steps-of-cca",
    "href": "canonical-correlation.html#steps-of-cca",
    "title": "25  Canonical correlation analysis",
    "section": "25.3 Steps of CCA",
    "text": "25.3 Steps of CCA\nThe general steps for conducting CCA are as follows:\n\n25.3.1 Data Preparation\nFirst, identify the two sets of variables, for instance \\mathbf X and \\mathbf Y. The number of variables in each set can be different. Inspect the data for missing values or outliers and handle them appropriately. Standardization or normalization is recommended if the two sets of variables have different units or scales.\n\n\n25.3.2 Construct covariance matrices\nCompute the covariance matrices for the two sets of variables. CCA relies on the covariance matrices to determine the linear relationships between the two sets.\n\n\n25.3.3 Derive canonical variables\nThe goal of CCA is to find matrices \\mathbf A and \\mathbf B such that the correlation between \\mathbf U = \\mathbf X \\cdot \\mathbf A and \\mathbf V = \\mathbf Y \\cdot \\mathbf B is maximized. This involves solving the following maximization problem:\n\n\\max \\; \\text{corr}(\\mathbf U, \\mathbf V) = \\max \\; \\frac{\\text{cov}(\\mathbf U, \\mathbf V)}{\\sqrt{\\text{var}(\\mathbf U) \\cdot \\text{var}(\\mathbf V)}}\n\nwhere \\mathbf U and \\mathbf V are the canonical variables, i.e., linear combinations \\mathbf U = \\mathbf X \\cdot \\mathbf A and \\mathbf V = \\mathbf Y \\cdot \\mathbf B.\nAfter obtaining the canonical variables, compute the canonical correlation coefficient, which measures the correlation between the two canonical variables.\n\n\n25.3.4 Significance test\nPerform significance test using statistical methods such as the chi-square test or Wilks’ Lambda to evaluate whether the canonical correlation is significant and meaningful. Based on the significance level, determine how many pairs of canonical variables should be retained for further interpretation.\n\n\n25.3.5 Interpret canonical variables\nThe higher the canonical correlation coefficient, the stronger the linear relationship between the two sets of variables. Each pair of canonical variables should be interpreted in terms of their real-world meaning. Canonical loadings indicate how much each original variable contributes to the canonical variables. These loadings help understand which variables in the original sets drive the correlation.\n\n\n25.3.6 Report results\nSummarize the analysis, including the canonical correlation coefficients, significance tests, and canonical loadings, and discuss their relevance to the research.\n\n\n25.3.7 Visualization (optional)\nUse graphical representations, such as scatter plots of the canonical variables, to visually explore the relationships between the two sets of variables.\n\nExample 1 \nIn order to explore the relationship between mental health and well-being, a university conducted a survey on the mental health and subjective well-being of first-year students. Performed canonical correlation analysis on the data of 674 students, focusing on three mental health indicators (depression score, anxiety score, and stress score) and two subjective well-being indicators (life attitude score and emotional state score).\n\n\n  Download data \n\n\ndf &lt;- read_csv(\"datasets/ex25-01.csv\", show_col_types = F) \nX &lt;- select(df, 1:3) |&gt; scale()\nY &lt;- select(df, 4:5) |&gt; scale()\n\nThe cancor() function can be used to perform canonical correlation analysis. Here’s an example:\n\nres.cc &lt;- cancor(X, Y) \nres.cc\n\n#&gt; $cor\n#&gt; [1] 0.70874979 0.05777605\n#&gt; \n#&gt; $xcoef\n#&gt;                  [,1]         [,2]        [,3]\n#&gt; DEPRES  -0.0184807659  0.056669704 -0.03975040\n#&gt; ANIETY  -0.0007031538 -0.064838367 -0.04134194\n#&gt; PRESSUR -0.0213237972  0.002855532  0.07657176\n#&gt; \n#&gt; $ycoef\n#&gt;                [,1]        [,2]\n#&gt; ALTITUD 0.001226327 -0.04739979\n#&gt; EMOTE   0.037820164  0.02859859\n#&gt; \n#&gt; $xcenter\n#&gt;        DEPRES        ANIETY       PRESSUR \n#&gt;  7.560718e-17 -1.948656e-16  1.004801e-17 \n#&gt; \n#&gt; $ycenter\n#&gt;       ALTITUD         EMOTE \n#&gt; -3.750709e-16 -2.223740e-16\n\n# Extract canonical variables\nU &lt;- scale(X) %*% res.cc$xcoef  \nV &lt;- scale(Y) %*% res.cc$ycoef  \n\n\nCCA result\n\nThe first canonical correlation is 0.7087, meaning there is a moderately strong relationship between the first pair of canonical variables.\nThe second canonical correlation is 0.0578, indicating a very weak or almost negligible relationship between the second pair of canonical variables.\n\n\nThe p.asym() function from CCP packages can be used to test significance in canonical correlation analysis.\n\nres.test &lt;- pluck(res.cc, \"cor\") |&gt; \n  p.asym(nrow(X), ncol(X), ncol(Y), tstat = \"Wilks\")\n\n#&gt; Wilks' Lambda, using F-approximation (Rao's F):\n#&gt;               stat    approx df1  df2   p.value\n#&gt; 1 to 2:  0.4960125 93.634745   6 1338 0.0000000\n#&gt; 2 to 2:  0.9966619  1.121999   2  670 0.3262391\n\n\n\nSignificance test result\n\nThe first canonical correlation is significant, as indicated by the small Wilks’ Lambda and p-value less than 0.001.\nThe second canonical correlation is not significant, as shown by the large Wilks’ Lambda and p-value greater than 0.05. Therefore, only the first canonical variate pair shows a strong relationship between the two sets of variables.\n\n\nUse the plt.asym() function to plot asymptotic distributions used to test the statistical significance of canonical correlation coefficients.\n\npar(cex = 0.8)\nplt.asym(res.test, rhostart = 1)\nplt.asym(res.test, rhostart = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUse scatter plots of the canonical variables to visually explore the relationships between two sets of variables.\n\nU &lt;- scale(X) %*% res.cc$xcoef  # Canonical variables for X\nV &lt;- scale(Y) %*% res.cc$ycoef  # Canonical variables for Y\n\ndata &lt;- data.frame(U1 = U[, 1], U2 = U[, 2], V1 = V[, 1], V2 = V[, 2])\n\nggplot(data, aes(x = U1, y = V1)) +\n  geom_point(size = 1) +\n  labs(title = \"Scatter Plot of Canonical Variables (U1 vs V1)\",\n       x = \"U1\", y = \"V1\") +\n  theme(text = element_text(size = 8))\n\nggplot(data, aes(x = U2, y = V2)) +\n  geom_point(size = 1) +\n  labs(title = \"Scatter Plot of Canonical Variables (U2 vs V2)\",\n       x = \"U2\", y = \"V2\") +\n  theme(text = element_text(size = 8))",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Canonical correlation analysis</span>"
    ]
  },
  {
    "objectID": "structural-equation.html",
    "href": "structural-equation.html",
    "title": "26  Structural equation model",
    "section": "",
    "text": "26.1 Prerequisite\nStructural equation modeling (SEM) is a statistical technique that allows for the estimation of complex relationships between observed and latent variables. SEM combines the strengths of path analysis, confirmatory factor analysis (CFA), and regression analysis, allowing for the simultaneous handling of measurement errors, latent variables, and observed variables. It enables us to test complex causal relationships. The SEM introduced in this chapter is primarily used for CFA. CFA is widely used in fields like psychology, education, and social sciences to validate theoretical constructs and measurement instruments.\nlibrary(tidyverse)\nlibrary(lavaan)\nlibrary(semPlot)",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Structural equation model</span>"
    ]
  },
  {
    "objectID": "structural-equation.html#overview",
    "href": "structural-equation.html#overview",
    "title": "26  Structural equation model",
    "section": "26.2 Overview",
    "text": "26.2 Overview\n\n26.2.1 Confirmatory factor analysis\nConfirmatory factor analysis (CFA) is often used to test whether a hypothesized factor structure fits the observed data. Unlike exploratory factor analysis (EFA, see Section 24.3.1 ), which is used to discover the underlying structure of data without predefined assumptions, CFA is applied when researchers have specific theories or models about the relationships between observed variables and their underlying latent factors. It is frequently used in scale design for estimating parameters and assessing the model’s goodness-of-fit, Example 1 is an scale for stroke patient-reported outcome measure. Based on this example, the readers can better understand the concepts and principles of CFA and SEM.\n\nExample 1 \nA group utilized the stroke patient-reported outcome measure (hereafter referred to as Stroke-PROM) to survey 295 stroke patients in order to assess their treatment outcomes. The Stroke-PROM includes four dimensions with 20 items in the physiological domain, three dimensions with 14 items in the psychological domain, two dimensions with 7 items in the social domain, and one dimension with 5 items in the treatment domain. The structure of Stroke-PROM is shown in Table 26.1 .\n\n\n\n\n\nTable 26.1: Brief description of Stroke-PROM structure\n\n\n\n\n\n\n\n\n\n\n\n\n\nDomain\nDimension\nVariable\nNumber_of_Items\nItems_Included\n\n\n\n\nPhysiological domain\nSomatic symptoms\nSOS\n7\nPHD1 ~ PHD7\n\n\n\nCognitive ability\nCOG\n4\nPHD8 ~ PHD11\n\n\n\nVerbal communication\nVEC\n4\nPHD12 ~ PHD15\n\n\n\nSelf-help skill\nSHS\n5\nPHD16 ~ PHD20\n\n\nPsychological domain\nAnxiety\nANX\n5\nPSD1 ~ PSD5\n\n\n\nDepression\nDEP\n5\nPSD6 ~ PSD10\n\n\n\nAvoidance\nAVO\n4\nPSD11 ~ PSD14\n\n\nSocial domain\nSocial contact\nSOC\n3\nSOR1 ~ SOR3\n\n\n\nFamily support\nFAS\n4\nSOR4 ~ SOR7\n\n\nTreatment domain\nSatisfaction\nSAT\n5\nTHA1 ~ THA5\n\n\n\n\n\n\n\n\n\n\n26.2.2 Components of SEM\nThe structure of a SEM consists of several key components:\nObserved variables\nObserved variables, also known as manifest variables, these are the variables that are directly measured in a study. They are the actual data collected from participants or experiments. For example, in Table 26.1 , items or questions that participants respond to are observed variables because they are directly measured.\nLatent variables\nLatent variables are variables that are not directly measured but are inferred from observed variables. Latent variables represent underlying constructs or factors that are assumed to influence the observed variables. In Table 26.1 , examples of latent variables include psychological traits like anxiety, depression, or avoidance, which cannot be measured directly but can be inferred through a set of observed vairables.\nExogenous variables\nExogenous variables are the independent variables in the model. They are not explained by any other variables within the model and are used to explain the variation in the endogenous variables. Exogenous variables are typically assumed to be external to the model.\nEndogenous variables\nEndogenous variables are the dependent variables in the model that are explained by other variables. They are influenced by both exogenous variables and other endogenous variables. In SEM, endogenous variables can be either latent or observed variables.\nMeasurement model\nThe measurement model describes the relationships between latent variables and observed variables. It is typically estimated using CFA. For each latent variable \\boldsymbol \\eta and its corresponding observed variables \\mathbf y, the measurement model can be expressed as:\n\n\\mathbf y = \\boldsymbol \\Lambda_\\mathbf y \\boldsymbol \\eta ​+ \\boldsymbol \\epsilon\n\nwhere \\mathbf y is a vector of all observed variables, \\boldsymbol \\eta is a vector of all latent variables. \\boldsymbol \\Lambda_\\mathbf y ​is the factor loadings matrix, representing the strength of the relationship between each observed variable and its corresponding latent variable, and \\boldsymbol \\epsilon is the measurement error vector, representing the portion of each observed variable that is not explained by the latent variable.\nStructural model\nThe structural model describes the causal relationships among latent variables. It is similar to path analysis but can include more complex causal relationships, such as direct effects, indirect effects, and mediating effects. The structural model can be expressed as:\n\n\\boldsymbol \\eta = \\mathbf B \\boldsymbol \\eta + \\boldsymbol \\Gamma \\boldsymbol \\xi + \\boldsymbol \\zeta\n\nwhere \\boldsymbol \\eta is a vector of endogenous latent variables , \\boldsymbol \\xi is a vector of exogenous latent variables, \\mathbf B is the path coefficients matrix, representing the causal relationships among endogenous latent variables, \\boldsymbol \\Gamma is the effects matrix, representing the influence of exogenous latent variables on endogenous latent variables, \\boldsymbol \\zeta is the residual vector, representing the portion of the endogenous latent variables that is not explained by the model.\nCombining the measurement model and the structural model, we get the complete SEM model. Suppose we have p observed variables and q latent variables, where m are endogenous latent variables and n are exogenous latent variables. The combined model can be expressed as:\n\n\\left\\{\n\\begin{array}{l}\n\\mathbf y = \\boldsymbol \\Lambda_y \\boldsymbol \\eta + \\boldsymbol \\epsilon \\\\\n\\mathbf x = \\boldsymbol \\Lambda_x \\boldsymbol \\xi + \\boldsymbol \\delta \\\\\n\\boldsymbol \\eta = \\mathbf B \\boldsymbol \\eta + \\boldsymbol \\Gamma \\boldsymbol \\xi + \\boldsymbol \\zeta\n\\end{array}\n\\right.\n​\nwhere \\mathbf y is a vector of endogenous observed variables, \\mathbf x is a vector of exogenous observed variables, \\boldsymbol \\Lambda_\\mathbf y is the factor loadings matrix for endogenous observed variables, \\boldsymbol \\Lambda_\\mathbf x is the factor loadings matrix for exogenous observed variables, \\boldsymbol \\epsilon is the measurement error vector for endogenous observed variables, \\boldsymbol \\delta is the measurement error vector for exogenous observed variables, \\boldsymbol \\eta is a vector of endogenous latent variables, \\boldsymbol \\xi is a vector of exogenous latent variables, \\mathbf B is the path coefficients matrix for endogenous latent variables, \\boldsymbol \\Gamma is the effects matrix for the influence of exogenous latent variables on endogenous latent variables, \\boldsymbol \\zeta is the residual vector for endogenous latent variables.\n\n  Download data \n\n\ndf &lt;- read_csv(\"datasets/ex26-01.csv\", show_col_types = F) \n\n\nm_model &lt;- '\n  SOS =~ PHD1 + PHD2 + PHD3 + PHD4 + PHD5 + PHD6 + PHD7\n  ANX =~ PSD1 + PSD2 + PSD3 + PSD4 + PSD5\n'\ns_model &lt;- '\n  ANX ~ SOS\n'\n\nfull_model &lt;- c(m_model, s_model)\n\nres.cfa &lt;- cfa(m_model, df)\nres.sem &lt;- sem(full_model, df)\n\nsemPaths(\n  res.sem, what = \"std\", residuals = T, rotation = 2, edge.label.cex = 0.8,\n  fade = F, layout = \"tree\", intercepts = F)\n\n\n\n\n\n\n\n\n\nm_model &lt;- '\n  SOS =~ PHD1 + PHD2 + PHD3 + PHD4 + PHD5 + PHD6 + PHD7\n  COG =~ PHD8 + PHD9 + PHD10 + PHD11\n  VEC =~ PHD12 + PHD13 + PHD14 + PHD15\n  SHS =~ PHD16 + PHD17 + PHD18 + PHD19 + PHD20\n  ANX =~ PSD1 + PSD2 + PSD3 + PSD4 + PSD5\n  DEP =~ PSD6 + PSD7 + PSD8 + PSD9 + PSD10\n  AVO =~ PSD11 + PSD12 + PSD13 + PSD14\n'\ns_model &lt;- '\n  ANX ~ SOS\n'\n\nfull_model &lt;- c(m_model, s_model)\n\nres.cfa &lt;- cfa(m_model, df)\nres.sem &lt;- sem(full_model, df)\n\nsemPaths(res.cfa, \"std\", edge.label.cex = 0.8, fade = F)\nsemPaths(res.sem, what = \"std\", residuals = T, rotation = 2, edge.label.cex = 0.8,\n  fade = F, layout = \"tree\", intercepts = F)",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Structural equation model</span>"
    ]
  },
  {
    "objectID": "structural-equation.html#evaluation-of-construct-validity",
    "href": "structural-equation.html#evaluation-of-construct-validity",
    "title": "26  Structural equation model",
    "section": "26.3 Evaluation of construct validity",
    "text": "26.3 Evaluation of construct validity\nCFA is essentially a measurement model within SEM and is often used to assess the construct validity of a measure or scale, ensuring its accuracy in measuring the intended constructs.\nThe key steps involved in CFA are as follows:\n\nModel specification: Based on existing theory or hypotheses, the relationships between latent variables and observed variables are defined.\nParameter estimation: Statistical methods (such as maximum likelihood estimation) are used to estimate the parameters of the model, determining the associations between latent and observed variables.\nModel evaluation: Various fit indices (e.g., Chi-square test, CFI, TLI, RMSEA) are used to evaluate whether the model fits the data well.\nModel modification: Based on the evaluation results, adjustments may be made to improve the model’s fit.\nInterpretation of results: The path coefficients and relationships between latent variables are analyzed to interpret the measurement results.\n\n\nExample 2 \nUse CFA to evaluate the construct validity of the Stroke-PROM scale in the physiological domain, i.e. SOS, COG, VEC, and SHS. This domain consists of four dimensions, corresponding to four latent factors. R software was used for modeling, the maximum likelihood estimation method was used to assess the construct validity of the different dimensions of the scale.\n\n\n  Download data \n\n\ndf &lt;- read_csv(\"datasets/ex26-02.csv\", show_col_types = F) \n\nThe Stroke-PROM scale in the physiological domain includes four latent variables, i.e. SOS, COG, VEC, and SHS. Each latent variable is measured by multiple observed variables (i.e., the individual items or questions in the scale).\nUsing the lavaan package, you can define and evaluate the model. First, define the relationships between the latent variables and their observed variables. Then use the cfa() function to fit the model use your data.\n\nmodel &lt;- '\n  SOS =~ PHD1 + PHD2 + PHD3 + PHD4 + PHD5 + PHD6 + PHD7\n  COG =~ PHD8 + PHD9 + PHD10 + PHD11\n  VEC =~ PHD12 + PHD13 + PHD14 + PHD15\n  SHS =~ PHD16 + PHD17 + PHD18 + PHD19 + PHD20\n'\nfit.cfa &lt;- cfa(model, data = df)\n\nAssess the goodness-of-fit using indices such as:\n\nChi-square test: A significant value suggests a misfit, but it’s sensitive to large sample sizes.\nCFI (comparative fit index): Values &gt; 0.90 or 0.95 indicate a good fit.\nTLI (Tucker-Lewis index): Values &gt; 0.90 or 0.95 are desirable.\nRMSEA (root mean square error of approximation): Values &lt; 0.08 indicate acceptable fit; &lt; 0.05 indicates a good fit.\nSRMR (standardized root mean square residual): Values &lt; 0.08 are considered acceptable.\n\n\nfit.cfa |&gt; fitMeasures(c(\"chisq\", \"cfi\", \"tli\", \"rmsea\", \"srmr\"))\n\n#&gt;   chisq     cfi     tli   rmsea    srmr \n#&gt; 630.894   0.867   0.846   0.098   0.082\n\n\nCheck if each observed variable loads significantly on its corresponding latent factor.\n\n1fit.cfa |&gt; summary(standardized = T, rsquare = T)\n2fit.cfa |&gt; standardizedSolution(output = \"text\")\n\n\n1\n\nNote that SEs and tests are still based on unstandardized estimates.\n\n2\n\nUse standardizedSolution() to obtain SEs and test statistics for standardized estimates.\n\n\n\n\nVisualize the relationships between latent and observed variables using semPaths() function from semPlot package:\n\n\n\n\n\n\n\n\n\nIf the model fit is not adequate, consider modifying the model based on the modification indices, which suggest potential areas for improvement:\n\nfit.cfa |&gt; modindices(sort. = T, power = T, maximum.number = 10)\n\n#&gt;       lhs op   rhs     mi    epc sepc.all delta   ncp power decision\n#&gt; 280 PHD14 ~~ PHD15 84.094  0.428    0.612   0.1 4.583 0.572  **(m)**\n#&gt; 291 PHD16 ~~ PHD17 76.363  0.402    0.600   0.1 4.720 0.584  **(m)**\n#&gt; 265 PHD12 ~~ PHD13 52.654  0.472    0.559   0.1 2.368 0.337  **(m)**\n#&gt; 181  PHD5 ~~  PHD6 51.566  0.305    0.740   0.1 5.547 0.654  **(m)**\n#&gt; 91    VEC =~ PHD16 33.919  0.480    0.343   0.1 1.471 0.228  **(m)**\n#&gt; 111  PHD1 ~~  PHD2 29.218  0.478    0.328   0.1 1.277 0.204  **(m)**\n#&gt; 266 PHD12 ~~ PHD14 22.832 -0.261   -0.330   0.1 3.350 0.448  **(m)**\n#&gt; 90    VEC =~ PHD11 22.010  0.602    0.479   0.1 0.607 0.122  **(m)**\n#&gt; 286 PHD15 ~~ PHD16 21.051  0.231    0.291   0.1 3.937 0.510  **(m)**\n#&gt; 75    COG =~ PHD16 20.641  0.314    0.209   0.1 2.091 0.304  **(m)**\n\n\nBy conducting CFA on the Stroke-PROM scale, you can determine whether the physiological domain’s structure, consisting of SOS, COG, VEC, and SHS, demonstrates construct validity.",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Structural equation model</span>"
    ]
  },
  {
    "objectID": "structural-equation.html#sem-analysis",
    "href": "structural-equation.html#sem-analysis",
    "title": "26  Structural equation model",
    "section": "26.4 SEM analysis",
    "text": "26.4 SEM analysis\n\n26.4.1 Simple SEM\n\nExample 3 \nConstruct a structural equation model using the somatic symptoms (SOS) from the physiological domain and anxiety (ANX) from the psychological domain of the Stroke-PROM scale. The model includes only one exogenous latent variable and one endogenous latent variable.\n\n\ndf &lt;- read_csv(\"datasets/ex26-03.csv\", show_col_types = F) \n\n\nmodel &lt;- '\n  # Measurement model\n  SOS =~ PHD1 + PHD2 + PHD3 + PHD4 + PHD5 + PHD6 + PHD7\n  ANX =~ PSD1 + PSD2 + PSD3 + PSD4 + PSD5\n  # Structural relationships\n  ANX ~ SOS\n'\nfit.sem &lt;- sem(model, df)\n\n\nfit.sem |&gt; summary(standardized = T, rsquare = T) \nfit.sem |&gt; standardizedSolution(output = \"text\") \n\n\nres.sem |&gt; fitMeasures(c(\"chisq\", \"rmsea\", \"aic\", \"gfi\", \"cfi\"))\n\n#&gt;     chisq     rmsea       aic       gfi       cfi \n#&gt;  1472.818     0.080 27602.396     0.766     0.856\n\n\n\nfit.sem |&gt; semPaths(\n  what = \"std\", style = \"lisrel\", residuals = T, edge.label.cex = 0.6, \n  fade = F, layout = \"tree\", intercepts = T, rotation = 1,\n  sizeMan = 3, sizeLat = 8, sizeLat2 = 6, residScale = 6\n  ) \n\n\n\n\n\n\n\n\n\nfit.sem |&gt; modindices(sort. = T, power = T, maximum.number = 10)\n\n#&gt;      lhs op  rhs     mi    epc sepc.all delta   ncp power decision\n#&gt; 78  PHD5 ~~ PHD6 60.918  0.346    0.823   0.1 5.090 0.616  **(m)**\n#&gt; 40  PHD1 ~~ PHD2 28.090  0.467    0.323   0.1 1.289 0.206  **(m)**\n#&gt; 96  PSD1 ~~ PSD2 23.615  0.250    0.338   0.1 3.776 0.493  **(m)**\n#&gt; 105 PSD4 ~~ PSD5 17.261  0.199    0.362   0.1 4.366 0.552  **(m)**\n#&gt; 43  PHD1 ~~ PHD5 13.183 -0.223   -0.261   0.1 2.652 0.370  **(m)**\n#&gt; 62  PHD3 ~~ PHD5 11.768 -0.227   -0.246   0.1 2.287 0.327  **(m)**\n#&gt; 79  PHD5 ~~ PHD7 11.438 -0.177   -0.252   0.1 3.632 0.478  **(m)**\n#&gt; 101 PSD2 ~~ PSD4 10.212 -0.163   -0.222   0.1 3.827 0.499  **(m)**\n#&gt; 54  PHD2 ~~ PHD6  9.938 -0.162   -0.228   0.1 3.795 0.495  **(m)**\n#&gt; 38   ANX =~ PHD6  9.019 -0.204   -0.170   0.1 2.160 0.312  **(m)**\n\n\n\n\n26.4.2 Complete SEM\n\nExample 4 \nConstruct a structural equation model using the somatic symptoms (SOS) from the physiological domain and anxiety (ANX) from the psychological domain of the Stroke-PROM scale. The model includes only one exogenous latent variable and one endogenous latent variable.\n\n\ndf &lt;- read_csv(\"datasets/ex26-04.csv\", show_col_types = F) \n\n\nmodel &lt;- '\n  # Measurement model\n  SOS =~ PHD1 + PHD2 + PHD3 + PHD4 + PHD5 + PHD6 + PHD7\n  COG =~ PHD8 + PHD9 + PHD10 + PHD11\n  ANX =~ PSD1 + PSD2 + PSD3 + PSD4 + PSD5\n  DEP =~ PSD6 + PSD7 + PSD8 + PSD9 + PSD10\n  AVO =~ PSD11 + PSD12 + PSD13 + PSD14\n  # Structural relationships\n  ANX ~ SOS\n  DEP ~ COG\n  DEP ~ AVO\n  DEP ~ ANX\n  AVO ~ COG\n  AVO ~ ANX\n'\nres.sem &lt;- sem(model, df)\nres.sem\n\n#&gt; lavaan 0.6-19 ended normally after 41 iterations\n#&gt; \n#&gt;   Estimator                                         ML\n#&gt;   Optimization method                           NLMINB\n#&gt;   Number of model parameters                        57\n#&gt; \n#&gt;   Number of observations                           295\n#&gt; \n#&gt; Model Test User Model:\n#&gt;                                                       \n#&gt;   Test statistic                               747.369\n#&gt;   Degrees of freedom                               268\n#&gt;   P-value (Chi-square)                           0.000\n\n\n\nres.sem |&gt; fitMeasures(c(\"chisq\", \"rmsea\", \"aic\", \"gfi\", \"cfi\"))\n\n#&gt;     chisq     rmsea       aic       gfi       cfi \n#&gt;   747.369     0.078 20129.701     0.828     0.891\n\n\n\nres.sem |&gt; semPaths(\n  what = \"std\", style = \"lisrel\", residuals = T, edge.label.cex = 0.6, \n  fade = F, layout = \"tree\", intercepts = T, rotation = 1,\n  sizeMan = 3, sizeLat = 8, sizeLat2 = 6, residScale = 6\n  ) \n\n\n\n\n\n\n\n\n\nres.sem |&gt; modindices(sort. = T, power = T, maximum.number = 10)\n\n#&gt;       lhs op   rhs     mi    epc sepc.all delta    ncp power decision\n#&gt; 457 PSD11 ~~ PSD12 81.374  0.369    1.141   0.1  5.966 0.685  **(m)**\n#&gt; 253  PHD5 ~~  PHD6 60.698  0.319    0.727   0.1  5.961 0.685  **(m)**\n#&gt; 113   ANX =~  PSD6 45.656  0.625    0.517   0.1  1.168 0.191  **(m)**\n#&gt; 448  PSD9 ~~ PSD10 39.472  0.214    0.449   0.1  8.595 0.834  *epc:m*\n#&gt; 430  PSD6 ~~ PSD10 30.656 -0.164   -0.414   0.1 11.420 0.922  *epc:m*\n#&gt; 141   DEP =~ PSD14 30.594  0.837    0.587   0.1  0.437 0.101  **(m)**\n#&gt; 163  PHD1 ~~  PHD2 27.556  0.460    0.320   0.1  1.305 0.208  **(m)**\n#&gt; 372  PSD1 ~~  PSD2 25.430  0.256    0.338   0.1  3.875 0.503  **(m)**\n#&gt; 459 PSD11 ~~ PSD14 21.826 -0.190   -0.393   0.1  6.020 0.689  **(m)**\n#&gt; 462 PSD13 ~~ PSD14 16.927  0.203    0.272   0.1  4.107 0.527  **(m)**\n\n\nHere we write a function named extract_sem_eq(), to directly extract the measurement model and the structural model from the structural equation model, and format it into a readable equation.\n\nextract_sem_eq &lt;- function(object, std = TRUE, digits = 2) {\n  # Extract parameter estimates\n  params &lt;- parameterEstimates(object, standardized = std)\n  \n  # Filter for measurement model (latent =~ observed)\n  m_model &lt;- params[params$op == \"=~\", ]\n  \n  # Filter for structural model (latent ~ latent or observed ~ latent)\n  s_model &lt;- params[params$op == \"~\", ]\n  \n  # Generate measurement model equations\n  cat(\"Measurement Model Equations:\\n\")\n  for (lhs_var in unique(m_model$lhs)) {\n    # Get all rows corresponding to the lhs variable (latent variable)\n    eq_rows &lt;- m_model[m_model$lhs == lhs_var, ]\n    \n    # Create the equation for this latent variable\n    equation &lt;- paste0(lhs_var, \" = \")\n    \n    # Loop over each observed variable (rhs) and its coefficient\n    for (i in 1:nrow(eq_rows)) {\n      rhs_var &lt;- eq_rows$rhs[i]\n      coef &lt;- round(eq_rows$std.all[i], digits)\n      \n      # Add the term to the equation\n      equation &lt;- paste0(equation, coef, \" \", rhs_var)\n      \n      # Add a plus sign if not the last term\n      if (i &lt; nrow(eq_rows)) {\n        equation &lt;- paste0(equation, \" + \")\n      }\n    }\n    \n    # Output the equation\n    cat(equation, \"\\n\")\n  }\n  \n  # Generate structural model equations\n  cat(\"\\nStructural Model Equations:\\n\")\n  for (lhs_var in unique(s_model$lhs)) {\n    # Get all rows corresponding to the lhs variable\n    eq_rows &lt;- s_model[s_model$lhs == lhs_var, ]\n    \n    # Create the equation for this lhs variable\n    equation &lt;- paste0(lhs_var, \" = \")\n    \n    # Loop over each predictor (rhs) and its coefficient\n    for (i in 1:nrow(eq_rows)) {\n      rhs_var &lt;- eq_rows$rhs[i]\n      coef &lt;- round(eq_rows$std.all[i], digits)\n      \n      # Add the term to the equation\n      equation &lt;- paste0(equation, coef, \" \", rhs_var)\n      \n      # Add a plus sign if not the last term\n      if (i &lt; nrow(eq_rows)) {\n        equation &lt;- paste0(equation, \" + \")\n      }\n    }\n    \n    # Output the equation\n    cat(equation, \"\\n\")\n  }\n}\n\n\nres.sem |&gt; extract_sem_eq()\n\n#&gt; Measurement Model Equations:\n#&gt; SOS = 0.47 PHD1 + 0.52 PHD2 + 0.47 PHD3 + 0.78 PHD4 + 0.82 PHD5 + 0.81 PHD6 + 0.59 PHD7 \n#&gt; COG = 0.74 PHD8 + 0.82 PHD9 + 0.78 PHD10 + 0.58 PHD11 \n#&gt; ANX = 0.71 PSD1 + 0.71 PSD2 + 0.83 PSD3 + 0.73 PSD4 + 0.85 PSD5 \n#&gt; DEP = 0.77 PSD6 + 0.86 PSD7 + 0.76 PSD8 + 0.7 PSD9 + 0.85 PSD10 \n#&gt; AVO = 0.88 PSD11 + 0.85 PSD12 + 0.73 PSD13 + 0.66 PSD14 \n#&gt; \n#&gt; Structural Model Equations:\n#&gt; ANX = 0.63 SOS \n#&gt; DEP = 0.25 COG + 0.44 AVO + 0.37 ANX \n#&gt; AVO = 0.33 COG + 0.56 ANX",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Structural equation model</span>"
    ]
  },
  {
    "objectID": "structural-equation.html#issues-need-consideration",
    "href": "structural-equation.html#issues-need-consideration",
    "title": "26  Structural equation model",
    "section": "26.5 Issues need consideration",
    "text": "26.5 Issues need consideration\nWhen applying SEM, several key issues must be considered to ensure the model’s validity, reliability, and interpretability:\n\n26.5.1 Theoretical foundation\nThe strength SEM lies in testing complex theoretical hypothesis. Therefore, model construction must be supported by strong theoretical foundations, including relationships between latent and observed variables, as well as causal paths between latent variables. The causal relationships within the model should be logical and theoretically justified, avoiding meaningless or unexplained paths.\n\n\n26.5.2 Model identifiability\nThe identifiability determines whether model parameters can be uniquely estimated. An identifiable model requires sufficient observed data to estimate all parameters. A general rule is to have enough observed variables (typically at least three to four) for each latent variable to ensure that the parameters are estimable.\n\n\n26.5.3 Sample size\nSEM is a complex statistical technique that generally requires a large sample size for robust results. Too small a sample size may lead to unstable models and biased parameter estimates. Relationship between sample size and model complexity: More complex models typically require larger sample sizes. A general guideline is to have at least 10–20 observations per free parameter.\n\n\n26.5.4 Data characteristics\nSEM typically assumes multivariate normality. If the data significantly deviate from normality, robust estimation methods or techniques like bootstrapping may be required. Missing data is a critical issue in SEM. Common approaches include full information maximum likelihood (FIML), mean imputation, or other missing data techniques. Proper handling of missing data is crucial to avoid biased results.\n\n\n26.5.5 Path coefficients\nTest whether the path coefficients in the model are statistically significant by examining the standard errors and p-values. Significant paths indicate meaningful relationships between latent variables. In the measurement model, ensure that the factor loadings are significant and of reasonable magnitude (typically between 0.4 and 0.7).\n\n\n26.5.6 Model modification\nModification indices can suggest potential improvements to the model, but changes should be theory-driven. Overreliance on modification indices may lead to a model that deviates from the original theoretical framework. Be cautious when modifying the model by adding or removing paths, especially if they are not supported by theory, as this can lead to overfitting.\n\n\n26.5.7 Types of effects\nThe concepts of direct effects, indirect effects, and total effects are key to understanding how variables influence one another within a model.\nDirect effect: The direct effect refers to the immediate impact of one variable on another without any intervening variables. It is represented by a single path (or arrow) in the model. For example, in a model where variable X directly affects variable Y, the direct effect is the influence of X on Y without any mediation. If X directly affects Y, the direct effect measures how much a change in stress levels immediately influences health outcomes.\nIndirect effect: The indirect effect occurs when the relationship between two variables is mediated by one or more intervening variables. This effect is calculated by multiplying the coefficients of the paths that link the two variables through the mediator(s). For instance, if X affects M (a mediator) and M, in turn, affects Y, the indirect effect is the product of the path coefficients from X to M and from M to Y.\nTotal effect: The total effect is the sum of the direct and indirect effects. It represents the overall impact of one variable on another, including both the direct influence and any mediated (indirect) pathways. The total effect can be thought of as the complete influence of X on Y, accounting for all possible paths, whether direct or mediated.\nExample: If X (parental support) directly affects Y (academic performance) and also indirectly affects Y through M (self-esteem), the total effect captures both the direct influence of parental support on academic performance and its indirect effect through self-esteem.\n\n\n26.5.8 Model interpretability\nSEM results can be complex, especially with many path coefficients. When interpreting the results, focus on the main effects and relationships while keeping the explanation concise and clear. Supplement quantitative findings with qualitative insights from the research context can improve the interpretation of SEM results.",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Structural equation model</span>"
    ]
  },
  {
    "objectID": "multilevel-model.html",
    "href": "multilevel-model.html",
    "title": "27  Multilevel model",
    "section": "",
    "text": "27.1 Prerequisite\nA multilevel model (also known as hierarchical linear modeling or mixed-effects modeling) is a statistical approach used when data are organized at more than one level. This type of model is particularly useful when the assumptions of independence between observations are violated, which often happens when data are grouped or clustered.\nlibrary(tidyverse)\nlibrary(lmerTest)\nlibrary(brms)\nlibrary(metafor)",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Multilevel model</span>"
    ]
  },
  {
    "objectID": "multilevel-model.html#mixed-model",
    "href": "multilevel-model.html#mixed-model",
    "title": "27  Multilevel model",
    "section": "27.2 Mixed model",
    "text": "27.2 Mixed model\nA mixed model (or mixed-effects model) is a statistical model that combines both fixed effects and random effects. These models are often used when data have a hierarchical or grouped structure, allowing for correlation within groups and enabling analysis across multiple levels of data. Mixed models are common in fields like psychology, biology, and medical research, where data may involve repeated measures or nested data structures.\nKey components of a mixed model:\n\nFixed effects: Parameters that apply consistently across all individuals or experimental units, such as a treatment effect or a general time effect.\nRandom effects: Parameters that vary across levels of a grouping variable (such as participants or regions), accounting for individual differences or group-specific deviations. Random effects help control for variability due to these groupings.\nResidual variability: Captures the unexplained variability within each level of the grouping structure.\n\nA mixed model can be expressed as:\n\nY_{ij} = \\beta_0 + \\beta_1 X_{ij} + u_{0j} + u_{1j}X_{ij} + \\epsilon_{ij}\n\nwhere Y_{ij} is the response variable for observation i in group j, \\beta_0 and \\beta_1 are fixed effects coefficients, X_{ij}is the predictor variable for observation i in group j, u_{0j} is the random intercept for group j, capturing random differences in initial levels between groups and typically assumed to follow a normal distribution with mean 0 and variance \\sigma_u^2, u_{ij} is the random slope for group j, allowing the effect of the predictor X_{ij} to vary across groups,\\epsilon_{ij} is the residual error for each observation.\nApplications of mixed models:\n\nRepeated measures: Analyzing data collected from the same individuals at multiple time points.\nHierarchical data: For data that involve multiple nested levels, such as patients within hospitals.\nRandom intercepts and slopes: Random intercepts allow different groups to have different baseline levels, while random slopes allow for different relationships between predictors and outcomes within each group.\n\n\nExample 1 \nA researcher randomly selected 10 patients with mild hypertension and measured their diastolic blood pressure before and after taking a medication. Analyze the changes in diastolic blood pressure before and after treatment.\n\n\n  Download data \n\n\ndf &lt;- read_csv(\"datasets/ex27-01.csv\", show_col_types = F)\n\n\nfit &lt;- lmer(bp ~ state + (1 |subject ), data = df) \nfit |&gt; summary()\n\n#&gt; Linear mixed model fit by REML. t-tests use Satterthwaite's method [\n#&gt; lmerModLmerTest]\n#&gt; Formula: bp ~ state + (1 | subject)\n#&gt;    Data: df\n#&gt; \n#&gt; REML criterion at convergence: 113.9\n#&gt; \n#&gt; Scaled residuals: \n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -1.1422 -0.5311  0.1307  0.4070  1.5714 \n#&gt; \n#&gt; Random effects:\n#&gt;  Groups   Name        Variance Std.Dev.\n#&gt;  subject  (Intercept) 63.511   7.969   \n#&gt;  Residual              4.889   2.211   \n#&gt; Number of obs: 20, groups:  subject, 10\n#&gt; \n#&gt; Fixed effects:\n#&gt;             Estimate Std. Error       df t value Pr(&gt;|t|)    \n#&gt; (Intercept) 126.2000     2.6153   9.6662   48.25 7.56e-13 ***\n#&gt; state       -16.0000     0.9888   9.0000  -16.18 5.83e-08 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Correlation of Fixed Effects:\n#&gt;       (Intr)\n#&gt; state -0.189\n\n\n\nfit |&gt; equatiomatic::extract_eq(mean_separate = T, use_coefs = T)\n\n\n\\begin{aligned}\n  \\operatorname{\\widehat{bp}}_{i}  &\\sim N \\left(\\mu, \\sigma^2 \\right) \\\\\n    \\mu &=126.2_{\\alpha_{j[i]}} - 16_{\\beta_{1}}(\\operatorname{state}) \\\\\n    \\alpha_{j}  &\\sim N \\left(0, 7.97 \\right)\n    \\text{, for subject j = 1,} \\dots \\text{,J}\n\\end{aligned}\n\n\n\n\n\n\n\n\n\nPackages for linear mixed models\n\n\n\nThe nlme, lme4 and lmerTest packages are all used for fitting linear mixed-effects models, but they serve different purposes and offer distinct functionalities. Here’s a detailed comparison:\n\n\n\n\n\n\n\n\n\n\n\nFeature\nnlme\nlme4\nlmerTest\n\n\n\n\nModel types\nSupports both linear and nonlinear mixed-effects models.\nSupports linear mixed-effects models (no nonlinear support).\nSame as lme4, but with added p-values and ANOVA tables for hypothesis testing.\n\n\nOptimization\nUses maximum likelihood (ML) or restricted maximum likelihood (REML) for estimation, but optimization methods are generally slower.\nAlso uses ML and REML, but employs faster and more efficient optimization methods, particularly for large datasets.\nSame as lme4 (uses lme4 for model fitting).\n\n\nNonlinear modeling\nSupports nonlinear mixed-effects models, making it more flexible for modeling complex, nonlinear relationships.\nDoes not support nonlinear models, limited to linear mixed-effects models.\nSame as lme4, limited to linear mixed-effects models.\n\n\nCorrelation structures\nCan handle more complex correlation structures (e.g., autocorrelation, spatial correlation, heteroscedasticity).\nLimited support for correlation structures, primarily for random effects.\nSame as lme4.\n\n\nHandling of missing data\nCan handle missing data via the na.action argument and imputation methods.\nRequires complete data, no built-in missing data handling beyond the standard options.\nSame as lme4.\n\n\nGrouping levels\nOffers greater flexibility for nested or crossed random effects at multiple levels.\nSupports nested and crossed random effects, but not as flexible in terms of specifying correlation structures.\nSame as lme4.\n\n\np-values for fixed effects\nProvides p-values for fixed effects by default.\nDoes not provide p-values by default (as it’s debated in the mixed-effects model community).\nProvides p-values for fixed effects using Satterthwaite’s or Kenward-Roger’s approximation.\n\n\nModel complexity\nBetter for handling more complex models with nonlinear effects and autocorrelated errors.\nFaster for simpler, large linear mixed-effects models.\nSame as lme4 but adds hypothesis testing tools like p-values and ANOVA.\n\n\nPerformance\nSlower, especially with large datasets and complex models.\nFaster and more efficient, particularly with large datasets.\nSame as lme4.\n\n\nUser interface\nSlightly more complicated interface (uses lme() for linear models and nlme() for nonlinear).\nSimpler syntax for linear mixed-effects models (uses lmer()).\nSame as lme4.\n\n\nModel comparison\nAllows comparison using likelihood ratio tests (e.g., anova() for nested models).\nSupports model comparison but does not provide p-values.\nSame as lme4, but includes p-values for hypothesis testing.\n\n\nDiagnostic plots\nMore diagnostic plots available for assessing model fit, residuals, and random effects.\nFewer built-in diagnostic plots, but can be extended with additional functions.\nSame as lme4.\n\n\n\nIn general, use lme4 for large, linear mixed-effects models; use lmerTest for p-values and hypothesis testing; and use nlme for more complex models, particularly nonlinear ones or those requiring advanced correlation structures.\nThe brm() function from brms package can fit a Bayesian generalized linear multivariate multilevel model. The brms is a high-level package that provides an intuitive interface for fitting Bayesian models using Stan on the backend, abstracting away the need to write Stan code manually. It’s similar to the lme4 package but for Bayesian models.\n\nbayes_fit &lt;- brm(bp ~ state + (1 |subject), data = df, silent = 2)\n\n#&gt; \n#&gt; SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\n#&gt; Chain 1: \n#&gt; Chain 1: Gradient evaluation took 8e-05 seconds\n#&gt; Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.8 seconds.\n#&gt; Chain 1: Adjust your expectations accordingly!\n#&gt; Chain 1: \n#&gt; Chain 1: \n#&gt; Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\n#&gt; Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\n#&gt; Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\n#&gt; Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\n#&gt; Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\n#&gt; Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\n#&gt; Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\n#&gt; Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\n#&gt; Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\n#&gt; Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\n#&gt; Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\n#&gt; Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\n#&gt; Chain 1: \n#&gt; Chain 1:  Elapsed Time: 0.122 seconds (Warm-up)\n#&gt; Chain 1:                0.051 seconds (Sampling)\n#&gt; Chain 1:                0.173 seconds (Total)\n#&gt; Chain 1: \n#&gt; \n#&gt; SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\n#&gt; Chain 2: \n#&gt; Chain 2: Gradient evaluation took 7e-06 seconds\n#&gt; Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds.\n#&gt; Chain 2: Adjust your expectations accordingly!\n#&gt; Chain 2: \n#&gt; Chain 2: \n#&gt; Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\n#&gt; Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\n#&gt; Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\n#&gt; Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\n#&gt; Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\n#&gt; Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\n#&gt; Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\n#&gt; Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\n#&gt; Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\n#&gt; Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\n#&gt; Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\n#&gt; Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\n#&gt; Chain 2: \n#&gt; Chain 2:  Elapsed Time: 0.084 seconds (Warm-up)\n#&gt; Chain 2:                0.047 seconds (Sampling)\n#&gt; Chain 2:                0.131 seconds (Total)\n#&gt; Chain 2: \n#&gt; \n#&gt; SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\n#&gt; Chain 3: \n#&gt; Chain 3: Gradient evaluation took 3e-06 seconds\n#&gt; Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.\n#&gt; Chain 3: Adjust your expectations accordingly!\n#&gt; Chain 3: \n#&gt; Chain 3: \n#&gt; Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\n#&gt; Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\n#&gt; Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\n#&gt; Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\n#&gt; Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\n#&gt; Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\n#&gt; Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\n#&gt; Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\n#&gt; Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\n#&gt; Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\n#&gt; Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\n#&gt; Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\n#&gt; Chain 3: \n#&gt; Chain 3:  Elapsed Time: 0.082 seconds (Warm-up)\n#&gt; Chain 3:                0.046 seconds (Sampling)\n#&gt; Chain 3:                0.128 seconds (Total)\n#&gt; Chain 3: \n#&gt; \n#&gt; SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\n#&gt; Chain 4: \n#&gt; Chain 4: Gradient evaluation took 2e-06 seconds\n#&gt; Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.02 seconds.\n#&gt; Chain 4: Adjust your expectations accordingly!\n#&gt; Chain 4: \n#&gt; Chain 4: \n#&gt; Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\n#&gt; Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\n#&gt; Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\n#&gt; Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\n#&gt; Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\n#&gt; Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\n#&gt; Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\n#&gt; Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\n#&gt; Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\n#&gt; Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\n#&gt; Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\n#&gt; Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\n#&gt; Chain 4: \n#&gt; Chain 4:  Elapsed Time: 0.108 seconds (Warm-up)\n#&gt; Chain 4:                0.053 seconds (Sampling)\n#&gt; Chain 4:                0.161 seconds (Total)\n#&gt; Chain 4:\n\nbayes_fit |&gt; summary() \n\n#&gt;  Family: gaussian \n#&gt;   Links: mu = identity; sigma = identity \n#&gt; Formula: bp ~ state + (1 | subject) \n#&gt;    Data: df (Number of observations: 20) \n#&gt;   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n#&gt;          total post-warmup draws = 4000\n#&gt; \n#&gt; Multilevel Hyperparameters:\n#&gt; ~subject (Number of levels: 10) \n#&gt;               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#&gt; sd(Intercept)     8.65      2.42     5.12    14.27 1.00      673     1456\n#&gt; \n#&gt; Regression Coefficients:\n#&gt;           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#&gt; Intercept   126.46      2.79   121.43   132.19 1.01      746     1007\n#&gt; state       -15.99      1.26   -18.54   -13.51 1.00     2348     1523\n#&gt; \n#&gt; Further Distributional Parameters:\n#&gt;       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#&gt; sigma     2.66      0.89     1.59     4.87 1.01      755      899\n#&gt; \n#&gt; Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n#&gt; and Tail_ESS are effective sample size measures, and Rhat is the potential\n#&gt; scale reduction factor on split chains (at convergence, Rhat = 1).",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Multilevel model</span>"
    ]
  },
  {
    "objectID": "multilevel-model.html#variance-component-model",
    "href": "multilevel-model.html#variance-component-model",
    "title": "27  Multilevel model",
    "section": "27.3 Variance component model",
    "text": "27.3 Variance component model\nA variance component model is used to handle multilevel data, particularly when the data exhibits clear within-group and between-group variability. The primary objective of this model is to decompose the total variance into different variance components, often used to measure variability at different levels.\nA random intercept model is a specific type of variance component model. In a random intercept model, each group has its own intercept, which represents the group-specific deviation from the overall mean. However, the slope (relationship between predictors and the outcome) is assumed to be fixed across groups. The random intercept introduces between-group variation, which is one of the variance components in the model.\nVariance component models are typically a special case of mixed-effects models. In these models, random effects capture variability across different levels (such as individual or group-level differences), while fixed effects describe the average relationship between variables.\nA variance component model can be expressed as:\n\nY_{ij} = \\mu + u_j + e_{ij}\n\nwhere Y_{ij} is a observation i within group j, \\mu is overall mean, u_j is random effect for group j, representing the variability between groups, assumed to follow u_j \\sim N(0, \\sigma_u^2), e_{ij} is residual (within-group error), assumed to follow e_{ij} \\sim N(0, \\sigma_e^2).\nThe total variance S_{Y_{ij}} is decomposed into two parts:\n\nS_{Y_{ij}} = \\sigma_u^2 + \\sigma_e^2\n\nwhere \\sigma_u^2 is the variance between groups (random effect), \\sigma_e^2 is the variance within groups (residual error).\nVariance component models are widely used in the analysis of nested data. For example:\n\nPatients (individuals) are nested within hospitals (groups). The total variance can be separated into hospital-level variation and patient-level variation.\n\nVariance component models are typically estimated using maximum likelihood estimation (MLE) or restricted maximum likelihood estimation (REML). The packages like lme4 or nlme are used to fit variance component models.\n\nExample 2 \nA researcher intends to use a meta-analysis to understand the effect of migraines on adult depression. The literature search was conducted up and 16 eligible studies were included. Each study was independent and had the same research hypothesis. The available data include the odds ratios (OR) for each study, as well as study-level explanatory variables such as sample size, whether adjustments for education level were made, and study region, as shown in the data file. Please use R to perform a variance components model analysis.\n\n\n  Download data \n\n\ntb &lt;- read_csv(\"datasets/ex27-02.csv\", show_col_types = F) |&gt; \n  mutate(\n    id = as.factor(id),\n    edu_adj = as.factor(edu_adj),\n    study_rgn = as.factor(study_rgn)\n  )\n\n\nrma(\n  yi = log(or),\n  vi = 1 / sqrt(n_sample), \n  data = tb, \n  mods = ~ edu_adj + study_rgn , \n  method = \"REML\") \n\n#&gt; \n#&gt; Mixed-Effects Model (k = 16; tau^2 estimator: REML)\n#&gt; \n#&gt; tau^2 (estimated amount of residual heterogeneity):     0.1306 (SE = 0.0590)\n#&gt; tau (square root of estimated tau^2 value):             0.3614\n#&gt; I^2 (residual heterogeneity / unaccounted variability): 89.74%\n#&gt; H^2 (unaccounted variability / sampling variability):   9.75\n#&gt; R^2 (amount of heterogeneity accounted for):            0.00%\n#&gt; \n#&gt; Test for Residual Heterogeneity:\n#&gt; QE(df = 13) = 100.0999, p-val &lt; .0001\n#&gt; \n#&gt; Test of Moderators (coefficients 2:3):\n#&gt; QM(df = 2) = 0.0784, p-val = 0.9616\n#&gt; \n#&gt; Model Results:\n#&gt; \n#&gt;             estimate      se     zval    pval    ci.lb   ci.ub      \n#&gt; intrcpt       0.6516  0.1934   3.3697  0.0008   0.2726  1.0307  *** \n#&gt; edu_adj1     -0.0446  0.2486  -0.1792  0.8578  -0.5319  0.4427      \n#&gt; study_rgn1    0.0610  0.2245   0.2718  0.7858  -0.3790  0.5010      \n#&gt; \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nVariance component models can be extended to more complex multilevel models. For example, in data with multiple nested levels, the model can be expanded to account for three or more levels, capturing additional random effects.",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Multilevel model</span>"
    ]
  },
  {
    "objectID": "multilevel-model.html#random-coefficient-model",
    "href": "multilevel-model.html#random-coefficient-model",
    "title": "27  Multilevel model",
    "section": "27.4 Random coefficient model",
    "text": "27.4 Random coefficient model\nA random coefficient model (also known as a random slopes model) is an extension of a random intercept model. In a random intercept model, only the intercept varies between groups, while the slope (relationship between predictors and the response) is fixed. In a random coefficient model, both the intercept and the slope can vary between groups, which allows for capturing the different relationships between predictors and the response across groups.\nSuppose we have data where measurements are taken on individuals across different time points, and we want to model not only the overall trend (intercept) but also allow the effect of time to vary by individual (random slopes). A random coefficient model with a random slope can be written as:\n\nY_{ij} = (\\beta_0 + b_{0i}) + (\\beta_1 + b_{1i}) \\cdot X_{ij} + \\epsilon_{ij}\n\nwhereY_{ij} is the outcome for individual i at time i, \\beta_0is the fixed intercept (average starting point),b_{0i} is the random intercept for individual i, \\beta_1 is the fixed slope (average rate of change over time), b_{1i} is the random slope for individual i (individual variation in the rate of change), X_{ij} is the time variable for individual i at time j, \\epsilon_{ij} is the residual error.\n\nExample 3 \nTo study the efficacy of treatments A and B on patients with leukopenia, a random method was used. 50 patients first received treatment A, followed by a break, and then received treatment B. Another 50 patients received treatment B first, followed by the same interval, and then treatment A. White blood cell counts were recorded after each treatment, as shown in the data.\n\n\n  Download data \n\n\ntb &lt;- read_csv(\n  \"datasets/ex27-03.csv\", \n  col_types = list(\n    id = col_factor(), period = col_factor(), treat = col_factor()\n  )) \n\nYou can use the lmer() function from the lmerTest package to fit a random coefficient model:\n\nfit1 &lt;- lmer(wbc ~ treat + period + (1 + dummy(period) || id), data = tb)\nfit1\n\n#&gt; Linear mixed model fit by REML ['lmerModLmerTest']\n#&gt; Formula: wbc ~ treat + period + (1 + dummy(period) || id)\n#&gt;    Data: tb\n#&gt; REML criterion at convergence: 271.9579\n#&gt; Random effects:\n#&gt;  Groups   Name          Std.Dev.\n#&gt;  id       (Intercept)   0.1644  \n#&gt;  id.1     dummy(period) 0.5773  \n#&gt;  Residual               0.8277  \n#&gt; Number of obs: 100, groups:  id, 50\n#&gt; Fixed Effects:\n#&gt; (Intercept)       treatA      period2  \n#&gt;      6.9358      -0.1889      -0.5024\n\nfit2 &lt;- lmer(wbc ~ treat + period + (0 + dummy(period) || id), data = tb)\nfit2\n\n#&gt; Linear mixed model fit by REML ['lmerModLmerTest']\n#&gt; Formula: wbc ~ treat + period + (0 + dummy(period) || id)\n#&gt;    Data: tb\n#&gt; REML criterion at convergence: 272.0055\n#&gt; Random effects:\n#&gt;  Groups   Name          Std.Dev.\n#&gt;  id       dummy(period) 0.5773  \n#&gt;  Residual               0.8440  \n#&gt; Number of obs: 100, groups:  id, 50\n#&gt; Fixed Effects:\n#&gt; (Intercept)       treatA      period2  \n#&gt;      6.9359      -0.1892      -0.5024\n\n\nIn this example code block:\n\nThe 0 + notation removes the intercept in random effects, so only random slopes are modeled.\nThe 1 + notation adds a random intercept along with the random slopes.\nThe || notation specifies that random slopes are uncorrelated, which can help if the model is too complex and causing convergence issues.\n\nYou might want to compare a random intercept model with a random coefficient model to see if adding random slopes improves the fit. This can be done with a likelihood ratio test:\n\nanova(fit1, fit2)\n\n#&gt; Data: tb\n#&gt; Models:\n#&gt; fit2: wbc ~ treat + period + (0 + dummy(period) || id)\n#&gt; fit1: wbc ~ treat + period + (1 + dummy(period) || id)\n#&gt;      npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)\n#&gt; fit2    5 276.01 289.04 -133.00   266.01                     \n#&gt; fit1    6 277.93 293.56 -132.96   265.93 0.0859  1     0.7695\n\n\nIf the random coefficient model significantly improves the fit compared to the random intercept model, it suggests that there is substantial individual variability in the slopes (i.e., the effect of time varies by individual).\nUsing period + treat || id treats both period and treat as uncorrelated random slopes within each id.\n\nlmer(wbc ~ treat + period + (0 + dummy(treat)  + dummy(period) || id), data = tb)\n\n#&gt; Linear mixed model fit by REML ['lmerModLmerTest']\n#&gt; Formula: wbc ~ treat + period + (0 + dummy(treat) + dummy(period) || id)\n#&gt;    Data: tb\n#&gt; REML criterion at convergence: 272.0055\n#&gt; Random effects:\n#&gt;  Groups   Name          Std.Dev.\n#&gt;  id       dummy(treat)  0.0000  \n#&gt;  id.1     dummy(period) 0.5773  \n#&gt;  Residual               0.8440  \n#&gt; Number of obs: 100, groups:  id, 50\n#&gt; Fixed Effects:\n#&gt; (Intercept)       treatA      period2  \n#&gt;      6.9359      -0.1892      -0.5024  \n#&gt; optimizer (nloptwrap) convergence code: 0 (OK) ; 0 optimizer warnings; 1 lme4 warnings\n\nlmer(wbc ~ treat + period + (1 + dummy(treat)  + dummy(period) || id), data = tb)\n\n#&gt; Linear mixed model fit by REML ['lmerModLmerTest']\n#&gt; Formula: wbc ~ treat + period + (1 + dummy(treat) + dummy(period) || id)\n#&gt;    Data: tb\n#&gt; REML criterion at convergence: 271.9579\n#&gt; Random effects:\n#&gt;  Groups   Name          Std.Dev.\n#&gt;  id       (Intercept)   0.1643  \n#&gt;  id.1     dummy(treat)  0.0000  \n#&gt;  id.2     dummy(period) 0.5773  \n#&gt;  Residual               0.8277  \n#&gt; Number of obs: 100, groups:  id, 50\n#&gt; Fixed Effects:\n#&gt; (Intercept)       treatA      period2  \n#&gt;      6.9358      -0.1889      -0.5024  \n#&gt; optimizer (nloptwrap) convergence code: 0 (OK) ; 0 optimizer warnings; 1 lme4 warnings\n\n\nYou can also fit a random coefficient model with the nlme package, though syntax differs slightly. If you prefer a Bayesian approach, the brms package also supports random coefficient models.\nThis random coefficient model is especially useful in longitudinal studies or hierarchical data, where you expect individual-level differences in how predictors affect the outcome.",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Multilevel model</span>"
    ]
  },
  {
    "objectID": "multilevel-model.html#multilevel-model-of-discrete-data",
    "href": "multilevel-model.html#multilevel-model-of-discrete-data",
    "title": "27  Multilevel model",
    "section": "27.5 Multilevel model of discrete data",
    "text": "27.5 Multilevel model of discrete data\nFor discrete data in a multilevel model, generalized linear mixed models (GLMMs) are commonly used. GLMMs extend linear mixed models to handle non-continuous outcomes, such as binary or multinomial responses.\nThe general form of a GLMM is:\n\n\\text{link}(\\mathbb{E}(Y)) = X\\beta + Zb\n\nwhere Y is the discrete dependent variable (e.g., binary 0 or 1), X \\beta is fixed effects, Zb is random effects, \\text {link}() is a link function to handle non-normality in discrete data, such as \\text {logit} (for logistic regression) or \\text {log} (for Poisson regression).\nYou can use the glmer() function from the lme4 package to fit these models. For a binary outcome (e.g., success/failure), you can use the \\text {logit} link function. For categorical outcomes with more than two categories, a multinomial (multiclass) model is appropriate. Since glmer() doesn’t directly support multinomial regression, you can use nnet::multinom() or brms::brm() for Bayesian multinomial mixed models. For count data, you can use a Poisson or negative binomial distribution.\n\nExample 4 \nBelow is a questionnaire survey data, conducted by 20 investigators on 135 rural community doctors, analyzed responses to the following three questions:\n\n“Patients often ask doctors to prescribe a specific antibiotic, have you encountered this situation?”\n“If a patient requests a specific antibiotic, do you agree to prescribe it?”\n“Some doctors believe that for patients whose medication costs are not reimbursable, they should prescribe more antibiotics or more expensive antibiotics. Do you agree with this opinion?”\n\nResponses to each question are binary (“Yes” or “No”), with no repeated measures for the questions. The number of doctors surveyed by each investigator varies, meaning the data are unbalanced.\n\n\n  Download data \n\n\ntb &lt;- read_csv(\"datasets/ex27-04.csv\", show_col_types = F) |&gt; \n  mutate(across(contains(c(\"id\", \"que\")), as.factor))\n\n\nglmer(que1 ~ doc_exp + (1 + doc_exp || ivt_id), data = tb, family = binomial(link = \"logit\")) \n\n#&gt; Generalized linear mixed model fit by maximum likelihood (Laplace\n#&gt;   Approximation) [glmerMod]\n#&gt;  Family: binomial  ( logit )\n#&gt; Formula: que1 ~ doc_exp + (1 + doc_exp || ivt_id)\n#&gt;    Data: tb\n#&gt;       AIC       BIC    logLik  deviance  df.resid \n#&gt;  449.0722  464.1455 -220.5361  441.0722       316 \n#&gt; Random effects:\n#&gt;  Groups   Name        Std.Dev.\n#&gt;  ivt_id   (Intercept) 0.261893\n#&gt;  ivt_id.1 doc_exp     0.007491\n#&gt; Number of obs: 320, groups:  ivt_id, 20\n#&gt; Fixed Effects:\n#&gt; (Intercept)      doc_exp  \n#&gt;    -0.19873      0.00394\n\n\n\n27.5.1 Reliability assessment\nMultilevel models for discrete data hold unique value in assessing questionnaire reliability, especially in studies where responses are influenced by hierarchical structures or clustering effects. Here’s why these models are particularly advantageous:\n\nAddressing hierarchical structure and nested effects\nQuestionnaire data often involve a natural hierarchy, such as responses collected by different interviewers or across diverse locations. Multilevel models allow for this complexity by adding random effects at various levels (e.g., interviewers or locations), thus accurately accounting for variations in responses due to these nested structures. This prevents underestimation of reliability and provides a more accurate picture of the questionnaire’s consistency.\nSuitability for discrete response variables\nResponses in reliability studies are typically discrete, such as binary “yes/no” answers or ordinal scales. Multilevel models are well-suited to handle such data through link functions like logit or probit, allowing for reliable modeling of these non-continuous variables. This ability to directly model the response distribution improves accuracy in reliability estimates, especially when items vary widely in response patterns.\nQuantifying item-specific and overall reliability\nBy treating items as a separate level in the model, multilevel approaches can isolate the variance associated with each questionnaire item through random effects. This item-level analysis provides crucial insights into the consistency of individual items and contributes to an overall reliability measure that is richer and more detailed than traditional metrics.\nEnabling complex reliability metrics beyond standard measures\nUnlike standard reliability coefficients (e.g., Cronbach’s α), multilevel models offer the flexibility to calculate advanced reliability metrics, such as hierarchical reliability coefficients. By leveraging variance components at various levels, multilevel models can reflect both item and respondent consistency, yielding a nuanced and holistic view of questionnaire reliability.\nControlling for systematic bias in questionnaire administration\nMultilevel models can include potential sources of systematic bias, such as interviewer differences or geographic variation, as either fixed or random effects. This feature helps control for external factors that may influence responses, ultimately resulting in more accurate and reliable assessments of the questionnaire’s consistency.\n\nIn practice, multilevel models can directly model questionnaire data with individual- and item-level variations, allowing researchers to differentiate between these sources of variability. This precise control over respondent- and item-specific effects makes multilevel modeling an invaluable tool in questionnaire reliability studies.",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Multilevel model</span>"
    ]
  },
  {
    "objectID": "comprehensive-evaluation.html",
    "href": "comprehensive-evaluation.html",
    "title": "28  Comprehensive evaluation methods",
    "section": "",
    "text": "28.1 Prerequisite\nComprehensive evaluation methods are a set of techniques used to assess complex systems or subjects by integrating multiple indicators or criteria into a single evaluation. This type of evaluation is often used in complex contexts where different factors contribute to the outcome, making a holistic measure more informative than individual indicators alone. These methods are especially useful for evaluating patient health, treatment efficacy and healthcare quality.\nlibrary(tidyverse)\nlibrary(visNetwork)\nlibrary(ahp)",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Comprehensive evaluation methods</span>"
    ]
  },
  {
    "objectID": "comprehensive-evaluation.html#overview",
    "href": "comprehensive-evaluation.html#overview",
    "title": "28  Comprehensive evaluation methods",
    "section": "28.2 Overview",
    "text": "28.2 Overview\n\n28.2.1 Classifications of comprehensive evaluation\nComprehensive evaluation methods can be classified based on different criteria. Here are some common classifications:\n\nBased on data type\n\nQuantitative evaluation: Uses numerical data to calculate scores, often involving statistical or mathematical techniques. Common methods include the weighted sum model, analytic hierarchy process, and principal component analysis.\nQualitative evaluation: Relies on non-numerical data, often from expert judgments or subjective opinions. Methods like the Delphi method, expert scoring, and case studies are typically used.\nMixed methods evaluation: Combines quantitative and qualitative data to provide a more complete assessment. Examples include the comprehensive scoring method and fuzzy comprehensive evaluation.\n\nBased on evaluation objective\n\nSingle-objective evaluation: Focuses on one primary goal, such as cost reduction or quality improvement. It’s commonly used in specific project evaluations or performance assessments.\nMulti-objective evaluation: Assesses multiple objectives or criteria simultaneously, such as cost, quality, and efficiency. Methods include the Weighted Scoring Method, analytic hierarchy process, and grey relational analysis.\n\nBased on model structure\n\nAdditive models: Use additive techniques, where scores for different criteria are weighted and summed to obtain a final score. The weighted sum model is an example.\nMultiplicative models: Combine criteria by multiplication, which can be effective when criteria are dependent. For example, multiplicative utility functions are sometimes used in economic and decision analysis.\nHybrid models: Use both additive and multiplicative elements or other transformations to aggregate scores. Fuzzy comprehensive evaluation often combines both approaches.\n\nBased on weighting mechanism\n\nEqual weighting: All criteria are given the same weight, often used when there is no clear prioritization among criteria.\nSubjective Weighting: Weights are determined based on expert judgment or subjective preferences, such as in Delphi studies or scoring based on stakeholder input.\nObjective Weighting: Weights are determined based on the variability or statistical properties of the data, such as in entropy weighting and principal component analysis.\n\nBased on complexity\n\nSimple scoring models: Use straightforward calculations, like the weighted Sum Model, suitable for cases with fewer criteria and simpler data.\nComplex multi-criteria decision models: Involve more advanced mathematical and statistical techniques, such as analytic hierarchy process, fuzzy comprehensive evaluation, and grey relational analysis, suitable for complex, large-scale evaluations with interrelated criteria.\n\n\n28.2.2 Steps for evaluation\nThe process of comprehensive evaluation typically follows these key steps, which help ensure that the evaluation is systematic, reliable, and meaningful.\n\nDefine the evaluation objective and criteria\nClearly outline the goal of the evaluation, such as assessing healthcare quality, treatment effectiveness, or patient satisfaction. Identify and define the specific criteria or indicators that will be evaluated. For instance, in healthcare, these could include mortality rates, patient recovery times, or resource utilization.\nSelect or design the evaluation method\nChoose an appropriate evaluation method based on the data type, goal, and context. Define how each criterion will be measured and, if needed, create a scoring scale for qualitative criteria.\nCollect and standardize data\nGather data for each criterion across all entities or individuals being evaluated (e.g., hospitals, patients). Standardize the data if indicators are measured on different scales, such as z-scores or min-max normalization. This ensures comparability across criteria.\nDetermine the weight of each criterion\nAssign weights to each criterion to reflect its relative importance. Weights can be determined using methods like expert judgment, entropy method, or analytic hierarchy process (AHP). In some cases, all criteria may be weighted equally, depending on the evaluation objective.\nCalculate the composite score\nUsing the selected evaluation method, calculate the synthetic (composite) score for each entity by combining the weighted criteria scores. Methods such as weighted sum or TOPSIS involve aggregating the scores, while PCA involve transformation or efficiency measures.\nAnalyze and rank results\nInterpret the scores to rank or categorize the entities based on their performance or risk. For example, high composite scores might indicate high-quality healthcare facilities, while lower scores may indicate areas needing improvement.\nValidate the results\nCheck the consistency and reliability of the results through sensitivity analysis or validation with external data if available. Ensure that the scores align with real-world expectations and that the weighting and criteria selection do not introduce bias.\nReport and make recommendations\nPresent the results in a clear format, using visual aids like tables, charts, or maps if necessary. Provide actionable insights or recommendations based on the evaluation. For instance, high-ranking facilities can serve as models, while lower-ranking ones can target specific areas for improvement.\n\nEach step is essential in creating a robust, transparent, and actionable synthetic evaluation. These evaluations provide a holistic view, supporting decision-makers in fields like healthcare, education, and public policy.\n\n\n28.2.3 Indicators selection\nThe selection of evaluation indicators is crucial to ensure that assessments are accurate, relevant, and actionable. The process typically involves several steps to ensure that chosen indicators align with the objectives of the evaluation and reflect important dimensions of the program.\nHere are some commonly used methods to ensure that selected indicators are robust, meaningful, and actionable:\n\nLiterature review\n\nIdentifying commonly used indicators from published studies, reviews, or meta-analyses in related fields. This provides a foundation of validated indicators that have proven effective. This is effective as an initial selection method to identify potential indicators and ensure consistency with existing research standards.\n\nDelphi method\n\nA structured, iterative approach using expert panels to gain consensus on the most relevant indicators through multiple rounds of surveys. This i useful when expert consensus is required, especially when indicators are subjective or context-specific, such as patient-reported outcomes or community health metrics. Surveys and rounds of consensus-building, often conducted manually or through survey software.\n\nCorrelation analysis\n\nExamines the correlation between potential indicators and the target outcome, removing indicators with low correlation or multicollinearity. R functions like cor() for correlation matrices and cor.test() for testing significance.\n\nFactor analysis or PCA\n\nReduces the dimensionality of indicator sets, grouping them based on shared variance and identifying key factors or principal components. This is suitable for identifying core indicators in large datasets where indicators might overlap in meaning or measure similar constructs. The prcomp() for PCA, factanal() for factor analysis, and PCA() from the FactoMineR package.\n\nRegression analysis\n\nUsing linear or logistic regression models to identify indicators that significantly predict a target outcome. Indicators with insignificant or low coefficients may be excluded. This is effective when evaluating predictive power for a specific outcome, often used in epidemiological or clinical research. R functions like lm() and glm() for linear or logistic regression.\n\nEntropy method\n\nAn objective approach based on information entropy, where indicators with high information variability (entropy) are prioritized. This is useful when unbiased, data-driven indicator selection is required, commonly in fields like environmental health or social sciences. You can use entropy() function from the infotheo package to calculate entropy.\n\nIndicator scoring and ranking\n\nIndicators are scored and ranked based on pre-defined criteria (e.g., relevance, reliability, cost-effectiveness), sometimes using weighted scores. This is frequently used in public health assessments, allowing for balanced consideration of multiple selection criteria. Scoring and ranking can be implemented by creating score functions or using simple calculations.\n\nSensitivity analysis\n\nEvaluates how changes in indicators affect the outcome, helping to determine the indicators that most influence results. This is applied in risk assessments and health economics to filter out indicators with minimal influence on the evaluation system. You can use the sensitivity analysis packages like sensitivity.\n\nCluster analysis\n\nGroups similar indicators into clusters, allowing the selection of representative indicators from each cluster. This is particularly useful when working with a high number of indicators with similar characteristics, often in large-scale health studies. You can use the kmeans() function for k-means clustering or hclust() for hierarchical clustering.\nEach method has specific advantages and is chosen based on factors like the complexity of the evaluation, data availability, stakeholder involvement, and the evaluation’s objectives. Often, a combination of these methods is used to ensure a thorough and well-rounded selection process.\n\n\n28.2.4 Indicator weights estimation\nThe estimation of weights for comprehensive evaluation indicators is essential in multi-criteria decision-making, public health assessments, and clinical research, as it ensures the relative importance of each indicator is accurately reflected. Here are common methods for estimating these weights:\n\nAnalytic hierarchy process\n\nIn analytic hierarchy process, indicators are compared pairwise based on expert judgment. Each comparison provides a relative importance score, from which a matrix is constructed. Eigenvalues and eigenvectors of this matrix are used to derive indicator weights. This is often used in complex decision-making scenarios where expert opinions are available, such as healthcare assessments. R packages like ahpsurvey or manually calculating eigenvectors can implement AHP.\n\nDelphi method\n\nExperts estimate weights through a structured, multi-round feedback process. Each round refines the weights based on group feedback until consensus is reached. This is useful for subjective or context-specific indicators, particularly when consensus is necessary (e.g., community health metrics). It is typically conducted through surveys.\n\nEntropy weight method\n\nThis objective method calculates entropy based on indicator variability. Indicators with greater variability (higher entropy) contribute more information and receive higher weights. This method is effective for objective weighting in studies where variability reflects importance, like environmental health or epidemiology.\n\nRegression coefficient method\n\nIn this method, indicators are regressed against an outcome variable. The resulting regression coefficients are standardized to obtain weights, with larger coefficients indicating greater influence. This method is useful when a target outcome exists (e.g., mortality rate) and can be predicted by the indicators.\n\nData envelopment analysis\n\nData envelopment analysis calculates efficiency scores based on the optimization of each observation relative to others. Weights are derived based on the efficiency maximization process. This method is commonly used in performance and efficiency studies, such as hospital productivity evaluations. The function dea() from the Benchmarking package can be used for this purpose.\n\nWeighted scoring or weighted sum model\n\nWeights are assigned based on expert judgment or historical data, with the final score being a weighted sum of indicators. This method is often applied in decision-making scenarios where indicators have known relative importance.\n\nPrincipal component analysis\n\nPrincipal component analysis (PCA) identifies the main components that explain the most variance in the data, which are then used to derive indicator weights based on component loadings. This method is suitable for large datasets where indicators may overlap in information; used widely in epidemiology and psychology.\n\nFactor analysis\n\nSimilar to PCA but with latent factors that group related indicators. Weights are derived from factor loadings, reflecting the relative importance of each indicator within factors. This method is applied when indicators are believed to correlate, such as in psychological or socio-economic assessments.\n\nSimple equal weights (uniform weighting)\n\nWhen all indicators are considered equally important, each indicator is assigned the same weight. This approach is straightforward and minimizes bias when there’s no clear rationale for differentiation. This is useful as a baseline method in exploratory studies or when no weight differentiation is justified.\nSelecting the right method depends on the availability of data, the need for subjective versus objective weighting, and the specific goals of the evaluation.\nThere are several functions and packages are available to support indicator selection and weight estimation for comprehensive evaluation:\nThere are several functions from MCAD package, such as weightedSum(), weightedProduct(), AHP() and TOPSIS() are used to apply weights to indicators and aggregate them based on MCDA criteria.\n\ndata &lt;- data.frame(\n  hospital = c(\"A\", \"B\", \"C\"),\n  patient_satisfaction = c(80, 85, 90),   # scale 0-100\n  treatment_success = c(70, 75, 95),      # scale 0-100\n  readmission_rate = c(0.15, 0.10, 0.05), # scale 0-1, lower is better\n  row.names = 1\n)\nweights &lt;- c(0.3, 0.5, 0.2)\nMCDA::weightedSum(data, weights)",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Comprehensive evaluation methods</span>"
    ]
  },
  {
    "objectID": "comprehensive-evaluation.html#comprehensive-scoring-method",
    "href": "comprehensive-evaluation.html#comprehensive-scoring-method",
    "title": "28  Comprehensive evaluation methods",
    "section": "28.3 Comprehensive scoring method",
    "text": "28.3 Comprehensive scoring method\nComprehensive scoring method, or weighted sum model, is a commonly used technique in evaluation and decision-making. This method allows for a synthetic assessment by combining multiple criteria into a single score.\nIn comprehensive evaluations, different scoring methods are applied based on the context and characteristics of the indicators. Here are some commonly used methods:\n\nCumulative addition method: Adds up the raw scores of each indicator to obtain a total score. Best suited for cases where all indicators share the same units and are of equal or similar importance.\n\n\\text{Score} = \\sum_{i=1}^{n} X_i\nwhere X_i is the score for the i-th indicator.\nMultiplicative method: Multiplies the raw scores of each indicator to produce the total score, emphasizing the interdependence of indicators. Used when high scores are required across all indicators to achieve a high overall score (e.g., if any indicator score is zero, the total score becomes zero).\n\n\n\\text{Score} = \\prod_{i=1}^{n} X_i\n\n\nAdditive-multiplicative method: Combines addition and multiplication, where certain indicators are added together and others are multiplied. Suitable for grouped weighting or where different sets of indicators have distinct weight considerations.\n\n\\text{Score} = (X_1 + X_2) \\times (X_3 + X_4)\n\nor\n\n\\text{Score} = X_1 \\times X_2 + X_3 \\times X_4\n\nWeighted addition method: Assigns a weight to each indicator and calculates the weighted sum of the scores. Ideal for cases where indicators have varying levels of importance, making this one of the most widely used methods.\n\n\\text{Score} = \\sum_{i=1}^{n} W_i \\times X_i\n\nwhere W_i is the weight of the i-th indicator and X_i is its score.\nWeighted additive-multiplicative method: Combines weighted addition and multiplication, where more heavily weighted indicators are multiplied, and less significant indicators are added, yielding a total score that reflects both weighting and indicator group characteristics. Applied in cases with clear weight disparities across groups of indicators.\n\n\\text{Score} = (W_1 \\times X_1 \\times X_2) + (W_2 \\times X_3)\n\n\nwhere W_1 and W_2 are the weights for different groups of indicators.\nCumulative Addition is ideal for straightforward summing but may fail to account for differences across indicators. Multiplicative Method is suitable for situations where the dependency among indicators is crucial, especially when a low score in one indicator significantly impacts the total. Weighted Addition Method is the most commonly used, particularly in scenarios with clear weight distribution across indicators. Weighted Additive-Multiplicative Method works well for cases where groups of indicators have distinct weights, balancing both additive and multiplicative effects. Selecting the best method according to the evaluation goals and specific circumstances ensures that the results are both reasonable and scientifically valid.\nHere’s an example to calculate the comprehensive score for hospitals based on three criteria.\n\ntb &lt;- data.frame(\n  hospital = c(\"A\", \"B\", \"C\"),\n  patient_satisfaction = c(80, 85, 90),   # scale 0-100\n  treatment_success = c(70, 75, 95),      # scale 0-100\n  readmission_rate = c(0.15, 0.10, 0.05), # scale 0-1, lower is better\n  row.names = 1\n)\n\nweights &lt;- c(0.4, 0.4, 0.2) # sum should equal 1\ntb |&gt; \n  mutate(\n    readmission_rate = (1 - readmission_rate) * 100) |&gt;   # scale 0-100\n    MCDA::weightedSum(weights)\n\n#&gt;  A  B  C \n#&gt; 77 82 93\n\ntb |&gt; \n  mutate(\n    readmission_rate = (1 - readmission_rate) * 100) |&gt;   # scale 0-100\n  apply(1, \\(x) sum(x * weights))\n\n#&gt;  A  B  C \n#&gt; 77 82 93",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Comprehensive evaluation methods</span>"
    ]
  },
  {
    "objectID": "comprehensive-evaluation.html#comprehensive-index-method",
    "href": "comprehensive-evaluation.html#comprehensive-index-method",
    "title": "28  Comprehensive evaluation methods",
    "section": "28.4 Comprehensive index method",
    "text": "28.4 Comprehensive index method\nThe comprehensive index method, also known as the synthetic index method, is commonly used for multi-criteria evaluation by calculating an overall score or index that reflects the performance across various indicators. This method is widely used in fields like healthcare, environmental studies, and social sciences to assess overall quality, risk, or effectiveness.\nFor example , the hospital work efficiency index (HWEI) is a metric used to evaluate the productivity and efficiency of hospital operations. This index typically integrates several performance indicators, including resource utilization, service quality, and operational costs, to provide a holistic measure of a hospital’s effectiveness in delivering healthcare.\nThe specific formula for the HWEI may vary depending on available data and organizational priorities, but it generally involves aggregating weighted scores for each component. Assume you have a data for three hospitals, with indicators normalized on a scale from 0 to 1:\n\nResource utilization score (e.g., occupancy rates and equipment usage)\nService quality score (e.g., patient satisfaction, readmission rates)\nCost efficiency score (e.g., cost per patient and cost per unit revenue)\n\nHere’s how you might calculate a HWEI:\n\ndata &lt;- data.frame(\n  Hospital = c(\"A\", \"B\", \"C\"),\n  Resource_Utilization = c(0.8, 0.7, 0.9),\n  Service_Quality = c(0.85, 0.65, 0.9),\n  Cost_Efficiency = c(0.75, 0.8, 0.7),\n  row.names = 1\n) \nweights &lt;- c(0.4, 0.4, 0.2)\n\nMCDA::weightedSum(data, weights)\n\n#&gt;    A    B    C \n#&gt; 0.81 0.70 0.86\n\n\nThe result give a single work efficiency index score for each hospital, making it easier to assess and compare their efficiency levels.",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Comprehensive evaluation methods</span>"
    ]
  },
  {
    "objectID": "comprehensive-evaluation.html#analytic-hierarchy-process",
    "href": "comprehensive-evaluation.html#analytic-hierarchy-process",
    "title": "28  Comprehensive evaluation methods",
    "section": "28.5 Analytic hierarchy process",
    "text": "28.5 Analytic hierarchy process\nThe analytic hierarchy process (AHP) is a structured decision-making framework used to rank and prioritize complex decisions by decomposing them into a hierarchy of criteria and alternatives. Created by Thomas Saaty in the 1970s, AHP is especially useful in situations where qualitative and quantitative aspects must be considered together. AHP involves breaking down the decision problem, evaluating each component using pairwise comparisons, and calculating weights for each criterion and alternative to find an optimal solution.\nSteps in the AHP:\n\nDefine the problem and goal: Clearly state the decision to be made and establish the overall goal.\nStructure the hierarchy: Decompose the decision problem into a hierarchy, with the goal at the top level, criteria and sub-criteria (if applicable) in the middle, and alternatives at the bottom.\nPairwise comparison and priority calculation: Conduct pairwise comparisons of the criteria and alternatives at each level of the hierarchy. Use a scale of importance (usually 1 to 9, where 1 means equal importance and 9 indicates extreme importance of one element over another). For each criterion or alternative, calculate a priority vector (weight) based on the comparison results.\nConsistency check: AHP includes a consistency index and consistency ratio to ensure judgments are not random or contradictory. The consistency ratio should typically be below 0.1 for the decision process to be considered reliable.\nCalculate final scores: Combine the weights of each level to derive a final score for each alternative, helping to identify the best choice.\n\n\nExample 1 \nTo evaluate the work quality of a hospital, the supervising department selects medical criteria, nursing criteria, and dietary criteria, to represent the overall goal. Each of these three sub-criteria is further represented by their sub-criteria. Figure 1 shows the target hierarchy for hospital work quality. Based on the data to assess the work quality of a hospital.\n\n\n  Download data \n\nTo create a hierarchical diagram, you can use the visNetwork package, which supports building hierarchical diagrams or tree structures for visualizing evaluation frameworks.\n\nnodes &lt;- data.frame(\n  id = 1:15, \n  label = c(\"Hospital Work Quality\", \n    \"Dietary Services\", \"Nursing Work\", \"Medical Services\",\n    \"Dietary Service Quality\", \"Nursing System\", \"Medical System\",   \n    \"Medical Service Quality\", \"Bed Utilization\",      \n    \"Food Quality Rate\", \"Execution Excellence Rate\", \"Execution Excellence Rate\",\n    \"ICU Admission Rate\", \"Clinical Efficacy Rate\", \"Bed Occupancy Rate\"\n  ),\n  shape = c(\"ellipse\", rep(\"box\", 14)),\n  group = c(\"A\", rep(\"B\", 3), rep(\"C\", 5), rep(\"D\", 6)),\n  size = c(10, rep(30, 3), rep(30, 5), rep(20, 6))\n)\n\nedges &lt;- data.frame(\n  from = c(1, 1, 1, 2, 3, 4, 4, 4, 5, 6, 7, 8, 8, 9),\n  to   = c(2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15),\n  arrows = \"to\"\n)\n\n# Render the network\nvisNetwork(nodes, edges) |&gt; \n  visHierarchicalLayout(\n    levelSeparation = 70,\n    sortMethod = \"directed\"\n  ) |&gt; \n  visOptions(highlightNearest = list(enabled = T, degree = 1, hover = T)) \n\n\n\n\n\n\n\nFigure 28.1: Hierarchical diagram of hospital work quality evaluation\n\n\n\n\n\ndf &lt;- read_csv(\"datasets/ex28-01.csv\", show_col_types = F) |&gt; \n  column_to_rownames(var = \"hosptl_code\")\n\nThis package lets you model and analyse complex decision making problems according to the AHP framework.\n\nhospitalAHP &lt;- Load(\"appendix/hospitl_quality.yml\") \n#Calculate(hospitalAHP)\n#Analyze(hospitalAHP) \nVisualize(hospitalAHP)\n\n\n\n\n\n\ncarAHP &lt;- Load(\"appendix/car1.yml\")\nCalculate(carAHP)\n#Visualize(carAHP)\n#AnalyzeTable(carAHP) \nAnalyze(carAHP) |&gt; knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeight\nOdyssey\nAccord Sedan\nCR-V\nElement\nAccord Hybrid\nPilot\nInconsistency\n\n\n\n\nBuy Car\n100.0%\n22.0%\n21.0%\n16.3%\n15.1%\n14.1%\n11.5%\n7.5%\n\n\n¦–Cost\n51.0%\n5.9%\n11.7%\n11.7%\n12.9%\n4.8%\n4.0%\n1.5%\n\n\n¦ ¦–Purchase Price\n24.9%\n2.3%\n6.1%\n6.1%\n9.1%\n0.6%\n0.6%\n6.8%\n\n\n¦ ¦–Fuel Cost\n12.8%\n2.2%\n1.9%\n2.1%\n2.3%\n1.7%\n2.6%\n0.0%\n\n\n¦ ¦–Resale Value\n8.2%\n1.1%\n1.9%\n2.9%\n1.1%\n0.9%\n0.3%\n3.2%\n\n\n¦ °–Maintenance Cost\n5.1%\n0.3%\n1.8%\n0.5%\n0.4%\n1.6%\n0.4%\n2.3%\n\n\n¦–Safety\n23.4%\n10.2%\n5.1%\n0.8%\n0.5%\n5.1%\n1.8%\n8.0%\n\n\n¦–Capacity\n21.5%\n5.7%\n2.8%\n3.1%\n1.5%\n2.8%\n5.6%\n0.0%\n\n\n¦ ¦–Passenger Capacity\n17.9%\n4.9%\n2.4%\n2.4%\n0.8%\n2.4%\n4.9%\n0.0%\n\n\n¦ °–Cargo Capacity\n3.6%\n0.8%\n0.3%\n0.7%\n0.7%\n0.3%\n0.7%\n0.4%\n\n\n°–Style\n4.1%\n0.3%\n1.5%\n0.6%\n0.1%\n1.5%\n0.2%\n10.1%",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Comprehensive evaluation methods</span>"
    ]
  },
  {
    "objectID": "comprehensive-evaluation.html#topsis",
    "href": "comprehensive-evaluation.html#topsis",
    "title": "28  Comprehensive evaluation methods",
    "section": "28.6 TOPSIS",
    "text": "28.6 TOPSIS\nTOPSIS (Technique for Order Preference by Similarity to an Ideal Solution) is a multi-criteria decision analysis technique designed to help decision-makers select the best option among several by comparing each option’s closeness to an ideal solution and a negative-ideal solution. The ideal solution is defined as the best possible value for each criterion, and the negative-ideal solution as the worst possible value.\nSteps of TOPSIS：\n1. Construct the decision matrix: Create a matrix where rows represent different alternatives and columns represent criteria to be evaluated.\n2. Normalize the decision matrix: To eliminate the effect of different units among criteria, normalize the matrix, typically using vector normalization:\n\nr_{ij} = \\frac{X_{ij}}{\\sqrt{\\sum_{i=1}^m X_{ij}^2}}\n\nwhere r_{ij} is the normalized value, x_{ij} is the original score, and m is the number of alternatives.\n3. Create the weighted normalized matrix: Apply weights w_j to each criterion to reflect their relative importance:\n\nv_{ij} = w_j \\cdot r_{ij}\n\n4. Determine the ideal and negative-ideal solutions: The ideal solution A^+is maximum value for benefit-type criteria and minimum for cost-type criteria. The negative-ideal solution A^- is minimum value for benefit-type criteria and maximum for cost-type criteria.\n5. Calculate distance to ideal and negative-ideal solutions: Compute the Euclidean distance of each alternative i from the ideal A^+ and negative-ideal A^- solutions:\n\nS_i^+ = \\sqrt{\\sum_{j=1}^n (v_{ij} - A_j^+)^2}\n\n\nS_i^- = \\sqrt{\\sum_{j=1}^n (v_{ij} - A_j^-)^2}\n\n6. Calculate the relative closeness: Evaluate each alternative’s relative closeness C_i to the ideal solution:\n\nC_i = \\frac{S_i^-}{S_i^+ + S_i^-}\n\nA value closer to 1 indicates a better option closer to the ideal solution.\n7. Rank alternatives: Rank alternatives by their C_i values, where the highest score indicates the best choice.\n\nExample 2 \nThe data below is from six districts of a city in 2018 on the quality of emergency response work for public health incidents. Please conduct a comprehensive evaluation using the 4 indicators: event discovery sensitivity, disposal effect index, standardized report index, and report timeliness.\n\n\n  Download data \n\n\ndf &lt;- read_csv(\"datasets/ex28-02.csv\", show_col_types = F) |&gt; \n  column_to_rownames(\"district\")\n\n\nweights &lt;- c(0.25, 0.25, 0.25, 0.25)  \nimpacts &lt;- c(\"-\", \"-\", \"+\", \"-\") \n\ntopsis::topsis(as.matrix(df), weights, impacts)\n\n#&gt;   alt.row     score rank\n#&gt; 1       1 0.6900941    3\n#&gt; 2       2 0.6218473    4\n#&gt; 3       3 0.7853366    2\n#&gt; 4       4 0.9045940    1\n#&gt; 5       5 0.3556175    6\n#&gt; 6       6 0.5405404    5\n\n\n\ntopsis.m &lt;- function (data = NULL, weights = NULL, benefits = NULL, method = \"recip\") {\n  # Argument checks\n  if (missing(data)) \n    stop(\"'data' must be provided and should be a numeric matrix or data frame\")\n  if (!is.matrix(data) & !is.data.frame(data)) \n    data &lt;- as.matrix(data)\n  if (is.null(rownames(data)))\n    rownames(data) &lt;- paste0(\"Sample_\", 1:n)\n  if (missing(benefits)) \n    stop(\"'benefits' must be provided as a character vector\")\n  if (length(weights) != ncol(data)) \n    stop(\"The length of 'weights' must equal the number of columns in 'data'\")\n  if (length(benefits) != ncol(data)) \n    stop(\"The length of 'benefits' must equal the number of columns in 'data'\")\n  if (!all(weights &gt; 0)) \n    stop(\"All 'weights' must be positive numbers\")\n  if (!all(benefits %in% c(\"+\", \"-\"))) \n    stop(\"'benefits' must contain only '+' (benefit) or '-' (cost) indicators\")\n  \n  # Define normalization functions\n  recip_norm &lt;- function(x, flag) {\n    lapply(seq_along(x), function(i) if (flag[i] == \"+\") x[[i]] else 1 / x[[i]])\n  }\n\n  iqr_norm &lt;- function(x, flag) {\n    lapply(seq_along(x), function(i) {\n      q1 &lt;- quantile(x[[i]], 0.25, na.rm = TRUE)\n      q3 &lt;- quantile(x[[i]], 0.75, na.rm = TRUE)\n      if (flag[i] == \"+\") (x[[i]] - q1) / (q3 - q1) \n      else (q3 - x[[i]]) / (q3 - q1)\n    })\n  }\n  \n  range_norm &lt;- function(x, flag) {\n    lapply(seq_along(x), function(i) {\n      if (flag[i] == \"+\") (x[[i]] - min(x[[i]])) / diff(range(x[[i]])) \n      else (max(x[[i]]) - x[[i]]) / diff(range(x[[i]]))\n    })\n  }\n  \n  # Apply chosen normalization method\n  norm_data &lt;- switch(\n    method,\n    recip = recip_norm(data, benefits),\n    iqr = iqr_norm(data, benefits),\n    range = range_norm(data, benefits),\n    stop(\"Unknown method specified\")\n  )\n  \n  norm_data &lt;- as.matrix(do.call(cbind, norm_data))  # Convert to matrix for calculations\n\n  # Compute weighted normalized matrix\n  A &lt;- sweep(norm_data, 2, sqrt(colSums(norm_data^2)), \"/\")\n  \n  weights &lt;- weights / sum(weights)  # Normalize weights\n  weightedA &lt;- A * weights\n\n  # Calculate positive and negative ideal solutions\n  u &lt;- apply(weightedA, 2, max)\n  l &lt;- apply(weightedA, 2, min)\n\n  # Distance functions for positive and negative ideals\n  du &lt;- apply(weightedA, 1, function(x) sqrt(sum((x - u)^2)))\n  dl &lt;- apply(weightedA, 1, function(x) sqrt(sum((x - l)^2)))\n\n  # TOPSIS score and ranking\n  score &lt;- dl / (dl + du)\n  \n  return(data.frame(\n    name = rownames(data),\n    score = score, \n    rank = rank(-score)))\n}\n\nThe topsis.m() function is ready to execute the TOPSIS method with customizable normalization methods (recip, iqr, and range).\n\nweights &lt;- c(0.25, 0.25, 0.25, 0.25)  \nbenefits &lt;- c(\"-\", \"-\", \"+\", \"-\") \ntopsis.m(df, weights, benefits, method = \"recip\")\n\n#&gt;   name     score rank\n#&gt; 1    A 0.4548596    3\n#&gt; 2    B 0.4131845    5\n#&gt; 3    C 0.5266299    2\n#&gt; 4    D 0.8866854    1\n#&gt; 5    E 0.4250024    4\n#&gt; 6    F 0.2679408    6",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Comprehensive evaluation methods</span>"
    ]
  },
  {
    "objectID": "comprehensive-evaluation.html#rank-sum-ratio",
    "href": "comprehensive-evaluation.html#rank-sum-ratio",
    "title": "28  Comprehensive evaluation methods",
    "section": "28.7 Rank sum ratio",
    "text": "28.7 Rank sum ratio\nThe rank sum ratio (RSR) method is a multi-criteria decision-making technique often used to evaluate and rank objects based on multiple indicators or criteria. The method works by ranking each object on each criterion, calculating a total rank for each object, and then normalizing these totals to produce a ratio between 0 and 1. This ratio is called the RSR, and it is a relative measure of how each object performs across the criteria. Here’s a step-by-step breakdown of how the RSR method works:\n1. Calculate ranks\nAssume there are m indicators and n evaluation objects, forming an n \\times m data matrix \\mathbf X = [X_{ij}] , where X_{ij} represents the value of the i-th object on the j-th indicator. Obtain ranks R_{ij} for each indicator in the matrix \\mathbf X = [X_{ij}], which indicates the rank of the i-th object under the j-th indicator.\n2. Calculate weighted rank sum\nMultiply each rank by its corresponding weight (if weights are assigned to indicators). Sum the weighted ranks for each sample:, yielding a total rank R_i:\n\nR_i = \\sum_{j=1}^{m} w_jR_{ij}\n\n3. Calculate the rank sum ratio\nNormalize the rank sum by dividing by the product of the sample count and the total weight sum. The RSR for each sample is:\n\nRSR_i = \\frac{R_i}{n \\cdot \\sum_{j=1}^m w_j}\n\nThis gives an RSR_i value on a [0, 1] scale, facilitating comparison and interpretation.\n\nRanking and evaluation\n\nObjects are ranked and categorized based on RSR_i values; a higher value indicates a better overall performance across indicators.\n\nExample 3 \nThe data below is collected from a province for a given year, including prenatal check-up rate X1 (%), maternal mortality rate X2 (per 100,000), and perinatal mortality rate X3 (‰) across 10 regions. Perform a comprehensive evaluation based on the three indicators.\n\n\n  Download data \n\n\ndf &lt;- read_csv(\"datasets/ex28-03.csv\", show_col_types = F) |&gt; \n  column_to_rownames(var = \"area\")\n\nThere is no built-in function in R for RSR, here we write a function named rsr() to implement RSR:\n\nrsr &lt;- function(data, weights = NULL, direction = c(\"+\", \"-\"), grades = 3) {\n  if (!is.data.frame(data)) {\n    stop(\"Input data must be a data frame.\")\n  }\n  \n  n &lt;- nrow(data)\n  m &lt;- ncol(data)\n  \n  # Assign row names if not provided\n  if (is.null(rownames(data))) rownames(data) &lt;- paste0(\"Sample_\", 1:n)\n  \n  # Handle weights\n  weights &lt;- if (is.null(weights)) rep(1 / m, m) else weights / sum(weights)\n  if (length(weights) != m) \n    stop(\"Length of weights must match the number of columns in data.\")\n  \n  # Direction (\"+\" means bigger is better, \"-\" means smaller is better)\n  direction &lt;- rep(direction, length.out = m)\n  \n  # Calculate the rank of each indicator\n  ranks &lt;- lapply(1:m, function(j) {\n    if (direction[j] == \"+\") rank(data[[j]]) else rank(-data[[j]])\n  })\n  \n  # Convert a list to a matrix\n  ranks &lt;- do.call(cbind, ranks)\n  \n  # Calculate the weighted rank sum\n  weighted_ranks &lt;- sweep(ranks, 2, weights, \"*\")\n  rsr &lt;- rowSums(weighted_ranks)\n  \n  # Standardize RSR values\n  rsr &lt;- rsr / (n * sum(weights))\n  sort &lt;- rank(-rsr)\n  # Adjust tie ranking\n  adjust_ties &lt;- function(x){\n    unique_x &lt;- sort(unique(x))\n    return(match(x, unique_x)) \n  }\n  sort &lt;- adjust_ties(sort)\n  # sort by grading\n  bucket_lbl &lt;- cut(-rsr, breaks = grades, labels = F, include.lowest = T)\n  \n  result &lt;- data.frame(\n    name = rownames(data), ranks = ranks, rsr = rsr, \n    sort = sort, grade = bucket_lbl\n  )\n  \n  class(result) &lt;- c(\"rsr\", \"data.frame\")  # 为结果添加类\n  return(result)\n}\n\n\ndirections &lt;- c(\"+\", \"-\", \"-\")\nrsr(df, direction = directions)\n\n#&gt;    name ranks.1 ranks.2 ranks.3       rsr sort grade\n#&gt; 1     A      10       2       6 0.6000000    4     2\n#&gt; 2     B       7       3       2 0.4000000    7     2\n#&gt; 3     C       9       7       7 0.7666667    2     1\n#&gt; 4     D       3       4       5 0.4000000    7     2\n#&gt; 5     E       2       8       8 0.6000000    4     2\n#&gt; 6     F       5       6       9 0.6666667    3     1\n#&gt; 7     G       6       5       4 0.5000000    6     2\n#&gt; 8     H       8       9      10 0.9000000    1     1\n#&gt; 9     I       4      10       3 0.5666667    5     2\n#&gt; 10    J       1       1       1 0.1000000    8     3",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Comprehensive evaluation methods</span>"
    ]
  },
  {
    "objectID": "community-intervention.html",
    "href": "community-intervention.html",
    "title": "29  Community intervention trials",
    "section": "",
    "text": "29.1 Prerequisite\nCommunity intervention trials are used to evaluate the impact of interventions at the community or group level, often applied in public health to assess the effectiveness of policies, education programs, or preventive measures. Below is a detailed guide for designing such studies.\nlibrary(tidyverse)",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Community intervention trials</span>"
    ]
  },
  {
    "objectID": "community-intervention.html#basic-concepts",
    "href": "community-intervention.html#basic-concepts",
    "title": "29  Community intervention trials",
    "section": "29.2 Basic concepts",
    "text": "29.2 Basic concepts\nThe term “community” here refers to a geographical area or a group of individuals sharing certain characteristics (e.g., patients in the same hospital or residents in the same neighborhood). In cluster randomized trials, a community represents the fundamental unit for random assignment of interventions. Community data often have a nested or hierarchical structure, for instance, patients nested within hospitals, which are nested within regions.\n\n29.2.1 Intracluster correlation\nIn community intervention trials, intracluster correlation and between-cluster variation are critical statistical concepts that significantly impact trial design, sample size estimation, and data analysis.\nThe intracluster correlation measures the degree of similarity among individuals within the same community (cluster) in terms of the outcome variable. The intracluster correlation coefficient (ICC, denoted as \\rho) quantifies the degree of similarity or clustering within groups or communities. It is a measure used in studies with hierarchical or clustered data to describe how strongly individuals within the same group resemble each other in terms of the outcome variable.\nBetween-cluster variation reflects the natural variability across clusters, regardless of the intervention. It accounts for inherent differences between communities, such as disparities in healthcare access, cultural practices, or baseline health conditions, which may influence outcomes even in the absence of an intervention.\nA high ICC indirectly indicates substantial between-cluster variation relative to total variability. A low ICC reflects that within-cluster variability dominates, meaning individuals within a cluster are less similar. High ICC decreases the effective sample size because individuals within the same cluster contribute less independent information. Between-cluster variation needs to be accounted for during both design and analysis phases to ensure accurate conclusions.\n\n\n29.2.2 Design effect\nICC undermines the assumption of independence between individuals within the same cluster. Ignoring ICC can underestimate the required sample size, increasing the risk of type II error (failure to detect a true effect). Adjustment of sample size using the design effect (DE) is necessary. The adjusted sample size is n_{\\text{adjusted}} = n_{\\text{traditional}} \\cdot \\text{DE}, where \\text{DE} = 1 + (m - 1) \\cdot \\rho, m is the average cluster size.\nIf ICC and between-cluster variation are ignored, standard errors of effect estimates may be underestimated, leading to inflated type I error rates (false positives). Traditional statistical methods may overstate the significance of intervention effects. For example, in a two-sample t-test comparing means, if n = 100 and \\rho = 0.01, the type I error probability at a significance level of \\alpha = 0.05 will increase from 0.05 to 0.166, a more than threefold increase in the false-positive rate. For F-tests comparing multiple group means, the false-positive rate due to the design effect will be even higher.\nAdvanced statistical techniques like multilevel models or generalized estimating equations are recommended, which can handle the clustering and ensure valid inference.\nWhen the outcome variable is continuous, the ICC \\rho can be estimated as the ratio of between-cluster variance to the total variance (the sum of between-cluster and within-cluster variances). Suppose there are k clusters, each with m individuals. The mean square errors for between-cluster and within-cluster variations are denoted as \\text{MS}_A and \\text{MS}_W, respectively. The estimate for \\rho is given by the following ANOVA-based formula:\n\n\\hat{\\rho} = \\frac{\\text{MS}_A - \\text{MS}_W}{\\text{MS}_A + (m-1)\\text{MS}_W} = \\frac{S_A^2}{S_A^2 + S_W^2}\n\\tag{29.1}\nWhere S_A^2 = (\\text{MS}_A - \\text{MS}_W)/m and S_W^2 = \\text{MS}_W are the observed variances between and within clusters.\nFor cases where the number of individuals per cluster is unequal, let m_j represent the number of individuals in the j-th cluster (j = 1, 2, \\ldots, k). The effective average cluster size m_0 can be calculated as: m_0 = \\frac{1}{k-1} \\left( M - \\sum_{j=1}^k \\frac{m_j^2}{M} \\right) \nWhere M = \\sum_{j=1}^k m_j is the total number of individuals across all clusters. This value m_0 can replace m in Equation 29.1 for cases with unequal cluster sizes.\nWhen the outcome variable is binary (e.g., proportions such as incidence or mortality rates), the ICC \\rho can be estimated as follows:\n\n\\hat{\\rho} = \\frac{\\sum_{j=1}^k \\hat{P}_j(1 - \\hat{P}_j)}{k(m-1)\\hat{P}(1 - \\hat{P})}\n\nWhere \\hat{P}jis the observed proportion in the j-th cluster (j = 1, 2, \\ldots, k), \\hat{P} is the overall proportion across all clusters, calculated as \\hat{P} = \\sum{j=1}^k m_j \\hat{P}_j / M. In practice, the value of \\rho depends not only on the measurement results of the outcome indicator but also on the size of the clusters.\n\n\n29.2.3 Commonly used design methods\nWhen conducting a community intervention trial, three fundamental aspects must be considered: 1. Selection of the intervention 2. Inclusion and exclusion criteria for individuals and communities 3. Evaluation of intervention effects. Based on these considerations, it is essential to adopt a scientifically rational trial design. The most commonly used design methods for community intervention trials include:\n1. Completely randomized design\nThe completely randomized design is the simplest and most straightforward design method used in community intervention trials. In this approach, communities (or clusters) are randomly assigned to the intervention group or the control group without considering any pre-existing characteristics of the communities.\nIt is best suited for trials with a large number of communities where balancing baseline characteristics through randomization alone is sufficient. It is less ideal for trials with small sample sizes or significant between-community variability, where matched or stratified designs might be more appropriate.\n2. Matched-pair design\nThe matched-pair design is a widely used method in community intervention trials to improve balance and reduce variability between intervention and control groups. In this design, communities are paired based on similar baseline characteristics, and one community from each pair is randomly assigned to the intervention group while the other is assigned to the control group.\nCommunities are matched into pairs using pre-specified criteria such as demographics (e.g., population size, socioeconomic status) , geographic location, or pre-intervention outcome measures. It is particularly valuable in small-scale trials or when the number of clusters is limited, making balance between groups critical.\n3. Stratified randomization design\nThe stratified randomization design is a method used in community intervention trials to ensure that the intervention and control groups are balanced with respect to specific characteristics that may influence the outcomes of interest. Communities are grouped into strata based on pre-determined characteristics, and randomization occurs separately within each stratum.\nCommunities are divided into strata (groups) based on factors such as geographic region, population size, socioeconomic status, or baseline outcome measures. It is especially beneficial in large-scale trials with significant variability among communities, as it ensures balanced allocation while allowing for flexibility in group sizes and analytical methods.",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Community intervention trials</span>"
    ]
  },
  {
    "objectID": "community-intervention.html#completely-randomized-community-intervention-trials",
    "href": "community-intervention.html#completely-randomized-community-intervention-trials",
    "title": "29  Community intervention trials",
    "section": "29.3 Completely randomized community intervention trials",
    "text": "29.3 Completely randomized community intervention trials\n\n29.3.1 Sample size estimation\nSample size estimation in community intervention trials must consider intracluster correlation (ICC) because individuals within the same community tend to exhibit correlated responses. Ignoring ICC can lead to underestimation of the required sample size, increasing the risk of type II error and reducing statistical power.\n1. Continuous outcome\nFor a two-arm trial comparing means:\n\nn = \\frac{(Z_{1-\\alpha/2} + Z_{1-\\beta})^2 \\cdot 2 \\sigma^2}{(\\mu_1 - \\mu_2)^2} \\cdot DE\n\nwhere Z_{1-\\alpha/2} is the critical value for the two-tailed test, Z_{1-\\beta} is the critical value for the power, \\sigma^2 is the population variance, \\mu_1 and \\mu_2 are the pupulation means of two groups, DE is the design effect, \\text{DE} = 1 + (m - 1) \\cdot \\rho, m is the average cluster size.\n\nss_crd.means &lt;- function(d, sd, m, icc, alpha = 0.05, power = 0.9) {\n  z_alpha &lt;- qnorm(1 - alpha / 2)\n  z_beta &lt;- qnorm(power)\n  \n  # Calculate the design effect\n  de &lt;- 1 + (m - 1) * icc\n  n &lt;- (2 * sd^2 * (z_alpha + z_beta)^2) / d^2\n  \n  # Adjust for design effect\n  n_adjusted &lt;- ceiling(n * de)\n  k &lt;- ceiling(n_adjusted / m)\n  \n  NOTE &lt;-  \"n is the sample size for each group, k is the number of communities for each group\"\n  METHOD &lt;- \"Two-arm trial comparing means sample size calculation\"\n  structure(\n    list(\n      n = n_adjusted, k = k, alpha = alpha, power = power, \n      method = METHOD, note = NOTE), \n    class = \"power.htest\")\n}\n\n\nExample 1 \nIn a community intervention trial investigating salt restriction for the prevention of hypertension, diastolic blood pressure is used as the trial effect indicator to estimate the required sample size for the study. The standard deviation of diastolic blood pressure in the population is \\delta = 8.886 mmHg, the intracluster correlation coefficient for diastolic blood pressure is \\rho = 0.075, the average number of individuals per community is 500, the expected trial effect size is a reduction of at least 3mmHg in the mean diastolic blood pressure in the intervention group compared to the control group after one year, a two-sided significance level of \\alpha = 0.05, statistical power of 1 - \\beta = 0.90.\n\n\nd &lt;- 3; sd &lt;- 8.886; m = 500; icc &lt;- 0.075\nss_crd.means(d, sd, m, icc)\n\n#&gt; \n#&gt;      Two-arm trial comparing means sample size calculation \n#&gt; \n#&gt;               n = 7085\n#&gt;               k = 15\n#&gt;           alpha = 0.05\n#&gt;           power = 0.9\n#&gt; \n#&gt; NOTE: n is the sample size for each group, k is the number of communities for each group\n\n\n2. Binary outcome\nFor a two-arm trial comparing proportions:\n\nn = \\frac{(Z_{1-\\alpha/2} + Z_{1-\\beta})^2 {[p_1 (1-p_1) + p_2 (1-p_2)}]}{(p_1 - p_2)^2} \\cdot DE\n\nWhere p_1 and p_2 are the proportions in the intervention and control groups.\n\nss_crd.proportion &lt;- function(p1, p2, m, icc, alpha = 0.05, power = 0.8) {\n  z_alpha &lt;- qnorm(1 - alpha / 2)\n  z_beta &lt;- qnorm(power)\n  \n  # Calculate the design effect\n  de &lt;- 1 + (m - 1) * icc\n  n &lt;- (z_alpha + z_beta)^2 * (p1 * (1 - p1) + p2 * (1 - p2)) / (p1 - p2)^2\n  \n # Adjust for design effect\n  n_adjusted &lt;- ceiling(n * de)\n  k &lt;- ceiling(n_adjusted / m)\n\n  NOTE &lt;-  \"n is the sample size for each group, k is the number of communities for each group\"\n  METHOD &lt;- \"Two-arm trial comparing proportions sample size calculation\"\n  structure(\n    list(\n      n = n_adjusted, k = k, alpha = alpha, power = power, \n      method = METHOD, note = NOTE), \n    class = \"power.htest\")\n}\n\n\nExample 2 \nIn a school intervention trial on student smoking rates, the study aim is to compare the effects of two interventions: “school-wide no-smoking campaigns” and “standard smoking cessation education programs,” on smoking cessation rates among adolescents over two years. From 24 schools, the intracluster correlation coefficient is estimated to be \\rho = 0.01. In the trial design, 100 students are randomly selected from each school. The expected smoking cessation rates for the two groups are P_1 = 0.06 and P_2 = 0.04, respectively. Under the conditions of a two-sided significance level \\alpha = 0.05 and statistical power 1 - \\beta = 0.8, how many individuals are required for each group?\n\n\np1 &lt;- 0.06; p2 &lt;- 0.04; m &lt;- 100; icc &lt;- 0.01\nss_crd.proportion(p1, p2, m, icc)\n\n#&gt; \n#&gt;      Two-arm trial comparing proportions sample size calculation \n#&gt; \n#&gt;               n = 3702\n#&gt;               k = 38\n#&gt;           alpha = 0.05\n#&gt;           power = 0.8\n#&gt; \n#&gt; NOTE: n is the sample size for each group, k is the number of communities for each group\n\n\n3. Incidence densities\nComparison of incidence densities is commonly used in studies analyzing events with low occurrence rates, measured as events per person-year, such as chronic disease incidence studies. In cluster-randomized trials, if the intervention’s effect is measured by a reduction in incidence density, the sample size calculation should account for person-years at risk within each community.\nAssume a study involving k communities, each with m individuals, randomly allocated to two groups. All individuals are followed for t person-years. The goal is to determine whether H_0: \\lambda_1 = \\lambda_2 holds under a specified significance level (\\alpha) and power (1 - \\beta). Here, \\lambda_1 and \\lambda_2 are the estimated incidence densities for the intervention and control groups, respectively. The variance of community-specific incidence rates within each group is denoted by \\sigma_1^2 and \\sigma_2^2 . Assuming an equal coefficient of variation ( CV ) between groups, defined as CV = \\sigma_1 / \\lambda_1 = \\sigma_2 / \\lambda_2, the coefficient of variation can be interpreted as the intraclass correlation coefficient (\\rho) for continuous variables. A higher CV indicates greater community-level variation, leading to a larger required sample size.\nThe estimated number of communities per group is given by:\n\nk = \\frac{(Z_{\\alpha} + Z_{\\beta})^2 (\\lambda_1 + \\lambda_2)}{t \\cdot (\\lambda_1 - \\lambda_2)^2} \\cdot \\text{VIF}\n\nwhere the variance inflation factor (VIF), accounting for the clustering effect, is calculated as:\n\n\\text{VIF} = 1 + \\frac{CV^2 (\\lambda_1^2 + \\lambda_2^2)}{\\lambda_1 + \\lambda_2} t.\n\nWhen there is no variation between communities ( CV = 0 ), VIF = 1 , and the formula simplifies to the conventional sample size calculation for cohort studies.\n\nss_crd_incidence &lt;- function(p1, p2, cv, t, alpha = 0.05, power = 0.8) {\n  z_alpha &lt;- qnorm(1 - alpha / 2)\n  z_beta &lt;- qnorm(power) \n  vif &lt;- 1 + (cv^2 * (p1^2 + p2^2)) / (p1 + p2) * t\n  \n  # Calculate the sample size per group\n  u &lt;- (z_alpha + z_beta)^2 * (p1 + p2)\n  v &lt;- t * (p1 - p2)^2\n  k &lt;- ceiling(u / v * vif)\n  \n  NOTE &lt;-  \"k is the number of communities for each group\"\n  METHOD &lt;- \"Two-arm trial comparing incidence density sample size calculation\"\n  structure(\n    list(\n      k = k, lambda1 = p1, lambda2 = p2, alpha = alpha, power = power, \n      t = t, cv = cv, method = METHOD, note = NOTE), \n    class = \"power.htest\")\n}\n\n\nExample 3 \nIn a community intervention trial for the prevention of HIV infection, the incidence density (\\lambda_2) in the control group is 0.01 per person-year, while the expected incidence density in the intervention group (\\lambda_1) is reduced to 0.005 per person-year. The coefficient of variation (CV) is estimated to be 0.25. Assuming that 1,000 individuals are randomly selected from each community, with a follow-up period of 2 years, yielding 2,000 person-years of observation per community. Under a two-sided test with \\alpha = 0.05 and a power of 1 - \\beta = 0.8, how many communities are required for each group?\n\n\nlambda1 &lt;- 0.01; lambda2 &lt;- 0.005; cv &lt;- 0.25; t &lt;- 2000        \n\nss_crd_incidence(lambda1, lambda2, cv, t)\n\n#&gt; \n#&gt;      Two-arm trial comparing incidence density sample size calculation \n#&gt; \n#&gt;               k = 5\n#&gt;         lambda1 = 0.01\n#&gt;         lambda2 = 0.005\n#&gt;           alpha = 0.05\n#&gt;           power = 0.8\n#&gt;               t = 2000\n#&gt;              cv = 0.25\n#&gt; \n#&gt; NOTE: k is the number of communities for each group",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>Community intervention trials</span>"
    ]
  },
  {
    "objectID": "scale-develop.html",
    "href": "scale-develop.html",
    "title": "30  Medical scales",
    "section": "",
    "text": "30.1 Prerequisite\nMedical scales, also known as clinical or health assessment scales, are standardized tools used to measure various aspects of health, including physical functioning, psychological well-being, symptom severity, and quality of life. They are widely used in clinical practice, research, and patient self-assessments to provide objective data that can inform diagnoses, monitor changes over time, evaluate treatment outcomes, and enhance communication between healthcare providers and patients.\nlibrary(tidyverse)\nlibrary(performance)\nlibrary(psych)\nlibrary(mirt)\nlibrary(effectsize)",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Medical scales</span>"
    ]
  },
  {
    "objectID": "scale-develop.html#designing-of-medical-scales",
    "href": "scale-develop.html#designing-of-medical-scales",
    "title": "30  Medical scales",
    "section": "30.2 Designing of medical scales",
    "text": "30.2 Designing of medical scales\nUnderstanding the structure of scales is key to constructing reliable, valid, and practical measurement tools. Their design not only determines the effectiveness of data collection but also affects the reliability and validity of the results.\n\n30.2.1 Essential elements of scales\nThe following outlines the essential elements that compose a medical scale, the design principles, and the critical considerations in their construction.\n1. Defining the objective of the scale\nBefore constructing a medical scale, clearly define its purpose. Is the scale intended to measure symptom severity, treatment efficacy, or patient-reported outcomes? Defining the objective helps determine the scope, target population, and the type of items to include, ensuring that each element aligns with the desired outcome.\nExample: The Hamilton depression rating scale was designed to measure the severity of depression, focusing on symptoms such as mood, insomnia, and cognitive function.\n2. Selecting and structuring items\nItems are the questions or statements to which respondents provide answers. Each item should reflect a single, specific aspect of the concept being measured, avoiding ambiguity. The items are generally classified based on content, such as symptoms or behaviors, and must be relevant, easily interpretable, and culturally sensitive.\nItem types:\n\nDirect symptom items: “How often do you feel fatigued?”\nBehavioral frequency items: “How many times per week do you experience loss of appetite?”\nFunctional impact items: “To what extent does the condition impact daily activities?”\n\n3. Response format and scaling\nResponse options provide the range through which respondents express the degree or frequency of the measured attribute. The choice of response format should balance ease of use with sensitivity to detect meaningful differences.\nTypes of response scales:\n\nDichotomous: Yes/No answers, often used in preliminary screenings.\nLikert scale: Allows respondents to express agreement or intensity (e.g., 1-5).\nFrequency/severity scales: Ranges from “Never” to “Always” or “None” to “Severe.”\n\nConsideration: A larger scale range may increase precision but could introduce response variability if it becomes too granular.\n4. Scoring mechanism\nThe scoring system aggregates responses into a meaningful score that reflects the overall measure of the concept. This can be a simple sum, a weighted score, or a more complex formula if some items are given more importance.\n\nSummative Scoring: Adding up individual item scores for a total score.\nWeighted Scoring: Assigning higher weights to critical items to emphasize their importance.\nInterpreting Scores: Establishing cutoff points or score ranges to categorize outcomes, such as “mild,” “moderate,” or “severe.”\n\n5. Reliability and validity testing\nTo ensure consistency and accuracy, a scale must undergo rigorous testing for reliability (e.g., test-retest reliability) and validity (e.g., construct validity).\n\nReliability: Ensures that the scale consistently measures the concept over time and across contexts.\nValidity: Verifies that the scale truly measures what it intends to.\n\nEach of these properties is critical in confirming that the scale will provide accurate and reproducible measurements in diverse settings.\n6. Cultural and linguistic adaptation\nIf a scale will be used in diverse populations, cultural and linguistic adaptations must be made. This ensures relevance and accuracy in diverse contexts and avoids bias.\nTranslation and Adaptation Process:\n\nForward and Backward Translation: Ensures the scale retains its meaning across languages.\nCultural Relevance Review: Local experts assess the scale for cultural appropriateness.\n\n7. Practical Considerations\nFinally, practical elements such as the length of the scale, ease of administration, and respondent burden play a role in the scale’s overall usability in real-world settings. Shorter scales are generally preferred for quick assessments, while longer scales may provide more detailed insights but could risk higher drop-out rates.\n\n\n30.2.2 Scale item selection\nItem selection is a critical step in developing a high-quality scale, ensuring items are representative, clear, and capable of distinguishing among responses effectively. Thoughtful item selection lays the foundation for a scale’s validity and reliability, making it a useful, accurate tool in real-world applications.\n1. Initial item generation and content review\nItem selection begins with generating an initial pool of items, often informed by literature reviews, expert interviews, focus groups, or surveys to ensure comprehensive coverage of the target concept.\n\nExpert review: Submit the items to domain experts for content review to ensure that each item is relevant to the construct being measured.\nParticipant interviews: Gather feedback from potential respondents via interviews or preliminary testing to assess the clarity and readability of the items.\n\n2. Relevance analysis\nStatistical methods or expert ratings can evaluate item relevance, helping to discard items that do not strongly align with the measurement goals.\n\nRating method: Experts rate items based on relevance, retaining items with high scores.\nContent validity index (CVI): Calculate the CVI, which indicates the proportion of experts who rate an item as “relevant” or “highly relevant.” Items with a CVI above 0.8 are typically considered suitable.\n\n3. Item discrimination analysis\nDiscrimination analysis helps identify items that effectively differentiate among respondent groups. Items with higher discrimination are more sensitive to varying levels of the construct being measured.\n\nItem-total correlation analysis: Use Pearson or Spearman correlations to assess the relationship between each item and the total score; items with correlations below 0.3 can be considered for removal.\nInternal consistency (reliability): Test the scale’s internal reliability using Cronbach’s alpha, eliminating items with low or negative contributions to reliability.\n\n4. Simplicity and Comprehensibility\nItems should be simple and easy to understand to avoid confusion and ensure respondents can answer accurately.\n\nWording Review: Ensure items are clearly worded and avoid double negatives, ambiguous terms (e.g., “often,” “usually”), and technical jargon.\nCognitive Interviewing: Test items in cognitive interviews to ensure that respondents interpret them as intended and to identify any ambiguities.\n\n5. Adequate number of items\nWhile an excess of items may lead to fatigue and diminish data quality, too few items may fail to capture the target construct comprehensively. Striking a balance between informativeness and response burden is crucial.\n\nRedundancy analysis: Evaluate items for redundancy, consolidating or removing highly correlated items.\nBalance comprehensiveness and conciseness: Ensure each subdimension has adequate item support without introducing unnecessary or repetitive items.\n\n6. Pilot testing and data analysis\nPilot testing provides empirical data for further item refinement, using data-driven methods like factor analysis to evaluate the construct validity of each item.\n\nExploratory factor analysis: Group items into potential factors to ensure a logical and coherent item structure.\nConfirmatory factor analysis: Verify the factor structure, ensuring the item structure aligns with the theoretical model.\n\n\nExample 1 \nIn a pilot-survey on the quality of life among 206 patients with and without hypertension, an analysis was conducted on the quality of 12 items across three dimensions of the physical domain—namely, pain and discomfort, energy and fatigue, and sleep and rest.\n\n\n  Download data \n\n\ndf &lt;- read_csv(\"datasets/ex30-01.csv\", show_col_types = F) \n\nItem-total correlation analysis:\n\nselect(df, contains(\"f\")) |&gt; \n  sapply(function(x) {\n    test &lt;- cor.test(~ x + score, data = df)\n    data.frame(\n      cor.coef = round(test$estimate, 3),\n      p.value = round(test$p.value, 3)\n    ) \n  }) |&gt; \n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nf11\nf12\nf13\nf14\nf21\nf22\nf23\nf24\nf31\nf32\nf33\nf34\n\n\n\n\ncor.coef\n-0.166\n-0.226\n-0.24\n-0.089\n0.243\n-0.181\n0.227\n-0.276\n0.212\n-0.317\n0.237\n-0.316\n\n\np.value\n0.027\n0.002\n0.001\n0.241\n0.001\n0.016\n0.002\n0\n0.005\n0\n0.002\n0\n\n\n\n\n\nInternal consistency (reliability)\n\nselect(df, contains(\"f\")) |&gt; \n  item_reliability()\n\n#&gt;    term alpha_if_deleted item_discrimination\n#&gt; 1   f11            0.521               0.087\n#&gt; 2   f12            0.424               0.469\n#&gt; 3   f13            0.410               0.537\n#&gt; 4   f14            0.442               0.392\n#&gt; 5   f21            0.581              -0.078\n#&gt; 6   f22            0.452               0.388\n#&gt; 7   f23            0.556              -0.046\n#&gt; 8   f24            0.424               0.465\n#&gt; 9   f31            0.553              -0.037\n#&gt; 10  f32            0.466               0.314\n#&gt; 11  f33            0.569              -0.080\n#&gt; 12  f34            0.475               0.277\n\n\n\n\n30.2.3 Item response theory\nItem response theory (IRT) is a statistical framework used in psychometrics and educational testing to analyze the relationship between individual responses to assessment items (questions) and the latent traits or abilities those items aim to measure. Unlike traditional methods, IRT assumes that the probability of a specific response to an item is a function of both the item’s characteristics and the respondent’s underlying trait level. Here’s a structured breakdown:\n1. Core principles of IRT\nLatent trait: IRT models assume a latent trait (often referred to as “theta” or ability) that each respondent has, which influences their probability of a particular response. Examples of latent traits include intelligence, proficiency, or psychological characteristics like anxiety.\nItem characteristics: Each item has specific properties that affect how it discriminates between respondents with different levels of the trait. These properties include difficulty, discrimination, and sometimes guessing.\n2. Key IRT models\n1-parameter logistic model (1PL), also known as the Rasch model, which assumes that items differ only in difficulty. In the 1PL model, only the difficulty of the item is taken into account.\n\nP(X = 1 \\mid \\theta) = \\frac{e^{\\theta - b}}{1 + e^{\\theta - b}}\n\nwhere P(X = 1 \\mid \\theta) is the probability of a correct response (scored as 1), \\theta is the latent trait or ability of the person, b is the difficulty parameter of the item.\nThis model assumes that all items have the same discrimination, meaning that each item is equally good at distinguishing between individuals with different ability levels.\n2-parameter logistic model (2PL): This model adds an item discrimination parameter, allowing each item to have a different slope or discriminatory power.\n\nP(X = 1 \\mid \\theta) = \\frac{e^{a(\\theta - b)}}{1 + e^{a(\\theta - b)}}\n\nwhere a is the discrimination parameter, indicating how steeply the probability changes with ability around the difficulty level. Other parameters are as defined above.\nThe discrimination parameter a makes the model more flexible, allowing it to account for items that are more or less sensitive to changes in the respondent’s ability.\n3-parameter logistic model (3PL): This model includes an additional guessing parameter, which allows for a non-zero probability of correctly guessing the answer, even for low-ability individuals.\n\nP(X = 1 \\mid \\theta) = c + (1 - c) \\frac{e^{a(\\theta - b)}}{1 + e^{a(\\theta - b)}}\n\nwhere c is the guessing parameter, representing the probability of a correct answer by guessing. Other parameters are as defined above.\nThe guessing parameter c accounts for the chance that respondents with very low ability might still answer the item correctly by guessing, which is particularly relevant for multiple-choice items.\n3. Applications of IRT\nEducational testing: IRT helps create adaptive tests, where the difficulty of subsequent questions depends on previous answers, providing a more precise estimate of a student’s ability.\nPsychological assessments: Commonly used in psychological testing to refine and validate scales, such as those measuring depression, anxiety, or quality of life.\nTest development and scaling: Allows for comparability of scores across different versions of a test by linking scales.\n4. Advantages of IRT\nPrecision: Provides more detailed information about each item and the respondent’s ability level, compared to classical test theory.\nTest adaptability: Allows for computerized adaptive testing (CAT), which can significantly reduce the number of items required to estimate a trait level accurately.\nInvariance: The estimated item parameters remain stable across populations, assuming the model fits well, making scores more generalizable.\n5. Interpreting IRT models\nItem characteristic curve (ICC): Graphically represents the probability of a correct response as a function of the latent trait, showing how an item performs at different ability levels.\nItem information function: Shows how much information an item provides about a specific ability level, which is crucial in test construction.\n\n\ndata &lt;- select(df, contains(\"f\")) \nmod &lt;- mirt(data, model = 1, verbose = F)  \n# Plot ICC\nitemplot(mod, item = 1, type = 'trace') \n\n\n\n\n\n\n\n\n\ntheta &lt;- matrix(seq(-1,1, by = .1))\niteminfo(extract.item(mod, 2), theta)\n\n#&gt;  [1] 0.6214112 0.6176567 0.6140703 0.6115853 0.6109246 0.6125437 0.6166047\n#&gt;  [8] 0.6229832 0.6313063 0.6410172 0.6514569 0.6619519 0.6718958 0.6808104\n#&gt; [15] 0.6883796 0.6944497 0.6990002 0.7020954 0.7038293 0.7042820 0.7034962",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Medical scales</span>"
    ]
  },
  {
    "objectID": "scale-develop.html#reliability-validity-and-responsiveness",
    "href": "scale-develop.html#reliability-validity-and-responsiveness",
    "title": "30  Medical scales",
    "section": "30.3 Reliability, validity and responsiveness",
    "text": "30.3 Reliability, validity and responsiveness\nIn the development and evaluation of scales, reliability, validity, and responsiveness are key properties that help ensure the tool’s utility and accuracy for measuring the intended construct. Here’s a structured overview of these three essential analyses:\n\n30.3.1 Reliability\nReliability assesses the consistency of the scale over repeated applications or among different raters. Key methods include:\nInternal consistency: Evaluates the consistency of items within a single scale, often using Cronbach’s alpha. A high Cronbach’s alpha (usually ≥0.70) indicates good internal consistency.\nTest-retest reliability: Measures stability over time by administering the scale to the same subjects at two different points in time and calculating the correlation.\nInter-rater reliability: Assesses agreement among different raters, typically using Cohen’s kappa or intraclass correlation coefficient (ICC) for continuous measures.\n\n\n30.3.2 Validity\nValidity determines whether the scale accurately measures what it is intended to measure. Types of validity include:\nContent validity: Ensures the scale items adequately cover the entire construct. This is often established through expert judgment.\nConstruct validity: Examines whether the scale reflects the theoretical construct it aims to measure. This can include:\n\nConvergent validity: Assesses whether scores are related to those from other scales measuring the same or similar constructs.\nDiscriminant validity: Confirms that the scale does not correlate highly with measures of different constructs.\n\nCriterion validity: Looks at how well the scale predicts or correlates with an established criterion, which can be:\n\nConcurrent validity: Compared against a current criterion.\nPredictive validity: Tested by determining how well the scale predicts future outcomes.\n\n\nExample 2 \nA doctor used the WHOQOL-100 scale to survey the quality of life of 50 healthy individuals, and repeated the survey one week later. The results are recorded in the data below. F1 to F24 represent the scores for the 24 dimensions from the first survey, T1 is the total score from the first survey, and T2 is the total score from the second survey. Q1 represents the respondents’ overall rating of their quality of life from the first survey, with a maximum score of 100 points.\n\n\n  Download data \n\n\ndf &lt;- read_csv(\"datasets/ex30-02.csv\", show_col_types = F)  \n\nCronbach’s alpha\n\nselect(df, contains(\"f\")) |&gt; cronbachs_alpha()\n\n#&gt; [1] 0.8046482\n\nselect(df, contains(\"f\")) |&gt; alpha(warnings = F, discrete = F) |&gt; pluck(1)\n\n#&gt;  raw_alpha std.alpha   G6(smc) average_r      S/N        ase    mean\n#&gt;  0.8046482 0.7969285 0.9209604 0.1405358 3.924374 0.03665875 11.1575\n#&gt;        sd  median_r\n#&gt;  1.071537 0.1707912\n\n\nConstruct validity\n\ndata &lt;- select(df, contains(\"f\"))\n\nmodel &lt;- '\n  F1 =~ f1 + f2 + f3\n  F2 =~ f4 + f5 + f6 + f7 + f8\n  F3 =~ f9 + f10 + f11 + f12\n  F4 =~ f13 + f14 + f15\n  F5 =~ f16 + f17 + f18 + f19 + f20 + f21 + f22 + f23\n  F6 =~ f24\n'\nlavaan::cfa(model, data, estimator = \"ML\") \n\n#&gt; Warning: lavaan-&gt;lav_object_post_check():  \n#&gt;    some estimated lv variances are negative\n\n\n#&gt; lavaan 0.6-19 ended normally after 123 iterations\n#&gt; \n#&gt;   Estimator                                         ML\n#&gt;   Optimization method                           NLMINB\n#&gt;   Number of model parameters                        62\n#&gt; \n#&gt;   Number of observations                            50\n#&gt; \n#&gt; Model Test User Model:\n#&gt;                                                       \n#&gt;   Test statistic                               396.589\n#&gt;   Degrees of freedom                               238\n#&gt;   P-value (Chi-square)                           0.000\n\n\n\n\n30.3.3 Responsiveness\nThe responsiveness measures the ability of an instrument (like a survey or scale) to detect change over time, often in response to an intervention. This is particularly relevant in longitudinal studies, where the goal is to observe treatment effects or changes in condition.\nThere are several ways to quantify responsiveness, including standardized effect size measures, such as the standardized response mean, Cohen’s d, and responsiveness index. Minimal clinically important difference defines the smallest change that is meaningful to patients, thus aiding interpretation of scale changes. Here’s an overview of how to calculate some of these in R:\n\nExample 3 \nA psychiatrist developed a neurosis scale to evaluate improvements in neurotic symptoms among patients with psychological disorders. The scale was used to assess 72 psychiatric inpatients before and after treatment. Measurement data can be downloaded from the link below. Calculate the responsiveness of this scale.\n\n\n  Download data \n\nThe standardized response mean is calculated as the mean change score divided by the standard deviation of the change scores.\n\ndf &lt;- read_csv(\"datasets/ex30-03.csv\", show_col_types = F)  \ndf |&gt; \n  summarize(srm = mean(X2 - X1) / sd(X2 - X1))\n\n#&gt; # A tibble: 1 × 1\n#&gt;     srm\n#&gt;   &lt;dbl&gt;\n#&gt; 1 0.842\n\n\nCohen’s d is another standardized effect size, which is the mean difference between two time points divided by the pooled standard deviation. We can use the cohens_d() function from effectsize packages to compute the effect size indices for standardized differences due to different cases:\n\ncohens_d(\"X2\", \"X1\", data = df, paired = T, verbose = F)\n\n#&gt; Cohen's d |       95% CI\n#&gt; ------------------------\n#&gt; 0.84      | [0.57, 1.11]\n\n\nEach of these properties is crucial for ensuring that a scale is not only statistically sound but also practically applicable for its intended clinical or research purpose. Evaluating reliability, validity, and responsiveness through carefully chosen statistical analyses provides confidence in the scale’s results and insights into patient outcomes.",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Medical scales</span>"
    ]
  },
  {
    "objectID": "scale-develop.html#medical-application-of-scales",
    "href": "scale-develop.html#medical-application-of-scales",
    "title": "30  Medical scales",
    "section": "30.4 Medical application of scales",
    "text": "30.4 Medical application of scales\n\nExample 4 \nIn a clinical study evaluating the efficacy of two chemotherapy regimens, quality of life was used as an outcome measure. Cancer patients were randomly divided into two groups: Group A received a low-dose combination chemotherapy regimen, while Group B received a high-dose monotherapy regimen. Each group included 10 patients, and quality of life was assessed 1 month, 3 months, and 12 months after chemotherapy. The scale consisted of 5 items, with each item scored from a minimum of 1 to a maximum of 5, resulting in a total score range of 5 to 25. The survey results for both groups are recorded in the data below.\n\n\n  Download data \n\n\ndf &lt;- read_csv(\"datasets/ex30-04.csv\", show_col_types = F)  \n\n\nSeparate a character column into multiple columns with a regular expression.\n\nThe primary concern of the analysis is whether there is a difference in treatment effects (quality of life) between the two groups. This difference includes whether the trend in changes after treatment is the same—that is, whether the rise and fall of the curves are consistent—and whether the quality of life levels of patients after treatment are similar, meaning whether the two curves coincide on the vertical axis. Here the profile analysis method is applied to the data.\n\ntb &lt;- df |&gt; \n  pivot_longer(everything(), names_to = \"time\", values_to = \"score\") |&gt; \n  separate(time, sep = \"_\", into = c(\"group\", \"time\"), convert = T) |&gt; \n  pivot_wider(\n    names_from = \"time\", names_prefix = \"t\",  \n    values_from = \"score\", values_fn = list) |&gt;   \n  unnest()\n\n#&gt; Warning: `cols` is now required when using `unnest()`.\n#&gt; ℹ Please use `cols = c(t1, t3, t12)`.\n\nres &lt;- with(tb, profileR::pbg(cbind(t1, t3, t12), group = group, original.names = T))\n\nres |&gt; summary()\n\n#&gt; Call:\n#&gt; profileR::pbg(data = cbind(t1, t3, t12), group = group, original.names = T)\n#&gt; \n#&gt; Hypothesis Tests:\n#&gt; $`Ho: Profiles are parallel`\n#&gt;   Multivariate.Test Statistic Approx.F num.df den.df    p.value\n#&gt; 1             Wilks 0.7471198 2.877024      2     17 0.08391063\n#&gt; 2            Pillai 0.2528802 2.877024      2     17 0.08391063\n#&gt; 3  Hotelling-Lawley 0.3384735 2.877024      2     17 0.08391063\n#&gt; 4               Roy 0.3384735 2.877024      2     17 0.08391063\n#&gt; \n#&gt; $`Ho: Profiles have equal levels`\n#&gt;             Df Sum Sq Mean Sq F value Pr(&gt;F)  \n#&gt; group        1  40.14   40.14   4.083 0.0584 .\n#&gt; Residuals   18 176.94    9.83                 \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; $`Ho: Profiles are flat`\n#&gt;          F df1 df2      p-value\n#&gt; 1 22.38023   2  17 1.728925e-05\n\npluck(res, \"data.summary\") |&gt; \n  t() |&gt; \n  as_tibble(rownames = \"group\") |&gt; \n  pivot_longer(\n    c(t1, t3 ,t12), names_to = \"time\", names_prefix = \"t\", \n    values_to = \"score\", names_transform = list(time = as.integer)) |&gt; \n  ggplot(aes(x = time, y = score, color = group, linetype = group)) +\n  geom_point(shape = 1, size = 2) +\n  geom_line() +\n  labs(x = \"Time(month)\", y = \"Average score\") +\n  theme(legend.title = element_blank())",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Medical scales</span>"
    ]
  },
  {
    "objectID": "meta-analysis.html",
    "href": "meta-analysis.html",
    "title": "31  Meta analysis",
    "section": "",
    "text": "31.1 Prerequisite\nMeta-analysis is a statistical technique used to combine results from multiple studies on the same topic, often in medicine or psychology to achieve a more accurate and comprehensive conclusion. By aggregating findings, meta-analysis allows researchers to identify patterns, assess overall effects, and make generalizations that may be more robust than results from any single study.\nlibrary(tidyverse)\nlibrary(metafor)\nlibrary(meta)",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Meta analysis</span>"
    ]
  },
  {
    "objectID": "meta-analysis.html#introduction-to-meta-analysis",
    "href": "meta-analysis.html#introduction-to-meta-analysis",
    "title": "31  Meta analysis",
    "section": "31.2 Introduction to meta-analysis",
    "text": "31.2 Introduction to meta-analysis\nMeta-analysis offers a comprehensive overview of a particular research question by quantitatively integrating data from individual studies. By pooling results, meta-analysis enhances statistical power and provides a more precise estimate of the overall effect.\n\n31.2.1 Steps in a meta-analysis\nBelow are the key steps involved in conducting a meta-analysis, outlining a systematic approach that ensures reliability and rigor.\n1. Formulating the research question and defining inclusion criteria\nThe process begins by formulating a clear and focused research question. This typically includes specifying the population, intervention, comparison, and outcomes (PICO) of interest. Next, inclusion and exclusion criteria are established to determine which studies are relevant to the meta-analysis. These criteria may be based on study design, population characteristics, type of intervention, and outcome measures. Clear criteria at the outset help ensure consistency and relevance throughout the analysis.\n2. Conducting a comprehensive literature search\nA thorough literature search is conducted across relevant databases (such as PubMed, Cochrane Library, PsycINFO, and Google Scholar) to identify all studies that meet the inclusion criteria. Grey literature, including unpublished studies, dissertations, and conference abstracts, is also reviewed to reduce publication bias. Every search strategy and its outcomes are documented to provide transparency in the search process.\n3. Screening studies and assessing eligibility\nAfter collecting potential studies, an initial screening of titles and abstracts is performed to identify eligible studies. Full-text reviews follow for studies that pass the initial screen, applying inclusion and exclusion criteria rigorously. Throughout this stage, it’s essential to maintain a record of excluded studies along with the reasons for their exclusion.\n4. Extracting data\nData extraction entails gathering key information from each study, such as sample sizes, effect sizes, standard deviations, p-values, and any other relevant characteristics. A structured data extraction form helps ensure accuracy and consistency. This step may also involve collecting study design information and indicators of study quality to support subsequent analyses.\n5. Assessing study quality and risk of bias\nStudy quality and risk of bias are evaluated to assess the reliability of each included study. Standard tools such as the Cochrane risk of bias tool (for randomized trials) or the Newcastle-Ottawa scale (for observational studies) are commonly used for this purpose. Evaluating study quality helps interpret the results accurately, highlighting potential sources of bias that may affect the analysis.\n6. Selecting a meta-analytic model\nThe fixed-effect model assumes that all studies share the same true effect size. A weighted average is calculated using weights that are the inverse of the standard error. The fixed-effect model is appropriate when heterogeneity among studies is low. The random-effects model assumes study-specific effect sizes vary randomly around a mean effect, which may arise from differences in populations, interventions, or conditions. The weighting incorporates both within-study and between-study variance.\nIf heterogeneity is substantial (e.g., I² &gt; 50%), the random-effects model is typically chosen. A random-effects model is often preferred for meta-analyses as it allows for generalizing results across varied studies.\n7. Calculating effect sizes\nEach study’s effect size is calculated and standardized to allow for consistent comparisons across studies. Effect sizes such as Cohen’s d (for continuous outcomes), odds ratios or relative risk, or correlation coefficients (for correlational studies) are common choices. These effect sizes, along with their variances, are essential for combining study results in the next steps.\n8. Pooling results\nThe calculated effect sizes are then pooled to obtain an overall effect estimate. Statistical software can be used to perform this pooling, applying weights based on study precision or sample size, giving greater influence to more reliable studies. This pooling provides a summary effect size that reflects the cumulative evidence across studies.\n9. Assessing heterogeneity\nHeterogeneity, or the variation in study outcomes, is assessed using statistical tests. Cochran’s Q test determines if variability is likely due to chance, while the I² statistic quantifies the proportion of variability due to heterogeneity rather than random error. High heterogeneity suggests differences in study populations, interventions, or methodologies, indicating a need for further investigation or subgroup analysis.\nA large Q and a p-value &lt; 0.05 suggest significant heterogeneity. An I² of 0-40% suggests low heterogeneity, 30-60% moderate, 50-90% substantial, and 75-100% considerable heterogeneity.\n10. Checking for publication bias\nPublication bias is assessed to ensure that the meta-analysis results are not skewed by the absence of unpublished or null-result studies. Funnel plots visually display asymmetry, indicating potential bias, while statistical tests like Egger’s test further evaluate its likelihood. Addressing publication bias strengthens the validity of the results.\n11. Performing sensitivity and subgroup analyses\nSensitivity analyses test the robustness of the results by excluding certain studies (e.g., high-risk studies or outliers) to evaluate their influence on the overall findings. Subgroup analyses examine differences in effects across categories, such as age, gender, or intervention type, providing insights into how study characteristics may impact results.\n12. Interpreting results\nThe results of the meta-analysis are interpreted with attention to the effect size, confidence intervals, heterogeneity, and potential biases. The overall significance of the findings is discussed, and the practical implications of the effect size are considered, not just its statistical significance. Any limitations of the analysis are also acknowledged.\n13. Reporting findings\nA final report or publication is prepared, summarizing the research question, methodology, results, and conclusions. Following guidelines such as PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) ensures a transparent and complete presentation. This report allows others to understand, evaluate, and replicate the findings, further enhancing the impact of the meta-analysis.\nIn conclusion, a well-conducted meta-analysis offers valuable insights by systematically integrating findings from multiple studies. Through careful planning and rigorous methodology, it provides a powerful tool for synthesizing evidence, informing practice, and guiding future research.\n\n\n31.2.2 R packages\nThere are several packages in R that facilitate meta-analysis. The meta is a comprehensive package for conducting meta-analyses, including fixed- and random-effects models, subgroup analysis, and meta-regression. The metafor offers advanced functionalities, including different effect size calculations, mixed-effects models, and meta-regression. The robumeta supports robust variance estimation for meta-analyses where there are dependent effect sizes.",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Meta analysis</span>"
    ]
  },
  {
    "objectID": "meta-analysis.html#pooling-odds-ratios",
    "href": "meta-analysis.html#pooling-odds-ratios",
    "title": "31  Meta analysis",
    "section": "31.3 Pooling odds ratios",
    "text": "31.3 Pooling odds ratios\nPooling odds ratios is a common approach in meta-analysis, especially for binary outcomes in case-control studies. The process involves selecting effect sizes, here is odds rations, testing for heterogeneity, choosing an appropriate model, and calculating a combined effect size.\nFor each study, calculate the odds ratio between the case and control groups, which reflects the association strength between exposure and outcome.\n\n\\text{OR} = \\frac{\\text{odds of event in exposed group}}{\\text{odds of event in control group}} = \\frac{\\frac{a}{b}}{\\frac{c}{d}} = \\frac{a \\cdot d}{b \\cdot c}\n\nwhere a is the number of cases with exposure and outcome, b is the number of cases with exposure but no outcome, c is the number of cases without exposure but with outcome, d is the number of cases without exposure and without outcome.\nFor ease in calculations and to meet normality assumptions, the OR is transformed to its natural logarithm, resulting in a log OR. Calculate the standard error for the log OR as well.\n\nExample 1 \nPerform a meta-analysis on the 9 case-control studies of the relationship between extremely low-frequency electromagnetic fields and childhood leukemia in Table 31.1 .\n\n\ndf &lt;- read_csv(\"datasets/ex31-01.csv\", show_col_types = F) |&gt;\n  mutate(id = c(1:9), .before = 1) \n\ndf |&gt; knitr::kable(align = \"c\")\n\n\n\nTable 31.1: Data of 9 case-control studies\n\n\n\n\n\n\nid\na\nb\nc\nd\n\n\n\n\n1\n63\n92\n29\n126\n\n\n2\n103\n95\n113\n112\n\n\n3\n25\n218\n17\n195\n\n\n4\n18\n162\n25\n252\n\n\n5\n27\n70\n52\n207\n\n\n6\n14\n70\n15\n126\n\n\n7\n29\n52\n16\n61\n\n\n8\n122\n89\n92\n113\n\n\n9\n49\n55\n30\n74\n\n\n\n\n\n\n\n\n\n31.3.1 metafor package\nThe metafor package can be used for OR pooling. Here we use the escalc() function to calculate the log odds ratios and their standard errors from the data.\n\ndat &lt;- escalc(measure = \"OR\", ai = a, bi = b, ci = c, di = d, data = df) \ndat\n\n#&gt; \n#&gt;   id   a   b   c   d     yi     vi \n#&gt; 1  1  63  92  29 126 1.0903 0.0692 \n#&gt; 2  2 103  95 113 112 0.0720 0.0380 \n#&gt; 3  3  25 218  17 195 0.2742 0.1085 \n#&gt; 4  4  18 162  25 252 0.1133 0.1057 \n#&gt; 5  5  27  70  52 207 0.4288 0.0754 \n#&gt; 6  6  14  70  15 126 0.5188 0.1603 \n#&gt; 7  7  29  52  16  61 0.7543 0.1326 \n#&gt; 8  8 122  89  92 113 0.5210 0.0392 \n#&gt; 9  9  49  55  30  74 0.7874 0.0854\n\n\nThis function calculates various effect sizes or outcome measures (with argument measure = \"OR\") and the corresponding sampling variances that are commonly used in meta-analysis.\nThen use the rma() function to conduct a meta-analysis. You can choose a fixed-effects model (method=\"FE\") or a random-effects model (e.g. method=\"REML\").\n\nres &lt;- rma(yi, vi, data = dat, slab = 1:nrow(dat), method = \"FE\") \nres |&gt; summary()\n\n#&gt; \n#&gt; Fixed-Effects Model (k = 9)\n#&gt; \n#&gt;   logLik  deviance       AIC       BIC      AICc   \n#&gt;  -3.5880   13.1805    9.1760    9.3732    9.7474   \n#&gt; \n#&gt; I^2 (total heterogeneity / total variability):   39.30%\n#&gt; H^2 (total variability / sampling variability):  1.65\n#&gt; \n#&gt; Test for Heterogeneity:\n#&gt; Q(df = 8) = 13.1805, p-val = 0.1058\n#&gt; \n#&gt; Model Results:\n#&gt; \n#&gt; estimate      se    zval    pval   ci.lb   ci.ub      \n#&gt;   0.4719  0.0899  5.2494  &lt;.0001  0.2957  0.6481  *** \n#&gt; \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nHere’s a breakdown of the meta-analysis results using the metafor package:\n\nModel summary:\n\nModel type: A fixed-effects model was used, based on data from 9 studies (k = 9).\n\n\n\nLog likelihood and deviance\n\nlogLik: -3.5880 – the log likelihood of the model.\nDeviance: 13.1805 – measures the goodness of fit, where lower deviance values indicate better fit.\nAIC (Akaike Information Criterion): 9.1760 – a measure for model comparison, where lower values are generally preferred.\nBIC (Bayesian Information Criterion): 9.3732 – similar to AIC but penalizes model complexity more, with lower values indicating better fit.\nAICc: 9.7474 – a corrected AIC value for small sample sizes.\n\n\nHeterogeneity assessment:\n\nI² (39.30%): This indicates that approximately 39.3% of the total variability in effect sizes is due to heterogeneity (differences between studies) rather than sampling error. This value suggests some moderate heterogeneity across studies.\n\n\n\nH² (1.65): Reflects the ratio of total variability to sampling variability. A value of 1.65 indicates that the total variability is moderately higher than sampling variability, which aligns with the I² result.\nTest for heterogeneity: Q-statistic: 13.1805, with a p-value of 0.1058. Since the p-value is above 0.05, there isn’t statistically significant evidence of heterogeneity among studies. This suggests that the studies’ effect sizes are relatively similar.\n\nModel results:\n\nEstimate: 0.4719 – This is the log odds ratio, indicating the overall estimated effect across studies.\nStandard Error (SE): 0.0899 – Measures the precision of the overall estimate. Lower SE values indicate more precise estimates.\nz-value: 5.2494 – Measures the significance of the estimate. A high z-value indicates a strong effect.\np-value: &lt; 0.0001 – This is highly significant, meaning that the overall effect is statistically significant.\n95% Confidence interval (CI): [0.2957, 0.6481] – The interval suggests that the true log odds ratio lies between 0.2957 and 0.6481 with 95% confidence.\n\nGiven the highly significant p-value and positive confidence interval bounds, this suggests a positive effect, with evidence that there is a statistically significant association or effect size across the studies analyzed.\n\nYou can create a forest plot to visualize individual and overall effect sizes and confidence intervals.\n\nforest(\n  res, atransf= exp, addpred = T, refline = 0, xlab = \"Odds Ratio\", \n  cex = 0.8, col = \"blue\", border = \"blue\", mlab = \"Pooling\",\n  header  = c(\"Study\", \"OR [95% CI]\"), ilab = cbind(a, b, c, d), \n  ilab.xpos = c(-3.1,-2.7,-2.3,-1.9), shade=\"zebra\")\n\ntext(c(-3.1,-2.7,-2.3,-1.9), 11, c(\"a\", \"b\", \"c\", \"d\"), cex = 0.9, font=2)\n\n\n\n\n\n\n\n\nInterpreting the forest plot:\n\nIndividual studies: Each row represents a study, with the study’s effect size and confidence interval displayed as a horizontal line.\nOverall effect: The diamond shape at the bottom shows the pooled effect size from the meta-analysis.\nLine of no effect: This is often set to 1 for odds ratios (or 0 for mean differences). Points that cross this line are not statistically significant individually.\n\nThe pooled OR indicates the overall risk in the exposed group relative to the control group. If OR &gt; 1, the exposure is associated with a higher risk; if OR &lt; 1, it’s associated with a lower risk. Results are statistically significant if the confidence interval does not include 1.\nTo create a funnel plot, you can use the funnel() function on your meta-analysis model object. The funnel plot helps to visually assess publication bias by plotting each study’s effect size against its standard error.\n\nfunnel(\n  res, yaxis = \"sei\", pch = 1, label = T, legend = F, cex = 0.8,  \n  cex.axis = 0.8, cex.lab = 0.8, atransf= exp, xlab = \"Odds Ratio\")\n\n\n\n\n\n\n\n\nInterpreting the funnel plot:\n\nSymmetry: In a funnel plot, symmetric patterns (a balanced “inverted funnel” shape) typically indicate low publication bias.\nAsymmetry: If the plot is asymmetric, with more studies clustered on one side, it might suggest the presence of publication bias or small-study effects.\nScatter: Points closer to the top of the funnel (representing studies with smaller standard errors and larger sample sizes) tend to be more concentrated around the overall effect estimate, while points further down (with larger standard errors) are expected to be more spread out.\n\nIf you notice asymmetry in your funnel plot, you may consider conducting further statistical tests for publication bias, such as Egger’s test or the trim-and-fill method.\n\n\n31.3.2 meta package\nThe meta package is widely used for conducting meta-analysis, offering a variety of functions to analyze different types of data, such as binary data and continuous data. The metabin() function is used for binary (event/non-event) data, allowing estimation of odds ratio (OR), risk ratio (RR), or risk difference (RD).\n\nres &lt;- metabin(\n  event.e = a, n.e = a + b, event.c = c, n.c = c + d,\n  sm = \"OR\", method = \"Inverse\", data = df, studlab = id)\n\nsummary(res)\n\n#&gt;       OR           95%-CI %W(common) %W(random)\n#&gt; 1 2.9753 [1.7769; 4.9817]       11.7       12.1\n#&gt; 2 1.0746 [0.7333; 1.5748]       21.3       16.2\n#&gt; 3 1.3154 [0.6897; 2.5090]        7.4        9.2\n#&gt; 4 1.1200 [0.5922; 2.1181]        7.6        9.3\n#&gt; 5 1.5354 [0.8964; 2.6299]       10.7       11.5\n#&gt; 6 1.6800 [0.7665; 3.6824]        5.0        6.9\n#&gt; 7 2.1262 [1.0414; 4.3408]        6.1        8.0\n#&gt; 8 1.6837 [1.1424; 2.4813]       20.6       16.0\n#&gt; 9 2.1976 [1.2392; 3.8971]        9.5       10.7\n#&gt; \n#&gt; Number of studies: k = 9\n#&gt; Number of observations: o = 3008 (o.e = 1353, o.c = 1655)\n#&gt; Number of events: e = 839\n#&gt; \n#&gt;                          OR           95%-CI    z  p-value\n#&gt; Common effect model  1.6031 [1.3441; 1.9120] 5.25 &lt; 0.0001\n#&gt; Random effects model 1.6367 [1.2891; 2.0781] 4.04 &lt; 0.0001\n#&gt; \n#&gt; Quantifying heterogeneity (with 95%-CIs):\n#&gt;  tau^2 = 0.0534 [0.0000; 0.3306]; tau = 0.2310 [0.0000; 0.5750]\n#&gt;  I^2 = 39.3% [0.0%; 72.1%]; H = 1.28 [1.00; 1.89]\n#&gt; \n#&gt; Test of heterogeneity:\n#&gt;      Q d.f. p-value\n#&gt;  13.18    8  0.1058\n#&gt; \n#&gt; Details of meta-analysis methods:\n#&gt; - Inverse variance method\n#&gt; - Restricted maximum-likelihood estimator for tau^2\n#&gt; - Q-Profile method for confidence interval of tau^2 and tau\n#&gt; - Calculation of I^2 based on Q\n\n\nThe forest plot is commonly used in meta-analysis to display the effect size of each study along with the overall effect size.\n\nforest(\n  res, layout = \"RevMan5\", random = F, common = T, prediction = T, \n  text.predict = \"Prediction CI\", pooled.events = T, pooled.totals = T,\n  xlab = \"Odds Ratio\", ref = 1, header.line = \"below\", details = T,\n  fontsize = 10, fs.details = 8, squaresize = 0.5, colgap = \"2mm\", \n  just = \"center\", just.studlab = \"center\", col.diamond.common = \"red\")\n\n\n\n\n\n\n\n\nA funnel plot is used to assess publication bias. A symmetric funnel plot suggests low publication bias, whereas asymmetry indicates potential bias.\n\nfunnel(\n  res, type = \"standard\", cex = 0.8, cex.lab = 0.8, \n  cex.axis = 0.8, studlab = T)",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Meta analysis</span>"
    ]
  },
  {
    "objectID": "meta-analysis.html#pooling-risk-difference",
    "href": "meta-analysis.html#pooling-risk-difference",
    "title": "31  Meta analysis",
    "section": "31.4 pooling risk difference",
    "text": "31.4 pooling risk difference\nPooling risk difference is commonly used in meta-analysis to summarize the absolute effect of an intervention across various study populations. The risk difference itself measures the absolute difference in the probability of an event occurring between two groups, often an intervention group and a control group.\nThe risk difference (RD), also known as the absolute risk reduction, is calculated as the difference between the event rates (probabilities of an outcome) in two groups: the intervention group and the control group. This can be further expressed as:\n\n\\text{RD} = \\frac{a}{a + b} - \\frac{c}{c + d}\n\nwhere ais the number of events in the intervention group, b is the number of non-events in the intervention group, c is the number of events in the control group, and d is the number of non-events in the control group.\nRD &gt; 0 indicates a higher risk (or probability of the event) in the intervention group than in the control group. RD &lt; 0 indicates a lower risk in the intervention group than in the control group. RD = 0 suggests no difference in risk between the two groups. For example, if RD = 0.10 (or 10%), it suggests that the intervention group has a 10% higher absolute event rate compared to the control group.\n\nExample 2 \nTo investigate the efficacy of domestic ranitidine in treating peptic ulcers, four randomized controlled trials meeting the criteria were collected. Cimetidine was used as the control drug, and the study results are shown in Table 31.2 . Please perform a meta-analysis to combine the difference in ulcer healing rates between domestic ranitidine and the control drug.\n\n\ndf &lt;- read_csv(\"datasets/ex31-02.csv\", show_col_types = F) |&gt;\n  mutate(id = c(1:4), .before = 1) \n\ndf |&gt; knitr::kable(align = \"c\")\n\n\n\nTable 31.2: Data of 4 randomized controlled trials\n\n\n\n\n\n\nid\nn1\np1\nn2\np2\n\n\n\n\n1\n7\n1.0000\n14\n0.9286\n\n\n2\n36\n0.8333\n25\n0.8000\n\n\n3\n62\n0.8710\n64\n0.6875\n\n\n4\n32\n0.7813\n26\n0.6923\n\n\n\n\n\n\n\n\nTo perform pooling of risk differences, you can use the metabin() function from the meta package, specifying sm = \"RD\":\n\nres &lt;- metabin(\n  event.e = p1 * n1, n.e = n1, event.c = p2 * n2, n.c = n2,\n  sm = \"RD\", method = \"Inverse\", data = df, studlab = id)\nres |&gt; summary()\n\n#&gt;       RD            95%-CI %W(common) %W(random)\n#&gt; 1 0.0714 [-0.1548; 0.2976]       17.1       17.1\n#&gt; 2 0.0333 [-0.1652; 0.2318]       22.2       22.2\n#&gt; 3 0.1835 [ 0.0426; 0.3244]       44.0       44.0\n#&gt; 4 0.0890 [-0.1390; 0.3170]       16.8       16.8\n#&gt; \n#&gt; Number of studies: k = 4\n#&gt; Number of observations: o = 266 (o.e = 137, o.c = 129)\n#&gt; Number of events: e = 211.0026\n#&gt; \n#&gt;                          RD           95%-CI    z p-value\n#&gt; Common effect model  0.1152 [0.0218; 0.2087] 2.42  0.0157\n#&gt; Random effects model 0.1152 [0.0218; 0.2087] 2.42  0.0157\n#&gt; \n#&gt; Quantifying heterogeneity (with 95%-CIs):\n#&gt;  tau^2 = 0 [0.0000; 0.0496]; tau = 0 [0.0000; 0.2227]\n#&gt;  I^2 = 0.0% [0.0%; 84.7%]; H = 1.00 [1.00; 2.56]\n#&gt; \n#&gt; Test of heterogeneity:\n#&gt;     Q d.f. p-value\n#&gt;  1.75    3  0.6257\n#&gt; \n#&gt; Details of meta-analysis methods:\n#&gt; - Inverse variance method\n#&gt; - Restricted maximum-likelihood estimator for tau^2\n#&gt; - Q-Profile method for confidence interval of tau^2 and tau\n#&gt; - Calculation of I^2 based on Q\n#&gt; - Continuity correction of 0.5 in studies with zero cell frequencies\n\n\n\nforest(\n  res, layout = \"RevMan5\", random = F, common = T, prediction = T, \n  text.predict = \"Prediction CI\", pooled.events = T, pooled.totals = T,\n  xlab = \"Odds Ratio\", ref = 1, header.line = \"below\", details = T,\n  fontsize = 10, fs.details = 8, squaresize = 0.5, colgap = \"2mm\", \n  just = \"center\", just.studlab = \"center\", col.diamond.common = \"red\")\n\n\n\n\n\n\n\n\n\nfunnel(\n  res, type = \"standard\", cex = 0.8, pch = 1, cex.lab = 0.8, \n  cex.axis = 0.8, studlab = T, pos.studlab = 3)",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Meta analysis</span>"
    ]
  },
  {
    "objectID": "general-estimat-equation.html",
    "href": "general-estimat-equation.html",
    "title": "32  Generalized estimating equation",
    "section": "",
    "text": "32.1 Prerequisites\nGeneralized estimating equation (GEE) is a statistical technique used for analyzing correlated data, particularly in longitudinal studies or repeated measurements where observations are not independent. GEE is useful for handling a range of data distributions and works well in situations where measurements are taken multiple times on the same subjects.\nlibrary(tidyverse)\nlibrary(ordinal)\nlibrary(geepack)\nlibrary(repolr)",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Generalized estimating equation</span>"
    ]
  },
  {
    "objectID": "general-estimat-equation.html#introduction-of-gee",
    "href": "general-estimat-equation.html#introduction-of-gee",
    "title": "32  Generalized estimating equation",
    "section": "32.2 Introduction of GEE",
    "text": "32.2 Introduction of GEE\n\n32.2.1 Key features of GEE\nHandles repeated measures: GEE is well-suited for analyzing data from subjects measured multiple times, where these repeated measurements are likely to be correlated.\nFlexible modeling: GEE can handle various distributions, including binary, Poisson, and normal distributions.\nRobust Estimation: GEE provides consistent estimates even if the correlation structure is not perfectly specified.\n\n\n32.2.2 Correlation structures in GEE\nGEE requires specifying a correlation structure to model the relationship between repeated measures. Common structures include:\nIndependent: Assumes no correlation between measurements.\nExchangeable: Assumes a constant correlation between measurements from the same subject.\nAutoregressive: Assumes that correlations decrease as time points grow further apart.\nUnstructured: Allows each pair of measurements to have its own correlation, though it requires a larger sample size to estimate reliably.\n\n\n32.2.3 Applications of GEE\nGEE is widely used in fields like medicine, psychology, ecology, and social sciences for analyzing:\nClinical trials: Evaluating patient outcomes over multiple time points.\nMulti-center studies: Analyzing data from different locations while accounting for within-center correlations.\nLongitudinal research: Assessing changes over time, such as in developmental studies or tracking treatment effects.",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Generalized estimating equation</span>"
    ]
  },
  {
    "objectID": "general-estimat-equation.html#using-gee-in-r",
    "href": "general-estimat-equation.html#using-gee-in-r",
    "title": "32  Generalized estimating equation",
    "section": "32.3 Using GEE in R",
    "text": "32.3 Using GEE in R\n\nExample 1 \nA researcher randomly selected 149 patients who used a pain relief pump after gynecological abdominal surgery. The subjects were randomly divided them into a control group and an treatment group, with different postoperative care methods applied. The researcher observed the patients’ pain at five different time points and assessed the pain levels using a numerical rating scale. The question is whether the two methods differ in their effectiveness in relieving the patients’ pain.\n\n\n  Download data \n\n\ndf &lt;- read_csv(\"datasets/ex32-01.csv\", , show_col_types = F) \n\nThe pain ratings are categorized as ordinal data, with measurements taken at multiple time points, making it repeated measures data. For such data, the GEE approach is commonly used. It can handle ordinal data and accounts for the correlation within the data.\n\n32.3.1 geepack package\nThe geepack package provides the several functions to fit GEE models. The geeglm() is a general GEE function that can model various types of data, including binary, count, and continuous outcomes, with flexibility in specifying link functions. The ordgee() function is specifically tailored for ordinal response data within GEE models. This function handles ordinal outcomes by fitting a GEE model with an ordinal link, such as the cumulative logit model, which is suitable for ordered categorical data.\nThe following table shows the results of model:\n\ndata &lt;- df |&gt; \n  pivot_longer(\n    cols = starts_with(\"t\"),\n    names_to = \"time\",\n    values_to = \"pain\",\n    names_prefix = \"t\"\n  ) |&gt; \n  mutate(\n    subject = as.integer(subject),\n    group = factor(group, labels = c(\"ctl\", \"trt\")),\n    time = as.integer(time)\n  ) |&gt; \n  as.data.frame()\n\ngeeglm(\n  pain ~ time + group + time:group, \n  id = subject, \n  corstr = \"unstructured\", \n  data = data\n) |&gt; \n  tidy()\n\n#&gt; # A tibble: 4 × 5\n#&gt;   term          estimate std.error statistic p.value\n#&gt;   &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1 (Intercept)     2.55      0.0718   1261.   0      \n#&gt; 2 time           -0.515     0.0295    306.   0      \n#&gt; 3 grouptrt       -0.345     0.113       9.26 0.00234\n#&gt; 4 time:grouptrt   0.0829    0.0437      3.60 0.0577\n\n\nThe model analyzes the effects of time, treat, and the interaction of time and treat on the response variable (pain degree) :\n\n(Intercept)\n\nEstimate: 2.549 Standard Error: 0.072 Statistic: 1261.13 p-value: 0.000\nInterpretation: The intercept represents the estimated average level of the response variable pain when both time and group are at their reference levels (e.g., at the baseline time point and in the control group). The p-value indicates that this estimate is highly statistically significant, meaning there’s strong evidence that the intercept is different from zero.\n\ntime\n\nEstimate: -0.5154 Standard Error: 0.02395 Statistic: 305.61 p-value: 0.000\nInterpretation: The coefficient for time represents the change in the response variable pain for each unit increase in time (e.g., for each additional time point). The negative estimate suggests that pain decreases over time. This effect is also highly statistically significant .\n\ngroup\n\nEstimate: -0.3446 Standard Error: 0.1132 Statistic: 9.26 p-value: 0.00234\nInterpretation: The coefficient for group (experimental group) indicates the difference in the response variable pain between the experimental group and the reference group (likely the control group). The negative estimate suggests that pain is lower in the experimental group compared to the control group. This difference is statistically significant (p-value &lt; 0.01).\n\ntime:group\n\nEstimate: 0.0829 Standard Error: 0.0437 Statistic: 3.6 p-value: 0.0577\nInterpretation: The interaction term time:group represents how the effect of time on pain differs between the experimental group and the control group. The positive estimate suggests a slight increase in pain over time in the experimental group compared to the control group. However, the p-value of 0.0577 indicates that this interaction is not statistically significant, meaning there’s no strong evidence that the effect of time on pain differs between the groups.\n\ngeepack::ordgee(\n  ordered(pain) ~ time * group, id = subject, \n  corstr = \"independence\", data = data) |&gt; \n  summary()\n\n#&gt; \n#&gt; Call:\n#&gt; geepack::ordgee(formula = ordered(pain) ~ time * group, id = subject, \n#&gt;     data = data, corstr = \"independence\")\n#&gt; \n#&gt; Mean Model:\n#&gt;  Mean Link:                 logit \n#&gt;  Variance to Mean Relation: binomial \n#&gt; \n#&gt;  Coefficients:\n#&gt;                    estimate san.se wald p\n#&gt; Inter:0       -4.903146e+01      0  Inf 0\n#&gt; Inter:1       -6.339917e+01      0  Inf 0\n#&gt; Inter:2       -7.956222e+01      0  Inf 0\n#&gt; Inter:3       -4.485464e+15      0  Inf 0\n#&gt; time           2.517124e+00      0  Inf 0\n#&gt; grouptrt      -1.437689e+02      0  Inf 0\n#&gt; time:grouptrt  3.265732e+01      0  Inf 0\n#&gt; \n#&gt; Scale is fixed.\n#&gt; \n#&gt; Correlation Model:\n#&gt;  Correlation Structure:     independence \n#&gt; \n#&gt; Returned Error Value:    0 \n#&gt; Number of clusters:   149   Maximum cluster size: 5\n\n\nThe geeglm() and ordgee() are both functions used to fit models for clustered or repeated measures data, but they differ in the types of data they handle and the specific models they fit.\ngeeglm()\nThis function is a more general function that fits GEE models for a wide variety of outcome distributions (e.g., Gaussian, binomial, Poisson). It is not restricted to ordinal data and can handle any type of response variable (continuous, binary, count, etc.) that can be modeled with a generalized linear model (GLM).\nIt allows you to specify the link function and the error distribution appropriate for your data (e.g., logit for binary outcomes, identity for continuous outcomes). It estimates population-averaged effects rather than subject-specific effects.\nUsage: It’s used for a broad range of data types and is versatile for many different kinds of clustered or repeated measures data.\nExample Scenario: If you’re analyzing binary outcomes (e.g., success/failure) collected at multiple time points across different clusters (e.g., hospitals), geeglm would be the appropriate choice.\nordgee()\nThis function is specifically designed for fitting GEE models to ordinal response data. This is useful when your outcome variable is ordinal (e.g., Likert scale data, ordered categorical data) and you have repeated measures or clustered data.\nIt accounts for the ordinal nature of the data by using a cumulative logit link function or other appropriate link functions for ordinal outcomes.\nUsage: It’s particularly used in situations where the response variable has a natural order but the distances between the categories are not assumed to be equal. It extends the GEE framework to handle the special case of ordinal outcomes.\nExample Scenario: If you’re analyzing patient satisfaction scores (e.g., 1 to 5) collected at multiple time points for different treatment groups, ordgee would be appropriate.\nKey Differences:\n\nOutcome Type: ordgee: Specifically for ordinal outcomes. geeglm: For a wide range of outcome types (continuous, binary, count, etc.).\nLink Functions: ordgee: Uses link functions appropriate for ordinal data (e.g., cumulative logit). geeglm: Allows specification of various link functions depending on the distribution of the response variable.\nFlexibility: ordgee: More specialized, focusing on ordinal outcomes. geeglm: More flexible, applicable to various types of response variables.\nInterpretation: ordgee: Focuses on the odds of being in a higher versus lower category of the ordinal outcome. geeglm: Focuses on population-averaged effects across the levels of the outcome.\n\nWhen to use which: Use geeglm() when your response variable is not ordinal, or you are dealing with binary, continuous, or count data, and you need to account for repeated measures or clustering. Use ordgee() when you have ordinal response data and need to account for repeated measures or clustering.\n\n\n32.3.2 repolr package\nThe package allows regression models to be fitted to repeated ordinal scores, for the proportional odds model, using a generalized estimating equation (GEE) methodology. The algorithm estimates the correlation parameter by minimizing the generalized variance of the regression parameters at each step of the fitting algorithm.\nThe repolr() function from the repolr package is used to handle this problem.\n\nrepolr(\n  pain ~ time * group, data = data, subjects = \"subject\", \n  times = c(1, 2, 3, 4, 5), categories = 4) |&gt; \n  summary()\n\n#&gt; \n#&gt; repolr: 2016-02-26 version 3.4 \n#&gt; \n#&gt; Call:\n#&gt; repolr(formula = pain ~ time * group, subjects = \"subject\", data = data, \n#&gt;     times = c(1, 2, 3, 4, 5), categories = 4)\n#&gt; \n#&gt; Coefficients: \n#&gt;                coeff     se.robust  z.robust  p.value \n#&gt; cuts1|2         -3.5676    0.2590   -13.7745    0.0000\n#&gt; cuts2|3          0.2612    0.2238     1.1671    0.2432\n#&gt; cuts3|4          3.8177    0.7406     5.1549    0.0000\n#&gt; time             1.3319    0.1066    12.4944    0.0000\n#&gt; grouptrt         0.8689    0.3121     2.7840    0.0054\n#&gt; time:grouptrt   -0.1712    0.1357    -1.2616    0.2071\n#&gt; \n#&gt; Correlation Structure:  independence \n#&gt; Fixed Correlation:  0\n\n\nMain effects: Both time and group have statistically significant effects on pain. Pain decreases over time and is lower in the experimental group. Interaction: The interaction between time and group is not statistically significant, suggesting that the rate of change in pain over time does not differ significantly between the experimental and control groups.\nGEE is a powerful approach for repeated measures and longitudinal data analysis, allowing researchers to account for correlation within subjects and to draw meaningful conclusions about treatment effects or changes over time. It is robust to correlation misspecification, making it suitable for complex correlated data structures. It provides reliable parameter estimates across various data distributions. However, GEE is less effective with small sample sizes, where its estimations may be less reliable. The choice of correlation structure requires statistical expertise, as incorrect specification can impact results.\n\n\n32.3.3 ordinal package\nThis package facilitates analysis of ordinal (ordered categorical data) via cumulative link models (CLMs) and cumulative link mixed models (CLMMs). Robust and efficient computational methods gives speedy and accurate estimation. A wide range of methods for model fits aids the data analysis.\n\nclm(ordered(pain) ~ time * group, data = data) |&gt; \n  summary()\n\n#&gt; formula: ordered(pain) ~ time * group\n#&gt; data:    data\n#&gt; \n#&gt;  link  threshold nobs logLik  AIC     niter max.grad cond.H \n#&gt;  logit flexible  745  -696.28 1406.57 7(0)  1.24e-08 5.8e+02\n#&gt; \n#&gt; Coefficients:\n#&gt;               Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; time          -1.40000    0.09288 -15.073  &lt; 2e-16 ***\n#&gt; grouptrt      -0.83443    0.27030  -3.087  0.00202 ** \n#&gt; time:grouptrt  0.09529    0.10695   0.891  0.37296    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Threshold coefficients:\n#&gt;     Estimate Std. Error z value\n#&gt; 0|1  -5.2738     0.2922 -18.051\n#&gt; 1|2  -3.7553     0.2561 -14.666\n#&gt; 2|3   0.2093     0.1856   1.128\n#&gt; 3|4   3.7766     0.5929   6.369\n\n\n\nclmm(ordered(pain) ~ time * group + (1 | subject), data = data) |&gt;\n  summary()\n\n#&gt; Cumulative Link Mixed Model fitted with the Laplace approximation\n#&gt; \n#&gt; formula: ordered(pain) ~ time * group + (1 | subject)\n#&gt; data:    data\n#&gt; \n#&gt;  link  threshold nobs logLik  AIC     niter     max.grad cond.H \n#&gt;  logit flexible  745  -633.82 1283.64 484(1456) 3.41e-05 3.4e+02\n#&gt; \n#&gt; Random effects:\n#&gt;  Groups  Name        Variance Std.Dev.\n#&gt;  subject (Intercept) 2.887    1.699   \n#&gt; Number of groups:  subject 149 \n#&gt; \n#&gt; Coefficients:\n#&gt;               Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; time           -1.9697     0.1311 -15.025   &lt;2e-16 ***\n#&gt; grouptrt       -1.2470     0.4172  -2.989   0.0028 ** \n#&gt; time:grouptrt   0.1234     0.1224   1.008   0.3135    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Threshold coefficients:\n#&gt;     Estimate Std. Error z value\n#&gt; 0|1  -7.5386     0.4905 -15.368\n#&gt; 1|2  -5.3973     0.4195 -12.865\n#&gt; 2|3   0.2524     0.2953   0.855\n#&gt; 3|4   4.7771     0.6956   6.867\n\n\nCLMMs explicitly model random effects, which allows for subject-specific inferences. This means you can account for the variability between subjects, leading to more accurate estimates of the fixed effects.\nCLMMs provide estimates for both fixed effects (population-level effects) and random effects (subject-level variability), which can be useful if you’re interested in understanding both individual and group-level patterns.\nCLMMs are likelihood-based models, which can provide more efficient estimates when the model assumptions are met. CLMMs assume a particular distribution (e.g., normally distributed random effects), and if this assumption is violated, the model may not perform well.\nCLMMs can be more computationally intensive, especially with large datasets or complex models with many random effects.\nGEE is robust to misspecification of the correlation structure within clusters. It focuses on estimating population-averaged effects rather than subject-specific effects, which can be more stable when model assumptions are not fully met.\nGEE tends to be less computationally demanding compared to CLMM, making it a better choice for large datasets.\nGEE can handle various types of correlation structures, making it flexible for different kinds of clustered data.\nGEE does not model random effects, which means it does not provide subject-specific inferences. It’s primarily focused on population-averaged effects. • Less Efficient with Small Samples: GEE might be less efficient with small sample sizes compared to CLMM because it does not fully account for subject-level variability.\nWhen to choose which:\nUse GEE if you are more interested in population-averaged effects, have a large dataset, or want a method that is more robust to correlation structure misspecification.\nUse CLMM if you are interested in subject-specific inferences, need to account for random effects, and are working with a relatively smaller dataset where computational demands are manageable.\nIn summary, if your primary interest is in understanding individual-level variability and you can handle the computational load, CLMM might be the better choice. If you’re more interested in population-level trends and want a more robust approach with less concern for the specific correlation structure, GEE would be a better option.",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Generalized estimating equation</span>"
    ]
  },
  {
    "objectID": "data-preprocess.html",
    "href": "data-preprocess.html",
    "title": "33  Data preprocessing",
    "section": "",
    "text": "33.1 Prerequisites\nlibrary(tidyverse)\nlibrary(outliers)\nlibrary(dataMaid)\nlibrary(validate)\nlibrary(pointblank)\nlibrary(assertr)\nlibrary(janitor)\nlibrary(VIM)",
    "crumbs": [
      "Practical skills",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Data preprocessing</span>"
    ]
  },
  {
    "objectID": "data-preprocess.html#data-validation",
    "href": "data-preprocess.html#data-validation",
    "title": "33  Data preprocessing",
    "section": "33.2 Data validation",
    "text": "33.2 Data validation\nData validation is a critical process in ensuring data accuracy, completeness, and consistency, especially before analysis or modeling. It improves the reliability of results and ensures data quality. Below are key aspects and methods for data validation:\n\n33.2.1 Data integrity checks\n1. Duplicate records\nIdentify and remove duplicate records, especially in unique identifiers to ensure each record is unique.\n\ndf &lt;- data.frame(\n  id = c(1:5,5),\n  weight_kg = c(70, 75, 80, 65, 72, 72),\n  height_cm = c(170, 175, 180, 165, 177, 177)\n)\n1length(unique(df$id)) == nrow(df)\n\nx &lt;- c(9:20, 1:5, 3:7, 0:8)\n2x[!duplicated(x)]\nunique(x)\ndf |&gt; \n  filter(!duplicated(id))\n\n\n1\n\nCheck the id column is unique or not\n\n2\n\nExtract unique elements\n\n\n\n\n#&gt; [1] FALSE\n#&gt;  [1]  9 10 11 12 13 14 15 16 17 18 19 20  1  2  3  4  5  6  7  0  8\n#&gt;  [1]  9 10 11 12 13 14 15 16 17 18 19 20  1  2  3  4  5  6  7  0  8\n#&gt;   id weight_kg height_cm\n#&gt; 1  1        70       170\n#&gt; 2  2        75       175\n#&gt; 3  3        80       180\n#&gt; 4  4        65       165\n#&gt; 5  5        72       177\n\n\n2. Range and logical checks\nEnsure data falls within reasonable ranges. For example, age values should typically range between 0 and 120, and negative income values may indicate input errors.\n\nx &lt;- c(34, 55, 130, 15)\n1x_is_in_range &lt;- (x &gt;= 0) & (x &lt;= 120)\nx_is_in_range\n\nx_ifelse &lt;- ifelse((x &gt;= 0) & (x &lt;= 120), \"Yes\", \"No\")\nx_ifelse\n\nx &lt;- c(34, 55, 130, 15)\ny &lt;- c(3, 5, 2, 5)\n2x_and_y &lt;- (x &gt;= 0 & x &lt;= 120) & (y == 5)\nx_and_y\n\n\n1\n\nRange check\n\n2\n\nLogical check\n\n\n\n\n#&gt; [1]  TRUE  TRUE FALSE  TRUE\n#&gt; [1] \"Yes\" \"Yes\" \"No\"  \"Yes\"\n#&gt; [1] FALSE  TRUE FALSE  TRUE\n\n\n\n\n33.2.2 Data consistency checks\n1. Type consistency\nEnsure that data types are consistent for the same variable, such as “Gender” containing only values like “Male” or “Female.”\n\ndf &lt;- data.frame(\n  id = 1:4,\n  gender = c(\"Male\", \"Female\", \"Female\", \"Make\"),\n  birth_date = c(\"2003-10-01\", \"2003-13-25\", \"2011/01/01\", \"2000-10-04\")\n)\ndf\n\n#&gt;   id gender birth_date\n#&gt; 1  1   Male 2003-10-01\n#&gt; 2  2 Female 2003-13-25\n#&gt; 3  3 Female 2011/01/01\n#&gt; 4  4   Make 2000-10-04\n\n# Check gender format\ndf$gender &lt;- factor(df$gender, levels = c(\"Male\", \"Female\"))\nis_valid_gender &lt;- !is.na(df$gender)\ndf$valid_gender &lt;- is_valid_gender\n\n# Check date format\ndf$birth_date &lt;- as.Date(df$birth_date, format=\"%Y-%m-%d\")\nis_valid_date &lt;- !is.na(df$birth_date)\ndf$valid_date &lt;- is_valid_date\ndf\n\n#&gt;   id gender birth_date valid_gender valid_date\n#&gt; 1  1   Male 2003-10-01         TRUE       TRUE\n#&gt; 2  2 Female       &lt;NA&gt;         TRUE      FALSE\n#&gt; 3  3 Female       &lt;NA&gt;         TRUE      FALSE\n#&gt; 4  4   &lt;NA&gt; 2000-10-04        FALSE       TRUE\n\n# Check date type\ndf |&gt; str()\n\n#&gt; 'data.frame':    4 obs. of  5 variables:\n#&gt;  $ id          : int  1 2 3 4\n#&gt;  $ gender      : Factor w/ 2 levels \"Male\",\"Female\": 1 2 2 NA\n#&gt;  $ birth_date  : Date, format: \"2003-10-01\" NA ...\n#&gt;  $ valid_gender: logi  TRUE TRUE TRUE FALSE\n#&gt;  $ valid_date  : logi  TRUE FALSE FALSE TRUE\n\n\n2. Unit consistency\nEnsure that data is consistent in units, especially when values might be recorded in different units (e.g., heights, weights).\n\ndf &lt;- data.frame(\n  id = 1:5,\n  weight_kg = c(70, 75, 80, 65, 72),\n  height_cm = c(170, 175, 1.8, 165, 1.72)\n)\n\nYou can write functions to automatically check unit consistency. For example, check if height is all in centimeters.\n\ncheck_height_units &lt;- function(df, x, unit) {\n  if (!(x %in% names(df))) {\n    stop(paste(\"Column\", x, \"does not exist in the dataframe\"))\n  }\n  \n  if (unit == \"cm\") {\n    out_of_range &lt;- which(!(df[[x]] &gt; 100 & df[[x]] &lt; 220))\n    if (length(out_of_range) == 0) {\n      return(TRUE)\n    } else {\n      message(\n        \"The following rows have heights outside the range: \", \n        paste(out_of_range, collapse = \", \")\n      )\n      return(FALSE)\n    }\n  } else {\n    stop(\"Unsupported unit\")\n  }\n}\n\n# Run the check\nis_cm &lt;- check_height_units(df, \"height_cm\", \"cm\")\n\n#&gt; The following rows have heights outside the range: 3, 5\n\nis_cm\n\n#&gt; [1] FALSE\n\n\nThis function will display a message listing the rows with values outside the expected range, making it easier to locate and address discrepancies.\n\n\n33.2.3 Missing values\nHandling and identifying missing data is crucial in data analysis and statistical modeling, as missing values can introduce bias or instability in the model. Below is an overview of methods for identifying, classifying, and handling missing data.\n1. Identifying missing data\n\nx &lt;- c(0:4)\n1is.na(x) &lt;- c(2, 4)\nx                    \nanyNA(x) \nis.na(x)\n\n\n1\n\nSet the positions with indices 2 and 4 in the vector x to NA.\n\n\n\n\n#&gt; [1]  0 NA  2 NA  4\n#&gt; [1] TRUE\n#&gt; [1] FALSE  TRUE FALSE  TRUE FALSE\n\n\n\ndf &lt;- data.frame(\n  id = 1:6,\n  x1 = c(19.8, 23.5, 22.4, 21.4, 20.7, 21.6),\n  x2 = c(3.2, 2.4, 3.2, NA, 3.5, 2.9)\n)\n\n# Check the number of missing values in each column\nmissing_values &lt;- sapply(df, \\(x) sum(is.na(x)))\nmissing_values\n\n#&gt; id x1 x2 \n#&gt;  0  0  1\n\nmissing_values &lt;- colSums(is.na(df))\nmissing_values\n\n#&gt; id x1 x2 \n#&gt;  0  0  1\n\n# Check the percentage of missing values for each column\nmissing_percentage &lt;- (colSums(is.na(df)) / nrow(df)) * 100\nmissing_percentage\n\n#&gt;       id       x1       x2 \n#&gt;  0.00000  0.00000 16.66667\n\n\n\ndf |&gt; \n  mutate(\n    flag = is.na(x2),\n    label = ifelse(flag, id, NA)\n  ) |&gt; \n  filter(flag) |&gt; \n  select(\"ID\" = id, \"value\" = x2) \n\n#&gt;   ID value\n#&gt; 1  4    NA\n\n\n2. Types of missing data\nThere are three main types of missing data, based on the missing data mechanism:\n(1) Missing completely at random (MCAR): The missingness is unrelated to observed or unobserved data. For example, data lost due to a system error.\n(2) Missing at random (MAR): The missingness is related to other observed variables, but not to the missing value itself. For example, income might be related to education level, and people with higher income are less likely to disclose their age.\n(3) Missing not at random (MNAR): The missingness is related to the missing value itself. For example, lower-income individuals may be more likely to omit income information.\n3 Methods for determining the type of missing data\nHere is a data with missing values.\n\ndf &lt;- read_csv(\"datasets/ex33-01.csv\", show_col_types = F)\n\n(1) Exploring associations with observed values\nFirst, you should create a missing value indicator variable (1 for missing, 0 for non-missing):\n\ndf &lt;- mutate(df, indicator = ifelse(is.na(max_pul), 1, 0))\n\nLogistic regression for missingness: Check if missingness in a variable depends on the observed values of other variables.\n\nglm(indicator ~ age + weight + time, data = df, family = binomial) |&gt; \n  summary()\n\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = indicator ~ age + weight + time, family = binomial, \n#&gt;     data = df)\n#&gt; \n#&gt; Coefficients:\n#&gt;              Estimate Std. Error z value Pr(&gt;|z|)\n#&gt; (Intercept) -12.08300    9.88699  -1.222    0.222\n#&gt; age          -0.01656    0.11294  -0.147    0.883\n#&gt; weight        0.12499    0.08536   1.464    0.143\n#&gt; time          0.14855    0.36349   0.409    0.683\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 23.582  on 21  degrees of freedom\n#&gt; Residual deviance: 20.536  on 18  degrees of freedom\n#&gt; AIC: 28.536\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 5\n\n\nGroup Comparisons: Compare observed variable distributions for groups with and without missing values in another variable:\n\nt.test(y ~ indicator, data = df, var.equal = T)\n\n#&gt; \n#&gt;  Two Sample t-test\n#&gt; \n#&gt; data:  y by indicator\n#&gt; t = 0.81105, df = 20, p-value = 0.4269\n#&gt; alternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n#&gt; 95 percent confidence interval:\n#&gt;  -0.505975  1.149740\n#&gt; sample estimates:\n#&gt; mean in group 0 mean in group 1 \n#&gt;        6.355882        6.034000\n\n\n(2) R packages\nThe mcar() function from the mice package can be used to test whether missingness is contingent upon the observed variables:\n\ndata(nhanes, package = \"mice\")\nres &lt;- mice::mcar(nhanes, method = \"auto\") \n\n#&gt; \n#&gt;  iter imp variable\n#&gt;   1   1  bmi  hyp  chl\n#&gt;   1   2  bmi  hyp  chl\n#&gt;   1   3  bmi  hyp  chl\n#&gt;   1   4  bmi  hyp  chl\n#&gt;   1   5  bmi  hyp  chl\n#&gt;   2   1  bmi  hyp  chl\n#&gt;   2   2  bmi  hyp  chl\n#&gt;   2   3  bmi  hyp  chl\n#&gt;   2   4  bmi  hyp  chl\n#&gt;   2   5  bmi  hyp  chl\n#&gt;   3   1  bmi  hyp  chl\n#&gt;   3   2  bmi  hyp  chl\n#&gt;   3   3  bmi  hyp  chl\n#&gt;   3   4  bmi  hyp  chl\n#&gt;   3   5  bmi  hyp  chl\n#&gt;   4   1  bmi  hyp  chl\n#&gt;   4   2  bmi  hyp  chl\n#&gt;   4   3  bmi  hyp  chl\n#&gt;   4   4  bmi  hyp  chl\n#&gt;   4   5  bmi  hyp  chl\n#&gt;   5   1  bmi  hyp  chl\n#&gt;   5   2  bmi  hyp  chl\n#&gt;   5   3  bmi  hyp  chl\n#&gt;   5   4  bmi  hyp  chl\n#&gt;   5   5  bmi  hyp  chl\n\nres\n\n#&gt; \n#&gt; Missing data patterns: 2 used, 3 removed.\n#&gt; Cases used: 20 \n#&gt; \n#&gt; Hawkins' test: median chi^2 (4) = 1.590164, median p = 0.810559\n#&gt; \n#&gt; \n#&gt; Interpretation of results:\n#&gt;  Hawkins' test is not significant; there is no evidence to reject the assumptions of multivariate normality and MCAR.\n\n\nThe test is not significant, conclude that there is no evidence against multivariate normality of the data, nor against MCAR.\n\n\n\n\n\n\nNote\n\n\n\nAn MCAR test can only indicate whether missingness is MCAR or MAR. The procedure cannot distinguish MCAR from MNAR, so a non-significant result does not rule out MNAR. If the test is significant, it indicates that the data is not MCAR but could be MAR or MNAR.\n\n\nThe mcar_test() function from naniar package uses Little’s test statistic to assess if data is MCAR. The null hypothesis in this test is that the data is MCAR, and the test statistic is a chi-squared value.\n\nnaniar::mcar_test(nhanes)\n\n#&gt; # A tibble: 1 × 4\n#&gt;   statistic    df p.value missing.patterns\n#&gt;       &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;            &lt;int&gt;\n#&gt; 1      8.00     9   0.534                5\n\n\n4. Methods for handling missing data\n(1) Deletion methods\nRow deletion: Remove rows with missing values. This is suitable if missing values are minimal and are MCAR.\nColumn deletion: Remove columns with a high proportion of missing values, especially if those variables are not critical to the analysis.\n\n# Remove rows that contain missing values\ndf_row_del &lt;- na.omit(df)\ndf_row_del\n\n#&gt; # A tibble: 16 × 8\n#&gt;      id   age weight  time pulse max_pul     y indicator\n#&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1     2    44   85.8  8.65   156     168  7.24         0\n#&gt; 2     4    40   76.0 12.0    176     180  6.09         0\n#&gt; 3     5    44   81.4 13.1    174     176  5.26         0\n#&gt; 4     6    44   73.0 10.1    168     168  6.74         0\n#&gt; 5     7    45   66.4 11.1    176     176  5.97         0\n#&gt; 6     8    54   83.1 10.3    166     170  6.91         0\n#&gt; # ℹ 10 more rows\n\n# Removes columns that contain missing values\ndf_col_del = df[, colSums(is.na(df)) == 0]\ndf_col_del\n\n#&gt; # A tibble: 22 × 6\n#&gt;      id   age weight  time     y indicator\n#&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1     1    44   89.5 11.4   5.95         1\n#&gt; 2     2    44   85.8  8.65  7.24         0\n#&gt; 3     3    38   89.0  9.22  6.65         1\n#&gt; 4     4    40   76.0 12.0   6.09         0\n#&gt; 5     5    44   81.4 13.1   5.26         0\n#&gt; 6     6    44   73.0 10.1   6.74         0\n#&gt; # ℹ 16 more rows\n\n\n(2) Imputation methods\nMean imputation: Fill missing values with the mean of the variable. Useful when missing values are few and data is approximately normally distributed.\nMedian imputation: Use the median for imputation, suitable for skewed data.\nMode imputation: For categorical variables, fill missing values with the most frequent category.\nForward or backward fill: Use previous or next values for imputation in time-series data.\nInterpolation: Use linear or polynomial interpolation for continuous ordered data.\nMultiple imputation: Use multiple models (like regression models) to impute missing values multiple times, generating multiple datasets. This approach reduces uncertainty.\n\ndf &lt;- data.frame(\n  id = 1:6,\n  x1 = c(19.8, 23.5, 22.4, 21.4, 20.7, 21.6),\n  x2 = c(3.2, 2.4, 3.2, NA, 3.5, 2.9)\n)\n# mean\nmutate(df, x2_filled = ifelse(is.na(x2), mean(x2, na.rm = T), x2))\n\n#&gt;   id   x1  x2 x2_filled\n#&gt; 1  1 19.8 3.2      3.20\n#&gt; 2  2 23.5 2.4      2.40\n#&gt; 3  3 22.4 3.2      3.20\n#&gt; 4  4 21.4  NA      3.04\n#&gt; 5  5 20.7 3.5      3.50\n#&gt; 6  6 21.6 2.9      2.90\n\n# median\nmutate(df, x2_filled = ifelse(is.na(x2), median(x2, na.rm = T), x2))\n\n#&gt;   id   x1  x2 x2_filled\n#&gt; 1  1 19.8 3.2       3.2\n#&gt; 2  2 23.5 2.4       2.4\n#&gt; 3  3 22.4 3.2       3.2\n#&gt; 4  4 21.4  NA       3.2\n#&gt; 5  5 20.7 3.5       3.5\n#&gt; 6  6 21.6 2.9       2.9\n\n# mode\nmode &lt;- names(which.max(table(df$x2)))\nmutate(df, x2_filled = ifelse(is.na(x2), mode, x2))\n\n#&gt;   id   x1  x2 x2_filled\n#&gt; 1  1 19.8 3.2       3.2\n#&gt; 2  2 23.5 2.4       2.4\n#&gt; 3  3 22.4 3.2       3.2\n#&gt; 4  4 21.4  NA       3.2\n#&gt; 5  5 20.7 3.5       3.5\n#&gt; 6  6 21.6 2.9       2.9\n\n\n\n# interpolation\nwith(df, approx(x = which(!is.na(x2)), y = x2[!is.na(x2)], xout = seq_along(x2))$y)\n\n#&gt; [1] 3.20 2.40 3.20 3.35 3.50 2.90\n\n\n\n# Multiple imputation using mice packages\nlibrary(mice)\n\n#&gt; \n#&gt; Attaching package: 'mice'\n\n\n#&gt; The following object is masked from 'package:stats':\n#&gt; \n#&gt;     filter\n\n\n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     cbind, rbind\n\nmice(df, m = 3, method = 'pmm', maxit = 50, seed = 500, printFlag = F) |&gt; \n  complete()\n\n#&gt;   id   x1  x2\n#&gt; 1  1 19.8 3.2\n#&gt; 2  2 23.5 2.4\n#&gt; 3  3 22.4 3.2\n#&gt; 4  4 21.4 2.4\n#&gt; 5  5 20.7 3.5\n#&gt; 6  6 21.6 2.9\n\n\n\nExample 1 \nThe data collected from 22 healthy middle-aged men include age (years), weight (kg), time required to run 2,000 meters (min), pulse rate during running (beats/min), maximum pulse rate during running (beats/min), and arterial oxygen partial pressure (kPa). The data are shown in Table 2. Among them, the pulse rate during running and the maximum pulse rate during running have 1 and 5 missing values, respectively. Analyze the missing values in the table.\n\n\n  Download data \n\n\ndf &lt;- read_csv(\"datasets/ex33-01.csv\", show_col_types = F)\n\n\nres &lt;- aggr(df, plot = F)\npluck(res, \"missings\") |&gt; as_tibble()\n\n#&gt; # A tibble: 7 × 2\n#&gt;   Variable Count\n#&gt;   &lt;chr&gt;    &lt;int&gt;\n#&gt; 1 id           0\n#&gt; 2 age          0\n#&gt; 3 weight       0\n#&gt; 4 time         0\n#&gt; 5 pulse        1\n#&gt; 6 max_pul      5\n#&gt; # ℹ 1 more row\n\n# Draw a summary map of missing values\nplot(\n  res, col = c('navyblue','red'), numbers = T, prop = F, combined = F,\n  varheight = T, border= \"lightblue\", cex.lab = 0.8, cex.axis = 0.8,\n  cex.numbers = 0.8, ylab=c(\"Histogram of missing data\", \"Pattern\")\n)\n\n\n\n\n\n\n\n\n\n# Plot the missing value matrix\nmatrixplot(df, cex.lab = 0.8, cex.axis = 0.8)\n\n#&gt; \n#&gt; Click in a column to sort by the corresponding variable.\n#&gt; To regain use of the VIM GUI and the R console, click outside the plot region.\n\n\n\n\n\n\n\n\n\n\nlibrary(naniar)\n\n#&gt; \n#&gt; Attaching package: 'naniar'\n\n\n#&gt; The following object is masked from 'package:validate':\n#&gt; \n#&gt;     all_complete\n\nvis_miss(df)       # Missing value visualization\ngg_miss_var(df)    # By missing variables\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(Amelia)\n\n#&gt; Loading required package: Rcpp\n\n\n#&gt; ## \n#&gt; ## Amelia II: Multiple Imputation\n#&gt; ## (Version 1.8.3, built: 2024-11-07)\n#&gt; ## Copyright (C) 2005-2024 James Honaker, Gary King and Matthew Blackwell\n#&gt; ## Refer to http://gking.harvard.edu/amelia/ for more information\n#&gt; ##\n\nmissmap(\n  df, col = c(\"red\", \"black\"), y.labels = paste0(\"s\", c(1:22)), \n  rank.order = F)\n\n#&gt; Warning: Unknown or uninitialised column: `arguments`.\n#&gt; Unknown or uninitialised column: `arguments`.\n\n\n\n\n\n\n\n\n\n\ndf |&gt; \n  mutate(flag = ifelse(is.na(max_pul), \"Yes\", \"No\")) |&gt; \n  select(y, flag) |&gt; \n  t.test(y ~ flag, data = _, var.equal = T)\n\n#&gt; \n#&gt;  Two Sample t-test\n#&gt; \n#&gt; data:  y by flag\n#&gt; t = 0.81105, df = 20, p-value = 0.4269\n#&gt; alternative hypothesis: true difference in means between group No and group Yes is not equal to 0\n#&gt; 95 percent confidence interval:\n#&gt;  -0.505975  1.149740\n#&gt; sample estimates:\n#&gt;  mean in group No mean in group Yes \n#&gt;          6.355882          6.034000\n\n\nThe p value of the t-test is &gt; 0.05, the variable max_pul are consistent with the MCAR hypothesis.\n(3) Model-based methods\nUse machine learning models (e.g., regression, random forest) to predict missing values. • Regression Imputation: Use other variables to build a regression model to predict missing values. • K-Nearest Neighbors (KNN): Impute values based on the nearest neighbors, suitable for smaller datasets. • Random Forest Imputation: Use a random forest model to predict missing values.\n(4) Modeling after imputation\nFor MAR and MNAR cases, simple imputation may be insufficient. Multiple imputation, which imputes values multiple times and creates multiple datasets, can account for imputation uncertainty. Results from models on these datasets are combined to produce a more robust outcome.\n(5) Indicator method\nIn some cases, it may be appropriate to create a new binary feature indicating whether the value is missing or not, incorporating missingness information into the model. This method is suitable for MNAR data.\n(6) Choosing a method\nMinimal missing data (&lt; 5%): Simple methods (e.g., mean or deletion) can work effectively.\nModerate missing data (5%-20%): Consider more sophisticated methods like interpolation, multiple imputation, or KNN.\nSubstantial missing data (&gt; 20%): Multiple imputation or model-based imputation is recommended. If missing data is severe, consider gathering additional data.\n\n\n33.2.4 Outlier checks\n1. Statistical methods\n(1) IQR method\nOutliers are typically defined as values that lie outside 1.5 times the interquartile range (IQR) from the first and third quartiles. This method is straightforward for numeric data.\n\nfind_outliers_iqr &lt;- function(x) {\n  Q1 &lt;- quantile(x, 0.25, na.rm = T)\n  Q3 &lt;- quantile(x, 0.75, na.rm = T)\n  IQR_value &lt;- IQR(x, na.rm = T)\n  lower_bound &lt;- Q1 - 1.5 * IQR_value\n  upper_bound &lt;- Q3 + 1.5 * IQR_value\n  outliers &lt;- x[x &lt; lower_bound | x &gt; upper_bound]\n  return(outliers)\n}\n\n# Apply to a variable\nx &lt;- c(7, 8, 3, 12, 5, 8, 9, 35, 10)\nfind_outliers_iqr(x)\n\n#&gt; [1] 35\n\n\n(2) Z-score method\nOutliers can also be detected by calculating Z-scores, which measure how many standard deviations an observation is from the mean. Here the threshold for outliers is a Z-score above 2.5 or below -2.5.\n\nfind_outliers_z &lt;- function(x, threshold = 2.5) {\n  z_scores &lt;- (x - mean(x, na.rm = T)) / sd(x, na.rm = T)\n  outliers &lt;- tibble(\n    index = which(z_scores &gt; threshold, arr.ind = T),\n    value = x[abs(z_scores) &gt; threshold]\n  )\n  return(outliers)\n}\n \nx &lt;- c(7, 8, 3, 12, 5, 8, 9, 35, 10)\nfind_outliers_z(x)\n\n#&gt; # A tibble: 1 × 2\n#&gt;   index value\n#&gt;   &lt;int&gt; &lt;dbl&gt;\n#&gt; 1     8    35\n\n\nAlternatively, you can directly use the functions from the outliers packages to detect outliers:\n\ngrubbs.test(x)\n\n#&gt; \n#&gt;  Grubbs test for one outlier\n#&gt; \n#&gt; data:  x\n#&gt; G = 2.561163, U = 0.077562, p-value = 0.0001756\n#&gt; alternative hypothesis: highest value 35 is an outlier\n\ndixon.test(x)\n\n#&gt; \n#&gt;  Dixon test for outliers\n#&gt; \n#&gt; data:  x\n#&gt; Q = 0.76667, p-value &lt; 2.2e-16\n#&gt; alternative hypothesis: highest value 35 is an outlier\n\nchisq.out.test(x)\n\n#&gt; \n#&gt;  chi-squared test for outlier\n#&gt; \n#&gt; data:  x\n#&gt; X-squared = 6.5596, p-value = 0.01043\n#&gt; alternative hypothesis: highest value 35 is an outlier\n\n\n2. Visual inspection\nThis method uses box plots to identify data points that deviate significantly.\n\ndf &lt;- tibble(\n  id = 1:9,\n  x = c(7, 8, 3, 12, 5, 8, 9, 35, 10),\n  grp = rep(\"1\", 9)\n)\ndf |&gt; \n  ggplot(aes(x = grp, y = x)) +\n  geom_boxplot(\n    fill = \"lightblue\", color = \"darkblue\", width = 0.4,\n    outlier.shape = 1, outlier.size = 2)  +\n  labs(x = \" \") +\n  theme(axis.text.x = element_blank())\n\n\n\n\n\n\n\n\n\ndf |&gt; \n  mutate(\n    Q1 = quantile(x, 0.25, na.rm = T),\n    Q3 = quantile(x, 0.75, na.rm = T),\n    IQR = Q3 - Q1,\n    lwr_bound = Q1 - 1.5 * IQR,  # 离群值下限\n    upr_bound = Q3 + 1.5 * IQR   # 离群值上线\n  ) |&gt; \n  mutate(\n    flag = ifelse(x &lt; lwr_bound | x &gt; upr_bound, T, F),\n    label = ifelse(flag, id, NA)\n  ) |&gt; \n  filter(flag) |&gt; \n  select(\"ID\" = id, \"value\" = x) \n\n#&gt; # A tibble: 1 × 2\n#&gt;      ID value\n#&gt;   &lt;int&gt; &lt;dbl&gt;\n#&gt; 1     8    35\n\n\n\n\n33.2.5 Distribution checks\nDistribution checks are essential for understanding the nature of a dataset and ensuring the assumptions required for certain analyses are met. You can use histograms and density plots to inspect data distributions and ensure they match expectations. If the distribution does not align with expectations (e.g., normal distribution), further investigation may be required. Here is an example of normality check:\nDistribution checks are essential for understanding the nature of a dataset and ensuring the assumptions required for certain analyses are met. If the distribution does not align with expectations (e.g., normal distribution), further investigation may be required. In R, various methods can be used to perform distribution checks, depending on the characteristics you want to examine. Here are some common types of distribution checks and how to conduct them:\n1. Normality check\nThis is used to check if data follows a normal distribution, this assumption is key for parametric tests like t-tests and ANOVA.\n\nx &lt;- c(2.5, 3.6, 2.8, 3.1, 3.9, 3.3, 3.1, 2.7, 3.2, 4.1)\n# Visual Check\nqqnorm(x)\nqqline(x, col = \"red\")\n\n# Statistical Test\nshapiro.test(x) # Shapiro-Wilk test\n\n#&gt; \n#&gt;  Shapiro-Wilk normality test\n#&gt; \n#&gt; data:  x\n#&gt; W = 0.96112, p-value = 0.7986\n\n\n\n\n\n\n\n\n\n2. Homogeneity of variance check\nHomogeneity of variance, also known as homoscedasticity, means that different groups in the dataset have similar variance. This is important for ANOVA and regression. If the assumption of homogeneity of variance is violated, it can impact the results of statistical tests by increasing the risk of type I or type II errors. Therefore, confirming that variances are equal across groups ensures that test statistics are reliable.\n\n# A sample data frame\nset.seed(123)\ndf &lt;- data.frame(\n  group = factor(rep(1:3, each = 10)),\n  x = c(\n    rnorm(10, mean = 10, sd = 2), \n    rnorm(10, mean = 12, sd = 2), \n    rnorm(10, mean = 14, sd = 2))\n)\n\nLevene’s test: Levene’s test is commonly used to assess homogeneity of variance across groups. It tests the null hypothesis that the variances of groups are equal.\n\nrstatix::levene_test(df, x ~ group)\n\n#&gt; # A tibble: 1 × 4\n#&gt;     df1   df2 statistic     p\n#&gt;   &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1     2    27  0.000498  1.00\n\n\nBartlett’s test: Bartlett’s test is another test for homogeneity of variances. It’s more sensitive to deviations from normality, so it’s typically used when data are normally distributed.\n\nbartlett.test(x ~ group, data = df)\n\n#&gt; \n#&gt;  Bartlett test of homogeneity of variances\n#&gt; \n#&gt; data:  x by group\n#&gt; Bartlett's K-squared = 0.11427, df = 2, p-value = 0.9445\n\n\nFligner-Killeen test: The Fligner-Killeen test is a non-parametric test for homogeneity of variances, making it more robust to non-normal distributions.\n\nfligner.test(x ~ group, data = df)\n\n#&gt; \n#&gt;  Fligner-Killeen test of homogeneity of variances\n#&gt; \n#&gt; data:  x by group\n#&gt; Fligner-Killeen:med chi-squared = 0.03923, df = 2, p-value = 0.9806\n\n\nAfter completing data validation, compile a report detailing the checks performed, any anomalies found, and corrective actions taken. This helps ensure data quality and provides a resource for the entire analysis team.\n\n\n33.2.6 Some R packages for data validation\nR has several packages designed specifically for data validation and verification, which can streamline the process of checking data quality. Here are some commonly used packages and their functions for data validation.\n1. dataMaid\nThe dataMaid package provides a complete set of data validation tools that can generate detailed reports to help you quickly understand data quality and issues. It’s especially helpful for exploring new datasets. With a single function makeDataReport(), you can check for issues like missing values, variable distributions, and extreme values, and then generate a descriptive report.\n\ndf &lt;- tibble(\n  id = 1:5,\n  gender = c(\"Male\", \"Female\", \"Female\", \"Male\", \"Male\"),\n  x1 = c(10, 20, NA, 40, 50),\n  x2 = c(2.3, 2.6, 3.2, 3.4, 20),\n  date = as.Date(c(\"2023-10-01\", \"2023-10-02\", \"2023.10.03\", \"2023-10-04\", \"2023-10-05\"))\n)\nmakeDataReport(\n  df, output = \"pdf\", file = \"data-check.Rmd\", replace = T,\n  render = F, onlyProblematic = T)\n\n#&gt; The default of 'doScale' is FALSE now for stability;\n#&gt;   set options(mc_doScale_quiet=TRUE) to suppress this (once per session) message\n\n\n2. Validate\nThe validate package provides powerful data validation capabilities that allow you to define and execute complex data validation rules. You can define validation rules to check for missing values, range constraints, consistency between variables, and more. The validator() function defines validation rules, confront() function apply validation rules to a dataset and generate a report.\n\ndf &lt;- tibble(\n  age = c(55, 67, 62, NA, 59, 70),\n  income = c(30, 18, 23, 30, 20, 15),\n  retired = c(\"Yes\", \"Yes\", \"Yes\", \"No\", \"No\", \"Yes\")\n)\n\nrules &lt;- validator(\n  age &gt;= 0,\n  age &lt;= 120,\n  income &gt;= 0,\n  if (retired == \"Yes\") age &gt;= 60\n)\nconfront(df, rules) |&gt; summary()\n\n#&gt;   name items passes fails nNA error warning\n#&gt; 1   V1     6      5     0   1 FALSE   FALSE\n#&gt; 2   V2     6      5     0   1 FALSE   FALSE\n#&gt; 3   V3     6      6     0   0 FALSE   FALSE\n#&gt; 4   V4     6      5     1   0 FALSE   FALSE\n#&gt;                                expression\n#&gt; 1                       age - 0 &gt;= -1e-08\n#&gt; 2                      age - 120 &lt;= 1e-08\n#&gt; 3                    income - 0 &gt;= -1e-08\n#&gt; 4 retired != \"Yes\" | (age - 60 &gt;= -1e-08)\n\n\n3. pointblank\nThe pointblank package provides a range of functions and methods to help you easily verify, validate, and report data, especially useful for data quality checks in ETL pipelines. Validation pipelines can be made using easily-readable, consecutive validation steps.\n\ndf &lt;- tibble(\n  age = c(55, 67, 62, NA, 59, 70),\n  income = c(30, 18, -23, 30, 20, 15),\n  retired = c(\"Yes\", \"Yes\", \"Yes\", \"No\", \"No\", \"Yes\")\n)\n\ncreate_agent(df) |&gt; \n  col_vals_between(age, left = 0, right = 120, na_pass = T) |&gt; \n  col_vals_between(income, left = 0, right = Inf, na_pass = T) |&gt; \n  col_vals_not_null(age) |&gt; \n  col_vals_in_set(retired, set = c(\"Yes\", \"No\")) |&gt; \n  interrogate()\n\n\n\n\n\n\n\nPointblank Validation\n\n\n\n\n[2024-11-25|23:25:20]\n\n\ntibble df\n\n\n\n\n\n\nSTEP\nCOLUMNS\nVALUES\nTBL\nEVAL\nUNITS\nPASS\nFAIL\nW\nS\nN\nEXT\n\n\n\n\n\n\n1\n\n\n\n\ncol_vals_between\n\n     \n\n\n col_vals_between()\n\n\n▮age\n\n\n[0, 120]\n\n\n\n      \n\n\n✓\n6\n6\n1\n0\n0\n—\n—\n—\n—\n\n\n\n\n2\n\n\n\n\ncol_vals_between\n\n     \n\n\n col_vals_between()\n\n\n▮income\n\n\n[0, Inf]\n\n\n\n      \n\n\n✓\n6\n5\n0.83333\n1\n0.16667\n—\n—\n—\n\n\nCSV\n\n\n\n\n\n\n3\n\n\n\n\ncol_vals_not_null\n\n       \n\n\n col_vals_not_null()\n\n\n▮age\n\n—\n\n\n      \n\n\n✓\n6\n5\n0.83333\n1\n0.16667\n—\n—\n—\n\n\nCSV\n\n\n\n\n\n\n4\n\n\n\n\ncol_vals_in_set\n\n     \n\n\n col_vals_in_set()\n\n\n▮retired\n\n\nYes, No\n\n\n\n      \n\n\n✓\n6\n6\n1\n0\n0\n—\n—\n—\n—\n\n\n\n2024-11-25 23:25:20 UTC &lt; 1 s 2024-11-25 23:25:20 UTC\n\n\n\n\n\n\n\n\n4. assertr\nThe assertr package is a useful tool for validating and verifying data to ensure that it meets specific conditions, making it particularly useful for data cleaning, quality checks, and pipeline automation. It allows you to perform checks on data frames, such as confirming value ranges, detecting missing or unexpected values, and enforcing data integrity.\n\ndf &lt;- tibble(\n  age = c(55, 67, 162, NA, 59, 70),\n  income = c(30, 18, -23, 30, 20, 15),\n  retired = c(\"Yes\", \"Yes\", \"Yes\", \"No\", \"No\", \"Yes\")\n)\n\ndf |&gt; assert(not_na, age, error_fun = error_return)\n\n#&gt; [[1]]\n#&gt; Column 'age' violates assertion 'not_na' 1 time\n#&gt;     verb redux_fn predicate column index value\n#&gt; 1 assert       NA    not_na    age     4    NA\n\ndf |&gt; assert(function(x) x &gt; 0, income, error_fun = error_return)\n\n#&gt; [[1]]\n#&gt; Column 'income' violates assertion 'function(x) x &gt; 0' 1 time\n#&gt;     verb redux_fn         predicate column index value\n#&gt; 1 assert       NA function(x) x &gt; 0 income     3   -23\n\ndf |&gt; assert(within_bounds(0, 120), age, error_fun = error_return)\n\n#&gt; [[1]]\n#&gt; Column 'age' violates assertion 'within_bounds(0, 120)' 1 time\n#&gt;     verb redux_fn             predicate column index value\n#&gt; 1 assert       NA within_bounds(0, 120)    age     3   162\n\n\n5. janitor\nThe janitor package is primarily for data cleaning, but it includes some useful functions for initial data checks, especially for dealing with missing and duplicate values.\n\ndf &lt;- tibble(\n  id = c(1:3, 3:5),\n  age = c(55, 67, 62, NA, 59, 70),\n  income = c(30, 18, -23, 30, 20, 15),\n  retired = c(\"Yes\", \"Yes\", \"Yes\", \"No\", \"No\", \"Yes\")\n)\n\n# Finds duplicate rows\nget_dupes(df, id) \n\n#&gt; # A tibble: 2 × 5\n#&gt;      id dupe_count   age income retired\n#&gt;   &lt;int&gt;      &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  \n#&gt; 1     3          2    62    -23 Yes    \n#&gt; 2     3          2    NA     30 No\n\n# Cleans names of an data.frame and results names are unique \nclean_names(df)  \n\n#&gt; # A tibble: 6 × 4\n#&gt;      id   age income retired\n#&gt;   &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  \n#&gt; 1     1    55     30 Yes    \n#&gt; 2     2    67     18 Yes    \n#&gt; 3     3    62    -23 Yes    \n#&gt; 4     3    NA     30 No     \n#&gt; 5     4    59     20 No     \n#&gt; 6     5    70     15 Yes",
    "crumbs": [
      "Practical skills",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Data preprocessing</span>"
    ]
  },
  {
    "objectID": "random-sampling.html",
    "href": "random-sampling.html",
    "title": "34  Random sampling",
    "section": "",
    "text": "34.1 Prerequisites\nRandom sampling is a statistical technique used to select a subset of individuals or observations from a larger population in such a way that each member of the population has an equal chance of being chosen. This method is often used to ensure that the sample represents the broader population, which helps to reduce bias and improve the generalizability of study results.\nlibrary(tidyverse)\nlibrary(rmarkdown)",
    "crumbs": [
      "Practical skills",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Random sampling</span>"
    ]
  },
  {
    "objectID": "random-sampling.html#basic-concepts",
    "href": "random-sampling.html#basic-concepts",
    "title": "34  Random sampling",
    "section": "34.2 Basic concepts",
    "text": "34.2 Basic concepts\n\n34.2.1 Population and sample\nIn statistics, population and sample are two key concepts used to make inferences about a larger group based on the analysis of a smaller group.\nPopulation\nThe population is the entire group of individuals, objects, or events that you are interested in studying. It includes all possible observations that could be made, so it represents the “whole.” Populations can be large (e.g., all residents of a country) or small (e.g., all patients in a hospital).\nExamples of populations:\n\nAll patients with hypertension in the United States.\nAll residents in a city when studying the incidence of respiratory diseases.\nAll infants born in a hospital over a year to assess newborn health outcomes.\n\nSample\nA sample is a subset of the population, selected in such a way that it represents the larger group. Sampling is done because studying the entire population can be impractical or impossible. In practice, obtaining samples might involve random sampling or other methods like stratified sampling to ensure specific subgroups are represented. The goal of taking a sample is to make inferences or predictions about the population based on this smaller, manageable group.\nExamples of samples:\n\n200 randomly selected patients with hypertension in a specific city to study treatment effectiveness.\nA group of 500 patients from a population of individuals over 65 to analyze common health outcomes in older adults.\nBlood samples taken from 150 randomly selected participants in a clinical trial to measure a biomarker.\n\n\n\n34.2.2 Parameters and statistics\nIn statistics, parameters and statistics are two fundamental concepts that distinguish the characteristics of a population from those of a sample.\nParameters\nParameters are numerical values that describe specific characteristics of a population. They represent true, often unknown values because measuring every individual in a population is usually impractical or impossible. Parameters are fixed values; however, they are generally estimated rather than measured directly.\nStatistics\nStatistics are numerical values calculated from a sample and serve as estimates of the corresponding population parameters. Statistics vary from sample to sample, as each sample might differ slightly from others due to random variation. They are used to make inferences about the population parameters.\nTable 34.1 shows the differences between parameters and statistics.\n\n\n\n\nTable 34.1: Key differences between parameter and statistic\n\n\n\n\n\n\n\n\n\n\n\nAspect\nParameter\nStatistic\n\n\n\n\nDefinition\nDescribes a population\nDescribes a sample\n\n\nRepresentation\nTrue values, often unknown\nEstimated values, calculated from a sample\n\n\nSymbol\n\\mu (mean), \\sigma^2 (variance), \\pi (proportion)\n\\bar{X} (mean), S^2 (variance), p (proportion)\n\n\nVariability\nConstant (fixed value)\nVaries from sample to sample\n\n\nPurpose\nDescribe entire population characteristics\nEstimate or infer population parameters\n\n\n\n\n\n\n\n\n\n\n34.2.3 Sampling frame and sampling\nIn survey research and statistics, sampling and the sampling frame are both critical concepts in designing and conducting studies:\nSampling\nSampling is the process of selecting a subset of individuals (the sample) from a larger group (the population) to estimate characteristics of the whole group. Effective sampling allows researchers to make inferences about the population without surveying everyone, which is especially useful for large populations. Different sampling methods exist, such as simple random sampling, stratified sampling, and cluster sampling, each tailored to specific research needs.\nSampling frame\nSampling frame is the actual list or database from which the sample is drawn. It ideally includes every member of the population to avoid selection bias. The quality of the sampling frame directly affects the quality of the sample: if the sampling frame is incomplete or outdated, certain groups within the population may be under- or overrepresented in the sample, which can lead to biased results.\nFor example, in a study of hospital patients, the sampling frame might be a complete patient registry, and the sampling method might involve randomly selecting a group from this registry for survey.\n\nExample 1 \nTo illustrate various sampling methods, here we use all the passengers in the Titanic disaster of 1912 as the population, and the passenger registry with details as the sampling frame.\n\n\npop &lt;- carData::TitanicSurvival |&gt; \n  mutate(\n    class = passengerClass, age = as.integer(age), .keep = \"unused\")\n\ncat(\"The total number of survived passengers is:\", nrow(pop))\n\n#&gt; The total number of survived passengers is: 1309",
    "crumbs": [
      "Practical skills",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Random sampling</span>"
    ]
  },
  {
    "objectID": "random-sampling.html#simple-random-sampling",
    "href": "random-sampling.html#simple-random-sampling",
    "title": "34  Random sampling",
    "section": "34.3 Simple random sampling",
    "text": "34.3 Simple random sampling\nSimple random sampling is a fundamental method of random sampling where each member of a population has an equal chance of being selected. This sampling technique is straightforward and is designed to minimize bias, ensuring that the sample is representative of the population.\nHere’s how you might perform a simple random sampling to select a sample of 10% subjects from a population:\n\nset.seed(2024)\nsimple_rs &lt;- pop |&gt; \n  slice_sample(prop = 0.1, replace = F) \nsimple_rs |&gt; paged_table()\n\n\n  \n\n\n\nIn this example, replace = FALSE ensures that each selected member can only appear in the sample once, maintaining independence and uniqueness in the sample selection.\nThe survival rates of different class can be estimated by the following code block:\n\nsimple_rs |&gt; \n  group_by(class, survived) |&gt; \n  summarise(n = n(), .groups = \"drop\") |&gt; \n  pivot_wider(\n    id_cols = everything(), names_from = c(survived), values_from = n\n  ) |&gt; \n  mutate(surv_rate = yes / (yes + no))\n\n#&gt; # A tibble: 3 × 4\n#&gt;   class    no   yes surv_rate\n#&gt;   &lt;fct&gt; &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;\n#&gt; 1 1st       9    23     0.719\n#&gt; 2 2nd      18     7     0.28 \n#&gt; 3 3rd      57    16     0.219\n\n\nSimple random sampling is often used in:\n\nSurvey research: To gather opinions or information from a subset of a population.\nQuality control: To inspect a sample of items for defects.\nClinical trials: To select patients randomly for different treatment groups.\n\nThis method works best when you have a complete list of the population and need an unbiased sample, especially when the population size is manageable.",
    "crumbs": [
      "Practical skills",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Random sampling</span>"
    ]
  },
  {
    "objectID": "random-sampling.html#stratified-random-sampling",
    "href": "random-sampling.html#stratified-random-sampling",
    "title": "34  Random sampling",
    "section": "34.4 Stratified random sampling",
    "text": "34.4 Stratified random sampling\nStratified random sampling is a method of sampling that involves dividing a population into distinct subgroups, called strata, that share specific characteristics. Within each stratum, a random sample is selected proportionally. This technique ensures that each subgroup is represented in the final sample, which can improve the accuracy and reliability of the study’s results, especially when the population has varying characteristics across groups.\nFor the Titanic passengers, if the goal is to study survival rates across different class, the strata could be first class, second class and third class. Suppose each class has different numbers of passengers: First, select a proportionate random sample from each class (10% ), then, combine these samples to create a final sample that represents all classes proportionally.\n\nset.seed(2024)\nstratified_rs &lt;- pop |&gt; \n  slice_sample(prop = 0.1, by = class) \n\nstratified_rs |&gt; paged_table()\n\n\n  \n\n\n\nThe survival rates of different class is calculated by the following code block:\n\nstratified_rs |&gt; \n  group_by(class, survived) |&gt; \n  summarise(n = n(), .groups = \"drop\") |&gt; \n  pivot_wider(\n    id_cols = everything(), names_from = c(survived), values_from = n\n  ) |&gt; \n  mutate(surv_rate = yes / (yes + no))\n\n#&gt; # A tibble: 3 × 4\n#&gt;   class    no   yes surv_rate\n#&gt;   &lt;fct&gt; &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;\n#&gt; 1 1st      12    20     0.625\n#&gt; 2 2nd      14    13     0.481\n#&gt; 3 3rd      55    15     0.214",
    "crumbs": [
      "Practical skills",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Random sampling</span>"
    ]
  },
  {
    "objectID": "random-sampling.html#systematic-sampling",
    "href": "random-sampling.html#systematic-sampling",
    "title": "34  Random sampling",
    "section": "34.5 Systematic sampling",
    "text": "34.5 Systematic sampling\nSystematic sampling is a sampling technique where you select every k-th individual from a population list to form a sample. This method is straightforward, often easy to implement, and useful when a truly random sample is not required. It provides a way to cover the entire population by using a structured approach, which can be more efficient than simple random sampling.\n\nset.seed(2024)\n\nsys_sample &lt;- function(df, size) {\n  N &lt;- nrow(df) \n  k &lt;- floor(N / size)\n  start &lt;- sample(1:k, 1)\n  indices &lt;- seq(start, by = k, length.out = size)\n  \n  return(df[indices, ])\n}\n\nsystematic_rs &lt;- sys_sample(pop, 130) \nsystematic_rs |&gt; paged_table()",
    "crumbs": [
      "Practical skills",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Random sampling</span>"
    ]
  },
  {
    "objectID": "random-sampling.html#cluster-sampling",
    "href": "random-sampling.html#cluster-sampling",
    "title": "34  Random sampling",
    "section": "34.6 Cluster sampling",
    "text": "34.6 Cluster sampling\nCluster Sampling is a probability sampling technique in which the population is divided into distinct groups, or “clusters.” Then, instead of selecting individual members from the entire population, random clusters are selected, and all or a sample of individuals within each selected cluster are included in the final sample. This method is commonly used in large-scale studies, especially when the population is spread across a wide geographic area, making individual sampling costly or impractical.\nSuppose you want to conduct a survey on household income in a city with 100 neighborhoods (clusters), each has 1000 families, and you plan to take a sample of 10 clusters, in each cluster 100 families are selected randomly. Here we write a clustering sampling function named cluster_sample() to perform this process:\n\ncluster_sample &lt;- function(data, id, n, size = NULL, prop = NULL) {\n  if (!xor(is.null(size), is.null(prop))) {\n    stop(\"Specify either 'size' or 'prop', but not both.\")\n  }\n  \n  # Step 1: Select clusters\n  selected_clusters &lt;- sample(unique(data |&gt; pull({{id}})), n)\n  \n  # Step 2: Sample families within each selected cluster\n  sampled_data &lt;- bind_rows(lapply(selected_clusters, function(cluster) {\n    # Filter data for the current cluster\n    cluster_data &lt;- data |&gt; filter({{id}} == cluster)\n    \n    # Determine the number of families available in the cluster\n    n_families &lt;- nrow(cluster_data)\n    \n    # Decide the number of samples based on fixed count or proportion\n    if (!is.null(size)) {\n      # Use a fixed number but limit to available families\n      n_to_sample &lt;- min(size, n_families)\n    } else {\n      # Use a proportion but ensure at least 1 family is selected\n      n_to_sample &lt;- max(1, round(n_families * prop))\n    }\n    \n    # Randomly sample families within the cluster\n    sampled_families &lt;- cluster_data |&gt; slice_sample(n = n_to_sample)\n    return(sampled_families)\n  }))\n  \n  return(sampled_data)\n}\n\n\nset.seed(2024)\nnhood_sizes &lt;- sample(800:1200, 100, replace = T)\n\nnhoods &lt;- tibble(\n  nhood_id = rep(1:100, times = nhood_sizes),\n  family_id = sequence(nhood_sizes),\n  income = rnorm(sum(nhood_sizes), mean = 50000, sd = 15000)  \n)\n# Example 1: Sample 100 families from each of 10 selected clusters\ncluster_sample(nhoods, id = nhood_id, n = 10, size = 100) |&gt; paged_table()\n\n\n  \n\n\n# Example 2: Sample 10% of families from each of 10 selected clusters\ncluster_sample(nhoods, id = nhood_id, n = 10, prop = 0.1) |&gt; paged_table()\n\n\n  \n\n\n\nCluster sampling is commonly used in:\n\nPublic health surveys: Randomly select neighborhoods (clusters) to analyze health indicators across a city.\nSocial research: Select geographic areas or households to study quality of life or social indicators\nEducational research: Choose schools or classes as clusters to assess academic performance among students.\n\nCluster sampling is a practical and cost-effective way to gather representative data in resource-limited scenarios. However, special attention is needed to ensure clusters are representative to avoid bias.",
    "crumbs": [
      "Practical skills",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Random sampling</span>"
    ]
  },
  {
    "objectID": "randomization.html",
    "href": "randomization.html",
    "title": "35  Randomization",
    "section": "",
    "text": "35.1 Prerequisites\nRandomization is a technique used in experiments and studies to assign participants to different groups, such as treatment and control groups, in a way that is entirely random. The goal is to reduce potential bias and to ensure that any differences between groups are due to chance rather than systematic differences. This approach helps to increase the validity of study results, as it balances confounding variables across groups.\nlibrary(tidyverse)\nlibrary(rmarkdown)\nlibrary(randomizeR)",
    "crumbs": [
      "Practical skills",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Randomization</span>"
    ]
  },
  {
    "objectID": "randomization.html#simple-randomization",
    "href": "randomization.html#simple-randomization",
    "title": "35  Randomization",
    "section": "35.2 Simple randomization",
    "text": "35.2 Simple randomization\nSimple Randomization is a straightforward technique in which each participant has an equal probability of being assigned to any of the available study groups. This approach is ideal for large samples, as it usually results in balanced group sizes and unbiased results. However, in smaller samples, there is a risk of group size imbalance.\n\nExample 1 \nSuppose you are conducting a clinical trial to test the efficacy of a new drug, and you have 50 subjects. You need to randomly assign them to a treatment group and a control group.\n\nHere’s how to perform simple randomization, where we want to assign participants to two groups:\n\nset.seed(2024)\nN = 50\nsimple_R &lt;- tibble(no = c(1:N), group = \"\") |&gt;\n  mutate(group = ifelse(runif(N) &gt; 0.5, \"A\", \"B\")) \nsimple_R |&gt; \n  group_by(group) |&gt; \n  summarise(n = n())\n\n#&gt; # A tibble: 2 × 2\n#&gt;   group     n\n#&gt;   &lt;chr&gt; &lt;int&gt;\n#&gt; 1 A        27\n#&gt; 2 B        23\n\n\nAlternatively, you can use randomizeR package to perform simple randomization.\n\nN = 50\nrarPar(N = N, K = 2) |&gt; \n  genSeq(r = 3, seed = 2024) |&gt; \n  pluck(\"M\") |&gt;    \n  apply(MARGIN = 2, \\(x) ifelse(x == 0, \"A\", \"B\")) |&gt; \n  as_tibble(.name_repair = ~ paste0(\"s\", (1:N))) \n\n#&gt; # A tibble: 3 × 50\n#&gt;   s1    s2    s3    s4    s5    s6    s7    s8    s9    s10   s11   s12  \n#&gt;   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n#&gt; 1 A     B     B     A     B     B     B     B     B     A     A     B    \n#&gt; 2 A     A     A     A     A     A     B     B     A     B     B     A    \n#&gt; 3 A     B     B     B     A     A     A     A     A     B     A     A    \n#&gt; # ℹ 38 more variables: s13 &lt;chr&gt;, s14 &lt;chr&gt;, s15 &lt;chr&gt;, s16 &lt;chr&gt;,\n#&gt; #   s17 &lt;chr&gt;, s18 &lt;chr&gt;, s19 &lt;chr&gt;, s20 &lt;chr&gt;, s21 &lt;chr&gt;, s22 &lt;chr&gt;, …",
    "crumbs": [
      "Practical skills",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Randomization</span>"
    ]
  },
  {
    "objectID": "randomization.html#complete-randomization",
    "href": "randomization.html#complete-randomization",
    "title": "35  Randomization",
    "section": "35.3 Complete randomization",
    "text": "35.3 Complete randomization\nComplete randomization is a method used in experimental design and clinical trials to assign participants to different treatment groups. It involves randomly distributing a predetermined, fixed number of participants across the groups to ensure balanced group sizes. Unlike simple randomization, which may result in unequal group sizes due to chance, complete randomization ensures that each group has the same or nearly the same number of participants.\nHere’s an example on how to perform complete randomization:\n\nset.seed(2024)\nN = 50\ninit &lt;- tibble(no = c(1:N), group = \"\")\n\ngroupA &lt;- init |&gt; \n  slice_sample(prop = 0.5, replace = F) \ngroupB &lt;- init |&gt; \n  setdiff(groupA)\ngroupA &lt;- mutate(groupA, group = \"A\")\ngroupB &lt;- mutate(groupB, group = \"B\")\ncomplete_R &lt;- rbind(groupA, groupB)\n\ncomplete_R |&gt; \n  group_by(group) |&gt; \n  summarise(n = n())\n\n#&gt; # A tibble: 2 × 2\n#&gt;   group     n\n#&gt;   &lt;chr&gt; &lt;int&gt;\n#&gt; 1 A        25\n#&gt; 2 B        25\n\n\nAlternatively, you can use randomizeR package to perform complete randomization.\n\nN = 50\ncrPar(N = N, K = 2) |&gt; \n  genSeq(r = 3, seed = 2024) |&gt; \n  pluck(\"M\") |&gt;    \n  apply(MARGIN = 2, \\(x) ifelse(x == 0, \"A\", \"B\")) |&gt; \n  as_tibble(.name_repair = ~ paste0(\"s\", (1:N))) \n\n#&gt; # A tibble: 3 × 50\n#&gt;   s1    s2    s3    s4    s5    s6    s7    s8    s9    s10   s11   s12  \n#&gt;   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n#&gt; 1 A     B     A     A     B     A     B     B     A     B     A     A    \n#&gt; 2 B     A     B     B     A     A     B     B     B     B     B     B    \n#&gt; 3 B     A     B     A     B     A     B     B     B     A     B     A    \n#&gt; # ℹ 38 more variables: s13 &lt;chr&gt;, s14 &lt;chr&gt;, s15 &lt;chr&gt;, s16 &lt;chr&gt;,\n#&gt; #   s17 &lt;chr&gt;, s18 &lt;chr&gt;, s19 &lt;chr&gt;, s20 &lt;chr&gt;, s21 &lt;chr&gt;, s22 &lt;chr&gt;, …",
    "crumbs": [
      "Practical skills",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Randomization</span>"
    ]
  },
  {
    "objectID": "randomization.html#block-randomization",
    "href": "randomization.html#block-randomization",
    "title": "35  Randomization",
    "section": "35.4 Block Randomization",
    "text": "35.4 Block Randomization\nBlock randomization is a technique used in experimental design to ensure that groups have a balanced number of participants over each block, reducing the potential for bias and imbalance. This is especially useful in clinical trials and other studies where ensuring equal group sizes within blocks is essential.\n\nExample 2 \nLet’s assume we have 50 subjects and want to assign them to two groups with a block size of 5.\n\nThe subjects are first divided into blocks, then within each block, subjects are assigned to groups in a specific ratio. Here is an example code for block randomization:\n\nset.seed(2024)\nN = 50\nblock_size = 5\n\ninit &lt;- tibble(\n  no = c(1:N), block = rep(1:block_size, each = N/ block_size), \n  group = \"\") \n\ngroupA &lt;- init |&gt; \n  slice_sample(prop = 0.6, by = block)\ngroupB &lt;- init |&gt; \n  setdiff(groupA)\ngroupA &lt;- mutate(groupA, group = \"A\")\ngroupB &lt;- mutate(groupB, group = \"B\")\nblock_R &lt;- rbind(groupA, groupB)\n\nblock_R |&gt; \n  group_by(group) |&gt; \n  summarise(n = n())\n\n#&gt; # A tibble: 2 × 2\n#&gt;   group     n\n#&gt;   &lt;chr&gt; &lt;int&gt;\n#&gt; 1 A        30\n#&gt; 2 B        20\n\n\nAlternatively, you can use randomizeR package to perform simple randomization.\n\nN = 50; rb = 10\nrpbrPar(N = N, rb = rb, K = 2) |&gt; \n  genSeq(r = 3, seed = 2024) |&gt; \n  pluck(\"M\") |&gt;    \n  apply(MARGIN = 2, \\(x) ifelse(x == 0, \"A\", \"B\")) |&gt; \n  as_tibble(.name_repair = ~ paste0(\"s\", (1:N))) \n\n#&gt; # A tibble: 3 × 50\n#&gt;   s1    s2    s3    s4    s5    s6    s7    s8    s9    s10   s11   s12  \n#&gt;   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n#&gt; 1 A     A     B     A     A     B     B     B     B     A     A     B    \n#&gt; 2 A     B     B     A     A     B     A     B     A     B     A     B    \n#&gt; 3 B     B     A     B     B     A     A     A     A     B     A     A    \n#&gt; # ℹ 38 more variables: s13 &lt;chr&gt;, s14 &lt;chr&gt;, s15 &lt;chr&gt;, s16 &lt;chr&gt;,\n#&gt; #   s17 &lt;chr&gt;, s18 &lt;chr&gt;, s19 &lt;chr&gt;, s20 &lt;chr&gt;, s21 &lt;chr&gt;, s22 &lt;chr&gt;, …",
    "crumbs": [
      "Practical skills",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Randomization</span>"
    ]
  },
  {
    "objectID": "randomization.html#stratified-randomization",
    "href": "randomization.html#stratified-randomization",
    "title": "35  Randomization",
    "section": "35.5 Stratified randomization",
    "text": "35.5 Stratified randomization\nStratified randomization is a technique used to ensure that specific characteristics (or “strata”) are evenly distributed across treatment groups in a study. This method helps control for confounding variables by balancing them across groups, which can be especially useful in clinical trials where factors like age, gender, or disease severity could impact the outcome.\n\nExample 3 \nSuppose we have a study where we need to randomly assign 100 subjects to two treatment groups, stratified by gender (male, female) and age group (&lt;30 years, ≥30 years). This approach ensures balanced distribution within each combination of gender and age group.\n\nHere is an example code for stratified randomization:\n\nset.seed(2024)\nN &lt;- 100\nratio = 0.5\ninit &lt;- tibble(\n  id = 1:N,\n  gender = sample(c(\"Male\", \"Female\"), N, replace = T),\n  age = sample(c(\"&lt;30\", \"≥30\"), N, replace = T)\n)\n\nstratified_r &lt;- init |&gt; \n  group_by(gender, age) |&gt; \n  mutate(group = if_else(row_number() &lt;= round(n() * ratio), \"A\", \"B\")) |&gt; \n  ungroup()\n\n# Shuffle within each stratum to add randomness\nstratified_r &lt;- stratified_r |&gt; \n  group_by(gender, age) |&gt; \n  sample_frac() |&gt; \n  ungroup()\n\nstratified_r |&gt; \n  group_by(group) |&gt; \n  summarise(n = n())  \n\n#&gt; # A tibble: 2 × 2\n#&gt;   group     n\n#&gt;   &lt;chr&gt; &lt;int&gt;\n#&gt; 1 A        50\n#&gt; 2 B        50\n\nstratified_r |&gt; \n  count(gender, age, group)\n\n#&gt; # A tibble: 8 × 4\n#&gt;   gender age   group     n\n#&gt;   &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;int&gt;\n#&gt; 1 Female &lt;30   A        14\n#&gt; 2 Female &lt;30   B        13\n#&gt; 3 Female ≥30   A        10\n#&gt; 4 Female ≥30   B        11\n#&gt; 5 Male   &lt;30   A        12\n#&gt; 6 Male   &lt;30   B        12\n#&gt; # ℹ 2 more rows",
    "crumbs": [
      "Practical skills",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Randomization</span>"
    ]
  },
  {
    "objectID": "randomization.html#minimization",
    "href": "randomization.html#minimization",
    "title": "35  Randomization",
    "section": "35.6 Minimization",
    "text": "35.6 Minimization\nMinimization is a randomization method often used in clinical trials to ensure balanced allocation of participants across treatment groups while taking specific covariates (such as age, gender, or disease severity) into account. Unlike simple or stratified randomization, minimization actively adjusts the assignment of each new participant based on existing imbalances across groups.\nHere’s how minimization works:\n1. Identify key covariates: Select the covariates that are important for balancing across treatment groups.\n2. Assign the first participants: Start by assigning the initial participants randomly to treatment groups.\n3. Calculate imbalance: For each new participant, calculate how their assignment to each group would impact the balance of covariates across groups.\n4. Assign to the least imbalanced group: The participant is assigned to the group that would result in the least imbalance, with a small probability of random assignment to avoid predictability.\n\nset.seed(2024)\nn &lt;- 100\ndf &lt;- tibble(\n  subject = 1:n, \n  gender = sample(c(\"Male\", \"Female\"), n, replace = T), \n  ageGroup = sample(c(\"&lt;30\", \"&gt;=30\"), n, replace = T),\n  group = \"\")\n\n# Define a combination of covariables\ncov_comb &lt;- expand.grid(\n  gender = c(\"Male\", \"Female\"), ageGroup = c(\"&lt;30\", \"&gt;=30\")) |&gt; \n  mutate(comb = paste0(gender, \"_\", ageGroup))\n\n# Select the processing group with the least imbalance\ncounter &lt;- matrix(0, nrow = 2, ncol = 4, dimnames = list(c(\"A\", \"B\"), cov_comb$comb))\n\nfor (i in 1:n) {\n  # Gets the covariate combination for the current subject\n  current_cov &lt;- paste0(df$gender[i], \"_\", df$ageGroup[i])\n  \n  # Calculate the imbalance score\n  imbalance_A &lt;- sum(counter[\"A\", ]) - counter[\"A\", current_cov]\n  imbalance_B &lt;- sum(counter[\"B\", ]) - counter[\"B\", current_cov]\n  \n  # Select the processing group with the least imbalance\n  if (abs(imbalance_A) &lt; abs(imbalance_B)) {\n    df$group[i] &lt;- \"A\"\n    counter[\"A\", current_cov] &lt;- counter[\"A\", current_cov] + 1\n  } else {\n    df$group[i] &lt;- \"B\"\n    counter[\"B\", current_cov] &lt;- counter[\"B\", current_cov] + 1\n  }\n}\n\nhead(df, 5)\n\n#&gt; # A tibble: 5 × 4\n#&gt;   subject gender ageGroup group\n#&gt;     &lt;int&gt; &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;\n#&gt; 1       1 Female &gt;=30     B    \n#&gt; 2       2 Male   &gt;=30     A    \n#&gt; 3       3 Male   &lt;30      B    \n#&gt; 4       4 Female &gt;=30     B    \n#&gt; 5       5 Male   &lt;30      A\n\n# Count the number of occurrences of each group\ndf |&gt; count(group)\n\n#&gt; # A tibble: 2 × 2\n#&gt;   group     n\n#&gt;   &lt;chr&gt; &lt;int&gt;\n#&gt; 1 A        55\n#&gt; 2 B        45",
    "crumbs": [
      "Practical skills",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>Randomization</span>"
    ]
  },
  {
    "objectID": "sample-size.html",
    "href": "sample-size.html",
    "title": "36  Sample size estimation",
    "section": "",
    "text": "36.1 Prerequisites\nSample size estimation is an important step when conducting clinical trials, survey studies, or other scientific research. It is essential for designing a study that is capable of detecting significant effects or differences. A reasonable sample size ensures that the study has sufficient statistical power to detect the effects of interest while avoiding unnecessary waste of resources.\nlibrary(tidyverse)\nlibrary(pwr)\nlibrary(effectsize)\nlibrary(powerSurvEpi)\nlibrary(rpact)",
    "crumbs": [
      "Practical skills",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Sample size estimation</span>"
    ]
  },
  {
    "objectID": "sample-size.html#experimental-studies",
    "href": "sample-size.html#experimental-studies",
    "title": "36  Sample size estimation",
    "section": "36.2 Experimental studies",
    "text": "36.2 Experimental studies\nThe required sample size in a experimental study is influenced by several key factors, which determine how large a sample is necessary to detect meaningful effects or differences. Here are the primary factors affecting sample size:\n1. Effect size\nEffect size is the magnitude of the difference or relationship the study aims to detect. Larger effect sizes require smaller sample sizes to detect, while smaller effect sizes require larger sample sizes to achieve the same power.\n2. Significance level (\\alpha)\nSignificance level is the probability of rejecting the null hypothesis when it is actually true (type I error), often set at 0.05. Lowering \\alpha (making the test more stringent) increases the required sample size because it reduces the chance of falsely detecting a significant effect.\n3. Statistical power (1-\\beta)\nStatistical power is the probability of correctly rejecting the null hypothesis when it is false, typically set at 0.8 or 0.9. Higher power requires a larger sample size, as it reduces the chance of a type II error (failing to detect a true effect).\n4. Population variability\nThe extent of variability or dispersion in the population, often reflected in standard deviation or variance. Higher variability necessitates a larger sample size to achieve a reliable estimate, as more variability makes it harder to detect true differences.\n5. Study design\nStudy design refers to the structure of the study, such as whether it includes repeated measures, matched pairs, or independent groups. Designs that control for variability (e.g., paired designs) generally require smaller sample sizes than independent designs.\n6. Sampling method\nSampling method refers to the approach of how participants are selected for the study (e.g., random sampling, stratified sampling). A sampling method that better represents the population can improve accuracy and reduce the sample size needed.\n7. Attrition or dropout rate\nAttrition rate denotes the expected rate at which participants may leave the study before it is completed. Higher expected dropout rates require oversampling to ensure adequate final sample size.\nUnderstanding these factors allows researchers to plan a sample size that balances feasibility with statistical rigor, ensuring the study’s findings are reliable and valid. Here’s a breakdown of sample size estimation methods for different study types and analysis.\n\n36.2.1 Comparison of means\n1. One-sample mean\nTo compare a single sample mean with a known population mean, the one-sample t-test is commonly used if the population variance is unknown and the sample size is relatively small. To estimate the required sample size of one-sample t-test, we need to consider several key factors:effect size \\Delta, significance level \\alpha, power 1-β, and population standard deviation \\sigma (or an estimate if unknown). The sample size n can be estimated as:\n\nn = \\left( \\frac{Z_{1-\\alpha/2} + Z_{1-\\beta}}{\\Delta / \\sigma} \\right)^2\n\nwhere Z_{1-\\alpha/2} is the Z-score for the significance level (two-tailed, if applicable), Z_{1-\\beta} is the Z-score for the desired power level.\n\nExample 1 \nA scientist wants to study the effect of a treatment on blood pressure in rats with acute myocardial ischemia. It is hypothesized that an average increase of 10 mmHg in arterial systolic pressure would be clinically significant. Given that the standard deviation of the treatment effect on blood pressure is 15 mmHg, how many rats are needed to detect this effect?\n\nThe power.t.test() function fro the stats package can be used to estimate the sample size needed for a one-sample t-test.\n\npower.t.test(\n  delta = 10, sd = 15, sig.level = 0.05, power = 0.9,\n  type = \"one.sample\", alternative = \"one.sided\")\n\n#&gt; \n#&gt;      One-sample t test power calculation \n#&gt; \n#&gt;               n = 20.69914\n#&gt;           delta = 10\n#&gt;              sd = 15\n#&gt;       sig.level = 0.05\n#&gt;           power = 0.9\n#&gt;     alternative = one.sided\n\n\nAlternatively, you can use the pwr.t.test() function from the pwr package:\n\npwr.t.test(\n  d = 10 / 15, sig.level = 0.05, power = 0.9,\n  type = \"one.sample\",alternative = \"greater\") \n\n#&gt; \n#&gt;      One-sample t test power calculation \n#&gt; \n#&gt;               n = 20.69917\n#&gt;               d = 0.6666667\n#&gt;       sig.level = 0.05\n#&gt;           power = 0.9\n#&gt;     alternative = greater\n\n\nBased on the results, the scientist would need 21 rats in order to have a 90% probability of detecting a significant increase in blood pressure of at least 10 mmHg with a one-sided test at a 5% significance level.\n2. Two-sample means\nA two-sample mean comparison is used to determine if there is a significant difference between the means of two independent groups. This type of test is commonly applied in scenarios like comparing the effect of two treatments or comparing measurements between two different populations. To estimate the sample size, we use a formula that considers the effect size, significance level, power, and variability within each group. This is typically done for a two-sample t-test.\nThe formula for estimating the required sample size n per group is:\n\nn = \\frac{2 \\sigma^2 (Z_{1-\\alpha/2} + Z_{1-\\beta})^2}{\\Delta^2}\n\nwhere \\sigma is the pooled standard deviation of the two groups, assumed to be the same for both groups (or an estimate from previous data), Z_{1-\\alpha/2} is the Z-score corresponding to the chosen significance level (this is often 1.96 if \\alpha = 0.05), Z_{1-\\beta} is the Z-score corresponding to the desired power level (0.84 for 80% power or 1.28 for 90% power), \\Delta is the effect size, defined as the minimum detectable difference in means between the two groups.\n\nExample 2 \nA scientist intends to evaluate the efficacy of a compound hypoglycemic capsule on diabetic mice. The plan is to randomly divide diabetic model mice into two equal groups. After four weeks, fasting blood glucose levels will be measured and compared between the two groups. Based on preliminary results, the expected fasting blood glucose in the control group is 16.5 mmol/L, while in the experimental group, it is 10.5 mmol/L. The standard deviation of fasting blood glucose in both groups is assumed to be equal, at 8.0 mmol/L. To detect a significant effect of the compound hypoglycemic capsule, with a significance level of \\alpha = 0.05 and power of 1 - \\beta = 0.90, how many mice are required in each group?\n\nThe power.t.test() function and pwr.t.test() function can perform this calculation for a two-sample t-test:\n\npower.t.test(delta = 6, sd = 8, sig.level = 0.05, power = 0.9, type = \"two.sample\")\n\n#&gt; \n#&gt;      Two-sample t test power calculation \n#&gt; \n#&gt;               n = 38.34601\n#&gt;           delta = 6\n#&gt;              sd = 8\n#&gt;       sig.level = 0.05\n#&gt;           power = 0.9\n#&gt;     alternative = two.sided\n#&gt; \n#&gt; NOTE: n is number in *each* group\n\n\n\npower.htest &lt;- pwr.t.test(\n  d = 6 / 8, sig.level = 0.05, power = 0.9,\n  type = \"two.sample\",alternative = \"two.sided\") \n\npower.htest\n\n#&gt; \n#&gt;      Two-sample t test power calculation \n#&gt; \n#&gt;               n = 38.34604\n#&gt;               d = 0.75\n#&gt;       sig.level = 0.05\n#&gt;           power = 0.9\n#&gt;     alternative = two.sided\n#&gt; \n#&gt; NOTE: n is number in *each* group\n\npower.htest |&gt; plot()\n\n\n\n\n\n\n\n\nWith these parameters, the output suggests that 39 mice each group are needed to reliably detect a significant difference in fasting blood glucose levels between the treatment and control groups.\n3. Multiple sample means\nWhen comparing multiple sample means, typically a one-way analysis of variance (ANOVA) is used to test if there are statistically significant differences among the means of three or more independent groups. The one-way ANOVA works under the assumption that the groups have approximately equal variances and that the data are normally distributed within each group.\nWhen comparing multiple sample means with ANOVA, the sample size estimation considers the desired power, significance level, number of groups, and effect size. For a one-way ANOVA with k groups, the sample size n required each group can be estimated using:\n\nn = \\frac{2 (Z_{1 - \\alpha/2} + Z_{1 - \\beta})^2}{f^2}\n\nwhere Z_{1 - \\alpha/2} is the critical value from the standard normal distribution for a two-sided test, Z_{1 - \\beta} is the critical value associated with the desired power, f, i.e. Cohen’s f, is the effect size for ANOVA, defined as:\n\nf = \\frac{\\sqrt{\\frac{\\sum_{i=1}^k (\\bar{X}_i - \\bar{X}_{\\text{grand}})^2}{k}}}{\\sigma}\n\nwhere k is the number of groups, \\bar{X}_i is the mean of the i-th group,\\bar{X}_{\\text{grand}} is the overall (grand) mean across all groups, and \\sigma is the pooled within-group standard deviation, which is expressed as:\n\n\\sigma = \\sqrt{\\frac{\\sum_{i=1}^k (n_i - 1)S_i^2}{\\sum_{i=1}^k (n_i - 1)}}\n\nwhere n_i is the sample size and S_i the standard deviation of group i.\nFor a balanced design with equal group sizes n , \\sigma is calculated as:\n\n\\sigma = \\sqrt{\\frac{\\sum_{i=1}^k S_i^2}{k}}\n\nHere we define a function named cohens.f() to compute Cohen’s f with the group means and the standard deviations:\n\n# Function to calculate Cohen's f for ANOVA\ncohens.f &lt;- function(means, sds) {\n  grand_mean &lt;- mean(means)\n  pooled_sd &lt;- sqrt(mean(sds^2))\n  sd_mean &lt;- sqrt(mean((means - grand_mean)^2))\n  \n  # Calculate Cohen's f\n  cohens_f &lt;- sd_mean / pooled_sd\n  return(cohens_f)\n}\n\n\nExample 3 \nA scientist plans to study the effects of Bifidobacterium and colchicine on liver fibrosis in mice. The mice will be randomly divided into three groups: a model (control) group, a Bifidobacterium infantis group, and a colchicine group. One primary outcome is the liver weight index in the mice. It is estimated that the liver weight index for each group at the end of the experiment will be 6.20, 5.40, and 4.70, respectively, with standard deviations of 1.87, 1.56, and 1.52. With a significance level of \\alpha = 0.05 and power of 1 - \\beta = 0.90, how many mice are needed in each group to detect a significant difference?\n\nThe power.anova.test() function and pwr.anova.test() function can perform this calculation for this example:\n\nmeans &lt;- c(6.20, 5.40, 4.70)\nsds &lt;- c(1.87, 1.56, 1.52)\n\npower.anova.test(\n  groups = 3, between.var = var(means), within.var = mean(sds^2),\n  sig.level = 0.05, power = 0.9) \n\n#&gt; \n#&gt;      Balanced one-way analysis of variance power calculation \n#&gt; \n#&gt;          groups = 3\n#&gt;               n = 31.87412\n#&gt;     between.var = 0.5633333\n#&gt;      within.var = 2.746967\n#&gt;       sig.level = 0.05\n#&gt;           power = 0.9\n#&gt; \n#&gt; NOTE: n is number in each group\n\n\n\nf &lt;- cohens.f(means, sds)\n\npower.htest &lt;- pwr.anova.test(k = 3, f = f, sig.level = 0.05, power = 0.9)\npower.htest\n\n#&gt; \n#&gt;      Balanced one-way analysis of variance power calculation \n#&gt; \n#&gt;               k = 3\n#&gt;               n = 31.8741\n#&gt;               f = 0.3697519\n#&gt;       sig.level = 0.05\n#&gt;           power = 0.9\n#&gt; \n#&gt; NOTE: n is number in each group\n\npower.htest |&gt; plot()\n\n\n\n\n\n\n\n\nFor the given parameters, the output shows that each group should include 32 subjects. This sample size ensures a 90% chance of detecting a meaningful difference between the groups.\n\n\n\n\n\n\nTip\n\n\n\nThe effectsize package provides various effect size calculations, including Cohen’s f for ANOVA directly from model objects. If you have original data collected from a pilot study, first conduct an ANOVA and then extract the effect size.\n\n\n\nset.seed(2024)\n\n# Example data frame for ANOVA analysis\ndata &lt;- data.frame(\n  group = factor(c(rep(\"A\", 15), rep(\"B\", 15), rep(\"C\", 15))),\n  x = c(\n    rnorm(15, 6.20, 1.87), rnorm(15, 5.40, 1.56), rnorm(15, 4.70, 1.52))\n)\n\nmodel &lt;- aov(x ~ group, data = data)\ncohens_f(model, partial = F, alternative = \"two.sided\")\n\n#&gt; # Effect Size for ANOVA (Type I)\n#&gt; \n#&gt; Parameter | Cohen's f |       95% CI\n#&gt; ------------------------------------\n#&gt; group     |      0.34 | [0.00, 0.62]\n\n\n\n\n36.2.2 Comparison of proportions\n1. One-sample proportion\nWhen comparing a sample proportion to a known population proportion, the goal is typically to determine whether the observed sample proportion differs significantly from the known or expected proportion in the population.\nFor a large sample (typically n \\geq 30, or more strictly when np \\geq 5 and n(1 - p) \\geq 5), a z-test is appropriate to compare the sample proportion to the known population proportion. This is because, with a large sample size, the distribution of the sample proportion approximates a normal distribution. The z-test statistic for this test is calculated as:\n\nz = \\frac{\\hat{p} - p_0}{\\sqrt{\\frac{p_0 (1 - p_0)}{n}}}\n\nwhere \\hat{p} is the sample proportion, p_0 is the known population proportion, n is the sample size.\nTo estimate the sample size needed for this test with specified significance level and power, use the following formula:\n\nn = \\frac{(Z_{1 - \\alpha/2} + Z_{1 - \\beta})^2 \\cdot p_0 (1 - p_0)}{(\\hat{p} - p_0)^2}\n\nwhere Z_{1 - \\alpha/2} is the Z-value for the significance level, Z_{1 - \\beta} is the Z-value associated with the desired power.\nFor small samples (when n &lt; 30 , or np &lt; 5 and n(1 - p) &lt; 5), the z-test is not suitable because the sample proportion distribution does not approximate a normal distribution. Instead, a binomial test is recommended for small sample sizes. The binomial test is a non-parametric test, appropriate when normality cannot be assumed. It directly evaluates the difference in proportions based on the binomial distribution, providing a more accurate result for small samples.\n\nExample 4 \nA pharmaceutical institute has developed a new drug expected to increase the effective rate of treating lung cancer model mice from the conventional 45% to 60%. To conduct an animal experiment, how many lung cancer model mice are required to detect an improvement of 15% in the effective rate of the new drug?\n\nHere we write a function named ss.one.prop() to compute the sample size based on the formula above:\n\nss.one.prop &lt;- function(p0, p, alpha = 0.05, power = 0.9, alternative = \"one.sided\") {\n  if (alternative == \"one.sided\") {\n    z_alpha &lt;- qnorm(1 - alpha)\n  } else if (alternative == \"two.sided\") {\n    z_alpha &lt;- qnorm(1 - alpha / 2)\n  } else {\n    stop(\"alternative must be 'one.sided' or 'two.sided'\")\n  }\n  z_beta &lt;- qnorm(power)\n  numerator &lt;- (z_alpha + z_beta)^2 * p0 * (1 - p0)\n  denominator &lt;- (p - p0)^2\n  n &lt;- numerator / denominator\n  \n  METHOD &lt;- \"One-sample comparison of proportion sample size calculation\"\n  \n  structure(\n    list(n = n, alpha = alpha, power = power, alternative = alternative, \n         method = METHOD), \n    class = \"power.htest\")\n}\n\n\nss.one.prop(p0 = 0.45, p = 0.6, alpha = 0.05, power = 0.9)\n\n#&gt; \n#&gt;      One-sample comparison of proportion sample size calculation \n#&gt; \n#&gt;               n = 94.20232\n#&gt;           alpha = 0.05\n#&gt;           power = 0.9\n#&gt;     alternative = one.sided\n\n\nAlternatively, you can use the pwr.p.test() function from the pwr packages:\n\nh &lt;- ES.h(0.6, 0.45)\n\npwr.p.test(h = h, sig.level = 0.05, power = 0.9, alternative = \"greater\")\n\n#&gt; \n#&gt;      proportion power calculation for binomial distribution (arcsine transformation) \n#&gt; \n#&gt;               h = 0.3015253\n#&gt;               n = 94.19357\n#&gt;       sig.level = 0.05\n#&gt;           power = 0.9\n#&gt;     alternative = greater\n\n\nIn summary, for a study designed to detect an effect size (h = 0.3015) with 90% power and a 5% significance level, you would need approximately 95 observations in a one-sided test to determine if the sample proportion is significantly greater than the population proportion.\n2. Two-sample proportions\nTo compare two independent sample proportions, we generally conduct a hypothesis test for the difference between two population proportions. This is typically done using a two-sample z-test for proportions. The test statistic follows a normal distribution (under large sample sizes) and is calculated as:\n\nZ = \\frac{\\hat{p}_1 - \\hat{p}_2}{\\sqrt{\\hat{p}(1 - \\hat{p}) \\left( \\frac{1}{n_1} + \\frac{1}{n_2} \\right)}}\n\nwhere \\hat{p}_1 and \\hat{p}_2 are the two sample proportions, n_1 and n_2 are the sample sizes of each group, \\hat{p} is the pooled sample proportion, which is the weighted average of the two sample proportions:\n\n\\hat{p} = \\frac{x_1 + x_2}{n_1 + n_2}\n\nwhere x_1 and x_2 are the number of successes (events of interest) in each group.\nTo estimate the sample size required for detecting a difference between two independent sample proportions, the following formula can be used:\n\nn_1 = n_2 = \\frac{\\left( Z_{\\alpha/2} + Z_{\\beta} \\right)^2} {2(\\text {sin}^{-1} \\sqrt{p_1} - \\text {sin}^{-1} \\sqrt{p_2})^2}\n\nwhere Z_{\\alpha/2} is the critical value for the two-sided test at significance level \\alpha, Z_{\\beta} is the critical value corresponding to the desired power.\nHere we write a function named ss.two.prop() to compute the sample size based on the formula above:\n\nss.two_prop &lt;- function(p1, p2, alpha = 0.05, power = 0.9, alternative = \"two.sided\") {\n  # Calculate Z-scores for significance level based on test type\n  if (alternative == \"two.sided\") {\n    z_alpha &lt;- qnorm(1 - alpha / 2)  # Two-sided test\n  } else if (alternative == \"one.sided\") {\n    z_alpha &lt;- qnorm(1 - alpha)  # One-sided test\n  } else {\n    stop(\"alternative must be 'two.sided', 'one.sided'\")\n  }\n  \n  z_beta &lt;- qnorm(power)\n  numerator &lt;- (z_alpha + z_beta)^2 \n  denominator &lt;- 2 * (asin(sqrt(p1)) - asin(sqrt(p2)))^2\n  n &lt;- numerator / denominator\n  \n  NOTE &lt;-  \"n is the sample size for each group\"\n  METHOD &lt;- \"Two-sample comparison of proportions sample size calculation\"\n  structure(\n    list(\n      n = n, alpha = alpha, power = power, alternative = alternative, \n      method = METHOD, note = NOTE), \n    class = \"power.htest\")\n}\n\n\nss.two_prop(p1 = 0.95, p2 = 0.80, alpha = 0.05, power = 0.9, alternative = \"two.sided\")\n\n#&gt; \n#&gt;      Two-sample comparison of proportions sample size calculation \n#&gt; \n#&gt;               n = 92.64515\n#&gt;           alpha = 0.05\n#&gt;           power = 0.9\n#&gt;     alternative = two.sided\n#&gt; \n#&gt; NOTE: n is the sample size for each group\n\n\nAlternatively, you can use the power.prop.test() function from the stats packages:\n\npower.prop.test(p1 = 0.95, p2 = 0.8, sig.level = 0.05,\n  power = 0.9, alternative = \"two\") \n\n#&gt; \n#&gt;      Two-sample comparison of proportions power calculation \n#&gt; \n#&gt;               n = 100.0618\n#&gt;              p1 = 0.95\n#&gt;              p2 = 0.8\n#&gt;       sig.level = 0.05\n#&gt;           power = 0.9\n#&gt;     alternative = two.sided\n#&gt; \n#&gt; NOTE: n is number in *each* group\n\n\nAnother calculation method is the pwr.2p.test() function from the pwr packages:\n\nh &lt;- ES.h(0.95, 0.8)\n\npower.htest &lt;- pwr.2p.test(h = h, sig.level = 0.05, power = 0.9, alternative = \"two\")\npower.htest\n\n#&gt; \n#&gt;      Difference of proportion power calculation for binomial distribution (arcsine transformation) \n#&gt; \n#&gt;               h = 0.4762684\n#&gt;               n = 92.64511\n#&gt;       sig.level = 0.05\n#&gt;           power = 0.9\n#&gt;     alternative = two.sided\n#&gt; \n#&gt; NOTE: same sample sizes\n\npower.htest |&gt; plot()\n\n\n\n\n\n\n\n\nIt should be noticed that the sample sizes calculated by power.prop.test() and pwr.2p.test() are different. The difference comes from the underlying methodologies they use to estimate effect size and power. Here’s a brief explanation of each:\n\npower.prop.test() uses a standard normal approximation for the difference between two proportions, calculating power based on a more traditional Z-test approach. It does not transform the proportions and operates directly on the difference in proportions (p1 - p2).\npwr.2p.test() from the pwr package uses Cohen’s h, which is based on the arcsine transformation of the proportions. Cohen’s h is a measure of effect size that is specifically adjusted for proportions, aiming to improve the estimation’s stability, especially when proportions are near 0 or 1.\n\nIf you are working with proportions that are close to 0 or 1, pwr.2p.test() may provide a more accurate sample size estimate due to Cohen’s h. If both proportions are more central, then either method could be appropriate, though results will vary due to these differences.\n3. Multiple sample proportions\nWhen comparing the proportions of multiple samples, we are typically interested in determining whether there are statistically significant differences among the groups. Chi-square test is used to determine if there is a significant association between categorical variables. It can be applied when you have one categorical variable from two or more independent samples. The formula for calculating sample size for each group in a multiple-proportion comparison is:\n\nn = \\frac{\\lambda} {2(\\sin^{-1} \\sqrt{p_{\\text{max}}} - \\sin^{-1} \\sqrt{p_{\\text{min}}})^2}\n\nwhere p_{\\text{max}}is the highest proportion (efficacy rate) among the groups, p_{\\text{min}} is the lowest proportion among the groups, \\lambda is the non-centrality parametera, which is a constant that depends on the desired significance level and power.\n\nExample 5 \nTo observe the effects of 3 methods for treating peptic ulcers in rats, it is preliminarily estimated that method A has an efficacy rate of 40%, method B has an efficacy rate of 50%, and method C has an efficacy rate of 65%. Assuming α = 0.05 and β = 0.10, to conclude that there are differences in efficacy rates, how many rats need to be observed in each group?\n\nFor a given power 1 - \\beta, significance level \\alpha, and effect size w, the sample size N can be calculated using the pwr.chisq.test() function, which is based on the non-central chi-squared distribution. This approach ensures accurate power calculations, especially for cases where the central approximation does not hold.\n\nk &lt;- 3\np0 = c(0.4, 0.5, 0.65);  p1 = c(0.6, 0.5, 0.35)\nP = matrix(c(p0, p1), nrow = k) / k\n1w1 = ES.w2(P)\nw2 &lt;- cohens_w(P, alternative = \"two.sided\") |&gt; pluck(1)\n\npwr.chisq.test(w = w1, df = 2, sig.level = 0.05, power = 0.9) \npwr.chisq.test(w = w2, df = 2, sig.level = 0.05, power = 0.9)\n\n\n1\n\nCalculate the effect size, with the same results\n\n\n\n\n#&gt; \n#&gt;      Chi squared power calculation \n#&gt; \n#&gt;               w = 0.2055947\n#&gt;               N = 299.3655\n#&gt;              df = 2\n#&gt;       sig.level = 0.05\n#&gt;           power = 0.9\n#&gt; \n#&gt; NOTE: N is the number of observations\n#&gt; \n#&gt; \n#&gt;      Chi squared power calculation \n#&gt; \n#&gt;               w = 0.2055947\n#&gt;               N = 299.3655\n#&gt;              df = 2\n#&gt;       sig.level = 0.05\n#&gt;           power = 0.9\n#&gt; \n#&gt; NOTE: N is the number of observations\n\n\n\n\n36.2.3 Linear correlation\nIn linear correlation analysis, sample size estimation is typically done to achieve a specified power in hypothesis test. The sample size required depends on the expected effect size (correlation coefficient), significance level \\alpha, and desired power 1 -\\beta. The sample size N can be estimated using the following formula:\n\nN = \\frac{(Z_{\\alpha / 2} + Z_{\\beta})^2}{\\left(0.5 \\times \\ln\\left(\\frac{1 + r}{1 - r}\\right)\\right)^2} + 3\n\nwhere r expected correlation coefficient (effect size), Z_{\\alpha / 2} is the critical value from the standard normal distribution for the significance level \\alpha (for a two-tailed test, use \\alpha / 2), Z_{\\beta} is the critical value from the standard normal distribution for the power level 1 - \\beta, and \\ln\\left(\\frac{1 + r}{1 - r}\\right)represents Fisher’s z-transformation of the correlation coefficient r, which converts the correlation coefficient to a standard normal form.\nThe following function named ss.linear.corr() can be used to calculate the sample size needed for correlation analysis based on the above formula:\n\nss.linear.corr &lt;- function(sig.level = 0.05, power = NULL, r, alternative = \"two.sided\") {\n  # Calculate z-scores\n  if (alternative == \"two.sided\") {\n    z_alpha &lt;- qnorm(1 - sig.level / 2)  # Two-sided test\n  } else if (alternative == \"one.sided\") {\n    z_alpha &lt;- qnorm(1 - alpha)  # One-sided test\n  } else {\n    stop(\"alternative must be 'two.sided', 'one.sided'\")\n  }\n  z_beta &lt;- qnorm(power)\n  \n  # Calculate Fisher's z transformation of r\n  z_r &lt;- 0.5 * log((1 + r) / (1 - r))\n  \n  # Calculate sample size\n  n &lt;- (z_alpha + z_beta)^2 / z_r^2 + 3\n  \n  METHOD &lt;- \"A approximate correlation sample size calculation\"\n  structure(\n    list(\n      n = n, r = r, sig.level = sig.level, power = power,\n      method = METHOD), \n    class = \"power.htest\")\n}\n\n\nExample 6 \nAccording to previous experience, the correlation coefficient between food intake and weight gain in rats is known to be 0.8. To obtain a statistically significant conclusion for this correlation at a significance level of \\alpha = 0.05 and power level of 1 - \\beta = 0.90, how many rats should be studied?\n\n\nss.linear.corr(sig.level = 0.05, power = 0.9, r = 0.8)\n\n#&gt; \n#&gt;      A approximate correlation sample size calculation \n#&gt; \n#&gt;               n = 11.70577\n#&gt;               r = 0.8\n#&gt;       sig.level = 0.05\n#&gt;           power = 0.9\n\n\nAlternative, you can use the pwr.r.test() function from the pwr packages:\n\npwr.r.test(sig.level = 0.05, power = 0.9, r = 0.8) \n\n#&gt; \n#&gt;      approximate correlation power calculation (arctangh transformation) \n#&gt; \n#&gt;               n = 11.16238\n#&gt;               r = 0.8\n#&gt;       sig.level = 0.05\n#&gt;           power = 0.9\n#&gt;     alternative = two.sided",
    "crumbs": [
      "Practical skills",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Sample size estimation</span>"
    ]
  },
  {
    "objectID": "sample-size.html#observational-studies",
    "href": "sample-size.html#observational-studies",
    "title": "36  Sample size estimation",
    "section": "36.3 Observational studies",
    "text": "36.3 Observational studies\nIn observational studies, the sample size required for estimating population parameters typically depends on the following factors:\n1. Confidence level (1 - \\alpha)\nIt is commonly set at 95% (corresponding to \\alpha = 0.05), the confidence level determines the width of the confidence interval around the estimate. Higher confidence levels increase the required sample size.\n2. Margin of error (d)\nThis is the allowable deviation between the estimate and the true population parameter, such as 5%. A smaller margin of error requires a larger sample size.\n3. Estimated proportion or effect size (p or \\theta)\nFor proportion estimates, an estimated proportion p from prior studies (e.g., 20% disease prevalence) is often used. For mean estimates, an anticipated standard deviation or measure of variability is needed.\n4. Standard deviation (\\sigma)\nFor continuous variables, an anticipated population standard deviation \\sigma is needed to account for variability. Higher variability increases the sample size requirement.\n5. Power (1 - \\beta)\nPower is the probability of detecting a true effect and is commonly set at 80% (corresponding to \\beta = 0.20). Higher power requires a larger sample size.\n6. Design effect\nFor studies using complex sampling designs (e.g., stratified or clustered sampling), a design effect is applied to account for the potential increase in required sample size due to the design.\n7. Number of strata or groups\nIf the study involves stratified or subgroup analysis, the sample size in each stratum or group must be adequate to ensure reliable estimates within each subgroup.\nThese parameters collectively help determine the appropriate sample size for cross-sectional, case-control, or cohort studies, often using statistical formulas or software specifically designed for sample size estimation.\n\n36.3.1 Cross-sectional study\nCross-sectional studies typically aim to estimate the prevalence or proportion of an outcome (such as a disease or behavior) in a population.\n1. Simple random sampling\nIn simple random sampling, the required sample size to estimate a population parameter accurately depends on the following formula and factors:\nFor estimating a proportion, the sample size n can be calculated using:\n\nn = \\frac{Z_{1-\\alpha/2}^2 \\cdot p \\cdot (1 - p)}{d^2}\n\nwhere Z_{1-\\alpha/2} is the Z-score corresponding to the significance level, p is the expected proportion or prevalence of the outcome (based on prior studies or estimates), d is the desired margin of error.\n\nss.srs.prop &lt;- function(p, d, alpha) {\n  z &lt;- qnorm(1 - alpha / 2)\n  n &lt;- (z^2 * p * (1 - p)) / d^2\n  n &lt;- ceiling(n)\n  \n  METHOD &lt;- \"Sample size calculation for estimating a proportion\"\n  structure(\n    list(\n      n = n, alpha = alpha, p = p, d = d, method = METHOD), \n    class = \"power.htest\")\n}\n\nFor estimating a mean, the sample size n is:\n\nn = \\frac{{Z_{1-\\alpha/2}^2 \\cdot \\sigma^2}}{{d^2}}\n\nwhere Z_{1-\\alpha/2} is the Z-score corresponding to the significance level, \\sigma is the estimated population standard deviation, d is the margin of error.\n\nss.srs.mean &lt;- function(sigma, d, alpha = 0.05) {\n  z &lt;- qnorm(1 - alpha / 2)\n  n &lt;- (z^2 * sigma^2) / d^2\n  n &lt;- ceiling(n)\n  \n  METHOD &lt;- \"Sample size calculation for estimating a mean\"\n  structure(\n    list(\n      n = n, alpha = alpha, sigma = sigma, d = d, method = METHOD), \n    class = \"power.htest\")\n}\n\nIf the population size N is finite and relatively small, the sample size can be adjusted using the finite population correction:\n\nn_{\\text{adj}} = \\frac{n}{1 + \\frac{n - 1}{N}}\n\nwhere n is the calculated sample size from the above formulas without the correction.\n\nadjusted.sample.size &lt;- function(x, N) {\n  n &lt;- x$n\n  n_adj &lt;- n / (1 + (n - 1) / N)\n  n_adj &lt;- ceiling(n_adj)\n  \n  METHOD &lt;- \"Sample size adjusted by the finite population correction\"\n  structure(\n    list(\n      N= N, n = n, n_adj = n_adj, alpha = alpha, sigma = sigma,\n      d = d, method = METHOD), \n    class = \"power.htest\")\n}\n\n\nExample 7 \nTo determine the prevalence of hypertension in a city’s population, given that the national prevalence of hypertension is 19.6% according to past data, and allowing for a margin of error of 0.01 with a significance level of \\alpha = 0.05, estimate the required sample size using simple random sampling.\n\n\np &lt;- 0.196; d &lt;- 0.01; alpha &lt;- 0.05\nss.srs.prop(p, d, alpha)\n\n#&gt; \n#&gt;      Sample size calculation for estimating a proportion \n#&gt; \n#&gt;               n = 6054\n#&gt;           alpha = 0.05\n#&gt;               p = 0.196\n#&gt;               d = 0.01\n\n\n\nExample 8 \nA factory has 5,500 workers, and a simple random sample is planned to assess the average white blood cell count of the workers to determine whether working conditions affect white blood cell levels. The allowable error margin is set to 0.1 × 10⁹/L, and with a significance level of \\alpha = 0.05, how many workers need to be surveyed at minimum? (Based on previous data, the standard deviation of employees’ white blood cell counts is 0.95 × 10⁹/L)\n\n\nalpha &lt;- 0.05; sigma &lt;- 0.95 * 10^9; d &lt;- 0.1 * 10^9  \nres &lt;- ss.srs.mean(sigma, d, alpha)\nadjusted.sample.size(res, N = 5500)\n\n#&gt; \n#&gt;      Sample size adjusted by the finite population correction \n#&gt; \n#&gt;               N = 5500\n#&gt;               n = 347\n#&gt;           n_adj = 327\n#&gt;           alpha = 0.05\n#&gt;           sigma = 9.5e+08\n#&gt;               d = 1e+08\n\n\n2. Stratified random sampling\nIn stratified random sampling, the formulas for calculating the total sample size needed to estimate a proportion and a mean are slightly different. Each involves breaking down the population into strata and using information within each stratum to improve the precision of the overall estimate.\nTo estimate a proportion with stratified random sampling, the sample size n can be estimated using the following formula, which takes into account finite population correction and the weights of each strata.\n\nn = \\frac{Z_{1-\\alpha/2}^2 \\sum_{h=1}^L W_h^2 \\frac{\\sqrt{p_h (1 - p_h)} \\sum N_h \\sqrt{p_h (1 - p_h)}}{N_h}}{d^2 + Z_{1-\\alpha/2}^2  \\frac{\\sum_{h=1}^L W_h p_h (1 - p_h)}{N}}\n\nwhere n is the total sample size across all strata, Z_{1-\\alpha/2} is the z-score corresponding to the desired confidence level, W_h = N_h / N is the weight for the h-th stratum, representing the proportion of the stratum within the total population, N_h is the population size of the h-th stratum, N is the overall population size, p_h is the estimated proportion in the h-th stratum, d is the allowable margin of error, and L total number of strata.\nOnce n is calculated, the sample size for each stratum n_h can be determined using Neyman (optimal) allocation:\n\nn_h = n \\times \\frac{N_h \\sqrt{p_h (1 - p_h)}}{\\sum_{h=1}^L N_h \\sqrt{p_h (1 - p_h)}}\n\nThis allocation minimizes the variance in estimating the overall proportion.\n\nss.strata.prop &lt;- function(N_h, p_h, d, alpha = 0.05) {\n  N &lt;- sum(N_h)\n  z &lt;- qnorm(1 - alpha / 2)\n  W_h &lt;- N_h / N \n  u &lt;- sum(N_h * sqrt(p_h * (1 - p_h)))\n  numerator &lt;- z^2 * sum(W_h^2 * sqrt(p_h * (1 - p_h)) * u / N_h)\n  denominator &lt;- d^2 + z^2 * sum(W_h * p_h * (1 - p_h) / N)\n  \n  n &lt;- numerator / denominator\n  n_h &lt;- (n * N_h / N) * (N_h - n / N) / (N_h -1)\n  n &lt;- ceiling(n)\n  n_h &lt;- ceiling(n_h)\n  \n  METHOD &lt;- \"Sample size calculation for estimating a proportion\"\n  structure(\n    list(n = n, n_h = n_h, alpha = alpha, d = d, method = METHOD), \n    class = \"power.htest\")\n}\n\nWhen estimating a mean, the total sample size n for stratified sampling, adjusted for finite population correction, can be estimated using:\n\nn = \\frac{Z_{1- \\alpha/2}^2 \\sum_{h=1}^L W_h^2 \\frac{S_h \\sum N_h S_h}{N_h}}{d^2 + Z_{1- \\alpha/2} ^2\\sum_{h=1}^L \\frac{W_h S_h^2}{N}}\n\nwhere S_h is the standard deviation within the h-th stratum (an estimate of the population variance within each stratum). Other terms are as defined above.\nSimilar to proportions, Neyman allocation for means is:\n\nn_h = n \\times \\frac{N_h S_h}{\\sum_{h=1}^L N_h S_h}\n\nwhere S_h is the standard deviation within each stratum. This allocation ensures efficient estimation by reducing sampling error.\n\nss.strata.mean &lt;- function(N_h, S_h, d, alpha = 0.05) {\n  N &lt;- sum(N_h)\n  z &lt;- qnorm(1 - alpha / 2)\n  \n  u &lt;- z^2 * sum((N_h / N)^2 * (S_h * sum(N_h * S_h)) / N_h)\n  v &lt;- d^2\n  w &lt;- z^2 * sum((N_h / N) * S_h^2 / N) \n  \n  # Calculate sample size\n  n &lt;- u / (v + w)\n  n_h &lt;- n * (N_h * S_h) / sum(N_h * S_h)\n  \n  n &lt;- ceiling(n)\n  n_h &lt;- ceiling(n_h)\n  \n  METHOD &lt;- \"Sample size calculation for estimating a mean\"\n  structure(\n    list(n = n, n_h = n_h, alpha = alpha, d = d, method = METHOD), \n    class = \"power.htest\")\n}\n\n\nExample 9 \nTo conduct a stratified random sampling survey on the annual medical expenses of workers in a certain factory, past survey data is provided in Table 36.1 . The goal is to understand the annual medical expenses of the factory’s employees, with an allowable margin of error, \\delta, not exceeding 5 yuan and with a confidence level of \\alpha = 0.05. What is the minimum number of workers that need to be surveyed?\n\n\n\n\n\nTable 36.1: Drug expenses of workers in each workshop of a factory\n\n\n\n\n\n\nworkshop\nn\nmean\nsd\n\n\n\n\n1\n1200\n180\n65\n\n\n2\n3200\n150\n50\n\n\n3\n600\n260\n90\n\n\n\n\n\n\n\n\n\nN_h &lt;- c(1200, 3200, 600)  # Population size for each stratum\nS_h &lt;- c(65, 50, 90) # Standard deviation for each stratum\nd &lt;- 5                     # Allowable error\nalpha &lt;- 0.05\n\nss.strata.mean(N_h, S_h, d, alpha = 0.05)\n\n#&gt; \n#&gt;      Sample size calculation for estimating a mean \n#&gt; \n#&gt;               n = 473\n#&gt;             n_h = 127, 259, 88\n#&gt;           alpha = 0.05\n#&gt;               d = 5\n\n\n3. Cluster sampling\nCluster sampling is a sampling method where the population is divided into separate groups, called clusters, and a random sample of these clusters is selected for study. Cluster sampling is often used when the population is large and geographically dispersed, as it can be more practical and cost-effective than simple random sampling.\nEstimating sample size for population proportion\nFor an infinite population, the required sample size for estimating a population proportion is calculated using:\n\nk = Z_{1-\\alpha/2}^2 \\sum \\frac{m_h^2 (p_h - p)^2}{(k_y - 1) \\bar m^2 d^2}\n\nFor a finite population, adjust the sample size as follows:\n\nk_\\text {adj} = k \\left(1 - \\frac{k_0}{K}\\right)\n\\tag{36.1}\nwhere Z_{1 - \\alpha / 2} is the Z-score corresponding to the desired confidence level, k_0 is the sample size for an infinite population, k is the number of clusters to be surveyed, m_h is the number of individuals surveyed in the h-th cluster, p_h is the event occurrence rate in the h-th cluster, \\bar m is the average number of individuals per cluster, p is the average occurrence rate, d is the allowable error margin, and K is the total number of clusters in the population.\n\nss.cluster.prop &lt;- function(m_h, p_h, K, d, alpha = 0.05) {\n  z &lt;- qnorm(1 - alpha / 2)\n  k_y &lt;- length(m_h)\n  m &lt;- sum(m_h) / k_y\n  p &lt;- sum(m_h * p_h) / sum(m_h)\n  sum_p &lt;- sum(m_h^2 * (p_h - p)^2 / ((k_y - 1) * m^2 * d^2))\n  k &lt;- z^2 * sum_p\n  k.adj &lt;- k * (1 - k / K)\n  \n  k &lt;- ceiling(k)\n  k.adj &lt;- ceiling(k.adj)\n  \n  METHOD &lt;- \"Sample size calculation for estimating a proportion\"\n  structure(\n    list(k = k, k.adj = k.adj, alpha = alpha, d = d, method = METHOD),\n    class = \"power.htest\")\n}\n\nEstimating sample size for population mean\nFor an infinite population, the required sample size for estimating a population mean is calculated using:\n\nk_0 = Z_{\\alpha/2}^2 \\sum \\frac{m_h (\\bar{X}_h - \\bar{X})^2}{(k_y - 1) \\bar m^2 d^2}\n\nwhere \\bar{X}_i is the mean of the observed index for the i-th cluster, \\bar{X} is the overall mean. Other variables have meanings similar to those in the proportion estimation formula.\nFor a finite population, adjust the sample size as Equation 36.1 .\n\nss.cluster.mean &lt;- function(m_h, x_h, K, d, alpha = 0.05) {\n  z &lt;- qnorm(1 - alpha / 2)\n  k_y &lt;- length(m_h)\n  m &lt;- sum(m_h) / k_y\n  x &lt;- sum(m_h * x_h) / sum(m_h)\n  sum_m &lt;- sum(m_h^2 * (p_h - p)^2 / ((k_y - 1) * m^2 * d^2))\n  k &lt;- z^2 * sum_m\n  k.adj &lt;- k * (1 - k / K)\n  \n  k &lt;- ceiling(k)\n  k.adj &lt;- ceiling(k.adj)\n  \n  METHOD &lt;- \"Sample size calculation for estimating a mean\"\n  structure(\n    list(k = k, k.adj = k.adj, alpha = alpha, d = d, method = METHOD),\n    class = \"power.htest\")\n}\n\n\nExample 10 \nTo estimate the prevalence of cardiovascular and cerebrovascular diseases in people over 40 in a city, a cluster sampling survey is planned across 62 communities. A preliminary survey was conducted in two districts, randomly selected by the survey team: In the first district, 5,368 people were surveyed, with 1,860 cases of cardiovascular and cerebrovascular diseases, resulting in a prevalence rate of 0.3465. In the second district, 6,236 people were surveyed, with 1,268 cases, resulting in a prevalence rate of 0.2033. The question is: How many districts need to be surveyed to estimate the prevalence, given a significance level of \\alpha = 0.05 and a margin of error \\delta = 0.1?\n\n\nm_h &lt;- c(5368, 6236)\np_h &lt;- c(0.3465, 0.2033)\nK &lt;- 62\nd &lt;- 0.1\nalpha &lt;- 0.05\nss.cluster.prop(m_h, p_h, K, d, alpha)\n\n#&gt; \n#&gt;      Sample size calculation for estimating a proportion \n#&gt; \n#&gt;               k = 4\n#&gt;           k.adj = 4\n#&gt;           alpha = 0.05\n#&gt;               d = 0.1\n\n\n\n\n36.3.2 Case-control study\nSample size estimation for a case-control study is essential to ensure adequate power to detect an association between exposure and outcome. The required sample size can be estimated based on the expected odds ratio for the exposure between cases and controls, the prevalence of exposure in the control group, desired significance level, power, and ratio of controls to cases. This ratio is often set to 1 (equal number of cases and controls), but can be different.\nFor a case-control study, the sample size n_1 (cases) and n_0 (controls) can be estimated using the following formula:\n\nn1 = \\frac{\\left(Z_{1-\\alpha/2} \\sqrt{2p(1 - p)} + Z_{1-\\beta} \\sqrt{p_1(1 - p_1) + p_0(1 - p_0)}\\right)^2}{(p_1 - p_0)^2}\n\nwhere Z_{1-\\alpha/2} and Z_{1-\\beta} are the Z-scores for the significance level \\alpha and desired power 1 - \\beta, respectively, p_0 is the the proportion of controls expected to have the exposure, p_1 is the expected proportion of exposure in cases, which can be calculated using the odds ratio (OR):\n\np_1 = \\frac{\\text {OR} \\times p_0}{1 + p_0 \\times (\\text {OR} - 1)}\n\np = \\frac{p_1 + p_0}{2} is the average of exposure rates in cases and controls. The sample size of control n_0 = k \\times n_1, where k is the ratio of controls to cases. The total sample size is then obtained by n = n_0 + n_1,\n\nss.case.control &lt;- function(p0, or, alpha, beta, k = 1){\n  z_alpha &lt;- qnorm(1 - alpha / 2)\n  z_beta &lt;- qnorm(1 - beta)\n  p1 &lt;- (or * p0) / (1 + p0 * (or - 1))\n  p &lt;- (p0 + p1) / 2\n  \n  u &lt;- (z_alpha * sqrt(2 * p * (1 - p)) + \n        z_beta * sqrt(p1 * (1 - p1) + p0 * (1 - p0)))^2\n  v &lt;- (p1 - p0)^2\n  n1 &lt;- ceiling(u / v)\n  n0 &lt;- ceiling(k * n1)\n\n  METHOD &lt;- \"Sample size calculation for case-control study\"\n  structure(\n    list(n0 = n0, n1 = n1, alpha = alpha, beta = beta, \n         OR = or, method = METHOD),\n    class = \"power.htest\")\n}\n\n\np0 &lt;- 0.3; or &lt;- 3; alpha &lt;- 0.05; beta &lt;- 0.1\nss.case.control(p0, or, alpha, beta)\n\n#&gt; \n#&gt;      Sample size calculation for case-control study \n#&gt; \n#&gt;              n0 = 73\n#&gt;              n1 = 73\n#&gt;           alpha = 0.05\n#&gt;            beta = 0.1\n#&gt;              OR = 3\n\n\n\n\n36.3.3 Cohort study\nIn cohort studies, sample size estimation typically depends on the incidence rates in the exposed and unexposed groups and the expected risk ratio (RR). The formula is similar to that used in case-control studies:\n\nn1 = \\frac{\\left(Z_{1-\\alpha/2} \\sqrt{2p(1 - p)} + Z_{1-\\beta} \\sqrt{p_1(1 - p_1) + p_0(1 - p_0)}\\right)^2}{(p_1 - p_0)^2}\n\nwhere p_1 and p_0 represent the expected incidence rates for the exposed and unexposed groups, respectively, p_1 can be calculated by RR: p_1 = p_0 \\times \\text {RR}.\n\nss.cohort &lt;- function(p0, rr, alpha, beta, k = 1){\n  z_alpha &lt;- qnorm(1 - alpha / 2)\n  z_beta &lt;- qnorm(1 - beta)\n  p1 &lt;- rr * p0\n  p &lt;- (p0 + p1) / 2\n  \n  u &lt;- (z_alpha * sqrt(2 * p * (1 - p)) + \n        z_beta * sqrt(p1 * (1 - p1) + p0 * (1 - p0)))^2\n  v &lt;- (p1 - p0)^2\n  n1 &lt;- ceiling(u / v)\n  n0 &lt;- ceiling(k * n1)\n\n  METHOD &lt;- \"Sample size calculation for cohort study\"\n  structure(\n    list(n0 = n0, n1 = n1, alpha = alpha, beta = beta, \n         RR = rr, method = METHOD),\n    class = \"power.htest\")\n}\n\n\np0 &lt;- 0.01; rr &lt;- 2; alpha &lt;- 0.05; beta &lt;- 0.1\nss.cohort(p0, rr, alpha, beta)\n\n#&gt; \n#&gt;      Sample size calculation for cohort study \n#&gt; \n#&gt;              n0 = 3103\n#&gt;              n1 = 3103\n#&gt;           alpha = 0.05\n#&gt;            beta = 0.1\n#&gt;              RR = 2",
    "crumbs": [
      "Practical skills",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Sample size estimation</span>"
    ]
  },
  {
    "objectID": "sample-size.html#survival-analysis",
    "href": "sample-size.html#survival-analysis",
    "title": "36  Sample size estimation",
    "section": "36.4 Survival analysis",
    "text": "36.4 Survival analysis\n\n36.4.1 Two survival rates\nThe sample size for comparing two survival rates, assuming exponential survival times and using the log-rank test. This approach typically involves estimating the number of events required for sufficient statistical power in survival analysis.\nThe getSampleSizeSurvival() function from rpact packages can be used for this purpose:\n\nExample 11 \nA scientist plans to conduct a study comparing the effects of a new surgical method with a traditional surgical method on the survival outcomes of patients with malignant adrenal tumors. The recruitment period for the study is set for 1 year, with all subjects entering the study uniformly, and a follow-up period of 2 years. It is known that the 2-year survival rate for the traditional surgical method is 50%, and the anticipated 2-year survival rate for the new surgical method is 80%. The estimated loss to follow-up rate for both groups is 6% per year, with a contamination rate of 3% in each group. Assuming a two-sided significance level of \\alpha = 0.05 and a power of 1 - \\beta = 0.90, what sample size is needed for this study?\n\n\nres &lt;- getSampleSizeSurvival(\n  alpha = 0.05, sided = 2, beta = 0.1, twoSidedPower = F, \n  pi1 = 0.5, pi2 = 0.8, eventTime = 24, followUpTime = 24, \n  accrualTime = c(0, 12), dropoutRate1 = 0.06, dropoutRate2 = 0.06, \n  typeOfComputation = \"Freedman\", allocationRatioPlanned = 1)\n\nres |&gt; \n  as_tibble() |&gt; \n  select(pi1, pi2, hazardRatio, nFixed1, nFixed2)\n\n#&gt; # A tibble: 1 × 5\n#&gt;     pi1   pi2 hazardRatio nFixed1 nFixed2\n#&gt;   &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1   0.5   0.8       0.431    48.8    48.8",
    "crumbs": [
      "Practical skills",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Sample size estimation</span>"
    ]
  },
  {
    "objectID": "pilot-project.html",
    "href": "pilot-project.html",
    "title": "37  A case to analyze",
    "section": "",
    "text": "37.1 Datasets\nIn this book, we used publicly available CDISC pilot study data, which is accessible through the CDISC GitHub repository. To streamline the process, we have downloaded all the datasets from the repository and converted them from .xpt format to .sas7bdat format for ease of use and compatibility. Then we stored them in the data/adam/ folder within this project. Additionally, The dataset structure adheres to the CDISC Analysis Data Model (ADaM) standard.\nThe SDTM-ADaM Pilot Project datasets were created to demonstrate the process of converting clinical trial data into formats that comply with the Study Data Tabulation Model (SDTM) and Analysis Data Model (ADaM) standards, which are set by the Clinical Data Interchange Standards Consortium (CDISC). These datasets are used to test, validate, and illustrate how to implement CDISC standards in real-world scenarios, helping pharmaceutical companies and regulatory agencies like the FDA ensure data quality and consistency in clinical trials.\nlibrary(haven)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nadsl &lt;- read_sas(\"data-adam/adsl.sas7bdat\")\n\nadsl |&gt; \n  select(ARM, ITTFL, EFFFL) |&gt; \n  filter(EFFFL == \"Y\") |&gt; \n  group_by(ARM) |&gt; \n  summarise(n = n())\n\n# A tibble: 3 × 2\n  ARM                      n\n  &lt;chr&gt;                &lt;int&gt;\n1 Placebo                 79\n2 Xanomeline High Dose    74\n3 Xanomeline Low Dose     81\nadsl |&gt; \n  glimpse()\n\nRows: 254\nColumns: 48\n$ STUDYID  &lt;chr&gt; \"CDISCPILOT01\", \"CDISCPILOT01\", \"CDISCPILOT01\", \"CDISCPILOT01…\n$ USUBJID  &lt;chr&gt; \"01-701-1015\", \"01-701-1023\", \"01-701-1028\", \"01-701-1033\", \"…\n$ SUBJID   &lt;chr&gt; \"1015\", \"1023\", \"1028\", \"1033\", \"1034\", \"1047\", \"1097\", \"1111…\n$ SITEID   &lt;chr&gt; \"701\", \"701\", \"701\", \"701\", \"701\", \"701\", \"701\", \"701\", \"701\"…\n$ SITEGR1  &lt;chr&gt; \"701\", \"701\", \"701\", \"701\", \"701\", \"701\", \"701\", \"701\", \"701\"…\n$ ARM      &lt;chr&gt; \"Placebo\", \"Placebo\", \"Xanomeline High Dose\", \"Xanomeline Low…\n$ TRT01P   &lt;chr&gt; \"Placebo\", \"Placebo\", \"Xanomeline High Dose\", \"Xanomeline Low…\n$ TRT01PN  &lt;dbl&gt; 0, 0, 81, 54, 81, 0, 54, 54, 54, 0, 0, 81, 81, 81, 0, 81, 81,…\n$ TRT01A   &lt;chr&gt; \"Placebo\", \"Placebo\", \"Xanomeline High Dose\", \"Xanomeline Low…\n$ TRT01AN  &lt;dbl&gt; 0, 0, 81, 54, 81, 0, 54, 54, 54, 0, 0, 81, 81, 81, 0, 81, 81,…\n$ TRTSDT   &lt;date&gt; 2014-01-02, 2012-08-05, 2013-07-19, 2014-03-18, 2014-07-01, …\n$ TRTEDT   &lt;date&gt; 2014-07-02, 2012-09-01, 2014-01-14, 2014-03-31, 2014-12-30, …\n$ TRTDUR   &lt;dbl&gt; 182, 28, 180, 14, 183, 26, 190, 10, 55, 182, 183, 183, 38, 18…\n$ AVGDD    &lt;dbl&gt; 0.0, 0.0, 77.7, 54.0, 76.9, 0.0, 54.0, 54.0, 54.0, 0.0, 0.0, …\n$ CUMDOSE  &lt;dbl&gt; 0, 0, 13986, 756, 14067, 0, 10260, 540, 2970, 0, 0, 14121, 26…\n$ AGE      &lt;dbl&gt; 63, 64, 71, 74, 77, 85, 68, 81, 84, 52, 84, 81, 75, 57, 79, 5…\n$ AGEGR1   &lt;chr&gt; \"&lt;65\", \"&lt;65\", \"65-80\", \"65-80\", \"65-80\", \"&gt;80\", \"65-80\", \"&gt;80…\n$ AGEGR1N  &lt;dbl&gt; 1, 1, 2, 2, 2, 3, 2, 3, 3, 1, 3, 3, 2, 1, 2, 1, 2, 2, 2, 3, 2…\n$ AGEU     &lt;chr&gt; \"YEARS\", \"YEARS\", \"YEARS\", \"YEARS\", \"YEARS\", \"YEARS\", \"YEARS\"…\n$ RACE     &lt;chr&gt; \"WHITE\", \"WHITE\", \"WHITE\", \"WHITE\", \"WHITE\", \"WHITE\", \"WHITE\"…\n$ RACEN    &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1…\n$ SEX      &lt;chr&gt; \"F\", \"M\", \"M\", \"M\", \"F\", \"F\", \"M\", \"F\", \"M\", \"M\", \"M\", \"F\", \"…\n$ ETHNIC   &lt;chr&gt; \"HISPANIC OR LATINO\", \"HISPANIC OR LATINO\", \"NOT HISPANIC OR …\n$ SAFFL    &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"…\n$ ITTFL    &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"…\n$ EFFFL    &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"…\n$ COMP8FL  &lt;chr&gt; \"Y\", \"N\", \"Y\", \"N\", \"Y\", \"N\", \"Y\", \"N\", \"Y\", \"Y\", \"Y\", \"Y\", \"…\n$ COMP16FL &lt;chr&gt; \"Y\", \"N\", \"Y\", \"N\", \"Y\", \"N\", \"Y\", \"N\", \"N\", \"Y\", \"Y\", \"Y\", \"…\n$ COMP24FL &lt;chr&gt; \"Y\", \"N\", \"Y\", \"N\", \"Y\", \"N\", \"Y\", \"N\", \"N\", \"Y\", \"Y\", \"Y\", \"…\n$ DISCONFL &lt;chr&gt; \"\", \"Y\", \"\", \"Y\", \"\", \"Y\", \"\", \"Y\", \"Y\", \"\", \"\", \"\", \"Y\", \"\",…\n$ DSRAEFL  &lt;chr&gt; \"\", \"Y\", \"\", \"\", \"\", \"Y\", \"\", \"Y\", \"Y\", \"\", \"\", \"\", \"Y\", \"\", …\n$ DTHFL    &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"…\n$ BMIBL    &lt;dbl&gt; 25.1, 30.4, 31.4, 28.8, 26.1, 30.4, 27.3, 23.9, 23.9, 21.9, 2…\n$ BMIBLGR1 &lt;chr&gt; \"25-&lt;30\", \"&gt;=30\", \"&gt;=30\", \"25-&lt;30\", \"25-&lt;30\", \"&gt;=30\", \"25-&lt;30…\n$ HEIGHTBL &lt;dbl&gt; 147.3, 162.6, 177.8, 175.3, 154.9, 148.6, 168.9, 158.2, 181.6…\n$ WEIGHTBL &lt;dbl&gt; 54.4, 80.3, 99.3, 88.5, 62.6, 67.1, 78.0, 59.9, 78.9, 71.2, 7…\n$ EDUCLVL  &lt;dbl&gt; 16, 14, 16, 12, 9, 8, 18, 22, 12, 14, 12, 10, 16, 15, 6, 16, …\n$ DISONSDT &lt;date&gt; 2010-04-30, 2006-03-11, 2009-12-16, 2009-08-02, 2011-09-29, …\n$ DURDIS   &lt;dbl&gt; 43.9, 76.4, 42.8, 55.3, 32.9, 42.0, 99.1, 40.7, 101.9, 44.2, …\n$ DURDSGR1 &lt;chr&gt; \"&gt;=12\", \"&gt;=12\", \"&gt;=12\", \"&gt;=12\", \"&gt;=12\", \"&gt;=12\", \"&gt;=12\", \"&gt;=12…\n$ VISIT1DT &lt;date&gt; 2013-12-26, 2012-07-22, 2013-07-11, 2014-03-10, 2014-06-24, …\n$ RFSTDTC  &lt;chr&gt; \"2014-01-02\", \"2012-08-05\", \"2013-07-19\", \"2014-03-18\", \"2014…\n$ RFENDTC  &lt;chr&gt; \"2014-07-02\", \"2012-09-02\", \"2014-01-14\", \"2014-04-14\", \"2014…\n$ VISNUMEN &lt;dbl&gt; 12, 5, 12, 5, 12, 6, 12, 4, 8, 12, 12, 12, 7, 12, 12, 7, 4, 7…\n$ RFENDT   &lt;date&gt; 2014-07-02, 2012-09-02, 2014-01-14, 2014-04-14, 2014-12-30, …\n$ DCDECOD  &lt;chr&gt; \"COMPLETED\", \"ADVERSE EVENT\", \"COMPLETED\", \"STUDY TERMINATED …\n$ DCREASCD &lt;chr&gt; \"Completed\", \"Adverse Event\", \"Completed\", \"Sponsor Decision\"…\n$ MMSETOT  &lt;dbl&gt; 23, 23, 23, 23, 21, 23, 10, 23, 20, 20, 19, 21, 22, 21, 10, 1…\nglimpse() makes it possible to see every column in a data frame. It’s a little like str() applied to a data frame but it tries to show you as much data as possible.\nOnce you read data in, the first step usually involves transforming it in some way to make it easier to work with in the rest of your analysis. Firstly, we use janitor::clean_names() to turn all column names of data frame adsl into snake case.",
    "crumbs": [
      "Practical skills",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>A case to analyze</span>"
    ]
  },
  {
    "objectID": "quarto.html",
    "href": "quarto.html",
    "title": "38  Quarto",
    "section": "",
    "text": "38.1 GitHub Action\n呈现和发布内容有几种不同的方法。下面，我们将提供一个使用 GitHub Actions 和 GitHub Pages 发布内容的指南。",
    "crumbs": [
      "Practical skills",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "quarto.html#github-action",
    "href": "quarto.html#github-action",
    "title": "38  Quarto",
    "section": "",
    "text": "38.1.1 执行代码\n如果你愿意，可以配置一个 GitHub Action 来执行 R 代码作为渲染的一部分。这虽然是最好的方法，但当在 GitHub Actions 等 CI 服务中执行代码时，请考虑以下要求:\n\n你需要在 CI 环境中重新配置所有依赖项（包括 R 以及所需的正确版本的包）。\n如果你的代码需要任何特殊权限（例如访问数据库或网络）， CI 服务器上也需要具备这些权限。\n你的项目可能包含无法轻易执行的文档（例如使用旧版包的几年前的博客文章）。这些文档可能需要单独启用冻结功能，以防止它们在 CI 上执行。\n\n\n\n38.1.2 先决条件\n确保代码可以在 GitHub Action 中执行的最佳方法是为项目使用 renv 虚拟环境。以下是一个完整的 GitHub Action 示例，它安装 R 和 renv.lock 中的包依赖项，然后执行代码并将输出渲染到 GitHub Pages：\n\n\n\n\n\n\nTip\n\n\n\n在 RStudio 左下窗口的 Terminal 选项卡中依次运行以下 Git 命令：\n\ngit remote add origin https://github.com/qbgaoo/r4ms.git\ngit branch -M main\ngit push -u origin main\n\n\n\n在 Quarto 项目中新建文本文件，命名为 publish.yml（当然也可以是其他命字），保存路径为 .github/workflows/publish.yml，在文件中添加如下内容：\non:\n  push:\n    branches: main\n  pull_request:\n    branches: main\n  # to be able to trigger a manual build\n  workflow_dispatch:\n  schedule:\n    # run every day at 11 PM\n    - cron: '0 23 * * *'\n\nname: Render and deploy Book to Github\n\njobs:\n  build-deploy:\n    runs-on: ubuntu-latest\n    permissions:\n      contents: write\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v4\n\n      - name: Set up Quarto\n        uses: quarto-dev/quarto-actions/setup@v2\n\n      - name: Install R\n        uses: r-lib/actions/setup-r@v2\n        with:\n          r-version: '4.4.1'\n\n      - name: Install R Dependencies\n        uses: r-lib/actions/setup-renv@v2\n        with:\n          cache-version: 1\n\n      - name: Render and Publish\n        uses: quarto-dev/quarto-actions/publish@v2\n        with:\n          target: gh-pages\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n这是一个 GitHub Actions 工作流配置文件，保存后将文件提交到 GitHub中，触发文件中的工作流。",
    "crumbs": [
      "Practical skills",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "memo.html",
    "href": "memo.html",
    "title": "39  Memo for future programming",
    "section": "",
    "text": "Here is a typical example for generating data using rep() function and sample() function.\n\nset.seed(123)\n# a data fraome to store the simulation data\ndf &lt;- tibble(\n  antibiotic = rep(c(\"A\", \"B\"), each = 100),\n  infection_type = rep(c(\"X\", \"Y\"), times = 100),\n  recovered = sample(c(T, F), 200, replace = T, prob = c(0.7, 0.3))\n)\n# create the contingency table\ncontingency_table &lt;- table(df$antibiotic, df$infection_type, df$recovered)",
    "crumbs": [
      "Practical skills",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Memo for future programming</span>"
    ]
  }
]