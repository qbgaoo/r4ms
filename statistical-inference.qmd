# Statistical inference

```{r}
#| echo: false

source("_common.R")
```

In statistics we often use samples to infer population characteristics, since it’s impractical or impossible usually to measure the entire population. This process is called statistical inference. It involves using statistical techniques to make estimates, test hypothesis, and make predictions about population parameters. In this chapter, we maily focus on the estimation of population means.

## Prerequisites

```{r}
#| message: false

library(tidyverse)
library(rmarkdown)
```

## Distribution of sample means

The distribution of sample means is a fundamental concept in statistics, describing the distribution of sample means obtained from multiple samples drawn from the same population. Here is a example to demonstrate the distribution of sample means.

```{r}
#| echo: false
#| eval: false

# This code block is to generate the 100 samples from a normal distribution
set.seed(150) 
thrombin_time <- rnorm(10000, mean = 17.5, sd = 1.25) |> 
  round(digits = 1)

# Set the number of samples and the sample size for each sample
n_samples <- 120
n <- 12
# A list used to store samples
samples <- vector("list", n_samples)

for (i in 1:n_samples) {
  samples[[i]] <- sample(thrombin_time, n, replace = F)
}

samples |> 
  as_tibble(.name_repair = "universal") |> 
  rename_with(~ paste0("s", c(1:n_samples))) |> 
  write_csv("ex05-01.csv")
```

::: example
Assume thrombin time follow the normal distribution of $\mu=17.5$, $\sigma=1.25$. A researcher randomly drew $120$ samples from the population, with each sample containing $n=12$ observations. The data is saved the the data below. The mean $\bar{X}$ and standard deviation $S$ for each sample are shown in @tbl-mean-distribution, please analyze the distribution of the $120$ means.
:::

<a href="datasets/ex05-01.csv" download="ex05-01.csv" class="btn btn-success"> <i class="bi bi-database-fill-down"></i> Download data </a>

```{r}
#| label: tbl-mean-distribution
#| tbl-cap: The n, median, mean and sd of the 120 random samples (s)
df <- read_csv("datasets/ex05-01.csv", show_col_types = F) |> 
  summarise_all(
    list(
      n       = ~ n(),        # <1>             
      median  = median,
      mean    = mean,
      sd      = sd        
    )
  ) |> 
  pivot_longer(
    cols      = everything(),
    names_to  = c("sample", ".value"),
    names_sep = "_"
  ) |> 
  mutate(across(c(mean, sd), \(x)round(x, 2)))

df |> paged_table()
```

1.  Here `n()` must use a style of anonymous function, unlike the other three ones.

It is clear that the mean of each sample is different, and also may not equal to the mean of the population. The difference between a population parameter and a corresponding sample statistic is called sampling error. It arises because only a random sample of the entire population is observed, which may not perfectly represent the entire population.

To visualize the distribution of the 120 sample means, here we create a histogram:

```{r}
df |> 
  select(mean) |> 
  pull() |> 
  hist(
  freq           = T,
  right          = F, 
  col            = "skyblue", 
  include.lowest = T,
  main           = " ",
  xlab           = "The average of the thrombin time",
  ylab           = "Frequency",
  ylim           = c(0, 26),
  labels         = T
)
```

From the figure above we can say that the distribution of the sample mean can be approximated by a normal distribution. The two sample statistics of mean $\bar{X}$ and standard deviation $S$ can be achieved by the following code:

```{r}
df |> 
  select(x = mean) |> 
  summarise(
    mean = mean(x),
    sd   = sd(x)
  ) |> 
  round(digits = 1) |> 
  knitr::kable(align = "c")
```

The mean of the sample means is $17.5$, equal to the population mean $\mu=1.75$, while the standard deviation of the sample mean is $0.4$, less than the standard deviation of population $\sigma = 1.25$.

### Standard error

The standard deviation of the distribution of the sample means is called the standard error (SE). It reflects the typical distance between a sample mean and the population mean. The standard error of sample means is given by:

$$\sigma_{\bar{X}} = \frac{\sigma}{\sqrt{n}}$$

However, the population parameter $\sigma$ is usually unknown, therefore, the sample standard deviation $S$ is used as an estimator for the population standard deviation. Therefore, an estimation of the standard error above mentioned is defined by:

$$S_{\bar{X}} = \frac{S}{\sqrt{n}}$$

As the sample size $n$ increases, the standard error decreases, meaning the sample means will be closer to the population mean.

It is need to be pointed out that the above formula for calculating the sampling error of the mean is only adapt to simple random sampling. For other sampling methods, there are corresponding formulas exist.

### Central Limit Theorem

Regardless of the population distribution, as the sample size $n$ becomes large (typically $n \geq 30$ is considered sufficient), the sampling distribution of the sample mean will approximate a normal distribution. This is the essence of the Central Limit Theorem in statistics.

For small sample sizes, if the population itself is normally distributed, the distribution of the sample means will also be normally distributed.

Because the sampling distribution of the sample means can be approximated by a normal distribution (especially for large $n$), it allows for the construction of confidence intervals for the population mean and the conducting of hypothesis tests. The normality of the sampling distribution justifies the use of *z*-test or *t*-test depending on whether the population variance is known or unknown and the sample size.

## t distribution

The sample mean $\bar{X}$ follows a normal distribution when the underlying population is normally distributed, or when the sample size is sufficiently large due to the Central Limit Theorem. However, when you’re converting the sample mean into a standardized form, the distribution you use depends on whether or not you know the population standard deviation $\sigma$.

-   If $\sigma$ is known, the sample mean $\bar{X}$ can be standardized using the formula:

$$
z = \frac{\bar{X} - \mu}{\sigma_{\bar{X}}} = \frac{\bar{X} - \mu}{\sigma / \sqrt{n}}
$$

In this case, the standardized variable $Z$ follows a standard normal distribution $N(0, 1)$.

-   If $\sigma$ is unknown, which is often the case in practice, you need to estimate it using the sample standard deviation $S$. The formula for standardizing the sample mean becomes:

$$
t = \frac{\bar{X} - \mu}{S_{\bar{X}}} = \frac{\bar{X} - \mu}{S / \sqrt{n}}
$$

Here, $t$ doesn't not follow a standard normal distribution anymore, it follows a *t*-distribution with degrees of freedom $\nu = n - 1$ ($n$ is the sample size).

The *t*-distribution, also known as Student’s *t*-distribution, is used in statistics when estimating population parameters when the sample size is small and/or the population variance is unknown. It’s especially important in confidence intervals, hypothesis testing, and regression analysis.

### Visual representation

The *t*-distribution can be plotted to show how it compares with the normal distribution.

```{r}
#| echo: false

tibble(
  t  = seq(-4, 4, length.out = 500),
  y1 = dt(t, df = 1),
  y2 = dt(t, df = 5),
  y3 = dt(t, df = Inf)
) |> 
  pivot_longer(
    cols      = contains("y"),
    values_to = "density",
    names_to  = "group"
  ) |> 
  ggplot(aes(x = t, y = density, linetype = group)) +
  geom_line() +
  scale_linetype_manual(
    name   = "",
    values = c("dashed", "solid", "dotdash"),
    labels = c("df = 1", "df = 5", "df = Inf")
  ) +
  theme(
    axis.text   = element_text(size = 12),
    axis.title  = element_text(size = 12),
    legend.text = element_text(size = 12)
  )
```

The *t*-distribution is similar to the standard normal distribution, but it has heavier tails. This means a higher probability of values further from the mean, especially with a smaller degree of freedom. As degrees of freedom increase, the *t*-distribution curve will converge toward the standard normal distribution curve.

The shape of the *t*-distribution depends on the degrees of freedom $\nu$, which is typically related to the sample size ($\nu = n - 1$). As the degrees of freedom increase, the *t*-distribution approaches the standard normal distribution. For large sample sizes ($\nu > 60$), the *t*-distribution and normal distribution are almost indistinguishable.

### Common uses

-   Confidence intervals

    The *t*-distribution is used to construct confidence intervals for the population mean when the population variance is unknown and the sample size is small.

-   *t*-test: Used to compare the means of two groups when the sample size is small and the population standard deviation is unknown.

-   Regression analysis: In linear regression, *t*-test are used to determine whether the coefficients of the independent variables are significantly different from zero.

## Estimation of population mean

Parameter estimation is a concept in statistics that involves using statistic off a sample to estimate parameters of the corresponding population. The sample should be random and large enough to provide an accurate estimate. A parameter is a numerical characteristic about a population, such as a mean, proportion, or standard deviation. Here we just talk about the estimation of population mean.

### Point estimation

This involves using sample data to calculate a single value that serves as the best guess for a population mean. However, obtaining an exact point estimate of the population mean from just one random sample is almost unattainable.

Calculate the sample mean $\bar{X}$: The sample mean is calculated using the formula:

$$
\bar{X} = \frac{1}{n} \sum_{i=1}^{n} x_i
$$

where $n$ is the sample size, and $x_i$ are the individual sample values. The sample mean ($\bar{X}$) serves as an estimate of the population mean ($\mu$).

However, a point estimate does not consider the sampling error and cannot tell you how close the estimate is likely to be to the true population mean. Instead of relying on a single point estimate, interval estimation are often used because they provide a range of values within which the true population mean is likely to lie along with a confidence level.

### Confidence interval

The method for calculating the confidence interval of the population mean varies depending on whether the population standard deviation $\sigma$ is known and the size of sample $n$. Typically, there are two types of methods: the *t*-distribution and the *z*-distribution (also known as the u-distribution). The following will introduce the methods for calculating the confidence interval of a single population mean and the confidence interval of the difference between two population means.

**Single population mean**

Confidence interval (CI) is associated with a confidence level, typically 95%, which indicates the degree of certainty that the true population mean falls in the range. The two-sided $1-\alpha$ confidence interval of single population mean can be formulated as:

$$
\text{CI} = \bar{X} \pm t_{\nu,\alpha/2} \cdot \frac{S}{\sqrt{n}} \quad \text{or} \quad \text{CI} = \bar{X} \pm z_{\alpha/2} \cdot \frac{S}{\sqrt{n}}
$$

Similarly, the one-sided $1-\alpha$ confidence interval for the population mean is given by：

$$
\mu> \bar{X} - t_{\nu,\alpha} \cdot \frac{S}{\sqrt{n}} \quad \text{or} \quad \mu> \bar{X} - z_\alpha \cdot \frac{S}{\sqrt{n}}
$$

$$
\mu < \bar{X} + t_{\nu,\alpha} \cdot \frac{S}{\sqrt{n}} \quad \text{or} \quad \mu < \bar{X} + z_\alpha \cdot \frac{S}{\sqrt{n}}
$$

where $t$ is the critical value from the *t*-distribution, it depends on the degree of freedom $\nu$ and the level of confidence $\alpha$; $z$ is the critical value from the standard normal distribution, it depends on the level of confidence $\alpha$.

::: example
A research group randomly selected 20 adult men in a area and measured their red blood cell count. Please calculate the 95% confidence interval for the population mean.
:::

<a href="datasets/ex05-02.csv" download="ex05-02.csv" class="btn btn-success"> <i class="bi bi-database-fill-down"></i> Download data </a>

Since the sample size is small, we would use the *t*-distribution to estimate the population mean. In practice, you can always use the *t*-distribution regardless of the size of samples, because as the increase of degrees of freedom , *t*-distribution approaches the standard normal distribution.

In this chapter, we write a pair of custom functions based on *t*-distribution to tackle such problems. They are named as `mean_CI()` and `mean_diff_CI()`.

```{r}
mean_CI  <- function(x, alternative = "two.sided", conf.level = 0.95){
  x_bar  <- mean(x, na.rm = T)
  s      <-  sd(x, na.rm = T)
  n      <- length(x)  
  stderr <- s / sqrt(n)
  
  type  <- alternative
  alpha <- 1- conf.level
  
  case_when(
    type == "two.sided" ~ {
      t_stat <- qt(1 - alpha / 2, df = n - 1)
      tibble( 
        mean       = x_bar,
        lower_ci   = x_bar - stderr * t_stat,
        upper_ci   = x_bar + stderr * t_stat,
        conf_level = conf.level
      )
    },
    
    type == "greater"   ~ {
      t_stat <- qt(alpha, df = n - 1)
      tibble(
        mean       = x_bar,
        lower_ci   = x_bar + stderr * t_stat,
        upper_ci   = Inf,
        conf_level = conf.level
      )
    },
      
    type == "less"      ~ {
      t_stat <- qt(alpha, df = n - 1)
      tibble( 
        mean       = x_bar,
        lower_ci   = -Inf,
        upper_ci   = x_bar - stderr * t_stat,
        conf_level = conf.level
     )
    }
  )
}
```

```{r}
read_csv("datasets/ex05-02.csv", show_col_types = F) |> 
  pull() |> 
  mean_CI() 
```

```{r}
read_csv("datasets/ex05-02.csv", show_col_types = F) |> 
  pull() |>
  DescTools::MeanCI()
```

**Difference between two population means**

The confidence interval for the difference between two population means is used to estimate the range within which the true difference between the means of two populations lies, based on sample data.

Randomly sample from two normal populations $N(\mu_1, \sigma^2)$ and $N(\mu_2, \sigma^2)$ with equal population variances but unequal population means. The sample sizes, means, and standard deviations of the two samples are denoted by $n_1$, $\bar{X}_1$, $S_1$ and $n_2$, $\bar{X}_2$, $S_2$ respectively. If the population variances are unknown and the sample sizes are small, then the two-sided ($1-\alpha$) confidence interval for the difference between the two population means ($\mu_1 - \mu_2$) is given by:

$$
(\bar{X}_1 - \bar{X}2) \pm t_{\alpha/2, \nu} \cdot \sqrt{S_c^2(\frac{1}{n_1} + \frac{1}{n_2})} \ , \quad S_c^2 = \frac{(n_1 -1)S_1^2 + (n_2 -1)S_2^2}{n_1 + n_2 -2} 
$$

Where $t_{\alpha/2, \nu}$ is the *t*-score with $\nu = n_1 + n_2 - 2$ degrees of freedom, $\alpha$ is the level of confidence, $S_c^2$ is called the pooled variance.

If thevariances of the two populations mentiond above are unknown and unequal, then the two-sided ($1-\alpha$) confidence interval for the difference between the two population means ($\mu_1 - \mu_2$) is given by:

$$
(\bar{X}_1 - \bar{X}2) \pm t_{\alpha/2, \nu} \cdot \sqrt{\frac{S_1^2}{n_1} + \frac{S_2^2}{n_2}}\ , \ 
\nu \approx \frac{\left(\frac{S_1^2}{n_1} + \frac{S_2^2}{n_2}\right)^2}{\frac{\left(\frac{S_1^2}{n_1}\right)^2}{n_1 - 1} + \frac{\left(\frac{S_2^2}{n_2}\right)^2}{n_2 - 1}}
$$

If the sample sizes of both samples are large (e.g., both are greater than 60)， the confidence interval for the difference between two population means $\mu_1 - \mu_2$ is given by:

$$
(\bar{X}_1 - \bar{X}2) \pm z_{\alpha/2} \cdot \sqrt{\frac{S_1^2}{n_1} + \frac{S_2^2}{n_2}}
$$

Where $\bar{X}_1$ and $\bar{X}_2$ are the sample means；$n_1$ and $n_2$ are the sample sizes. $S_1^2$ and $S_2^2$are the sample variances*.* $z_{\alpha/2}$ is the z-score corresponding to the desired confidence level.

```{r}
mean_diff_CI <- function(x, y, alternative = "two.sided", conf.level = 0.95, 
                         var.equal = FALSE){
  x <- na.omit(x)
  y <- na.omit(y)
  
  x_bar <- mean(x)
  v1    <- var(x)
  n1    <- length(x)  
  
  y_bar <- mean(y)
  v2    <- var(y)
  n2    <- length(y)  
  
  type  <- alternative
  alpha <- 1- conf.level
  diff  <-  x_bar - y_bar 
  
  if(var.equal) {
    df     <- n1 + n2 - 2
    var    <- ((n1 - 1) * v1 + (n2 - 1) * v2) / df
    stderr <- sqrt(var * (1 / n1 + 1 / n2))
  } else{
    stderr1 <- sqrt(v1/n1)
    stderr2 <- sqrt(v2/n2)
    stderr  <- sqrt(stderr1^2 + stderr2^2)
    df      <- stderr^4/(stderr1^4/(n1 - 1) + stderr2^4/(n2 - 1))
  }
  
  case_when(
    type == "two.sided" ~ {
      t_stat <- qt(1 - alpha / 2, df)
      tibble( 
        mean_x     = x_bar,
        mean_y     = y_bar,
        mean_diff  = diff,
        lower_ci   = diff - stderr * t_stat,
        upper_ci   = diff + stderr * t_stat,
        conf_level = conf.level
      )
    },
    
    type == "greater"   ~ {
      t_stat <- qt(alpha, df)
      tibble( 
        mean_x     = x_bar,
        mean_y     = y_bar,
        mean_diff  = diff,
        lower_ci   = diff + stderr * t_stat,
        upper_ci   = Inf,
        conf_level = conf.level
      )
    },
      
    type == "less"      ~ {
      t_stat <- qt(alpha, df)
      tibble( 
        mean_x     = x_bar,
        mean_y     = y_bar,
        mean_diff  = diff,
        lower_ci   = -Inf,
        upper_ci   = diff - stderr * t_stat,
        conf_level = conf.level
     )
    }
  )
}
```

::: example
To compare the difference of hemoglobin concentration between people with alpha thalassemia trait and silent carrier, a doctor choose a random sample of 42 patients from a patient database. 22 subjects have alpha thalassemia trait, 20 are silent_carrier. Have a try to analyze the 95% confidence interval of the two populations.
:::

<a href="datasets/ex05-03.csv" download="ex05-03.csv" class="btn btn-success"> <i class="bi bi-database-fill-down"></i> Download data </a>

```{r}
df <- read_csv("datasets/ex05-03.csv", show_col_types = F)
x <- pull(df, 1)
y <- pull(df, -1)
mean_diff_CI(x, y)
```

```{r}
DescTools::MeanDiffCI(x, y, na.rm = T)
```

### Interpreting the confidence interval

The results will give you a range that represents the interval within which the true difference between the population means or the true population mean (for single population) is expected to fall, with a certain level of confidence (usually 95%).

More, if the interval includes 0, this suggests that there may be no significant difference between the two population means. If the interval does not include 0, it suggests that there is a significant difference.
