[
  {
    "objectID": "nonparametric-test.html",
    "href": "nonparametric-test.html",
    "title": "12  Nonparametric Tests",
    "section": "",
    "text": "12.1 Prerequisite\nlibrary(tidyverse)\nlibrary(DescTools)\nlibrary(PMCMRplus)",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Nonparametric Tests</span>"
    ]
  },
  {
    "objectID": "nonparametric-test.html#wilcoxon-signed-rank-test",
    "href": "nonparametric-test.html#wilcoxon-signed-rank-test",
    "title": "12  Nonparametric Tests",
    "section": "12.2 Wilcoxon signed-rank test",
    "text": "12.2 Wilcoxon signed-rank test\nWilcoxon signed-rank test is often used for comparing paired samples or a single sample to a known median. It is an alternative to the paired t-test when data are not normally distributed. This test can be applied in situations where a before-and-after comparison is needed, such as evaluating the effect of a treatment, intervention, or condition on a specific group.\n\n12.2.1 Paired samples\nComparing the median of paired sample difference to zero is typically used to determine whether there is a significant difference between two related groups. The method most commonly applied is the Wilcoxon signed-rank test.\nThis test ranks the absolute differences between the pairs, then checks whether the median of these differences deviates from zero, indicating a statistically significant change between the two paired groups.\n\nExample 1: \nIn a paired design, two types of feed were administered to 8 pairs of rats, and the vitamin A content in their liver was measured. The data can be downloaded from the button below. Is there a significant difference in the vitamin A content in the livers of rats fed with different types of feed?\n\n\n  Download data \n\n\ndf &lt;- read_csv(\"datasets/ex12-01.csv\", show_col_types = F) \nstr(df)\n\n#&gt; spc_tbl_ [8 × 2] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n#&gt;  $ feedA: num [1:8] 3.55 2 3 3.95 3.8 3.75 3.45 3.05\n#&gt;  $ feedB: num [1:8] 2.45 2.4 1.8 3.2 3.25 2.7 2.5 1.75\n#&gt;  - attr(*, \"spec\")=\n#&gt;   .. cols(\n#&gt;   ..   feedA = col_double(),\n#&gt;   ..   feedB = col_double()\n#&gt;   .. )\n#&gt;  - attr(*, \"problems\")=&lt;externalptr&gt;\n\n\nFirst, test the normality of the difference between the pairs of observations.\n\nwith(df, shapiro.test(feedA - feedB))\n\n#&gt; \n#&gt;  Shapiro-Wilk normality test\n#&gt; \n#&gt; data:  feedA - feedB\n#&gt; W = 0.81051, p-value = 0.03705\n\n\nThe p-value is 0.03705, less than 0.05, indicating the difference does not follows a normal distribution. Then the signed-rank test can be conducted using wilcox.test() function with argument paired = TRUE.\n\nwith(df, wilcox.test(x = feedA, y = feedB, paired = T))\n\n#&gt; \n#&gt;  Wilcoxon signed rank exact test\n#&gt; \n#&gt; data:  feedA and feedB\n#&gt; V = 35, p-value = 0.01563\n#&gt; alternative hypothesis: true location shift is not equal to 0\n\n\n\n\n12.2.2 One sample\nThe comparison of a single sample median with the population median is used to assess whether the median of a sample significantly differs from a known or hypothesized population median. A common method for this comparison is the one-sample Wilcoxon signed-rank test (also known as the one-sample median test).\nThe test ranks the sample data and compares the sum of positive and negative ranks relative to the hypothesized median, determining whether the sample median is significantly different from the population median.\n\nExample 2: \nIt is known that the median urine nitrogen level of healthy individuals in a area is 45.3. A group randomly selected 12 workers from a factory in that area, and measured their fluoride content in urine. The data can be downloaded from the button below. Are the urine nitrogen levels of the factory workers higher than those of the healthy individuals in the area?\n\n\n  Download data \n\n\ndf &lt;- read_csv(\"datasets/ex12-02.csv\", show_col_types = F)\nstr(df)\n\n#&gt; spc_tbl_ [12 × 2] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n#&gt;  $ urine_fluorine: num [1:12] 44.2 45.3 46.4 49.5 51 ...\n#&gt;  $ diff          : num [1:12] -1.09 0 1.09 4.17 5.75 ...\n#&gt;  - attr(*, \"spec\")=\n#&gt;   .. cols(\n#&gt;   ..   urine_fluorine = col_double(),\n#&gt;   ..   diff = col_double()\n#&gt;   .. )\n#&gt;  - attr(*, \"problems\")=&lt;externalptr&gt;\n\n\n\nwith(df, wilcox.test(x = urine_fluorine, mu = 45.3, alter = \"greater\"))\n\n#&gt; Warning in wilcox.test.default(x = urine_fluorine, mu = 45.3, alter =\n#&gt; \"greater\"): cannot compute exact p-value with zeroes\n\n\n#&gt; \n#&gt;  Wilcoxon signed rank test with continuity correction\n#&gt; \n#&gt; data:  urine_fluorine\n#&gt; V = 65, p-value = 0.00255\n#&gt; alternative hypothesis: true location is greater than 45.3\n\n\nWhen you run the code above, a warning message “cannot compute exact p-value with zeroes” is occurring. The warning you’re encountering is because the wilcox.test() function has trouble computing an exact p-value when there are zero differences in the data (or the values themselves are zero), which can occur in certain datasets. To resolve this, you can use the following strategy: Ignore the exact p-value and rely on the asymptotic approximation, which can be computed without zero-value issues. You can do this by setting exact = FALSE.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Nonparametric Tests</span>"
    ]
  },
  {
    "objectID": "nonparametric-test.html#wilcoxon-rank-sum-test",
    "href": "nonparametric-test.html#wilcoxon-rank-sum-test",
    "title": "12  Nonparametric Tests",
    "section": "12.3 Wilcoxon rank-sum test",
    "text": "12.3 Wilcoxon rank-sum test\nThe Wilcoxon rank-sum test (also known as the Mann-Whitney U test) is a nonparametric test used to compare two independent groups to assess whether their population distributions differ. It is an alternative to the two-sample t-test when the assumption of normality is not met, making it robust for non-normally distributed data. It compares the ranks of the values rather than the actual data, making it suitable for ordinal and continuous data that is not normally distributed.\n\n12.3.1 Raw data\nRaw data refers to unprocessed or untransformed data that has been collected directly from a source. The data typically consists of observations or measurements in their original form, such as survey responses, experimental results, or sensor readings.\n\nExample 3: \nThe right hilar diameter (RD) was measured using X-ray in 10 lung cancer patients and 12 stage 0 silicosis workers. The data can be downloaded from the button below. The question is whether the RDs of lung cancer patients are higher than those of stage 0 silicosis workers?\n\n\n  Download data \n\n\ndf &lt;- read_csv(\n  \"datasets/ex12-03.csv\", \n  col_types = list(group = col_factor(), rd = col_double())\n)\nstr(df)\n\n#&gt; spc_tbl_ [22 × 2] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n#&gt;  $ group: Factor w/ 2 levels \"1\",\"2\": 1 1 1 1 1 1 1 1 1 1 ...\n#&gt;  $ rd   : num [1:22] 2.78 3.23 4.2 4.87 5.12 6.21 7.18 8.05 8.56 9.6 ...\n#&gt;  - attr(*, \"spec\")=\n#&gt;   .. cols(\n#&gt;   ..   group = col_factor(levels = NULL, ordered = FALSE, include_na = FALSE),\n#&gt;   ..   rd = col_double()\n#&gt;   .. )\n#&gt;  - attr(*, \"problems\")=&lt;externalptr&gt;\n\n\n\nwilcox.test(rd ~ group, data = df, alter = \"greater\", exact = F)\n\n#&gt; \n#&gt;  Wilcoxon rank sum test with continuity correction\n#&gt; \n#&gt; data:  rd by group\n#&gt; W = 86.5, p-value = 0.04318\n#&gt; alternative hypothesis: true location shift is greater than 0\n\n\n\n\n12.3.2 Frequency table data and ordinal data\nFrequency table data refers to data that is summarized in terms of counts or frequencies for different categories. Ordinal data, on the other hand, involves variables that have a clear, ordered ranking but the intervals between ranks are not necessarily equal. Here is an example of ordinal data.\n\nExample 4: \nA hospital treated two types of pediatric pneumonia with a new drug, and the efficacy is shown in. Determine whether the efficacy of the drug differs between the two types of pneumonia patients.\n\n\n  Download data \n\n\ndf &lt;- read_csv(\n  file = \"datasets/ex12-04.csv\", \n  col_types = list(\n    group = col_factor(),\n    efficacy = col_integer(), \n    freq = col_integer()\n  )\n)\nstr(df)\n\n#&gt; spc_tbl_ [8 × 3] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n#&gt;  $ group   : Factor w/ 2 levels \"1\",\"2\": 1 1 1 1 2 2 2 2\n#&gt;  $ efficacy: int [1:8] 1 2 3 4 1 2 3 4\n#&gt;  $ freq    : int [1:8] 65 18 30 13 42 6 23 11\n#&gt;  - attr(*, \"spec\")=\n#&gt;   .. cols(\n#&gt;   ..   group = col_factor(levels = NULL, ordered = FALSE, include_na = FALSE),\n#&gt;   ..   efficacy = col_integer(),\n#&gt;   ..   freq = col_integer()\n#&gt;   .. )\n#&gt;  - attr(*, \"problems\")=&lt;externalptr&gt;\n\n\nIf you have a frequency table that includes ordinal information, you can perform a rank-sum test with weighted data. First, the data is expanded based on frequencies, and then the rank-sum test is applied. Here we first expand the data using uncount() function in the tidyr package, then perform the Wilcoxon rank-sum test with wilcox.test():\n\ndf |&gt; \n  uncount(freq) |&gt; \n  wilcox.test(efficacy ~ group, data = _)\n\n#&gt; \n#&gt;  Wilcoxon rank sum test with continuity correction\n#&gt; \n#&gt; data:  efficacy by group\n#&gt; W = 4954.5, p-value = 0.5883\n#&gt; alternative hypothesis: true location shift is not equal to 0",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Nonparametric Tests</span>"
    ]
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "1  Preface",
    "section": "",
    "text": "1.1 Prerequisites\nWe’ve made a few assumptions about what you already know to get the most out of this book. You should have some basic knowledge about medical statistics, and it’s helpful if you have some basic R programming experience already.\nYou need some things to run the code in this book: R, RStudio and some preinstalled R packages. Packages are the fundamental units of reproducible R code. They include reusable functions, documentation that describes how to use them, and sample data.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "preface.html#prerequisites",
    "href": "preface.html#prerequisites",
    "title": "1  Preface",
    "section": "",
    "text": "1.1.1 R\nTo download R, go to CRAN, the comprehensive R archive network, https://cloud.r-project.org. A new major version of R comes out once a year, and there are 2-3 minor releases each year. It’s a good idea to update regularly. Upgrading can be a bit of a hassle, especially for major versions that require you to re-install all your packages, but putting it off only makes it worse. We recommend R 4.4.1 or later for this book.\n\n\n1.1.2 RStudio\nRStudio is an integrated development environment, or IDE, for R programming, which you can download from https://posit.co/download/rstudio-desktop/. RStudio is updated a couple of times a year, and it will automatically let you know when a new version is out, so there’s no need to check back. It’s a good idea to upgrade regularly to take advantage of the latest and greatest features. For this book, make sure you have at least RStudio 2024.04.2.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "preface.html#running-r-code",
    "href": "preface.html#running-r-code",
    "title": "1  Preface",
    "section": "1.2 Running R code",
    "text": "1.2 Running R code\nThe previous section showed you several examples of running R code. The code in the book looks like this:\n\n1 + 2\n\n#&gt; [1] 3\n\n\nIf you run the same code in your local console, it will look like this:\n&gt; 1 + 2\n[1] 3\nThere are two main differences. In your console, you type after the &gt;, called the prompt; we don’t show the prompt in the book. In the book, the output is commented out with #&gt;; in your console, it appears directly after your code. These two differences mean that if you’re working with an electronic version of the book, you can easily copy code out of the book and paste it into the console.\nThroughout the book, we use a consistent set of conventions to refer to code:\n\nFunctions are displayed in a code font and followed by parentheses, like sum() or mean().\nOther R objects (such as data or function arguments) are in a code font, without parentheses, like flights or x.\nSometimes, to make it clear which package an object comes from, we’ll use the package name followed by two colons, like dplyr::mutate() or nycflights13::flights. This is also valid R code.\nTo improves readability, variable names and function names are named using snake case.\n\nIn this course an introduction to basic statistical methods useful for biomedical data analysis will be given. Concepts are taught in an intuitive manner, alternating between short lectures and practicals. This allows for plenty of interaction and illustration with examples of practical interest. Participants who aim to use more complex methods can use the concepts and skills learned during the course as basis, as the vast majority of statistical methods are implemented in R.\nParticipants must be able to work with R and R packages to follow the course. Those with little or no experience in R must follow an introductory R course prior to following this course.\nIn addition, it is strongly advised to learn to work with RStudio and RMarkdown. Those with no prior knowledge of RMarkdown can follow the tutorials here. During the course we will practice further, and the RMarkdown cheatsheets may be useful.\nWho should attend\nResearchers who need to run their own statistical analyses, and want to do it in a transparent and reproducible manner. While most participants tend to be PhD students and postdocs, more senior researchers can also benefit from the course.\nR is a free, open-source software for statistical computing and graphics. In this course, you will learn the programming techniques to make the most of this powerful tool for processing, analysing and presenting your data. Please note that basic statistical knowledge is required. You will analyse data by writing functions and scripts to get reproducible and well documented results. You will learn how to create excellent graphics and how to adapt them to your needs. We will use data from clinical background to work throughout the course. Main topics covered during this course will be: Basics of the R language, data accessing, data manipulations, explore and summarise your data using descriptive statistics, graphics in R, data analysis with focus on basic statistics (e.g. hypothesis testing, linear and logistic regression).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "preface.html#colophon",
    "href": "preface.html#colophon",
    "title": "1  Preface",
    "section": "1.3 Colophon",
    "text": "1.3 Colophon\nThe book is written by Quarto, an online version of it is available at https://qbgaoo.github.io/r4ms/.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preface</span>"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "2  Introduction",
    "section": "",
    "text": "2.1 Medical statistics\nMedical statistics is the science of applying statistical techniques and principles to analyze data related to health, disease, treatment effectiveness, and public health issues. It involves the systematic collection, organization, description, and inference of health-related data to assist healthcare professionals in making evidence-based, scientific decisions.\nKey Components:\n1. Data Collection: Gathering health-related data systematically through experiments, observational studies, or clinical trials.\n2. Data Organization and Description: Using statistical measures, such as means, variances, and frequency distributions, to describe and summarize the basic characteristics of the data.\n3. Data Analysis: Applying inferential statistical methods, such as hypothesis testing, regression analysis, and analysis of variance (ANOVA), to uncover patterns and causal relationships in the data.\n4. Interpretation of Results: Applying the results of statistical analyses to the medical field, explaining the occurrence and progression of diseases, the effectiveness of treatments, and providing evidence-based support for clinical decisions and public health policies.\nMedical statistics is widely used in medical research, the design of clinical trials, the evaluation of diagnostic tests, the comparison of treatment outcomes, and the epidemiological study of diseases.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#population-and-sample",
    "href": "intro.html#population-and-sample",
    "title": "2  Introduction",
    "section": "2.2 Population and sample",
    "text": "2.2 Population and sample\nPopulation and sample are fundamental concepts in statistics, and they are key to understanding data analysis, particularly in the context of statistical inference.\n\n2.2.1 Population\nA population includes all members of a defined group that we are studying or collecting information on for data-driven decisions. The population represents the entire group of interest, which could be people, objects, events, or measurements.\nExamples: All adults living in a particular country. All patients treated for a particular condition in a hospital over a decade.\nCharacteristics:\nParameters: The numerical characteristics of a population are called parameters (e.g., population mean, population standard deviation).\nSize: The population can be finite (e.g., all students in a school) or infinite (e.g., all possible outcomes of rolling a die).\n\n\n2.2.2 Sample\nA sample is a subset of the population selected for study. It is often impractical or impossible to study an entire population, so researchers use a sample to draw conclusions about the population.\nExamples: 1,000 randomly selected adults from a country. 200 patients selected from the hospital’s records.\nStatistics: The numerical characteristics of a sample are called statistics (e.g., sample mean, sample standard deviation).\nSize: The size of the sample (denoted as n) is always smaller than the population size (denoted as N).\n\n\n2.2.3 Relationship between population and sample\nSampling: The process of selecting a sample from a population is called sampling. It is crucial that the sample is representative of the population to ensure that the conclusions drawn from the sample can be generalized to the population.\nInference: Statistical inference involves making predictions or generalizations about a population based on information from a sample. This is done using various statistical methods, including estimation and hypothesis testing.\nBias and Variability: A sample might not perfectly represent the population due to sampling bias or variability. Researchers use random sampling techniques to minimize these issues and improve the reliability of the inference.\nPractical Example\nImagine you want to study the average height of all adult women in a country (population). Measuring the height of every woman in the country would be impractical, so instead, you randomly select 500 women (sample) and measure their heights. The average height of these 500 women is a statistic. You then use this statistic to estimate the parameter—the average height of all adult women in the country.\nKey Points to Remember\nPopulation: Entire group of interest; parameters describe it.\nSample: Subset of the population; statistics describe it.\nInference: Drawing conclusions about the population based on sample data.\nRepresentativeness: A sample should represent the population to ensure valid inferences.\nUnderstanding the distinction between population and sample is essential for conducting valid and reliable statistical analyses.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro.html#data",
    "href": "intro.html#data",
    "title": "2  Introduction",
    "section": "2.3 Data",
    "text": "2.3 Data\nIn statistics, data can be classified into different types based on various characteristics. Understanding these data types is crucial for selecting appropriate statistical methods and accurately interpreting results. Below are the main types of data:\n\n2.3.1 Quantitative data\nQuantitative data (numerical data) refers to data that can be measured and expressed numerically. This type of data is critical in the medical field as it allows for precise measurements, comparisons, and statistical analysis, ultimately leading to better understanding, diagnosis, treatment, and prevention of diseases. Quantitative data can be further classified into two main types:\n1. Discrete Data\nDiscrete data consist of distinct, separate values that are countable. These values are typically whole numbers, and there is no intermediate value between them.\nExamples:\nNumber of patients: The number of patients admitted to a hospital.\nNumber of medications: The number of medications a patient is taking.\nNumber of occurrences: The number of heartbeats per minute (heart rate).\nCharacteristics of count data\n\nDiscrete: The data are non-negative integers, as they represent counts of occurrences.\nOverdispersion: Count data in medical settings often show overdispersion, where the variance exceeds the mean (e.g., some patients may have a high number of hospital visits while others have very few).\nSkewed Distribution: Count data are often positively skewed, with many individuals having low counts (e.g., no hospital visits) and fewer individuals having higher counts.\n\n2. Continuous Data\nContinuous data can take any value within a given range. These data represent measurements and can have decimal places, meaning they are not restricted to whole numbers.\nExamples:\nBlood pressure: Systolic and diastolic blood pressure measurements (e.g., 120/80 mmHg).\nBody temperature: Body temperature measured in degrees Celsius or Fahrenheit (e.g., 36.6°C).\nCholesterol levels: Blood cholesterol levels measured in milligrams per deciliter (e.g., 180 mg/dL).\nQuantitative data is fundamental in many areas of medicine:\n\nDiagnosis: • Quantitative measurements such as blood pressure, blood glucose levels, and cholesterol levels are used to diagnose conditions like hypertension, diabetes, and hyperlipidemia. 2. Treatment Monitoring: • Quantitative data is crucial for monitoring treatment effectiveness. For example, changes in tumor size (measured in millimeters or centimeters) can indicate whether cancer treatment is effective. 3. Epidemiology: • Quantitative data is used to track disease incidence and prevalence rates, mortality rates, and other public health indicators. • Example: Tracking the number of new COVID-19 cases per day during a pandemic. 4. Clinical Research: • In clinical trials, quantitative data is collected to assess the safety and efficacy of new drugs or treatments. • Example: Measuring the reduction in blood pressure in participants taking an antihypertensive drug. 5. Medical Imaging: • Quantitative data is used in medical imaging to measure organ size, tumor volume, and other anatomical features. • Example: Measuring the size of a kidney stone on an ultrasound image.\n\nStatistical Analysis of Quantitative Data in Medicine\nQuantitative data allows for a variety of statistical analyses, including:\n\nDescriptive Statistics: • Mean: The average value (e.g., average heart rate of a group of patients). • Median: The middle value when data is ordered (e.g., median age of patients in a study). • Standard Deviation: A measure of the variability or spread of data (e.g., variability in blood pressure readings).\nInferential Statistics: • t-tests and ANOVA: Used to compare means between two or more groups. • Regression Analysis: Used to model the relationship between a dependent variable and one or more independent variables. • Correlation: Used to assess the strength and direction of the relationship between two continuous variables.\nProbability and Distributions: • Quantitative data often follows specific statistical distributions (e.g., normal distribution), which are used in probability and inferential statistics.\n\nExamples of Quantitative Data in Medical Practice\nVital Signs Monitoring: • Continuous monitoring of heart rate, blood pressure, and oxygen saturation levels in critically ill patients. • Laboratory Tests: • Quantitative measurement of blood glucose levels, complete blood count (CBC), and electrolyte levels. • Growth and Development: • Tracking height, weight, and head circumference in pediatric patients over time to assess growth and development. • Pharmacokinetics: • Measuring drug concentrations in the blood over time to understand absorption, distribution, metabolism, and excretion.\nApplication of Quantitative Data in Medical Research\n\nRandomized Controlled Trials (RCTs): • Quantitative data is used to compare the effectiveness of new treatments or interventions with standard care or placebo. • Example: Measuring the reduction in blood pressure in patients receiving a new antihypertensive drug versus a placebo.\nLongitudinal Studies: • Quantitative data is collected over time to study changes in health outcomes, risk factors, or disease progression. • Example: Tracking changes in lung function in smokers versus non-smokers over several years.\nHealth Economics: • Quantitative data is used to evaluate the cost-effectiveness of medical interventions, including cost per quality-adjusted life year (QALY) gained. • Example: Calculating the cost-effectiveness of a new cancer treatment based on survival rates and costs.\n\nQuantitative data is a cornerstone of medical science, providing the foundation for diagnosis, treatment, research, and public health. Its ability to be precisely measured and analyzed makes it indispensable for advancing medical knowledge, improving patient care, and making informed decisions in healthcare.\n\n\n2.3.2 Qualitative data (Categorical data)\nQualitative data in medicine refers to non-numerical data that represent categories or groups. Unlike quantitative data, which deals with numbers and measurements, qualitative data describes characteristics or attributes that can be used to classify individuals, objects, or events into distinct groups. This type of data is crucial in medical research and practice for understanding patient demographics, disease classifications, and treatment outcomes. ain types:\n1. Nominal Data\nNominal data consists of categories that do not have a natural order or ranking. These categories are mutually exclusive, meaning that an individual or event can belong to only one category.\nBlood Type: Categories like A, B, AB, and O.\nGender: Male, Female, Other.\nPresence or Absence of a Condition: Yes/No responses (e.g., presence of hypertension).\n\n\n2.3.3 Ordinal Data\nOrdinal data consists of categories that have a meaningful order or ranking, but the intervals between the categories are not necessarily equal or meaningful.\nSeverity of Disease: Mild, Moderate, Severe.\nPain Scale: Numeric rating scale (e.g., 0–10), where 0 is no pain and 10 is the worst pain imaginable.\nStage of Cancer: Stages I, II, III, IV.\nQualitative data plays a vital role in various aspects:\n\nPatient Demographics:\n\nCategorization: Patients are often categorized based on attributes like age group, gender, race, and ethnicity, which are critical for understanding the distribution of diseases and tailoring medical interventions.\nExample: A study may categorize patients by age group (e.g., children, adults, elderly) to analyze how a particular disease affects different age groups.\n\nDisease Classification:\n\nDiagnosis: Diseases are classified into categories based on symptoms, genetic markers, or other clinical criteria. This classification helps in diagnosing, treating, and researching diseases.\nExample: Types of diabetes (Type 1, Type 2, Gestational Diabetes) are categorical variables that guide treatment plans.\n\nTreatment Outcomes:\n\nOutcome Measures: Treatment outcomes can be categorized as successful/unsuccessful, improved/no improvement, or recurrence/no recurrence. These qualitative outcomes are essential for evaluating the effectiveness of treatments.\nExample: A study on cancer treatment might categorize outcomes as “complete remission,” “partial remission,” or “no response.”\n\nQuality of Life and Patient Satisfaction:\n\nSurveys and Questionnaires: Patient satisfaction surveys and quality of life assessments often use ordinal scales (e.g., very satisfied, satisfied, neutral, dissatisfied, very dissatisfied) to gather qualitative data on patient experiences.\nExample: A patient satisfaction survey may categorize responses to a question about hospital care quality into five levels, from “very poor” to “excellent.”\n\n\nStatistical Analysis of Qualitative Data\n\nFrequency and Proportion: The most basic analysis involves counting the number of occurrences in each category and expressing them as frequencies or proportions (percentages).\nExample: Proportion of patients with a specific blood type in a population.\nChi-Square Tests: Used to determine if there is a significant association between two categorical variables.\nExample: Assessing the relationship between smoking status (smoker, non-smoker) and lung cancer diagnosis (yes, no).\n\n\n\nLogistic Regression: Used for modeling the probability of a binary outcome based on one or more predictor variables.\nExample: Predicting the likelihood of a disease relapse based on treatment type and other categorical factors.\nCross-Tabulation: A method used to examine the relationship between two or more categorical variables by displaying the data in a matrix format.\nExample: Cross-tabulating the relationship between gender and the prevalence of a specific medical condition.\n\nApplications in Medical Research\n\nEpidemiology:\n\nResearchers often use qualitative data to study the distribution and determinants of health-related states in populations.\nExample: Categorizing patients by exposure to a risk factor (e.g., exposure to asbestos: yes/no) and examining its relationship with disease occurrence.\n\nClinical Trials:\n\nClinical trials use qualitative data to classify participants, monitor treatment adherence, and assess treatment responses.\nExample: Categorizing patients based on their response to a new drug as “responsive,” “partially responsive,” or “non-responsive.”\n\nPublic Health:\n\nPublic health studies often categorize populations based on demographic factors to identify health disparities and target interventions.\nExample: Identifying vaccination rates across different ethnic groups to improve immunization programs.\nQualitative data is essential in the medical field for categorizing and analyzing various aspects of health and disease. It provides the basis for understanding patterns, making clinical decisions, and conducting research that ultimately improves patient care. Properly collecting, analyzing, and interpreting qualitative data ensures that medical interventions are tailored to the specific needs of patients and populations.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "basic-stat.html",
    "href": "basic-stat.html",
    "title": "Basic methods",
    "section": "",
    "text": "基本的医学统计方法涵盖了数据的收集、整理、描述、分析和解释，旨在帮助研究者理解疾病的发生、发展、治疗效果及健康状况的分布规律。以下是医学研究中常用的一些基本统计方法：\n\n1. 描述性统计\n\n频率分析：计算各类别或数值出现的频数和百分比。\n集中趋势测量：平均值、中位数和众数，用于描述数据的中心位置。\n离散程度测量：标准差、方差、四分位数间距和极差，反映数据的分散程度。\n\n\n\n2. 推断性统计\n\n参数检验：\n\nt检验：用于比较两组或几组独立或配对样本的均值差异，如学生t检验、配对t检验。\nANOVA（方差分析）：比较多个样本均值是否存在显著差异。\n\n非参数检验：\n\nMann-Whitney U检验（Wilcoxon秩和检验的两样本形式）：适用于连续数据但不符合正态分布的两组样本均值差异检验。\nKruskal-Wallis H检验：多组独立样本的中位数差异检验，适用于非正态分布数据。\n\n卡方检验（Chi-square test）：用于分析分类变量间的关系，如四格表检验。\n\n\n\n3. 回归分析\n\n线性回归：研究一个或多个自变量与一个连续因变量之间的线性关系。\n逻辑回归（Logistic Regression）：适用于因变量为二分类或多元分类的场合，用来估计事件发生概率。\n\n\n\n5. 随机化和抽样\n\n随机化：确保研究的组间可比性。\n抽样技术：包括简单随机抽样、分层抽样、整群抽样等，确保样本代表性。\n\n\n\n6. 误差与功效分析\n\n类型I和II错误的理解，以及统计功效分析，以确定研究设计是否足够强大以检测预期效应。\n\n选择适当的统计方法需依据研究目的、数据类型（定量或定性）、数据分布（正态与否）、样本大小等因素。理解这些基础统计概念和方法对于设计和解读医学研究至关重要。",
    "crumbs": [
      "Basic methods"
    ]
  },
  {
    "objectID": "quat-data-stat-desc.html",
    "href": "quat-data-stat-desc.html",
    "title": "3  Statistical description of quantitative data",
    "section": "",
    "text": "3.1 Prerequisite\nSetting up the required R packages.\nlibrary(tidyverse)\nClick on the download button above to download the data file. Save it in your working directory and import the data file into R using the code below.\nrbc &lt;- read_csv(\"datasets/ex03-01.csv\")\n\n#&gt; Rows: 138 Columns: 1\n#&gt; ── Column specification ─────────────────────────────────────────────────────\n#&gt; Delimiter: \",\"\n#&gt; dbl (1): rbc\n#&gt; \n#&gt; ℹ Use `spec()` to retrieve the full column specification for this data.\n#&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nHere we use the read_csv() function. The first argument is the most important: the path to the file. You can think about the path as the address of the file. The code above will work if you have a ex02-01.csv file in the datasets folder of your project.\nWhen you run read_csv(), it prints out a message telling you the number of rows and columns of data, the delimiter that was used, and the column specifications (names of columns organized by the type of data the column contains). It also prints out some information about retrieving the full column specification and how to quiet this message.\nThe data file has only one column with name rbc. Let’s check if there are some missing values present in it.\nrbc |&gt; \n  anyNA()\n\n#&gt; [1] FALSE\nThe output FALSE indicates no missing values is present.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Statistical description of quantitative data</span>"
    ]
  },
  {
    "objectID": "quat-data-stat-desc.html#prerequisite",
    "href": "quat-data-stat-desc.html#prerequisite",
    "title": "3  Statistical description of quantitative data",
    "section": "",
    "text": "Example 1: \nA researcher used a random sampling method to examine the red blood cell counts of 138 normal adult women. The measuring results are saved in a data file. Please use the data to create a frequency distribution table.\n\n\n  Download data",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Statistical description of quantitative data</span>"
    ]
  },
  {
    "objectID": "quat-data-stat-desc.html#visualization",
    "href": "quat-data-stat-desc.html#visualization",
    "title": "3  Statistical description of quantitative data",
    "section": "3.2 Visualization",
    "text": "3.2 Visualization\n\n3.2.1 Frequency table\nA frequency table mentioned here is a statistical tool that organizes data into intervals and lists the number of frequency in each interval. It helps summarize large dataset by showing how often each value or range of values occurs, making it easier to identify trends and the overall distribution of the data. This table is often used in conjunction with visual tools like histogram to enhance data interpretation.\nHere is the steps for creating a frequency table for continuous variables.\n\nComputing the minimum and maximum of the variable.\n\n\nmin_rbc &lt;- rbc |&gt; \n  min()\n\nmax_rbc &lt;- rbc |&gt; \n  max()\n\n\nDetermining the number of intervals. The number of intervals is typically between 10 and 15; here, we adopt 10. The seq() function is used to obtain upper and lower limits of the intervals.\n\n\nbins = 12\nbreaks &lt;- seq(min_rbc, max_rbc, length.out = bins + 1)\nbreaks\n\n#&gt;  [1] 3.070000 3.269167 3.468333 3.667500 3.866667 4.065833 4.265000 4.464167\n#&gt;  [9] 4.663333 4.862500 5.061667 5.260833 5.460000\n\n\n\nUsing the cut() function to dive each data into their respective intervals.\n\n\ncut(pull(rbc), breaks = breaks, include.lowest = T, right = F) |&gt; \n  head(40)\n\n#&gt;  [1] [3.87,4.07) [4.07,4.26) [4.26,4.46) [3.47,3.67) [5.06,5.26) [3.87,4.07)\n#&gt;  [7] [4.26,4.46) [3.67,3.87) [4.66,4.86) [4.07,4.26) [4.46,4.66) [4.07,4.26)\n#&gt; [13] [3.67,3.87) [4.07,4.26) [4.26,4.46) [3.07,3.27) [4.86,5.06) [3.87,4.07)\n#&gt; [19] [4.26,4.46) [3.47,3.67) [4.46,4.66) [3.87,4.07) [4.46,4.66) [4.07,4.26)\n#&gt; [25] [4.46,4.66) [3.87,4.07) [4.26,4.46) [3.47,3.67) [4.86,5.06) [3.87,4.07)\n#&gt; [31] [4.26,4.46) [4.07,4.26) [4.66,4.86) [4.07,4.26) [4.46,4.66) [4.07,4.26)\n#&gt; [37] [4.46,4.66) [3.67,3.87) [3.87,4.07) [4.07,4.26)\n#&gt; 12 Levels: [3.07,3.27) [3.27,3.47) [3.47,3.67) [3.67,3.87) ... [5.26,5.46]\n\n\n\nGenerating the frequency table.\n\n\ncut(pull(rbc), breaks = breaks, include.lowest = T, right = F) |&gt;\n  table() |&gt; \n  knitr::kable(col.names = c(\"interval\", \"freq\"), align = \"c\")\n\n\n\n\ninterval\nfreq\n\n\n\n\n[3.07,3.27)\n2\n\n\n[3.27,3.47)\n3\n\n\n[3.47,3.67)\n9\n\n\n[3.67,3.87)\n14\n\n\n[3.87,4.07)\n22\n\n\n[4.07,4.26)\n30\n\n\n[4.26,4.46)\n21\n\n\n[4.46,4.66)\n15\n\n\n[4.66,4.86)\n10\n\n\n[4.86,5.06)\n6\n\n\n[5.06,5.26)\n4\n\n\n[5.26,5.46]\n2\n\n\n\n\n\n\n\n3.2.2 Frequency histogram\nA frequency histogram is a graphical representation of a frequency table. It displays the distribution of numerical variales by showing the frequency (count) of a value within specific intervals (bins) on the x-axis, with the y-axis representing the frequency. Each bar in the histogram corresponds to an interval, and the height of the bar indicates how many valuess fall within that range. This visual tool is useful for quickly assessing the shape, spread, and central tendency of the data distribution.\nHere we supply two methods to plot a histogram.\n\nbaseggplot2\n\n\n\nhist(\n  x              = pull(rbc), \n  breaks         = breaks, \n  freq           = T,\n  right          = F, \n  col            = \"skyblue\", \n  include.lowest = T,\n  main           = \" \",\n  xlab           = \"Maximum heart rate\",\n  ylab           = \"Frequency\",\n  ylim           = c(0, 32),\n  labels         = T\n)\n\n\n\n\n\n\n\n\n\n\n\nrbc |&gt; \n  ggplot(aes(x = rbc)) +\n  geom_histogram(\n    fill   = \"skyblue\", \n    stat   = \"bin\",\n    color  = \"black\",\n    breaks = breaks,\n    closed = \"left\"\n  ) +\n  stat_bin(\n    geom   = \"text\", \n    aes(label = after_stat(count)),\n    breaks = breaks, \n    closed = \"left\",\n    size   = 4,\n    vjust  = - 0.3\n  ) +\n  labs(x = \"Maximum heart rate\", y = \"Frequency\") +\n  theme(\n    axis.title.x     = element_text(size = 12), \n    axis.title.y     = element_text(size = 12), \n    axis.text.x      = element_text(size = 11),  \n    axis.text.y      = element_text(size = 11),\n    panel.background = element_blank(),        \n    axis.line        = element_line(color = \"black\") \n  )",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Statistical description of quantitative data</span>"
    ]
  },
  {
    "objectID": "quat-data-stat-desc.html#measures-of-central-tendency",
    "href": "quat-data-stat-desc.html#measures-of-central-tendency",
    "title": "3  Statistical description of quantitative data",
    "section": "3.3 Measures of central tendency",
    "text": "3.3 Measures of central tendency\nCentral tendency is a statistical concept that refers to the measure of the center or typical value in a data set. It provides a summary of the data with a single value that represents the middle or average of the data. The most common measures of central tendency are:\n\n3.3.1 Mean\nThe arithmetic average of all values. It’s calculated by summing all the values and dividing by the number of values.\nFor a population:\n\n\\mu = \\frac{\\sum x_i}{N}\n\nFor a sample:\n\n\\bar{X} = \\frac{\\sum x_i}{n}\n\nwhere x_i represents values of a random variable X, and N and n are the sizes of the population and sample, respectively.\n\nrbc |&gt; \n  pull() |&gt; \n  mean()\n\n#&gt; [1] 4.227029\n\n\n\n\n3.3.2 Median\nThe middle value in a data set when the values are sorted in ascending order. If there is an even number of values, the median is the average of the two middle values. Unlike the mean, the median is not affected by outliers or skewed data, making it a robust indicator of central tendency. To find the median:\n\nSort the data set.\nIf the number of observations N is odd, the median is the middle value.\nIf N is even, the median is the average of the two central values.\n\n\nrbc |&gt; \n  pull() |&gt; \n  median()\n\n#&gt; [1] 4.23",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Statistical description of quantitative data</span>"
    ]
  },
  {
    "objectID": "quat-data-stat-desc.html#measures-of-dispersion",
    "href": "quat-data-stat-desc.html#measures-of-dispersion",
    "title": "3  Statistical description of quantitative data",
    "section": "3.4 Measures of dispersion",
    "text": "3.4 Measures of dispersion\nDispersion or variability, describe the spread or dispersion of data points in a data set. They provide insight into how much individual data points differ from the central value (mean, median, etc.). Common measures of dispersion include:\n\n3.4.1 Range\nThe difference between the maximum and minimum values in the data set.\n\n\\text{range} = \\text{max} - \\text{min}\n\n\nrange(rbc) |&gt; \n  diff()\n\n#&gt; [1] 2.39\n\n\n\n\n3.4.2 Interquartile range\nInterquartile range (IQR) is the range of the middle 50% of the data, calculated as the difference between the 75th percentile (Q3) and the 25th percentile (Q1).\n\n\\text{IQR} = Q3 - Q1\n\nYou can directly use the IQR() function to get IQR.\n\nrbc |&gt; \n  pull() |&gt; \n  IQR()\n\n#&gt; [1] 0.565\n\n\n\n\n3.4.3 Variance\nMeasures the average squared deviation of each data point from the mean.\nFor a population:\n\n\\sigma^2 = \\frac{1}{N} \\sum_{i=1}^N (x_i - \\mu)^2\n\nFor a sample:\n\nS^2 = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{X})^2\n\nwhere \\mu is the population mean, \\bar{X} is the sample mean, x_i represents each value, and N and n are are the sizes of the population and sample, respectively.\nYou can directly use the var() function to get variance.\n\nrbc |&gt; \n  pull() |&gt; \n  var()\n\n#&gt; [1] 0.1986751\n\n\n\n\n3.4.4 Standard deviation\nThe square root of the variance, providing a measure of spread in the same units as the data.\nFor a population:\n\n\\sigma = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N (x_i - \\mu)^2}\n\nFor a sample:\n\nS = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{X})^2}\n\nYou can directly use the sd() function to get standard deviation.\n\nrbc |&gt; \n  pull() |&gt; \n  sd()\n\n#&gt; [1] 0.4457298\n\n\n\n\n3.4.5 Coefficient of variation\nThe ratio of the standard deviation to the mean, expressed as a percentage, useful for comparing variability between variables with different units or scales.\nFor a population:\n\n\\text{CV} = \\frac{\\sigma}{\\mu} \\times 100%\n\nwhere \\sigma is the standard deviation and \\mu is the mean of a population.\nFor a sample:\n\n\\text{CV} = \\frac{S}{\\bar{X}} \\times 100%\n\nwhere S is the standard deviation and \\bar{X} is the mean of a sample.\n\nmean &lt;- rbc |&gt; \n  pull() |&gt; \n  mean()\n\nsd &lt;- rbc |&gt; \n  pull() |&gt; \n  sd()\n\nsd / mean * 100\n\n#&gt; [1] 10.54475",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Statistical description of quantitative data</span>"
    ]
  },
  {
    "objectID": "normal-distribution.html",
    "href": "normal-distribution.html",
    "title": "4  Normal distribution",
    "section": "",
    "text": "4.1 Prerequisite\nSetting up the required R packages in this chapter.\nlibrary(tidyverse)\nlibrary(nortest)\nlibrary(scales)\nlibrary(e1071)",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Normal distribution</span>"
    ]
  },
  {
    "objectID": "normal-distribution.html#normal-distribution",
    "href": "normal-distribution.html#normal-distribution",
    "title": "4  Normal distribution",
    "section": "4.2 Normal distribution",
    "text": "4.2 Normal distribution\nThe probability density function of a normal distribution is given by the formula:\nf(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}}\nwhere x is the variable, \\mu is the mean, and \\sigma is the standard deviation. The \\mu and \\sigma are two parameters of the distribution. The mean \\mu is a location parameter, which defines the central position, as shown in Figure 4.1 . The standard deviation \\sigma is the shape parameter, which defines the width and height of the distribution, as shown in Figure 4.2 .\n\n\n\n\n\n\n\n\nFigure 4.1: The normal curve with different means\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.2: The normal curve with different standard deviation\n\n\n\n\n\n\n4.2.1 Standard normal distribution\nAs shown above, the change of \\mu and \\sigma affects the shape of a normal distribution. For convenience, let\nz = \\frac{x - \\mu}{\\sigma}\nthen the above mentioned probability density function will become as:\nf(z) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{1}{2}z^2} which is called the standard normal distribution, where of \\mu = 0 and \\sigma = 1.\n\n\n4.2.2 Area under curve\nThe cumulative distribution function is the probability that a normal random variable X will be less than or equal to a given value x, which is defined by the formula:\nF(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\int_{-\\infty}^x e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}}The area under the curve of a normal distribution over a specific interval represents the probability that the random variable falls within that interval. This is computed using the cumulative distribution function.\nIn R, you can calculate the area under the curve between two points using the pnorm() function.\n\nExample 1: \nCalculating the area under the standard normal curve between -1.96 and 1.96.\n\n\narea_under_curve &lt;- pnorm(1.96, mean = 0, sd = 1) - pnorm(-1.96, mean = 0, sd = 1)\narea_under_curve\n\n#&gt; [1] 0.9500042\n\n\n\n\n4.2.3 Visualizing the area\nYou can use ggplot2 package in R to plot the normal distribution curve and shade the area under the curve. Here is example to shade the area of the left and right tails.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Normal distribution</span>"
    ]
  },
  {
    "objectID": "normal-distribution.html#sec-normality-test",
    "href": "normal-distribution.html#sec-normality-test",
    "title": "4  Normal distribution",
    "section": "4.3 Normality test",
    "text": "4.3 Normality test\nNormality test is used to determine whether a data follows a normal distribution. This is important because many statistical tests, including the t-test, assume that the data follows a normal distribution.\n\nExample 2: \nA scientist used a random sampling method to examine the red blood cell count of 29 normal adult men. The measuring results are saved in the below file. Please analyze its normality.\n\n\n  Download data \n\nYou can click on the download button above to download and the save it in your own folder. Here we import the data file into R and assign to a tibble named rbc.\n\nrbc &lt;- read_csv(\"datasets/ex04-01.csv\")\n\n#&gt; Rows: 29 Columns: 1\n#&gt; ── Column specification ─────────────────────────────────────────────────────\n#&gt; Delimiter: \",\"\n#&gt; dbl (1): rbc\n#&gt; \n#&gt; ℹ Use `spec()` to retrieve the full column specification for this data.\n#&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nTest of normality is used to determine if a variable is well-modeled by a normal distribution. This is an important assumption for many statistical tests. Histogram (Section 3.2.2) can be used as a visual tool to assess whether a variable follows a normal distribution, but it should be used with caution due to the reasons below:\n\nFor a small sample data, the histogram might not provide a clear picture of the distribution, making it harder to assess normality. With larger samples, the histogram gives a better indication but may still be misleading.\nThe appearance of the histogram can change significantly depending on the number of bins (or the bin width). Too few bins might obscure important features of the data, while too many bins might introduce noise.\nThe interpretation of a histogram is somewhat subjective. Two people might look at the same histogram and draw different conclusions about normality.\n\nSince histogram can sometimes be misleading or ambiguous, it’s a good practice to use it alongside other methods:\n\n4.3.1 Normality test method\nStatistical tests can provide a more formal assessment of normality, though they also have limitations and can be sensitive to sample size. Here we only show a few commonly used:\n\nShapiro-Wilk test: Best for small to medium-sized data.\nShapiro-Francia test: A variation of the Shapiro-Wilk test. It is generally more appropriate for dealing with larger sample size data compared to the Shapiro-Wilk test, particularly for data that is expected to be normally distributed.\nAnderson-Darling test: Gives more weight to the tails of the distribution.\n\nWe perform the Shapiro-Wilk test using shapiro.test(), which lies in the stats package of base R. The Shapiro-Francia test and Anderson-Darling test are performed by sf.test() and ad.test(), respectively. Both of them come from the nortest package, which need to be installed beforehand.\n\nShapiro-WilkShapiro-FranciaAnderson-Darling\n\n\n\nrbc |&gt; \n  pull() |&gt; \n  shapiro.test()\n\n#&gt; \n#&gt;  Shapiro-Wilk normality test\n#&gt; \n#&gt; data:  pull(rbc)\n#&gt; W = 0.98382, p-value = 0.9228\n\n\n\n\n\nrbc |&gt; \n  pull() |&gt; \n  sf.test()\n\n#&gt; \n#&gt;  Shapiro-Francia normality test\n#&gt; \n#&gt; data:  pull(rbc)\n#&gt; W = 0.97756, p-value = 0.682\n\n\n\n\n\nrbc |&gt; \n  pull() |&gt; \n  ad.test()\n\n#&gt; \n#&gt;  Anderson-Darling normality test\n#&gt; \n#&gt; data:  pull(rbc)\n#&gt; A = 0.22677, p-value = 0.7975\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nIf the p-value is greater than 0.05, the data is normally distributed (fail to reject the null hypothesis).\nIf the p-value is less than 0.05, the data is not normally distributed (reject the null hypothesis).\n\n\n\n\n\n4.3.2 Visualization method\nQ-Q Plot (Quantile-Quantile Plot) is a more reliable visual tool that plots the quantiles of the data against the quantiles of a theoretical normal distribution. Visual inspection of the data can be very informative and is always useful as a supplementary method.\nYou can create a Q-Q Plot using qqnorm() function. More, you can add a line to a theoretically normal quantile-quantile plot by qqline(), which passes through the first and third quartiles by default. These two functions can be found from the stats package of base R.\n\nrbc |&gt; \n  pull() |&gt; \n  qqnorm(main = \"\", datax = T)\n\nrbc |&gt; \n  pull() |&gt; \n  qqline(datax = T)\n\n\n\n\n\n\n\n\nAlternatively, you can use the ggplot2 package to create a Q-Q plot, which has more customization and flexibility. Here is an example.\n\nrbc |&gt;\n  ggplot(aes(sample = rbc)) +\n  geom_qq(shape = 1, size = 2.3) +\n  geom_qq_line() +\n  labs(x = \"Theoretical Quantiles\", y = \"Sample Quantiles\") +\n  coord_flip() +\n  theme(\n    axis.text  = element_text(size = 12),\n    axis.title = element_text(size = 12)\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nIf the data points fall approximately along the reference line in the Q-Q plot, the data is likely normally distributed.\nSignificant deviations from the line indicate departures from normality.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Normal distribution</span>"
    ]
  },
  {
    "objectID": "normal-distribution.html#medical-reference-range",
    "href": "normal-distribution.html#medical-reference-range",
    "title": "4  Normal distribution",
    "section": "4.4 Medical reference range",
    "text": "4.4 Medical reference range\nA medical reference range is the set of values that a medical test result falls within for a healthy population. These ranges are used by healthcare providers to interpret laboratory results and determine whether a patient’s test results are normal or indicate a potential health issue.\n\n4.4.1 Establishment\nReference ranges are typically established by testing a large sample of healthy individuals and determining the range within which a certain percentage (often 95%) of results fall. The middle 95% of the population’s values are considered the reference range, meaning that 2.5% of healthy individuals might naturally have results slightly below this range and another 2.5% slightly above it.\n\nExample 3: \nA investigator randomly sampled 180 normal adult males in a region, and measured the fibrinogen levels (g/L) in their venous blood. The data is saved in a file below. Have a try to establish the 95% medical reference range for fibrinogen level of normal adult males in that region.\n\n\n  Download data \n\nThe code chunk below can print the results directly.\n\nmed_ref_range &lt;- read_csv(\"datasets/ex04-02.csv\") |&gt; \n  pull() |&gt; \n  quantile(probs = c(0.025, 0.975)) |&gt; \n  round(digits = 2) |&gt; \n  print()\n\n#&gt; Rows: 180 Columns: 1\n#&gt; ── Column specification ─────────────────────────────────────────────────────\n#&gt; Delimiter: \",\"\n#&gt; dbl (1): fibrinogen\n#&gt; \n#&gt; ℹ Use `spec()` to retrieve the full column specification for this data.\n#&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n#&gt;  2.5% 97.5% \n#&gt;  1.77  3.63\n\n\nThe result indicates that the 95% medical reference range for the fibrinogen level of normal adult males is: (1.77, 3.63).\n\n\n4.4.2 Result interpretation\nA test result that falls within the reference range is usually considered normal. A result outside the reference range might indicate an abnormal condition, but this must be interpreted in the context of the patient’s overall health, symptoms, and medical history. Not all out-of-range results indicate disease; they might be normal for a specific individual due to factors like temporary stress, diet, or exercise.\n\n\n4.4.3 Considerations\nReference ranges are not absolute; what is normal for one individual may not be normal for another, especially at the edges of the range. Doctors consider a variety of factors, including patient history and symptoms, when interpreting test results. An out-of-range result may warrant further testing or a different interpretation based on the clinical context.\nReference ranges may be updated as new research and technologies emerge, so staying informed about the latest standards is important for accurate diagnosis and treatment.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Normal distribution</span>"
    ]
  },
  {
    "objectID": "statistical-inference.html",
    "href": "statistical-inference.html",
    "title": "5  Statistical inference",
    "section": "",
    "text": "5.1 Prerequisites\nlibrary(tidyverse)\nlibrary(rmarkdown)",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistical inference</span>"
    ]
  },
  {
    "objectID": "statistical-inference.html#distribution-of-sample-means",
    "href": "statistical-inference.html#distribution-of-sample-means",
    "title": "5  Statistical inference",
    "section": "5.2 Distribution of sample means",
    "text": "5.2 Distribution of sample means\nThe distribution of sample means is a fundamental concept in statistics, describing the distribution of sample means obtained from multiple samples drawn from the same population. Here is a example to demonstrate the distribution of sample means.\n\nExample 1: \nAssume thrombin time follow the normal distribution of \\mu=17.5, \\sigma=1.2. A researcher randomly drew 100 samples from a population of size 60000 , with each sample n=10 observations. The data is saved the the data below. The mean \\bar{X} and standard deviation S for each sample are shown in Table 5.1, please analyze the distribution of the 120 means.\n\n\n  Download data \n\n\ndf &lt;- read_csv(\"datasets/ex05-01.csv\", show_col_types = F) |&gt; \n  summarise_all(\n    list(\n1      n       = ~ n(),\n      median  = median,\n      mean    = mean,\n      sd      = sd        \n    )\n  ) |&gt; \n  pivot_longer(\n    cols      = everything(),\n    names_to  = c(\"sample\", \".value\"),\n    names_sep = \"_\"\n  ) |&gt; \n  mutate(across(c(mean, sd), \\(x)round(x, 2)))\n\ndf |&gt; paged_table()\n\n\n1\n\nHere n() must use a style of anonymous function, unlike the other three ones.\n\n\n\n\n\n\nTable 5.1: The n, median, mean and sd of the 100 random samples (s)\n\n\n\n\n  \n\n\n\n\n\n\nIt is clear that the mean of each sample is different, and also may not equal to the mean of the population. The difference between a population parameter and a corresponding sample statistic is called sampling error. It arises because only a random sample of the entire population is observed, which may not perfectly represent the entire population.\nTo visualize the distribution of the 120 sample means, here we create a histogram:\n\ndf |&gt; \n  select(mean) |&gt; \n  pull() |&gt; \n  hist(\n  freq           = T,\n  right          = F, \n  col            = \"lightblue\", \n  include.lowest = T,\n  main           = \" \",\n  xlab           = \"The average of the thrombin time\",\n  ylab           = \"Frequency\",\n  ylim           = c(0, 26),\n  labels         = T\n)\n\n\n\n\n\n\n\n\nFrom the figure above we can say that the distribution of the sample mean can be approximated by a normal distribution. The two sample statistics of mean \\bar{X} and standard deviation S can be achieved by the following code:\n\ndf |&gt; \n  select(x = mean) |&gt; \n  summarise(\n    mean = mean(x),\n    sd   = sd(x)\n  ) |&gt; \n  round(digits = 2) |&gt; \n  knitr::kable(align = \"c\")\n\n\n\n\nmean\nsd\n\n\n\n\n17.46\n0.36\n\n\n\n\n\nThe mean of the sample means is 17.5, equal to the population mean \\mu=1.75, while the standard deviation of the sample mean is 0.4, less than the standard deviation of population \\sigma = 1.25.\n\n5.2.1 Standard error\nThe standard deviation of the distribution of the sample means is called the standard error (SE). It reflects the typical distance between a sample mean and the population mean. The standard error of sample means is given by:\n\\sigma_{\\bar{X}} = \\frac{\\sigma}{\\sqrt{n}}\nHowever, the population parameter \\sigma is usually unknown, therefore, the sample standard deviation S is used as an estimator for the population standard deviation. Therefore, an estimation of the standard error above mentioned is defined by:\nS_{\\bar{X}} = \\frac{S}{\\sqrt{n}}\nAs the sample size n increases, the standard error decreases, meaning the sample means will be closer to the population mean.\nIt is need to be pointed out that the above formula for calculating the sampling error of the mean is only adapt to simple random sampling. For other sampling methods, there are corresponding formulas exist.\n\n\n5.2.2 Central Limit Theorem\nRegardless of the population distribution, as the sample size n becomes large (typically n \\geq 30 is considered sufficient), the sampling distribution of the sample mean will approximate a normal distribution. This is the essence of the Central Limit Theorem in statistics.\nFor small sample sizes, if the population itself is normally distributed, the distribution of the sample means will also be normally distributed.\nBecause the sampling distribution of the sample means can be approximated by a normal distribution (especially for large n), it allows for the construction of confidence intervals for the population mean and the conducting of hypothesis tests. The normality of the sampling distribution justifies the use of z-test or t-test depending on whether the population variance is known or unknown and the sample size.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistical inference</span>"
    ]
  },
  {
    "objectID": "statistical-inference.html#t-distribution",
    "href": "statistical-inference.html#t-distribution",
    "title": "5  Statistical inference",
    "section": "5.3 t distribution",
    "text": "5.3 t distribution\nThe sample mean \\bar{X} follows a normal distribution when the underlying population is normally distributed, or when the sample size is sufficiently large due to the Central Limit Theorem. However, when you’re converting the sample mean into a standardized form, the distribution you use depends on whether or not you know the population standard deviation \\sigma.\n\nIf \\sigma is known, the sample mean \\bar{X} can be standardized using the formula:\n\n\nz = \\frac{\\bar{X} - \\mu}{\\sigma_{\\bar{X}}} = \\frac{\\bar{X} - \\mu}{\\sigma / \\sqrt{n}}\n\nIn this case, the standardized variable Z follows a standard normal distribution N(0, 1).\n\nIf \\sigma is unknown, which is often the case in practice, you need to estimate it using the sample standard deviation S. The formula for standardizing the sample mean becomes:\n\n\nt = \\frac{\\bar{X} - \\mu}{S_{\\bar{X}}} = \\frac{\\bar{X} - \\mu}{S / \\sqrt{n}}\n\nHere, t doesn’t not follow a standard normal distribution anymore, it follows a t-distribution with degrees of freedom \\nu = n - 1 (n is the sample size).\nThe t-distribution, also known as Student’s t-distribution, is used in statistics when estimating population parameters when the sample size is small and/or the population variance is unknown. It’s especially important in confidence intervals, hypothesis testing, and regression analysis.\n\n5.3.1 Visual representation\nThe t-distribution can be plotted to show how it compares with the normal distribution.\n\n\n\n\n\n\n\n\n\nThe t-distribution is similar to the standard normal distribution, but it has heavier tails. This means a higher probability of values further from the mean, especially with a smaller degree of freedom. As degrees of freedom increase, the t-distribution curve will converge toward the standard normal distribution curve.\nThe shape of the t-distribution depends on the degrees of freedom \\nu, which is typically related to the sample size (\\nu = n - 1). As the degrees of freedom increase, the t-distribution approaches the standard normal distribution. For large sample sizes (\\nu &gt; 60), the t-distribution and normal distribution are almost indistinguishable.\n\n\n5.3.2 Common uses\n\nConfidence intervals\nThe t-distribution is used to construct confidence intervals for the population mean when the population variance is unknown and the sample size is small.\nt-test: Used to compare the means of two groups when the sample size is small and the population standard deviation is unknown.\nRegression analysis: In linear regression, t-test are used to determine whether the coefficients of the independent variables are significantly different from zero.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistical inference</span>"
    ]
  },
  {
    "objectID": "statistical-inference.html#estimation-of-population-mean",
    "href": "statistical-inference.html#estimation-of-population-mean",
    "title": "5  Statistical inference",
    "section": "5.4 Estimation of population mean",
    "text": "5.4 Estimation of population mean\nParameter estimation is a concept in statistics that involves using statistic off a sample to estimate parameters of the corresponding population. The sample should be random and large enough to provide an accurate estimate. A parameter is a numerical characteristic about a population, such as a mean, proportion, or standard deviation. Here we just talk about the estimation of population mean.\n\n5.4.1 Point estimation\nThis involves using sample data to calculate a single value that serves as the best guess for a population mean. However, obtaining an exact point estimate of the population mean from just one random sample is almost unattainable.\nCalculate the sample mean \\bar{X}: The sample mean is calculated using the formula:\n\n\\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\n\nwhere n is the sample size, and x_i are the individual sample values. The sample mean (\\bar{X}) serves as an estimate of the population mean (\\mu).\nHowever, a point estimate does not consider the sampling error and cannot tell you how close the estimate is likely to be to the true population mean. Instead of relying on a single point estimate, interval estimation are often used because they provide a range of values within which the true population mean is likely to lie along with a confidence level.\n\n\n5.4.2 Confidence interval\nThe method for calculating the confidence interval of the population mean varies depending on whether the population standard deviation \\sigma is known and the size of sample n. Typically, there are two types of methods: the t-distribution and the z-distribution (also known as the u-distribution). The following will introduce the methods for calculating the confidence interval of a single population mean and the confidence interval of the difference between two population means.\nSingle population mean\nConfidence interval (CI) is associated with a confidence level, typically 95%, which indicates the degree of certainty that the true population mean falls in the range. The two-sided 1-\\alpha confidence interval of single population mean can be formulated as:\n\n\\text{CI} = \\bar{X} \\pm t_{\\nu,\\alpha/2} \\cdot \\frac{S}{\\sqrt{n}} \\quad \\text{or} \\quad \\text{CI} = \\bar{X} \\pm z_{\\alpha/2} \\cdot \\frac{S}{\\sqrt{n}}\n\nSimilarly, the one-sided 1-\\alpha confidence interval for the population mean is given by：\n\n\\mu&gt; \\bar{X} - t_{\\nu,\\alpha} \\cdot \\frac{S}{\\sqrt{n}} \\quad \\text{or} \\quad \\mu&gt; \\bar{X} - z_\\alpha \\cdot \\frac{S}{\\sqrt{n}}\n\n\n\\mu &lt; \\bar{X} + t_{\\nu,\\alpha} \\cdot \\frac{S}{\\sqrt{n}} \\quad \\text{or} \\quad \\mu &lt; \\bar{X} + z_\\alpha \\cdot \\frac{S}{\\sqrt{n}}\n\nwhere t is the critical value from the t-distribution, it depends on the degree of freedom \\nu and the level of confidence \\alpha; z is the critical value from the standard normal distribution, it depends on the level of confidence \\alpha.\n\nExample 2: \nA research group randomly selected 20 adult men in a area and measured their red blood cell count. Please calculate the 95% confidence interval for the population mean.\n\n\n  Download data \n\nSince the sample size is small, we would use the t-distribution to estimate the population mean. In practice, you can always use the t-distribution regardless of the size of samples, because as the increase of degrees of freedom , t-distribution approaches the standard normal distribution.\nIn this chapter, we write a function named mean_CI() to tackle this problem.\n\n\nmean_CI  &lt;- function(x, alternative = \"two.sided\", conf.level = 0.95){\n  x_bar  &lt;- mean(x, na.rm = T)\n  s      &lt;-  sd(x, na.rm = T)\n  n      &lt;- length(x)  \n  stderr &lt;- s / sqrt(n)\n  \n  type  &lt;- alternative\n  alpha &lt;- 1- conf.level\n  \n  case_when(\n    type == \"two.sided\" ~ {\n      t_stat &lt;- qt(1 - alpha / 2, df = n - 1)\n      tibble( \n        mean       = x_bar,\n        lower_ci   = x_bar - stderr * t_stat,\n        upper_ci   = x_bar + stderr * t_stat,\n        conf_level = conf.level\n      )\n    },\n    \n    type == \"greater\"   ~ {\n      t_stat &lt;- qt(alpha, df = n - 1)\n      tibble(\n        mean       = x_bar,\n        lower_ci   = x_bar + stderr * t_stat,\n        upper_ci   = Inf,\n        conf_level = conf.level\n      )\n    },\n      \n    type == \"less\"      ~ {\n      t_stat &lt;- qt(alpha, df = n - 1)\n      tibble( \n        mean       = x_bar,\n        lower_ci   = -Inf,\n        upper_ci   = x_bar - stderr * t_stat,\n        conf_level = conf.level\n     )\n    }\n  )\n}\n\n\n\nread_csv(\"datasets/ex05-02.csv\", show_col_types = F) |&gt; \n  pull() |&gt; \n  mean_CI() \n\n#&gt; # A tibble: 1 × 4\n#&gt;    mean lower_ci upper_ci conf_level\n#&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n#&gt; 1  5.31     5.04     5.59       0.95\n\n\nAlternatively, you can use a function MeanCI() from DescTools package.\n\nread_csv(\"datasets/ex05-02.csv\", show_col_types = F) |&gt; \n  pull() |&gt;\n  DescTools::MeanCI()\n\n#&gt;     mean   lwr.ci   upr.ci \n#&gt; 5.312500 5.035249 5.589751\n\n\nDifference between two population means\nThe confidence interval for the difference between two population means is used to estimate the range within which the true difference between the means of two populations lies, based on sample data.\nRandomly sample from two normal populations N(\\mu_1, \\sigma^2) and N(\\mu_2, \\sigma^2) with equal population variances but unequal population means. The sample sizes, means, and standard deviations of the two samples are denoted by n_1, \\bar{X}_1, S_1 and n_2, \\bar{X}_2, S_2 respectively. If the population variances are unknown and the sample sizes are small, then the two-sided (1-\\alpha) confidence interval for the difference between the two population means (\\mu_1 - \\mu_2) is given by:\n\n(\\bar{X}_1 - \\bar{X}2) \\pm t_{\\alpha/2, \\nu} \\cdot \\sqrt{S_c^2(\\frac{1}{n_1} + \\frac{1}{n_2})} \\ , \\quad S_c^2 = \\frac{(n_1 -1)S_1^2 + (n_2 -1)S_2^2}{n_1 + n_2 -2}\n\nWhere t_{\\alpha/2, \\nu} is the t-score with \\nu = n_1 + n_2 - 2 degrees of freedom, \\alpha is the level of confidence, S_c^2 is called the pooled variance.\nIf thevariances of the two populations mentiond above are unknown and unequal, then the two-sided (1-\\alpha) confidence interval for the difference between the two population means (\\mu_1 - \\mu_2) is given by:\n\n(\\bar{X}_1 - \\bar{X}2) \\pm t_{\\alpha/2, \\nu} \\cdot \\sqrt{\\frac{S_1^2}{n_1} + \\frac{S_2^2}{n_2}}\\ , \\\n\\nu \\approx \\frac{\\left(\\frac{S_1^2}{n_1} + \\frac{S_2^2}{n_2}\\right)^2}{\\frac{\\left(\\frac{S_1^2}{n_1}\\right)^2}{n_1 - 1} + \\frac{\\left(\\frac{S_2^2}{n_2}\\right)^2}{n_2 - 1}}\n\nIf the sample sizes of both samples are large (e.g., both are greater than 60)， the confidence interval for the difference between two population means \\mu_1 - \\mu_2 is given by:\n\n(\\bar{X}_1 - \\bar{X}2) \\pm z_{\\alpha/2} \\cdot \\sqrt{\\frac{S_1^2}{n_1} + \\frac{S_2^2}{n_2}}\n\nWhere \\bar{X}_1 and \\bar{X}_2 are the sample means；n_1 and n_2 are the sample sizes. S_1^2 and S_2^2are the sample variances. z_{\\alpha/2} is the z-score corresponding to the desired confidence level.\nHere is a custom function named mean_diff_CI() to resolve this issue.\n\n\nmean_diff_CI &lt;- function(x, y, alternative = \"two.sided\", conf.level = 0.95, \n                         var.equal = FALSE){\n  x &lt;- na.omit(x)\n  y &lt;- na.omit(y)\n  \n  x_bar &lt;- mean(x)\n  v1    &lt;- var(x)\n  n1    &lt;- length(x)  \n  \n  y_bar &lt;- mean(y)\n  v2    &lt;- var(y)\n  n2    &lt;- length(y)  \n  \n  type  &lt;- alternative\n  alpha &lt;- 1- conf.level\n  diff  &lt;-  x_bar - y_bar \n  \n  if(var.equal) {\n    df     &lt;- n1 + n2 - 2\n    var    &lt;- ((n1 - 1) * v1 + (n2 - 1) * v2) / df\n    stderr &lt;- sqrt(var * (1 / n1 + 1 / n2))\n  } else{\n    stderr1 &lt;- sqrt(v1/n1)\n    stderr2 &lt;- sqrt(v2/n2)\n    stderr  &lt;- sqrt(stderr1^2 + stderr2^2)\n    df      &lt;- stderr^4/(stderr1^4/(n1 - 1) + stderr2^4/(n2 - 1))\n  }\n  \n  case_when(\n    type == \"two.sided\" ~ {\n      t_stat &lt;- qt(1 - alpha / 2, df)\n      tibble( \n        mean_x     = x_bar,\n        mean_y     = y_bar,\n        mean_diff  = diff,\n        lower_ci   = diff - stderr * t_stat,\n        upper_ci   = diff + stderr * t_stat,\n        conf_level = conf.level\n      )\n    },\n    \n    type == \"greater\"   ~ {\n      t_stat &lt;- qt(alpha, df)\n      tibble( \n        mean_x     = x_bar,\n        mean_y     = y_bar,\n        mean_diff  = diff,\n        lower_ci   = diff + stderr * t_stat,\n        upper_ci   = Inf,\n        conf_level = conf.level\n      )\n    },\n      \n    type == \"less\"      ~ {\n      t_stat &lt;- qt(alpha, df)\n      tibble( \n        mean_x     = x_bar,\n        mean_y     = y_bar,\n        mean_diff  = diff,\n        lower_ci   = -Inf,\n        upper_ci   = diff - stderr * t_stat,\n        conf_level = conf.level\n     )\n    }\n  )\n}\n\n\n\nExample 3: \nTo compare the difference of hemoglobin concentration between people with alpha thalassemia trait and silent carrier, a doctor choose a random sample of 42 patients from a patient database. 22 subjects have alpha thalassemia trait, 20 are silent_carrier. Have a try to analyze the 95% confidence interval of the two populations.\n\n\n  Download data \n\n\ndf &lt;- read_csv(\"datasets/ex05-03.csv\", show_col_types = F)\nx &lt;- pull(df, 1)\ny &lt;- pull(df, -1)\nmean_diff_CI(x, y, var.equal = T)\n\n#&gt; # A tibble: 1 × 6\n#&gt;   mean_x mean_y mean_diff lower_ci upper_ci conf_level\n#&gt;    &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n#&gt; 1   11.0   11.8    -0.715    -1.67    0.245       0.95\n\n\nSamely, you can use the function MeanDiffCI() from DescTools package, but this function does not have a direct argument to control for whether the variances of the two populations are equal.\n\nDescTools::MeanDiffCI(x, y, na.rm = T, var.equal = T)\n\n#&gt;   meandiff     lwr.ci     upr.ci \n#&gt; -0.7145455 -1.6678398  0.2387489\n\n\nAs you can see, the confidence intervals printed out from DescTools::MeanDiffCI and mean_diff_CI() have a little difference.\n\n\n5.4.3 Interpreting the confidence interval\nThe confidence interval represents a range within which the true population mean is expected to fall with a certain level of confidence (usually 95%).\nAs for the difference between two population means, the truth is the same. Moreover, if the interval of the difference between two population means includes 0, this suggests that there may be no significant difference between the two population means. If the interval does not include 0, it suggests that there is a significant difference.\nConsider the 100 samples drawn from normal populations N(17.5, 1.2) (see Example 1), we can construct the 95% confidence intervals for them. Look up the results in Table 5.2 , flag = TRUE indicates the confidence interval contain the population mean, and the vise is not true.\n\npop.mean = 17.5\n\ndf &lt;- read_csv(\"datasets/ex05-01.csv\", show_col_types = F) |&gt; \n  summarise_all(\n    list(\n      s.mean = mean,\n      ci     = mean_CI\n    )\n  ) |&gt; \n  pivot_longer(\n    cols      = everything(),\n    names_to  = c(\"sample\", \".value\"),\n    names_sep = \"_\"\n  ) |&gt; \n  mutate(\n    lower.ci = ci[[2]],\n    upper.ci = ci[[3]],\n    ci       = NULL\n  ) |&gt;\n  mutate(\n    flag = lower.ci &lt;= pop.mean & upper.ci &gt;= pop.mean,\n    sample = as.factor(c(1:100)),\n    across(c(2:4), ~ round(., digits = 2))\n  )\ndf |&gt; \n  paged_table()\n\n\n\nTable 5.2: The confidence intervals for the 100 samples\n\n\n\n\n  \n\n\n\n\n\n\nTo highlight the confidence intervals that do not contain the population mean with a different color,\nTo see more clearli, we visualize the confidence intervals for the means of 100 samples as below. The blue verticale line represents the population mean, black dot denotes each sample mean, and the short horizontal lines characterize the confidence intervals. The confidence intervals that do not contain the population mean are highlighted with a red color.\n\ndf |&gt; \n  ggplot(aes(x = s.mean, y = sample)) +\n  geom_point(size = 0.4) +\n  geom_vline(xintercept = pop.mean, linetype = \"dashed\", color = \"blue\") +\n  geom_errorbarh(\n    aes(xmin = lower.ci, xmax = upper.ci, color = flag), \n    height = 0.3,\n    linewidth = 0.4) +\n  xlim(15, 20) +\n  labs(x = \"Means of sample and 95% CI\", y = \"Sample index\") +\n  theme_light() +\n  theme(\n    axis.text = element_text(size = 6.5), \n    legend.position = \"none\",\n    axis.title  = element_text(size = 10)\n  )",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statistical inference</span>"
    ]
  },
  {
    "objectID": "hypothesis-test.html",
    "href": "hypothesis-test.html",
    "title": "6  Hypothesis test",
    "section": "",
    "text": "6.1 Fundamentals of hypothesis test",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Hypothesis test</span>"
    ]
  },
  {
    "objectID": "hypothesis-test.html#fundamentals-of-hypothesis-test",
    "href": "hypothesis-test.html#fundamentals-of-hypothesis-test",
    "title": "6  Hypothesis test",
    "section": "",
    "text": "6.1.1 The fundamentals\nThe small probability event method or proof by contradiction using small probability event, is a reasoning approach used in hypothesis test based on the principle of probability theory. The core idea is that if an event is highly unlikely to occur under a certain assumption, and it does occur, then the assumption is likely incorrect.\nIt start from the opposite assumption (H_0) to indirectly assess whether the problem of interest (H_1) holds true. This involves assuming that H_0 is true, calculating the test statistic, and then using the resulting p-value to determine whether H_0 can be upheld.\n\n\n6.1.2 Some concepts and notations\nBefore you learn and insight into more about hypothesis test, some concepts and notations need to be noticed.\nNull hypothesis (H_0)\nThis is the default assumption that there is no effect or no difference. It is the hypothesis that we seek to test against. For example: The average height of adult men in a certain city is 175 cm. (H_0: \\mu = 175 cm)\nAlternative hypothesis (H_1)\nThis is what you want to prove. It represents an effect, a difference, or a relationship that contradicts the null hypothesis. The hypothesis is that there is an effect or difference. For example: The average height of adult men in the city is not 175 cm. (H_1: \\mu \\ne 175 cm)\nTest statistic\nA standardized value calculated from the sample data that is used to determine whether to reject H_0, such as the t-statistic, z-statistic, or chi-square statistic.\nSignificance level (\\alpha)\nThe probability threshold for rejecting the null hypothesis. Commonly used values are 0.05, 0.01, or 0.10. α = 0.05 means there’s a 5% risk of rejecting the null hypothesis when it is actually true.\nP-value\nThe probability of obtaining a test statistic at least as extreme as the one observed, assuming the null hypothesis is true. If the p-value is less than α, the null hypothesis is rejected. The smaller the p-value, the stronger the evidence against the null hypothesis. For example, if the p-value is 0.03, there is a 3% chance of observing such a result if H₀ is true.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Hypothesis test</span>"
    ]
  },
  {
    "objectID": "hypothesis-test.html#steps-of-hypothesis-test",
    "href": "hypothesis-test.html#steps-of-hypothesis-test",
    "title": "6  Hypothesis test",
    "section": "6.2 Steps of hypothesis test",
    "text": "6.2 Steps of hypothesis test\nThe process of conducting a hypothesis test involves several key steps. Here’s an outline of the typical procedure:\n\nDefine H_0 and H_1.\nChoose the significance level \\alpha, typically 0.05.\nSelect the appropriate test.\nCompute the test statistic.\nDetermine the p-value.\nMake a decision based on the p-value and significance level.\n\nReject H_0: If the p-value ≤ α.\nFail to reject H_0: If the p-value &gt; α.\n\nDraw conclusions in the context of the research.\n\nIn the subsequet chapters, some typical test method will be introduced.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Hypothesis test</span>"
    ]
  },
  {
    "objectID": "hypothesis-test.html#considerations-for-hypothesis-test",
    "href": "hypothesis-test.html#considerations-for-hypothesis-test",
    "title": "6  Hypothesis test",
    "section": "6.3 Considerations for hypothesis test",
    "text": "6.3 Considerations for hypothesis test\n\n6.3.1 Type I error and type II error\nIt is known that hypothesis test employs the principle of proof by contradiction through small probability events. The conclusions drawn based on the p-value are probabilistic essentially, meaning that the conclusions may not be entirely correct. Two types of errors can occur.\nType I error (α) (false positive)\nA type I error occurs when you reject the null hypothesis (H_0) when it is actually true. For example, imagine that you are testing a new drug to see if it lowers blood pressure. The null hypothesis states that the drug has no effect. If you conclude that the drug does lower blood pressure when in fact it does not, you have made a type I error.\nThe probability of making a type I error is denoted by the significance level α .Common choices for α are 0.05 or 0.01, meaning there is a 5% or 1% risk of rejecting H₀ when it is true.\nType II error (β) (false negative)\nA type II error occurs when you fail to reject the null hypothesis (H_0) when the alternative hypothesis (H_1) is actually true. Continuing with the drug example, if the drug actually does lower blood pressure, but you conclude that it doesn’t, you’ve made a type II error.\nThe probability of making a type II error is denoted by \\beta. The power of a test (1-\\beta) represents the probability of correctly rejecting a false null hypothesis when the alternative hypothesis is true. In other words, it’s the likelihood that the test will detect an effect or difference when one actually exists.\nFor example , in a medical study designed to detect whether a new drug is effective, if the power of the test is 0.9, there is a 90% chance of correctly rejecting H_0 (that the drug has no effect) when H_1 (that the drug is effective) is true.\nFactors that affect β include:.\n\nSample size: Larger sample sizes generally increase the power of a test.\nEffect size: Larger differences or stronger effects are easier to detect, thus increasing power.\nSignificance level (α): Increasing α can increase power, but it also increases the risk of a type I error.\nVariance: Lower variability within the data increases power.\n\nIn hypothesis test, there is often a trade-off between the risks of type I and type II errors. Lowering the significance level α reduces the risk of a type I error but increases the risk of a type II error, and vice versa.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Hypothesis test</span>"
    ]
  },
  {
    "objectID": "hypothesis-test.html#considerations-in-hypothesis-test",
    "href": "hypothesis-test.html#considerations-in-hypothesis-test",
    "title": "6  Hypothesis test",
    "section": "6.4 Considerations in hypothesis test",
    "text": "6.4 Considerations in hypothesis test\n\n6.4.1 Study design\nStudy design is the prerequisite of hypothesis test. The groups to be compared should be balanced and comparable, meaning that aside from the primary factor under investigation (such as a new drug in a clinical trial versus a control drug), other factors (such as age, gender, disease duration, and severity) that could influence the results should be identical or similar between groups. The best way to ensure balance is randomization before treatment.\n\n\n6.4.2 Different tests for different types of data\nThe appropriate test should be selected based on the purpose of the analysis, the type and distribution of the data, the study design, the sample size, and the conditions under which different statistical methods are applicable. For example, paired t-tests should be used for paired design measurement data; for completely randomized design measurement data with small sample sizes (i.e., n ≤ 60) with equal variances, the two-sample t-test should be used. If variances are unequal, an approximate t-test should be used.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Hypothesis test</span>"
    ]
  },
  {
    "objectID": "ttest.html",
    "href": "ttest.html",
    "title": "7  t-test",
    "section": "",
    "text": "7.1 Prerequisites\nlibrary(tidyverse)",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>t-test</span>"
    ]
  },
  {
    "objectID": "ttest.html#one-sample-t-test",
    "href": "ttest.html#one-sample-t-test",
    "title": "7  t-test",
    "section": "7.2 One-sample t-test",
    "text": "7.2 One-sample t-test\nThe one-sample t-test is used to determine whether the mean of a single sample is significantly different from a known or hypothesized population mean. It is commonly used when you have a sample and want to test if its mean differs from a theoretical value or an expected value. Its test statistic t is calculated as follows:\n\nt = \\frac{\\bar{X} - \\mu_0}{S_{\\bar{X}}} = \\frac{\\bar{X} - \\mu_0}{S / \\sqrt{n}}\n\nwhere \\bar{X} si the sample mean , S is the sample standard deviation and n is the sample size, \\mu_0 is the hypothesized population mean.\nThe one-sample t-test is a straightforward but powerful tool for hypothesis testing in many research scenarios.\n\nExample 1: \nA doctor measured the hemoglobin concentration in 36 male workers involved in lead-related jobs. The data can be downloaded below. The question is whether the mean hemoglobin concentration (\\mu) of male workers involved in lead-related jobs differs from the mean of 140 (g/L) for normal adult males.\n\n\n  Download data \n\nR codes for one-sample t-test is:\n\nread_csv(\"datasets/ex07-01.csv\", show_col_types = F) |&gt; \n  t.test(mu = 140) \n\n#&gt; \n#&gt;  One Sample t-test\n#&gt; \n#&gt; data:  read_csv(\"datasets/ex07-01.csv\", show_col_types = F)\n#&gt; t = -2.1367, df = 35, p-value = 0.03969\n#&gt; alternative hypothesis: true mean is not equal to 140\n#&gt; 95 percent confidence interval:\n#&gt;  122.1238 139.5428\n#&gt; sample estimates:\n#&gt; mean of x \n#&gt;  130.8333\n\n\nThe results show t = -2.1367 , \\text{p-value} = 0.03969. At the significance level of \\alpha = 0.05 , reject H_0 and accept H_1, indicating that the difference is statistically significant. In the context of this case, it can be concluded that the average hemoglobin concentration of male workers engaged in lead work is lower than the average hemoglobin concentration of normal adult males.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>t-test</span>"
    ]
  },
  {
    "objectID": "ttest.html#paired-t-test",
    "href": "ttest.html#paired-t-test",
    "title": "7  t-test",
    "section": "7.3 Paired t-test",
    "text": "7.3 Paired t-test\nThe paired t-test is used to compare the means of two related groups. This test is typically used when the observations are paired in some meaningful way, such as measurements taken on the same subjects at two different times or under two different conditions.\n\nH_0 : The mean difference between the paired observations is zero (\\mu_d = 0).\nH_1 : The mean difference between the paired observations is not zero (\\mu_d \\neq 0).\n\nThe test statistic for the paired t-test is calculated as:\nt = \\frac{\\bar{d}}{S_d / \\sqrt{n}} Where \\bar{d} is the mean of the differences between the paired observations, S_d is the standard deviation of the differences, and n is the number of pairs.\n\nExample 2: \nTo compare whether the results of fat content measurement in lactic acid beverages differ between two methods, 10 samples of lactic acid beverages were randomly selected. The fat content was measured using both the Gerber-Gottlieb method and the fatty acid hydrolysis method. The data can be downloaded below. The question is whether the measurement results from the two methods are different?\n\n\n  Download data \n\nR codes for paired t-test is:\n\nread_csv(\"datasets/ex07-02.csv\", show_col_types = F) |&gt; \n  with(t.test(x1, x2, paired = T))\n\n#&gt; \n#&gt;  Paired t-test\n#&gt; \n#&gt; data:  x1 and x2\n#&gt; t = 7.926, df = 9, p-value = 2.384e-05\n#&gt; alternative hypothesis: true mean difference is not equal to 0\n#&gt; 95 percent confidence interval:\n#&gt;  0.1946542 0.3501458\n#&gt; sample estimates:\n#&gt; mean difference \n#&gt;          0.2724\n\n\nThe results show t = -2.1367 , \\text{p-value} &lt; 0.001. At the significance level of \\alpha = 0.05, reject H_0 and accept H_1, indicating that the difference is statistically significant. It can be concluded that the two methods yield different fat content measurements. As mean difference estimation is 0.2724 &gt; 0, the Gerber-Gottlieb method providing higher results.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>t-test</span>"
    ]
  },
  {
    "objectID": "ttest.html#two-sample-t-test",
    "href": "ttest.html#two-sample-t-test",
    "title": "7  t-test",
    "section": "7.4 Two-sample t-test",
    "text": "7.4 Two-sample t-test\nThe two-sample t-test (also known as the independent t-test) is used to determine whether there is a significant difference between the means of two independent groups. It is commonly used in experiments or studies where researchers want to compare the means of two different populations or conditions.\nIn medical study it is commonly used to compare the means of two samples in a completely randomized design. In this design, subjects are randomly assigned to two different treatment groups, and the researcher is interested in whether the means of these two samples represent different population means. Additionally, in observational studies, the two-sample t-test can be used to compare the means of two samples obtained through independent random sampling from two different populations.\nWhen both samples come from normal populations, and the sample sizes are relatively small, such as n_1 \\leq 60 or/and n_2 \\leq 60 , different testing methods should be used depending on whether the population variances are equal.\n\n7.4.1 The t-test for equal population variances\nThe t-test for equal population variances, often referred to as the pooled variance t-test, is used when comparing the means of two independent samples under the assumption that the two populations have the same variance. This assumption allows the variances of the two samples to be combined (or pooled) into a single estimate, which can then be used in the t-test formula.\nBefore conducting the combined t-test, it’s common to perform an F-test to check if the variances of the two samples are equal. If the p-value from the F-test is not significant, the pooled t-test can be justified. The pooled variance is calculated as a weighted average of the variances from the two samples:\n\nS_c^2 = \\frac{(n_1 - 1)S_1^2 + (n_2 - 1)S_2^2}{n_1 + n_2 - 2}\n\nHere, S_c^2 is the combined variance, S_1^2 and S_2^2 are the variances of the two samples, and n_1 and n_2 are the sample sizes.\nOnce the combined variance is calculated, the t-statistic for the two-sample t-test can be computed as:\n\nt = \\frac{\\bar{X}_1 - \\bar{X}_2}{\\sqrt{S_c^2 \\left(\\frac{1}{n_1} + \\frac{1}{n_2}\\right)}}\n\n\\bar{X}_1 and \\bar{X}_2 are the sample means, and n_1 and n_2 are the sample sizes. The degrees of freedom for the pooled variance t-test are calculated as n_1 + n_2 - 2. 5.\nH_0: There is no difference between the population means (\\mu_1 = \\mu_2).\nH_1: There is a difference between the population means (\\mu_1 \\neq \\mu_2).\nIf the variances are unequal, the Welch t-test (which does not assume equal variances) is recommended.\n\nExample 3: \nTo investigate the anti-atherosclerotic effects of tetrandrine on rabbits, an atherosclerosis rabbit model was established using a high-fat diet over 12 weeks. 16 rabbits successfully modeled after 12 weeks were randomly divided into two groups. The group A was given a high-fat diet of 100g/day, while the group B was given the same high-fat diet along with an additional 109mg/(kg·d) of tetrandrine mixed into the feed. After continuous feeding for 3 weeks, blood was drawn from the heart at the end of the experiment to measure the high-density lipoprotein (mmol/L) levels in the heart blood. The data can be downloaded below. Can it be concluded that the population means of high-density lipoprotein content in the heart blood differ between group A and B?\n\n\n  Download data \n\nR codes for two-sample t-test is:\n\nread_csv(\"datasets/ex07-03.csv\", show_col_types = F) |&gt; \n  mutate(grp = factor(grp, labels = c(\"A\", \"B\"))) |&gt; \n1  t.test(x ~ grp, data = _, var.equal = T)\n\n\n1\n\nUse the pipe operator to perform t-test with a group variable and a response variable.\n\n\n\n\n#&gt; \n#&gt;  Two Sample t-test\n#&gt; \n#&gt; data:  x by grp\n#&gt; t = 3.689, df = 14, p-value = 0.00243\n#&gt; alternative hypothesis: true difference in means between group A and group B is not equal to 0\n#&gt; 95 percent confidence interval:\n#&gt;  0.05494115 0.20755885\n#&gt; sample estimates:\n#&gt; mean in group A mean in group B \n#&gt;         0.77000         0.63875\n\n\nThe results show t = 3.689 , \\text{p-value} = 0.00243. At the \\alpha = 0.05 significance level, reject H_0 and accept H_1, indicating that the difference is statistically significant. It can be concluded that the population means of high-density lipoprotein content in the heart blood are different between group A and B. Considering the sample means in this case, it can be inferred that the population mean of high-density lipoprotein content in the heart blood is higher in group A than in the model group.\n\n\n7.4.2 Welch t-test for unequal population variances\nUnlike the combined variance t-test, the Welch t-test does not assume that the two populations have the same variance. This is particularly useful when you suspect or know that the variances are different, or when the sample sizes are quite different, which makes it more flexible than the combined variance t-test.\nThe t-statistic for the Welch t-test is calculated as:\n\nt = \\frac{\\bar{X}_1 - \\bar{X}_2}{\\sqrt{\\frac{S_1^2}{n_1} + \\frac{S_2^2}{n_2}}}\n\nHere, \\bar{X}_1 and \\bar{X}_2 are the sample means, S_1^2 and S_2^2 are the variances of the two samples, and n_1 and n_2 are the sample sizes.\nThe degrees of freedom for the Welch t-test are calculated using the Welch-Satterthwaite equation:\n\n\\nu = \\frac{\\left(\\frac{S_1^2}{n_1} + \\frac{S_2^2}{n_2}\\right)^2}{\\frac{\\left(\\frac{S_1^2}{n_1}\\right)^2}{n_1 - 1} + \\frac{\\left(\\frac{S_2^2}{n_2}\\right)^2}{n_2 - 1}}\n\nThis formula typically results in a non-integer value for the degrees of freedom, which is a characteristic of the Welch t-test. It is generally considered more robust than the combined variance t-test in situations where variances differ.\n\nExample 4: \nTo analyze the effect of blood glucose control on serum total cholesterol levels, a study was conducted on type 2 diabetes patients in a community. Hemoglobin A1c levels below 7.0% were used as the target for blood glucose control. Total cholesterol levels (mmol/L) were measured in 25 patients with poor blood glucose control (Group A) and 25 patients with good blood glucose control (Group B). The data can be downloaded below. Determine whether the mean total cholesterol levels of those with poor blood glucose control and those with good blood glucose control are equal or not.\n\n\n  Download data \n\nR codes for Welch t-test is:\n\nread_csv(\"datasets/ex07-04.csv\", show_col_types = F) |&gt; \n  t.test(x ~ grp, data = _, var.equal = F)    \n\n#&gt; \n#&gt;  Welch Two Sample t-test\n#&gt; \n#&gt; data:  x by grp\n#&gt; t = -2.8322, df = 36.672, p-value = 0.007465\n#&gt; alternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n#&gt; 95 percent confidence interval:\n#&gt;  -1.4177999 -0.2350001\n#&gt; sample estimates:\n#&gt; mean in group 1 mean in group 2 \n#&gt;          4.3176          5.1440\n\n\nThe results show t = -2.8322 , \\text{p-value} = 0.007465. At the \\alpha = 0.05 significance level, reject H_0 and accept H_1, indicating that the difference is statistically significant. Therefore, we can conclude that the mean total cholesterol levels between those with poor and good blood glucose control are different.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>t-test</span>"
    ]
  },
  {
    "objectID": "ttest.html#normality-test-and-f-test",
    "href": "ttest.html#normality-test-and-f-test",
    "title": "7  t-test",
    "section": "7.5 Normality test and F-test",
    "text": "7.5 Normality test and F-test\nWhen conducting a two-sample t-test, especially when comparing the means of two small samples, it is required that the corresponding populations follow a normal distribution and that the population variances are equal, known as homogeneity of variance. For paired t-tests, it is only necessary that the distribution of the differences between each pair of data points follows a normal distribution. Therefore, when performing a two-sample t-test with small samples, it is generally advisable to first conduct a homogeneity of variance test, particularly when there is a noticeable disparity between the sample variances. If the variances are homogeneous, a standard t-test is used; if not, an approximate t-test, such as Welch t-test, is applied. Additionally, it may be necessary to perform a normality test, though normality tests are more commonly used to establish medical reference ranges using normal distribution methods.\n\n7.5.1 Normality test\nNormality test has been discussed in Section 4.3 , for more details please move there.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>t-test</span>"
    ]
  },
  {
    "objectID": "ttest.html#sec-f-test",
    "href": "ttest.html#sec-f-test",
    "title": "7  t-test",
    "section": "7.6 F-test",
    "text": "7.6 F-test\nThe F-test was traditionally used to determine whether the variances of two populations are equal. However, this test assumes that the data follow a normal distribution, which is often not the case, especially when variances are unequal. Because of this limitation, the Levene’s test has become more popular in recent years. Levene’s test is more robust and does not rely on the specific distribution of the population. Levene’s test can be used to test the homogeneity of variances for both two populations and multiple populations. Here, only the F-test for comparing the variances of two samples is introduced.\nH_0: The variances of the two populations are equal (\\sigma_1^2 = \\sigma_2^2).\nH_1: The variances of the two populations are not equal (\\sigma_1^2 \\neq \\sigma_2^2).\nThe F-statistic is calculated as the ratio of the two sample variances:\n\nF = \\frac{S_1^2}{S_2^2}\n\nwhere S_1^2 and S_2^2 are the variances of the two samples. By convention, S_1^2 should be the larger variance, so the F-statistic is always greater than or equal to 1. The F-distribution depends on two parameters: the degrees of freedom for the numerator (\\nu_1 = n_1 - 1) and the denominator (\\nu_2 = n_2 - 1).\nIf the calculated F-statistic is is close to 1, the variances are similar. If it is much larger or smaller than 1, there may be a significant difference. If the p-value is less than the chosen significance level (e.g., 0.05), reject the null hypothesis, indicating that the variances are significantly different.\nThe F-test is sensitive to non-normality. If the data does not follow a normal distribution, the test may give misleading results. In such cases, non-parametric alternatives, such as the Levene’s test, may be more appropriate.\n\ntibble(\n  F_value   = seq(0, 5, length.out = 1000),\n  y1  = df(F_value, 8, 1),\n  y2  = df(F_value, 8, Inf)\n) |&gt; \n  pivot_longer(\n    cols = contains(\"y\"),\n    names_to = \"grp\",\n    values_to = \"Density\"\n  ) |&gt; \n  ggplot(aes(x = F_value, y = Density, linetype = grp)) + \n  geom_line(linewidth = 0.5) +\n  geom_text(\n    aes(x = 0.8, y = 0.3, hjust = 0,\n        label = paste(\"df1 =\", 8, \", df2 =\", 1))) +\n  geom_text(\n    aes(x = 1, y = 0.8, hjust = 0,\n    label = paste(\"df1 =\", 8, \", df2 =\", Inf))) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nExample 5: \nFor Example 3, use the F-test to determine whether the variances of high-density lipoprotein content in the heart blood of the two populations are unequal.\n\n\nread_csv(\"datasets/ex07-03.csv\", show_col_types = F) |&gt; \n  var.test(x ~ grp, data = _)\n\n#&gt; \n#&gt;  F test to compare two variances\n#&gt; \n#&gt; data:  x by grp\n#&gt; F = 2.299, num df = 7, denom df = 7, p-value = 0.2944\n#&gt; alternative hypothesis: true ratio of variances is not equal to 1\n#&gt; 95 percent confidence interval:\n#&gt;   0.4602708 11.4833515\n#&gt; sample estimates:\n#&gt; ratio of variances \n#&gt;           2.299011\n\n\nThe results show: F = 2.299, \\text{p-value} = 0.2944, at the significant level of \\alpha = 0.1, do not reject H_0. The difference is not statistically significant. Therefore, we cannot conclude that the variances of high-density lipoprotein content in the heart blood of the Qingfujian group and the model group are unequal. Consequently, Example 3 used a two-sample t-test under the assumption of equal variances.\n\nExample 6: \nFor Example 4, Use the F test to determine whether the population variances of serum total cholesterol levels differ between those with poor and good blood glucose control.\n\n\nread_csv(\"datasets/ex07-04.csv\", show_col_types = F) |&gt; \n  var.test(x ~ grp, data = _)\n\n#&gt; \n#&gt;  F test to compare two variances\n#&gt; \n#&gt; data:  x by grp\n#&gt; F = 3.5022, num df = 24, denom df = 24, p-value = 0.0032\n#&gt; alternative hypothesis: true ratio of variances is not equal to 1\n#&gt; 95 percent confidence interval:\n#&gt;  1.543310 7.947462\n#&gt; sample estimates:\n#&gt; ratio of variances \n#&gt;           3.502199\n\n\nThe results show: F = 3.5022, \\text{p-value} = 0.0032, at the significant level of \\alpha = 0.1, reject H_0, accept H_1. The difference is statistically significant. It can be considered that the population variances of total cholesterol levels differ between those with poor and good blood glucose control. Therefore, in Example 4, the two-sample t-test for unequal variances was used.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>t-test</span>"
    ]
  },
  {
    "objectID": "anova.html",
    "href": "anova.html",
    "title": "8  Analysis of Variance",
    "section": "",
    "text": "8.1 Prerequisite\nlibrary(tidyverse)\nlibrary(rmarkdown)\nlibrary(rstatix)\nlibrary(agricolae)\nlibrary(DescTools)",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "anova.html#the-basic-idea",
    "href": "anova.html#the-basic-idea",
    "title": "8  Analysis of Variance",
    "section": "8.2 The basic idea",
    "text": "8.2 The basic idea\nThe basic idea behind ANOVA is to analyze the variation within the data and partition it into two components: variation between groups and variation within groups. By comparing these two sources of variation, ANOVA can determine if the differences between group means are statistically significant.\n\n8.2.1 Total variance\nTotal variance in the context of ANOVA refers to the overall variability in the data. It represents the sum of the variability due to differences between group means (explained variance) and the variability within each group (unexplained variance or residual variance). The total variance can be decomposed as follows:\nThe total variance is calculated using the total sum of squares (SST), which quantifies the total variation in the data:\n\n\\text{SST} = \\sum_{i=1}^{N} (X_i - \\bar{X})^2\n\nwhere X_i is an individual observation, \\bar{X} is the overall mean of all observations, N is the total number of observations.\n\nHigh total variance: Indicates that the data points are widely spread out from the overall mean, suggesting high variability in the dataset.\nLow total variance: Indicates that the data points are closely clustered around the overall mean, suggesting low variability.\n\n\n\n8.2.2 Between-group variance\nBetween-group variance in ANOVA is a measure of how much the group means differ from the overall mean. It captures the variability that is due to the differences between the groups, rather than within them.\nThe between-group variance is calculated using the sum of squares between-groups (SSB), which is defined as:\n\n\\text{SSB} = \\sum_{j=1}^{k} n_j (\\bar{X}_j - \\bar{X})^2\n\nWhere k is the number of groups, n_j is the number of observations in group j , \\bar{X}_j is the mean of group j, \\bar{X} is the overall mean of all observations combined.\n\nHigh between-group variance: Indicates that the group means are significantly different from the overall mean, suggesting that the groups are different in a meaningful way.\nLow between-group variance: Indicates that the group means are similar to each other and to the overall mean, suggesting that the groups do not differ much.\n\n\n\n8.2.3 Within-group variance\nWithin-group variance in ANOVA measures the variability within each group. It captures the differences between individual data points and their respective group mean, indicating how spread out the data is within each group.\nThe within-group variance is calculated using the sum of squares within-groups (SSW), which is defined as:\n\n\\text{SSW} = \\sum_{j=1}^{k} \\sum_{i=1}^{n_j} (X_{ij} - \\bar{X}_j)^2\n\nWhere k is the number of groups, n_j is the number of observations in group j, X_{ij} is the i-th observation in group j, \\bar{X}_j is the mean of group j.\n\nHigh within-group variance: Indicates that there is a lot of variability within each group, meaning that individual observations differ significantly from their group mean.\nLow within-group variance: Indicates that individual observations within each group are close to their group mean, showing less variability.\n\nThe relationship between these components is given by:\n\n\\text{SST} = \\text{SSB} + \\text{SSW}\n\nIn ANOVA, the degrees of freedom associated with the within-group variance and between-group variance are essential for calculating the F-statistic. The degrees of freedom are calculated as:\n\n\\nu_{\\text{between}} = k - 1\n\n\n\\nu_{\\text{within}} = N - k\n\nWhere N is the total number of observations across all groups, k is the number of groups.\nFor example, suppose you have 3 groups with a total of 30 observations:\n\\nu_{\\text{between}} = k - 1 = 3 - 1 = 2, \\nu_{\\text{within}} = N - k = 30 - 3 = 27.\nIn the context of ANOVA, the mean square is a measure of variance and is used to compare variances between groups and within groups. It is calculated by dividing the sum of squares (SS) by the corresponding degrees of freedom.\nThe mean square between-groups (MSB) represents the average variance between the different group means. It is calculated as:\n\n\\text{MSB} = \\frac{\\text{SSB}}{\\nu_{\\text{between}}}\n\nThe mean square within-groups (MSW) represents the average variance within each group. It is calculated as:\n\n\\text{MSW} = \\frac{\\text{SSW}}{\\nu_{\\text{within}}}\n\n\n\n8.2.4 F-Statistic\nThe F-statistic is calculated by dividing the MSB by MSW:\n\nF = \\frac{\\text{MSB}}{\\text{MSW}}\n\nThe F follows an F-distribution with \\nu_{\\text{between}} and \\nu_{\\text{within}} degrees of freedom. It is used to assess the significance of the differences between group means. A larger F-value indicates that the between-group variance is relatively larger compared to the within-group variance, suggesting significant differences between the group means.\n\n\n8.2.5 Conditions for ANOVA\n\nNormality: The data within each group should be approximately normally distributed. If the data significantly deviate from normality, data transformation or non-parametric methods might be necessary.\nHomogeneity of variance: The variances across the groups should be equal, meaning the spread or variability within each group should be similar. This is a crucial assumption of ANOVA. If the variances are unequal, alternative methods like Welch’s ANOVA may be required.\nIndependence: The observations within each group and between groups should be independent of each other. This means that the measurement of one sample should not influence another.\nFixed factors: ANOVA typically assumes that the factors are fixed, meaning the levels of the factors are deliberately chosen and not randomly selected. If the factors are random, mixed-effects models or random-effects models should be considered.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "anova.html#anova-for-completely-randomized-design",
    "href": "anova.html#anova-for-completely-randomized-design",
    "title": "8  Analysis of Variance",
    "section": "8.3 ANOVA for completely randomized design",
    "text": "8.3 ANOVA for completely randomized design\nIn a completely randomized design, subjects are randomly assigned to different groups without any restrictions or blocks. Each patient has an equal chance of being assigned to any of the groups, ensuring that the groups are comparable and that the treatment effects can be attributed to the interventions rather than other factors.\n\nExample 1: \nTo clear the efficacy of three drugs, 120 patients were selected according to a inclusion criteria and evenly divided into three groups using a completely randomized design. How to use R to arrange random group assignments?\n\nIn R, you can use the sample() function to perform random group assignments and random drug allocation. Below is an example of how to achieve this:\n\nset.seed(100)\n\nn &lt;- 120\nk &lt;- 3\ngroup &lt;- sample(rep(1:k, each = n / k))\nblind_code &lt;- tibble(\n  id = 1:n,\n  group = factor(group, labels = c(\"A\", \"B\", \"C\"))\n) \n\npaged_table(blind_code)\n\n\n  \n\n\n\nIn the code above, set.seed(220) ensures that the randomization is reproducible. The sample() function shuffles the sequence generated by rep(1:k, each = n/k) to randomly assign patients to the three groups. The final result is stored in the blind_code data frame, which includes patient IDs and their assigned group. This way, each patient is not only assigned to a specific group.\nTo ensure that the randomization has been done correctly and that each group has the correct number of patients, you can check the distribution:\n\nblind_code |&gt; \n  group_by(group) |&gt; \n  summarise(n = n())\n\n#&gt; # A tibble: 3 × 2\n#&gt;   group     n\n#&gt;   &lt;fct&gt; &lt;int&gt;\n#&gt; 1 A        40\n#&gt; 2 B        40\n#&gt; 3 C        40\n\n\n\nExample 2: \nTo study the clinical efficacy of three lipid-lowering drugs, 120 patients were selected based on uniform inclusion criteria and randomly divided into three groups using a completely randomized design. Each group received one of the drugs in a double-blind trial. The effectiveness was evaluated based on the reduction in triglyceride levels before and after 6 weeks of treatment. To analyze whether there are any differences in the average reduction in triglyceride levels among the three groups.\n\n\n  Download data \n\nImport the data into R:\n\ndf &lt;- read_csv(\"datasets/ex08-01.csv\", show_col_types = F) |&gt; \n  mutate(group = factor(group, labels = c(\"A\", \"B\", \"C\")))\n\nBefore or after ANOVA, you need to check the assumptions of normality and homogeneity of variance. Here is a method of testing normality for multiple samples simultaneously.\n\nwith(df, tapply(diff_tri, INDEX = group, FUN = shapiro.test)) \n\n#&gt; $A\n#&gt; \n#&gt;  Shapiro-Wilk normality test\n#&gt; \n#&gt; data:  X[[i]]\n#&gt; W = 0.97245, p-value = 0.4289\n#&gt; \n#&gt; \n#&gt; $B\n#&gt; \n#&gt;  Shapiro-Wilk normality test\n#&gt; \n#&gt; data:  X[[i]]\n#&gt; W = 0.95872, p-value = 0.1514\n#&gt; \n#&gt; \n#&gt; $C\n#&gt; \n#&gt;  Shapiro-Wilk normality test\n#&gt; \n#&gt; data:  X[[i]]\n#&gt; W = 0.96547, p-value = 0.2564\n\n\nAn alternative method is written below:\n\ndf |&gt; \n  group_by(group) |&gt; \n  shapiro_test(diff_tri)\n\n#&gt; # A tibble: 3 × 4\n#&gt;   group variable statistic     p\n#&gt;   &lt;fct&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 A     diff_tri     0.972 0.429\n#&gt; 2 B     diff_tri     0.959 0.151\n#&gt; 3 C     diff_tri     0.965 0.256\n\n\nThe p-values above are all greater than 0.05, it says that the three populations meet the assumption of normal distribution.\nIn Section 7.6 we have discussed the test of homogeneity of variance using F-test. F-test is suitable for two samples, but here there are three samples. In Section 8.7 , we will talk about the comparison of multiple sample variances , and come back to this example again.\nHere we use the aov() function to perform a one-way ANOVA, where diff_tri is your dependent variable and group is the independent factor.\n\ndf |&gt; \n  aov(diff_tri ~ group, data = _) |&gt; \n  summary() \n\n#&gt;              Df Sum Sq Mean Sq F value   Pr(&gt;F)    \n#&gt; group         2  13.62   6.811   12.27 1.45e-05 ***\n#&gt; Residuals   117  64.92   0.555                     \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe results indicate: \\text{F}=12.3, p &lt; 0.01. At the significance level of \\alpha = 0.05 , reject H_0 and accept H_1 , indicating that the average reduction of triglyceride levels among patients taking different drugs is not all equal. This suggests that different drugs may have different effects on triglyceride reduction.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "anova.html#anova-for-randomized-block-design",
    "href": "anova.html#anova-for-randomized-block-design",
    "title": "8  Analysis of Variance",
    "section": "8.4 ANOVA for randomized block design",
    "text": "8.4 ANOVA for randomized block design\nA randomized block design is a type of experimental design in which subjects are divided into blocks based on certain characteristics (e.g., age, gender, baseline measurements) that are expected to influence the response to the treatments. Within each block, subjects are then randomly assigned to different treatment groups. This design helps to control for the variability within blocks, thereby increasing the precision of the experiment.\nRandomized block design is particularly useful when there are sources of variation that are not of primary interest but could affect the outcome, allowing the experimenter to control for these factors and focus on the treatment effects.\n\nExample 3: \nIn a study to compare the effectiveness of three different types of pain relief medications (A, B, and C) in postoperative patients, the patients’ baseline pain levels (low, medium, high) may influence their response to the medications. So you want to control for this variable in your study. Patients are first divided into blocks based on their baseline pain levels. Then, within each block, patients are randomly assigned to one of the three pain relief medications (A, B, or C).\n\nHere’s the R code to achieve the randomized block design:\n\nLet’s assume you have 36 patients, with 12 in each block (low, medium, high baseline pain levels).\n\n\nset.seed(100)  \n\nn &lt;- 36 # sample size\ntreat &lt;- c(\"drug A\", \"drug B\", \"drug C\")\nblock &lt;- c(\"low\", \"medium\", \"high\")\nk &lt;- length(treat)  # number of treatments\nb &lt;- length(block)  # number of blocks\n\nblind_code &lt;- data.frame(\n  id = 1:n,\n  bl_pain = rep(block, each = n / k)\n)\n\nblind_code |&gt; \n  paged_table()\n\n\n  \n\n\n\n\nNow, you can assign the patients within each block to one of the three medications (A, B, or C).\n\n\n# Randomly assign medications within each block\nblind_code  |&gt; \n  group_by(bl_pain) |&gt;   \n  mutate(\n    treatment = sample(rep(treat, each = n / (k * b)))\n  ) |&gt; \n  ungroup() |&gt; \n  paged_table()\n\n\n  \n\n\n\nThe output will show each patient’s ID, their baseline pain level, and the randomly assigned medication. The assignment will be random within each block, reflecting the randomized block design.\n\nExample 4: \nIn a trial to study the effects of three prenatal nutritional supplements on newborn weight, a randomized block design was used. Pregnant women with similar living locations, ages, and similar family economic status formed 10 blocks, each containing 3 pregnant women. Within each block, Pregnant women were randomly assigned to one of the three nutritional supplements (A, B, or C). Explore whether there is a difference in newborn birth weights among the three prenatal nutritional supplements?\n\n\n  Download data \n\n\nread_csv(\"datasets/ex08-02.csv\", show_col_types = F) |&gt; \n  mutate(\n    treat = factor(treat, labels = c(\"A\", \"B\", \"C\")),\n    block = factor(block)\n  ) |&gt; aov(weight ~ treat + block, data = _) |&gt; \n  summary()\n\n#&gt;             Df  Sum Sq Mean Sq F value  Pr(&gt;F)   \n#&gt; treat        2 1032327  516163   9.506 0.00152 **\n#&gt; block        9 1530670  170074   3.132 0.01877 * \n#&gt; Residuals   18  977340   54297                   \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "anova.html#anova-for-latin-square-design",
    "href": "anova.html#anova-for-latin-square-design",
    "title": "8  Analysis of Variance",
    "section": "8.5 ANOVA for latin square design",
    "text": "8.5 ANOVA for latin square design\nA Latin square design is used to control the effects of two extraneous variables. When there are three factors, each with the same number of levels, a Latin square design can systematically arrange experimental treatments to minimize confounding between the factors. This design is particularly useful in fields such as agriculture, education, or other research areas where it’s important to control multiple external variables.\n\n8.5.1 Key points\n\nStructure: A Latin Square Design is typically represented as an n × n square grid, where each row, column, and diagonal represent different external variables or factors. Each cell within the grid represents a combination of experimental conditions.\nTreatment allocation: Each treatment appears exactly once in each row and column of the grid, ensuring that all treatments are evenly distributed across the different levels of the two controlled variables.\nControlled variables: The design controls two primary extraneous variables (represented by rows and columns), thereby reducing their potential impact on the experimental outcomes. The third factor (usually the treatment) is randomly assigned to each cell in the grid.\n\n\n# Generate a 3x3 Latin Square Design\ntreatments &lt;- c(\"A\", \"B\", \"C\", \"D\", \"E\", \"F\")\nlatin_square &lt;- design.lsd(trt = treatments, seed = 42) \n\n# Display the Latin Square Design\nlatin_square$sketch \n\n#&gt;      [,1] [,2] [,3] [,4] [,5] [,6]\n#&gt; [1,] \"E\"  \"A\"  \"D\"  \"C\"  \"F\"  \"B\" \n#&gt; [2,] \"C\"  \"E\"  \"B\"  \"A\"  \"D\"  \"F\" \n#&gt; [3,] \"F\"  \"B\"  \"E\"  \"D\"  \"A\"  \"C\" \n#&gt; [4,] \"B\"  \"D\"  \"A\"  \"F\"  \"C\"  \"E\" \n#&gt; [5,] \"A\"  \"C\"  \"F\"  \"E\"  \"B\"  \"D\" \n#&gt; [6,] \"D\"  \"F\"  \"C\"  \"B\"  \"E\"  \"A\"\n\n\nThis code generates a 6x6 Latin dquare design where each treatment (A, B, C, D, E, F) appears exactly once in each row and column.\n\nExample 5: \nTo compare the size of skin blisters caused by six different drugs (A, B, C, D, E, F) after being injected into rabbits. A Latin square design was employed, using six rabbits, with each drug being injected into six different sites on each rabbit. The task is to perform an analysis of variance (ANOVA) to determine if there are any significant differences in the blister sizes caused by these drugs.\n\n\n  Download data \n\n\nread_csv(\"datasets/ex08-03.csv\", show_col_types = F) |&gt; \n  mutate(\n    rat_id = as.factor(rat_id),\n    part   = as.factor(part),\n    treat  = factor(treat, labels = c(\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"))\n  ) |&gt; \n  aov(herpes_size ~ treat + part + rat_id, data = _) |&gt; \n  summary()\n\n#&gt;             Df Sum Sq Mean Sq F value Pr(&gt;F)  \n#&gt; treat        5  667.1  133.43   3.906 0.0124 *\n#&gt; part         5   85.5   17.09   0.500 0.7723  \n#&gt; rat_id       5  250.5   50.09   1.466 0.2447  \n#&gt; Residuals   20  683.2   34.16                 \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "anova.html#cross-over-design",
    "href": "anova.html#cross-over-design",
    "title": "8  Analysis of Variance",
    "section": "8.6 Cross-over design",
    "text": "8.6 Cross-over design\nA cross-over design is commonly used in clinical trials where participants receive a sequence of different treatments. Each participant acts as their own control, meaning they receive all the treatments under study, but in a randomized order. This design is particularly useful for comparing the effects of treatments in situations where the response to a treatment is expected to return to baseline before the next treatment is administered.\n\n8.6.1 Key features\n\nParticipants as their own control: Since each participant receives multiple treatments, variability due to individual differences is minimized.\nRandomization: The order in which participants receive the treatments is randomized to avoid bias.\nWashout period: A period of time between treatments is often included to ensure that the effects of the previous treatment do not carry over into the next one.\nBalanced design: The design is usually balanced, meaning each treatment is administered the same number of times in each sequence.\n\nFor example, consider a clinical trial comparing the effects of two drugs, Drug A and Drug B, on blood pressure. In a cross-over design, each participant would receive both drugs in a randomized order. For example, some participants might receive Drug A first, followed by a washout period, and then receive Drug B. Others would receive Drug B first, followed by Drug A after the washout period. By comparing the effects of each drug within the same participants, the design controls for individual variability.\n\n# Number of subjects\nn &lt;- 10\n\n# Generate treatment sequences\nset.seed(200)  # For reproducibility\ntreat_seq &lt;- sample(rep(c(\"A\", \"B\"), each = 5))\n\n# Create a data frame for the study\ncross_over_design &lt;- tibble(\n  subject = 1:n,\n  phase1 = treat_seq,\n  phase2 = if_else(treat_seq == \"A\", \"B\", \"A\")\n)\n\ncross_over_design |&gt; \n  paged_table()\n\n\n  \n\n\n\nThis example generates a basic 2x2 cross-over design. For more complex designs, the package can accommodate more treatments and periods.\nCross-over designs are powerful tools in clinical research, especially when the treatment effects are short-lived and the sample size is limited.\n\nExample 6: \nA cross-over trial was conducted to measure 3H-cGMP levels in plasma using two scintillation liquids, A and B. In the first phase, samples from subjects 1, 3, 4, 7, and 9 were measured using liquid A, while samples from subjects 2, 5, 6, 8, and 10 were measured using liquid B. In the second phase, the measurement methods were switched, with subjects 1, 3, 4, 7, and 9 using liquid B and subjects 2, 5, 6, 8, and 10 using liquid A. Perform an analysis of variance (ANOVA) on the results of the cross-over trial.\n\n\n  Download data \n\n\nread_csv(\"datasets/ex08-04.csv\", show_col_types = F) |&gt; \n  mutate(\n    subject = as.factor(subject),\n    treat = factor(treat, labels = c(\"A\", \"B\")),\n    phase = factor(phase, labels = c(\"I\", \"II\")),\n    .keep = \"unused\"\n  ) |&gt; \n  aov(response ~ treat + phase + subject, data = _) |&gt; \n  summary()\n\n#&gt;             Df Sum Sq Mean Sq  F value   Pr(&gt;F)    \n#&gt; treat        1    198     198    4.019   0.0799 .  \n#&gt; phase        1    490     490    9.925   0.0136 *  \n#&gt; subject      9 551111   61235 1240.195 1.32e-11 ***\n#&gt; Residuals    8    395      49                      \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "anova.html#sec-multi-sample-comp",
    "href": "anova.html#sec-multi-sample-comp",
    "title": "8  Analysis of Variance",
    "section": "8.7 Comparison of multiple sample means",
    "text": "8.7 Comparison of multiple sample means\nAfter conducting an ANOVA and finding a significant difference between groups, you often need to pinpoint which groups differ. This is often done using post-hoc tests for multiple comparison. Multiple comparison tests control for type I error across the comparisons, meaning they adjust the significance level to prevent too many false positives.\nHere are some of the most common post-hoc tests:\n\nLeast Significant Difference (LSD) test: The LSD test is one of the oldest and simplest post-hoc tests, and it is essentially a series of t-tests between pairs of group means.\nTukey’s honest significant difference (HSD) test: Compares all possible pairs of means and is particularly effective when group sizes are equal.\nStudent-Newman-Keuls (SNK) test: It is derived from Tukey’s HSD, but is less conservative (finds more differences). Tukey controls the error for all comparisons, where SNK only controls for comparisons under consideration. This makes it more powerful but also more prone to type I error.\nDunnett’s test: Compares each treatment group to a control group, useful when you have a control group and several treatments.\nBonferroni correction: Adjusts the significance level for the number of comparisons being made. It’s conservative and controls for type I error. Useful when you have a small number of comparisons.\nScheffé’s Method: Allows for all possible contrasts, not just pairwise comparisons, making it a flexible but conservative choice.\n\n\n8.7.1 When to use which\nChoosing the right post-hoc test after an ANOVA depends on several factors, including your study design, the number of groups, your tolerance for type I error (false positive), and the nature of your data. Here’s a guide to help you decide when to select which post-hoc test:\n\nUse Tukey’s HSD if you need a balanced approach between controlling type I error and maintaining power, especially for all-pairwise comparisons.\nUse SNK if you want more power and are okay with slightly higher type I error risk.\nUse LSD if your primary goal is to detect differences, particularly in an exploratory setting, and you are less concerned about type I error.\nUse Bonferroni if you have a small number of comparisons and need strong control over type I error.\nUse Dunnett’s if your comparisons are focused on multiple treatments against a single control group.\n\n\nExample 7: \nIn Example 1, after ANOVA rejected the null hypothesis, we still want to know whihc pairs of drugs differ in the mean reduction of triglycerides?\n\nThe PostHocTest() function in DescTools package is a convenience wrapper for computing post-hoc test after having calculated an ANOVA.\n\ndf &lt;- read_csv(\"datasets/ex08-01.csv\", show_col_types = F) |&gt; \n  mutate(group = factor(group, labels = c(\"A\", \"B\", \"C\")))\n\naov_model &lt;- aov(diff_tri ~ group, data = df)\naov_model |&gt; summary()\n\n#&gt;              Df Sum Sq Mean Sq F value   Pr(&gt;F)    \n#&gt; group         2  13.62   6.811   12.27 1.45e-05 ***\n#&gt; Residuals   117  64.92   0.555                     \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\np&lt;0.05 indicates the difference between groups is significant. The multiple comparison between groups is conducted using different methods.\n\nc(\"hsd\", \"bonferroni\", \"lsd\", \"scheffe\", \"newmankeuls\") |&gt; \n  map(~ PostHocTest(aov_model, method = .)) \n\n#&gt; [[1]]\n#&gt; \n#&gt;   Posthoc multiple comparisons of means : Tukey HSD \n#&gt;     95% family-wise confidence level\n#&gt; \n#&gt; $group\n#&gt;         diff     lwr.ci     upr.ci    pval    \n#&gt; B-A -0.09200 -0.4874133  0.3034133 0.84549    \n#&gt; C-A -0.75625 -1.1516633 -0.3608367 4.1e-05 ***\n#&gt; C-B -0.66425 -1.0596633 -0.2688367 0.00034 ***\n#&gt; \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; \n#&gt; [[2]]\n#&gt; \n#&gt;   Posthoc multiple comparisons of means : Bonferroni \n#&gt;     95% family-wise confidence level\n#&gt; \n#&gt; $group\n#&gt;         diff     lwr.ci     upr.ci    pval    \n#&gt; B-A -0.09200 -0.4965707  0.3125707 1.00000    \n#&gt; C-A -0.75625 -1.1608207 -0.3516793 4.1e-05 ***\n#&gt; C-B -0.66425 -1.0688207 -0.2596793 0.00035 ***\n#&gt; \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; \n#&gt; [[3]]\n#&gt; \n#&gt;   Posthoc multiple comparisons of means : Fisher LSD \n#&gt;     95% family-wise confidence level\n#&gt; \n#&gt; $group\n#&gt;         diff     lwr.ci     upr.ci    pval    \n#&gt; B-A -0.09200 -0.4218756  0.2378756 0.58177    \n#&gt; C-A -0.75625 -1.0861256 -0.4263744 1.4e-05 ***\n#&gt; C-B -0.66425 -0.9941256 -0.3343744 0.00012 ***\n#&gt; \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; \n#&gt; [[4]]\n#&gt; \n#&gt;   Posthoc multiple comparisons of means: Scheffe Test \n#&gt;     95% family-wise confidence level\n#&gt; \n#&gt; $group\n#&gt;         diff     lwr.ci     upr.ci    pval    \n#&gt; B-A -0.09200 -0.5049876  0.3209876 0.85870    \n#&gt; C-A -0.75625 -1.1692376 -0.3432624 7.5e-05 ***\n#&gt; C-B -0.66425 -1.0772376 -0.2512624 0.00058 ***\n#&gt; \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; \n#&gt; [[5]]\n#&gt; \n#&gt;   Posthoc multiple comparisons of means : Newman-Keuls \n#&gt;     95% family-wise confidence level\n#&gt; \n#&gt; $group\n#&gt;         diff     lwr.ci     upr.ci    pval    \n#&gt; B-A -0.09200 -0.4218756  0.2378756 0.58177    \n#&gt; C-A -0.75625 -1.1516633 -0.3608367 4.1e-05 ***\n#&gt; C-B -0.66425 -0.9941256 -0.3343744 0.00012 ***\n#&gt; \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe results of different post-hoc tests are consistent. There is no difference between drug A and drug B, only the overall mean of glycerol reduction between drug C and drug A, drug C and drug B have statistical significance, and the glycerol reduction of drug C is less.\nHere is the Dunnett’s test, which can assign the control group.\n\nDunnettTest(diff_tri ~ group, data = df, control = \"B\")\n\n#&gt; \n#&gt;   Dunnett's test for comparing several treatments with a control :  \n#&gt;     95% family-wise confidence level\n#&gt; \n#&gt; $B\n#&gt;         diff    lwr.ci    upr.ci    pval    \n#&gt; A-B  0.09200 -0.280971  0.464971 0.80343    \n#&gt; C-B -0.66425 -1.037221 -0.291279 0.00023 ***\n#&gt; \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "anova.html#comparison-of-multiple-sample-variances",
    "href": "anova.html#comparison-of-multiple-sample-variances",
    "title": "8  Analysis of Variance",
    "section": "8.8 Comparison of multiple sample variances",
    "text": "8.8 Comparison of multiple sample variances\nIn statistical analysis, comparing multiple sample variances is important to determine whether the variances across different groups are equal. This is often referred to as testing for homogeneity of variances. There are several common statistical tests for this purpose:\n\nExample 8: \nFor data in Example 1, whether the reduction of triglyceride levels in three groups satisfies the assumption of homogeneity of variances?\n\n\n8.8.1 Bartlett’s Test\nBartlett’s test checks whether multiple sample variances are equal under the assumption that the data follow a normal distribution. It is sensitive to departures from normality, so it may not be appropriate for non-normal data.\nHere is the R code for Bartlett’s test:\n\nbartlett.test(diff_tri ~ group, data = df) \n\n#&gt; \n#&gt;  Bartlett test of homogeneity of variances\n#&gt; \n#&gt; data:  diff_tri by group\n#&gt; Bartlett's K-squared = 2.2184, df = 2, p-value = 0.3298\n\n\n\n\n8.8.2 Levene’s Test\nLevene’s test is a more robust alternative to Bartlett’s test. It tests for the equality of variances and is less sensitive to deviations from normality, making it suitable for non-normally distributed data.\nHere is the R code for this Levene’s test:\n\nLeveneTest(diff_tri ~ group, data = df)\n\n#&gt; Levene's Test for Homogeneity of Variance (center = median)\n#&gt;        Df F value Pr(&gt;F)\n#&gt; group   2  1.8226 0.1662\n#&gt;       117\n\n\n\n\n8.8.3 Fligner-Killeen Test\nThe Fligner-Killeen test is a non-parametric test that is robust against non-normal data. It uses ranks of the data to test for equality of variances across groups.\nHere is the R code for Fligner-Killeen test:\n\nfligner.test(diff_tri ~ group, data = df)\n\n#&gt; \n#&gt;  Fligner-Killeen test of homogeneity of variances\n#&gt; \n#&gt; data:  diff_tri by group\n#&gt; Fligner-Killeen:med chi-squared = 3.8501, df = 2, p-value = 0.1459",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "count-data-stat-desc.html",
    "href": "count-data-stat-desc.html",
    "title": "9  Statistical description of count data",
    "section": "",
    "text": "9.1 Prerequisite\nlibrary(tidyverse)\nAbsolute values and relative values are fundamental concepts in statistics, used to describe raw data and the relationships between data points, respectively.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Statistical description of count data</span>"
    ]
  },
  {
    "objectID": "count-data-stat-desc.html#absolute-value",
    "href": "count-data-stat-desc.html#absolute-value",
    "title": "9  Statistical description of count data",
    "section": "9.2 Absolute value",
    "text": "9.2 Absolute value\nAn absolute value refers to the raw, observed data that quantifies an event or phenomenon directly, it focus on the actual quantity of an event or measurement. For example:\n\nA hospital received 500 patients in one year.\nIn a clinical trial, there were 20 occurrences of heart attacks.\n\n\n9.2.1 Characteristics\n\nAbsolute values are often expressed with specific units, such as people, events, kilograms, etc.\nAbsolute values represent the measured quantity without any relation to other data points, no comparison involved.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Statistical description of count data</span>"
    ]
  },
  {
    "objectID": "count-data-stat-desc.html#relative-value",
    "href": "count-data-stat-desc.html#relative-value",
    "title": "9  Statistical description of count data",
    "section": "9.3 Relative Value",
    "text": "9.3 Relative Value\nA relative value expresses the relationship or proportion between two or more data points, often in the form of percentages, ratios, or multiples. For example:\n\nThe hospital treated 5% of the total population of the region in one year.\nThe relative risk of heart attacks in the drug group is 1.5 times that of the control group.\n\n\n9.3.1 Common forms\n\nPercentage: Expressed as a proportion of a part to the whole. For instance, 20% of patients in a clinical trial experienced side effects.\n\n\n\\text{Percentage} = \\left(\\frac{\\text{part}}{\\text{whole}}\\right) \\times 100%\n\n\nRate: This expresses the frequency of an event occurring in relation to a specific quantity of time, population, or other conditions. It is commonly used to measure the occurrence of phenomena over time or within a particular population in fields like epidemiology, demography, and clinical research.\n\nIncidence rate: The number of new cases of a disease occurring in a population during a specific time period, typically expressed as per 1,000 or 100,000 people.\n\n\n\\text{Incidence rate} = \\frac{\\text{Number of new cases}}{\\text{Total population}} \\times 1,000 \\text{ (or 100,000)}\n\n\nMortality Rate: The number of deaths in a given population over a specific time, often per 1,000 individuals.\n\n\n\\text{Mortality rate} = \\frac{\\text{Number of deaths}}{\\text{Total population}} \\times 1,000\n\n\nBirth Rate: The number of live births per 1,000 people in a population in a given time period.\n\n\n\\text{Birth rate} = \\frac{\\text{Number of live births}}{\\text{Total population}} \\times 1,000\n\n\nSurvival Rate: The proportion of people who survive a medical condition or treatment for a specific period of time.\n\nFor example, if a study finds that 50 new cases of diabetes occur in a population of 10,000 people during one year, the incidence rate would be:\n\\frac{50}{10,000} \\times 1,000 = 5 \\text{ new cases per 1,000 people per year}\nRatio: A comparison of two quantities, representing the relative frequency or strength of two phenomena. For example, the ratio of male to female visits to a clinic is 3:2.\n\n\n\\text{Ratio} = \\frac{\\text{value A}}{\\text{value B}}\n\n\nMultiple: Expresses how many times one quantity is greater or smaller than another. For example, the effectiveness of a new drug might be twice that of the standard treatment.\n\n\n\\text{Multiple} = \\frac{\\text{effectiveness of new drug}}{\\text{effectiveness of standard treatment}}\n\nRelative values are more useful for comparisons, such as comparing disease incidence rates, relative risks of a condition between different groups, or the effectiveness of different treatments. For example:\n\nAbsolute value: In a study, 150 patients received a specific treatment.\nRelative value: The treatment group had an 80% success rate, while the control group had a 50% success rate, giving a relative risk of 1.6.\n\n\n\n9.3.2 Rate and proportion\nRate includes a time element (or a denominator such as population size), indicating how frequently an event happens over time. Proportion is simply a part-to-whole comparison and does not necessarily involve time.\nRelative values compare events or measurements, highlighting the proportional or relative differences between groups or phenomena.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Statistical description of count data</span>"
    ]
  },
  {
    "objectID": "discrete-data-dist.html",
    "href": "discrete-data-dist.html",
    "title": "10  Distributions of discrete variables",
    "section": "",
    "text": "10.1 Binomial distribution\nThe binomial distribution is a discrete probability distribution that describes the number of successes in a fixed number of independent trials, where each trial has only two possible outcomes (often referred to as success or failure), and the probability of success remains constant in each trial.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Distributions of discrete variables</span>"
    ]
  },
  {
    "objectID": "discrete-data-dist.html#binomial-distribution",
    "href": "discrete-data-dist.html#binomial-distribution",
    "title": "10  Distributions of discrete variables",
    "section": "",
    "text": "10.1.1 Key Characteristics\n\nNumber of trials (n): The distribution considers a fixed number of trials, denoted by n.\nProbability of success (p): Each trial has the same probability of success, denoted by p. The probability of failure is 1 - p.\nIndependent trials: The outcome of one trial does not affect the outcome of any other trial.\n\n\n\n10.1.2 Probability mass function (PMF)\nThe probability of getting exactly k successes in n trials is given by the binomial probability formula:\n\nP(X = k) = \\binom{n}{k} p^k (1-p)^{n-k}\n\nWhere X is the random variable representing the number of successes, \\binom{n}{k} is the binomial coefficient, calculated as \\frac{n!}{k!(n-k)!}, p is the probability of success, k is the number of successes, where k = 0, 1, 2, \\ldots, n .\n\n\n10.1.3 Mean and standard deviation\nIn a binomial distribution, the mean (or expected value) and standard deviation of the number of success X is given by: mean (expected value) \\mu = np; standard deviation \\sigma = \\sqrt{n \\cdot p \\cdot (1 - p)}\nwhere n is the number of trials, p is the probability of success in each trial, 1 - p is the probability of failure in each trial.\nIn R, you can calculate the mean and standard deviation using the following code:\n\nn &lt;- 10  # Number of trials\np &lt;- 0.7  # Probability of success\n\n# Calculate mean\nmean &lt;- n * p\nprint(paste(\"Mean:\", mean))\n\n#&gt; [1] \"Mean: 7\"\n\n# Calculate standard deviation\nstd_dev &lt;- sqrt(n * p * (1 - p))\nprint(paste(\"Standard deviation:\", std_dev))\n\n#&gt; [1] \"Standard deviation: 1.44913767461894\"\n\n\n\nExample 1: \nSuppose you are conducting a clinical trial for a new medication. Each patient has a 70% chance of responding positively to the treatment. If you treat 10 patients, the binomial distribution can be used to model the number of positive responses.\n\nHere n is 10 patients, p is 0.70. You can calculate the probability of exactly 7 patients responding positively:\n\nP(X = 7) = \\binom{10}{7} (0.7)^7 (0.3)^3 = 0.2668\n\nSo, there’s a 26.68% chance that exactly 7 patients will respond positively. You can calculate binomial probabilities using the dbinom() function:\n\n# Probability of exactly 7 successes in 10 trials with p = 0.7\ndbinom(7, size = 10, prob = 0.7)\n\n#&gt; [1] 0.2668279\n\n\nFor the binomial distribution, the mean and standard deviation of the proportion of successes \\hat{p} are as follows: mean of \\hat{p} is \\mu_{\\hat{p}} = p, standard deviation of \\hat{p} (also called standard error) is:\n\\sigma_{\\hat{p}} = \\sqrt{\\frac{p(1 - p)}{n}}\nThe mean of the proportion of successes is the true success probability p. This standard deviation decreases as the number of trials n increases, which means that the proportion of successes becomes more stable with a larger sample size.\n\n\n10.1.4 Visualization\nYou can also plot the binomial distribution. The plot typically uses a bar chart to show the probability for each possible number of successes.\n\n# Define parameters\nn &lt;- 10  # Number of trials\np &lt;- 0.7  # Probability of success\n\n# Generate data\nx &lt;- 0:n\nprobabilities &lt;- dbinom(x, size = n, prob = p)\ndata &lt;- tibble(successes = x, probability = probabilities)\n\n# Create the plot\nggplot(data, aes(x = successes, y = probability)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\", color = \"black\") +\n  labs(x = \"Number of successes\", y = \"Probability\")\n\n\n\n\n\n\n\n\nWith n = 10 and p = 0.7, the generated plot will show the probability distribution for achieving different numbers of successes in 10 trials. When p = 0.7, the distribution typically peaks around 7 successes, meaning the most likely number of successes is near 9.\nThe shape of the distribution depends on the number of trials n and the probability of success p. When p = 0.5, the distribution is symmetric; when p is far from 0.5, the distribution becomes skewed. When n \\to \\infty , as long as p is not too close to 0 or 1, the binomial distribution approximates a normal distribution.\n\nn1 &lt;- 5  \nn2 &lt;- 10\np &lt;- 0.5\n\n# Generate data\nx1 &lt;- 0:n1\nx2 &lt;- 0:n2\nprob1 &lt;- dbinom(x1, size = n1, prob = p)\nprob2 &lt;- dbinom(x2, size = n2, prob = p)\ndf1 &lt;- tibble(successes = x1, probability = prob1)\ndf2 &lt;- tibble(successes = x2, probability = prob2)\n\n# Create the plot\nggplot(df1, aes(x = successes, y = probability)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\", color = \"black\") +\n  labs(x = \"Number of successes\", y = \"Probability\") +\nggplot(df2, aes(x = successes, y = probability)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\", color = \"black\") +\n  labs(x = \"Number of successes\", y = \"Probability\")\n\n\n\n\n\n\n\n\n\nn1 &lt;- 10  \nn2 &lt;- 50\np &lt;- 0.3\n\n# Generate data\nx1 &lt;- 0:n1\nx2 &lt;- 0:n2\nprob1 &lt;- dbinom(x1, size = n1, prob = p)\nprob2 &lt;- dbinom(x2, size = n2, prob = p)\ndf1 &lt;- tibble(successes = x1, probability = prob1)\ndf2 &lt;- tibble(successes = x2, probability = prob2)\n\n# Create the plot\nggplot(df1, aes(x = successes, y = probability)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\", color = \"black\") +\n  labs(x = \"Number of successes\", y = \"Probability\") +\nggplot(df2, aes(x = successes, y = probability)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\", color = \"black\") +\n  labs(x = \"Number of successes\", y = \"Probability\")\n\n\n\n\n\n\n\n\n\n\n10.1.5 Applications\nThe binomial distribution is widely applied in scenarios where there are only two possible outcomes (often referred to as “success” and “failure”) in repeated trials or experiments. Each trial is independent, and the probability of success remains constant across trials. Below are some key applications of the binomial distribution:\n\nConfidence interval (CI) estimation\nThe CI for a population proportion (or population rate) is a range of values that is likely to contain the true proportion p of a population, which can be estimated based on a sample proportion \\hat{p} = \\frac{X}{n}, where X is the number of successes and n is the sample size.\nThe Wald method is the traditional approach to estimate the confidence interval for a population proportion. The formula is:\n\\hat{p} \\pm Z_{\\alpha/2} \\times \\sqrt{\\frac{\\hat{p}(1 - \\hat{p})}{n}}\nWhere \\hat{p} is the sample proportion (i.e., the number of successes divided by the total sample size), Z_{\\alpha/2} is the critical value from the standard normal distribution for a given confidence level (e.g., 1.96 for 95% confidence), n is the sample size, \\sqrt{\\frac{\\hat{p}(1 - \\hat{p})}{n}} is the standard error of the proportion.\n\nExample 2: \nSuppose you take a sample of 200 people, and 50 people have a positive result. To calculate a 95% confidence interval for the population rate of positive.\n\nIf you want to manually calculate the Wald CI, you can write your own R code, for example:\n\n# Example: 50 successes in 200 trials\nx &lt;- 50  # Number of successes\nn &lt;- 200  # Total number of trials\np_hat &lt;- x / n  # Sample proportion\n\n# Z-value for 95% confidence\nz &lt;- 1.96\n\n# Standard error\nse &lt;- sqrt(p_hat * (1 - p_hat) / n)\n\n# Confidence interval\nlower &lt;- p_hat - z * se\nupper &lt;- p_hat + z * se\n\n# Output the result\ncat(\"95% CI: [\", lower, \",\", upper, \"]\\n\")\n\n#&gt; 95% CI: [ 0.1899875 , 0.3100125 ]\n\n\nThe binconf() function from the Hmisc package allows you to compute confidence intervals for proportions using several methods, including Wald, Wilson, and Agresti-Coull.\n\nbinconf(x = 50, n = 200, alpha = 0.05, method = \"asymptotic\")  \n\n#&gt;  PointEst     Lower     Upper\n#&gt;      0.25 0.1899886 0.3100114\n\n\nFor small samples or when \\hat{p} is close to 0 or 1, the Wald method might not perform well. Other methods, such as the Wilson score interval, Agresti-Coull interval, or Bayesian intervals, are often preferred in these cases.\nWilson Score Interval:\n\n\n\\frac{\\hat{p} + \\frac{Z^2}{2n}}{1 + \\frac{Z^2}{n}} \\pm \\frac{Z}{1 + \\frac{Z^2}{n}} \\sqrt{\\frac{\\hat{p}(1 - \\hat{p})}{n} + \\frac{Z^2}{4n^2}}\n\n\nOne-sample proportion test\nThis test determines whether the sample proportion is significantly different from the known population proportion. The test statistic for this comparison is based on the standard error of the sample proportion, which is given by:\nSE = \\sqrt{\\frac{p_0(1 - p_0)}{n}}where p_0 is the known population proportion, and n is the sample size.\nThe test statistic Z is calculated as:\n\nZ = \\frac{\\hat{p} - p_0}{SE}\n\n\nExample 3: \nSuppose each patient has a 70% chance of responding to a treatment. In a clinical trial, 13 out of a set of 20 patients taking this treatment have positive responses. Determine if there is a significant difference between the sample and population proportion.\n\nYou can perform the one-sample proportion test using the prop.test() function or binom.test() function in R:\n\n# Example: 13 successes in 20 trials, comparing to a known population proportion of 0.370\nx &lt;- 13  # Number of successes\nn &lt;- 20  # Sample size\np0 &lt;- 0.70  # Known population proportion\n\n# Perform one-sample proportion test\nprop.test(x = x, n = n, p = p0)\n\n#&gt; \n#&gt;  1-sample proportions test with continuity correction\n#&gt; \n#&gt; data:  x out of n, null probability p0\n#&gt; X-squared = 0.059524, df = 1, p-value = 0.8073\n#&gt; alternative hypothesis: true p is not equal to 0.7\n#&gt; 95 percent confidence interval:\n#&gt;  0.4094896 0.8369133\n#&gt; sample estimates:\n#&gt;    p \n#&gt; 0.65\n\n\n\nbinom.test(x = x, n = n, p = p0)\n\n#&gt; \n#&gt;  Exact binomial test\n#&gt; \n#&gt; data:  x and n\n#&gt; number of successes = 13, number of trials = 20, p-value = 0.6295\n#&gt; alternative hypothesis: true probability of success is not equal to 0.7\n#&gt; 95 percent confidence interval:\n#&gt;  0.4078115 0.8460908\n#&gt; sample estimates:\n#&gt; probability of success \n#&gt;                   0.65\n\n\nTwo-sample proportion test\nThis test determines whether there is a significant difference between the proportions of two independent samples. The test statistic is based on the difference between the sample proportions of the two groups:\n\nZ = \\frac{\\hat{p}_1 - \\hat{p}_2}{SE}\n\nWhere \\hat{p}_1 and \\hat{p}_2 are the sample proportions from the two groups, and SE is the standard error of the difference in proportions, calculated as:\n\nSE = \\sqrt{\\hat{p}(1 - \\hat{p}) \\left( \\frac{1}{n_1} + \\frac{1}{n_2} \\right)}\n\nHere, \\hat{p} is the pooled sample proportion:\n\n\\hat{p} = \\frac{X_1 + X_2}{n_1 + n_2}\n\nwhere X_1 and X_2 are the number of successes in the two groups, and n_1 and n_2 are the sample sizes of the two groups.\n\nExample 4: \nSuppose you have two independent groups: Group A 30 successes out of 150 trials; Group B 45 successes out of 200 trials. Perform the two-sample proportion test to check if the proportions in these two groups differ significantly.\n\n\nx &lt;- c(30, 45)  # Number of successes in each group\nn &lt;- c(150, 200)  # Sample sizes of each group\n\n# Perform two-sample proportion test\nprop.test(x = x, n = n)\n\n#&gt; \n#&gt;  2-sample test for equality of proportions with continuity correction\n#&gt; \n#&gt; data:  x out of n\n#&gt; X-squared = 0.18702, df = 1, p-value = 0.6654\n#&gt; alternative hypothesis: two.sided\n#&gt; 95 percent confidence interval:\n#&gt;  -0.11712834  0.06712834\n#&gt; sample estimates:\n#&gt; prop 1 prop 2 \n#&gt;  0.200  0.225",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Distributions of discrete variables</span>"
    ]
  },
  {
    "objectID": "discrete-data-dist.html#poisson-distribution",
    "href": "discrete-data-dist.html#poisson-distribution",
    "title": "10  Distributions of discrete variables",
    "section": "10.2 Poisson distribution",
    "text": "10.2 Poisson distribution\nThe Poisson distribution is a discrete probability distribution that expresses the probability of a given number of events occurring in a fixed interval of time or space, given that these events occur with a known constant rate and independently of the time since the last event. For examples, the number of phone calls received by a call center in an hour, the number of hospital admissions per day.\n\n10.2.1 Key characteristics\n\nEvents are independent: The occurrence of one event does not affect the probability of another event.\nThe rate is constant: The average number of events in a fixed interval is constant.\nRare events: The events are typically rare relative to the observation period or space.\n\n\n\n10.2.2 Probability mass function (PMF)\nThe probability of observing k events in a given interval is:\n\nP(X = k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}\n\nWhere X is the random variable representing the number of events, k is the number of events (a non-negative integer), \\lambda is the average number of events (rate parameter), e is the base of the natural logarithm.\n\n\n10.2.3 Mean and standard deviation\nIn a Poisson distribution, mean \\mu = \\lambda, variance \\sigma = \\sqrt\\lambda. This means that the mean and variance of the Poisson distribution are both equal to the rate parameter \\lambda.\n\nExample 5: \nIf the average number of patients arriving at a hospital per hour is 50, you could use a Poisson distribution with \\lambda = 50 to model the number of patients expected to arrive in any given hour.\n\nYou can use the rpois() function in R to generate random values from a Poisson distribution.\n\nset.seed(200)\n# Simulate the number of patients arriving in 8 hours from a Poisson distribution with a rate of 5\nlambda &lt;- 50\nk &lt;- 8\nrpois(n = k, lambda = lambda)\n\n#&gt; [1] 50 51 53 53 50 49 42 46\n\n\nYou can use the dpois() function to calculate the probability of a specific number of events.\n\n# Probability of observing exactly 5 events when lambda is 5\ndpois(x = k, lambda = lambda)\n\n#&gt; [1] 1.868596e-13\n\n\n\n\n10.2.4 Visualization\nYou can plot the Poisson distribution. Here’s an example where we plot the probability mass function (PMF) of the Poisson distribution for different values of k (the number of events), given a specific rate \\lambda.\n\nlambda &lt;- 5\nk_values &lt;- 0:5  # Values from 0 to 10 events\nprobabilities &lt;- dpois(k_values, lambda = lambda)\npoisson_data &lt;- tibble(k_values, probabilities)\n\nggplot(poisson_data, aes(x = k_values, y = probabilities)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\", color = \"black\") +\n  labs(x = \"Number of events (k)\", y = \"Probability\")\n\n\n\n\n\n\n\n\nThe resulting plot will show the probability mass function for the Poisson distribution with \\lambda = 5 . It will depict how likely different counts of events (0, 1, 2, 3, etc.) are.\nThe shape of the Poisson distribution depends on the parameter \\lambda, which represents the average number of events in a given time or space interval. As \\lambda changes, the shape of the distribution also changes in the following ways:\n\nFor very small values of \\lambda (e.g., close to 0), the distribution is skewed to the right. Most of the probability mass is concentrated around 0, with a sharp drop-off for higher values of k. The distribution is unimodal with its peak near k = 0 .2.\nAs \\lambda increases, the distribution becomes more symmetric. The peak of the distribution shifts to the right, centering around \\lambda, since the expected number of events increases. The skewness reduces as \\lambda increases.\nWhen \\lambda becomes large (e.g., \\lambda &gt; 10), the distribution approximates a normal distribution. The distribution becomes nearly symmetric and bell-shaped, with the mean and variance both close to \\lambda.\n\n\nlambda1 &lt;- 0.5\nlambda2 &lt;- 5\nk_values &lt;- 0:10  # Values from 0 to 10 events\nprob1 &lt;- dpois(k_values, lambda = lambda1)\nprob2 &lt;- dpois(k_values, lambda = lambda2)\ndf1 &lt;- tibble(k_values, probabilities = prob1)\ndf2 &lt;- tibble(k_values, probabilities = prob2)\n\nggplot(df1, aes(x = k_values, y = probabilities)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\", color = \"black\") +\n  labs(x = \"Number of events (k)\", y = \"Probability\") +\nggplot(df2, aes(x = k_values, y = probabilities)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\", color = \"black\") +\n  labs(x = \"Number of events (k)\", y = \"Probability\")\n\n\n\n\n\n\n\n\n\n\n10.2.5 Applications\nThe poisson.test() function is used to perform an exact test for the rate parameter \\lambda of a Poisson distribution. This function helps compare the observed count data (events) to an expected rate or two Poisson-distributed samples to check whether the observed events match a hypothesized or known rate.\n\nOne-sample comparison\nTo test whether the observed number of events follows a hypothesized Poisson rate \\lambda.\n\nExample 6: \nAn emergency room wants to determine if the average number of patients arriving per hour differs from a known average rate. Suppose the historical data shows an average of 8 patients arriving per hour. They observed 17 patients arriving in 2 hours.Test if the observed rates differ significantly from the historical average of 8 patients per hour.\n\n\n# Observed data\nx &lt;- 17  # Number of patients in the second sample\nt &lt;- 2   # Time period in hours\n# Hypothesized rate (historical average)\nlambda_0 &lt;- 8\n\npoisson.test(x, T = t, r = lambda_0) \n\n#&gt; \n#&gt;  Exact Poisson test\n#&gt; \n#&gt; data:  x time base: t\n#&gt; number of events = 17, time base = 2, p-value = 0.8016\n#&gt; alternative hypothesis: true event rate is not equal to 8\n#&gt; 95 percent confidence interval:\n#&gt;   4.951563 13.609323\n#&gt; sample estimates:\n#&gt; event rate \n#&gt;        8.5\n\n\nTwo-sample comparison\n\nTo test whether the rates from two Poisson-distributed samples are equal.\n\nExample 7: \nA researcher is studying the incidence of a rare disease in two different regions to see if the rate of new cases per year is the same. The researcher wants to compare the rates of the disease in two regions. 15 new cases of the disease were observed in 2 years in region A. 4 new cases of the disease were observed in 2 years in region B. Test if the disease incidence rates in the two regions are significantly different.\n\n\n# Observed data\nx1 &lt;- 15  # New cases in region A\nT1 &lt;- 2  # Number of years in region A\n\nx2 &lt;- 4  # New cases in region B\nT2 &lt;- 2  # Number of years in region B\n\npoisson.test(c(x1, x2), T = c(T1, T2))\n\n#&gt; \n#&gt;  Comparison of Poisson rates\n#&gt; \n#&gt; data:  c(x1, x2) time base: c(T1, T2)\n#&gt; count1 = 15, expected count1 = 9.5, p-value = 0.01921\n#&gt; alternative hypothesis: true rate ratio is not equal to 1\n#&gt; 95 percent confidence interval:\n#&gt;   1.194652 15.522225\n#&gt; sample estimates:\n#&gt; rate ratio \n#&gt;       3.75",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Distributions of discrete variables</span>"
    ]
  },
  {
    "objectID": "discrete-data-dist.html#negative-binomial-distribution",
    "href": "discrete-data-dist.html#negative-binomial-distribution",
    "title": "10  Distributions of discrete variables",
    "section": "10.3 Negative binomial distribution",
    "text": "10.3 Negative binomial distribution\nThe Negative Binomial distribution is a probability distribution used to model the number of trials required to achieve a certain number of successes in a sequence of independent and identically distributed Bernoulli trials. Unlike the binomial distribution, which counts the number of successes in a fixed number of trials, the negative binomial distribution counts the number of failures before achieving a fixed number of successes.\nFor Eexample, the number of patients treated before achieving 5 successful recoveries; The number of hospital visits until a patient recovers (achieves a fixed number of successful treatments).;The number of failures (e.g., adverse events or complications) before a certain number of successful outcomes (e.g., disease remission).\n\n10.3.1 Key characteristics\n\nParameter r: The number of successes we want to achieve.\nParameter p : The probability of success on each trial.\nSupport: The negative binomial distribution models the number of failures before the r-th success occurs.\n\n\n\n10.3.2 Probability mass function (PMF)\nThe probability that there are k failures before the r-th success is given by:\n\nP(X = k) = \\binom{k + r - 1}{k} p^r (1 - p)^k\n\nwhere X is the number of failures, r is the number of successes, and p is the probability of success.\n\n\n10.3.3 Mean and standard deviation\nFor negative binomial distribution, the mean and standard deviation are calculated using the number of successes r and the probability of success p.\nmean: \\mu = \\frac{r(1 - p)}{p} standard deviation: \\sigma = \\sqrt\\frac{r(1 - p)}{p^2}\nThe Negative binomial distribution is often used as an alternative to the Poisson distribution when the data show overdispersion, meaning the variance exceeds the mean.\n\n\n10.3.4 Applications\nThe negative binomial distribution can be used to describe the clustering of organisms and also to compare the differences in the population mean.\n\nClustering of organisms\nThe negative ninomial distribution is often used in ecological studies and public health to describe the clustering of organisms or events when the data are overdispersed (i.e., the variance is greater than the mean). This distribution is particularly useful in modeling situations where events (such as organisms) tend to cluster or aggregate rather than be randomly distributed.\n\nExample 8: \nA hospital is studying the clustering of bacterial infections in patients within a specific unit. The hospital collects data on the number of bacterial colonies detected in wound samples from 50 patients over a month. The mean number of bacterial colonies per patient is \\lambda = 10, but due to clustering, some patients have many colonies, while others have very few. This clustering leads to overdispersion, where the variance is greater than the mean. To model the number of bacterial colonies detected per patient and estimate the clustering pattern.\n\n\n# Simulate data for 50 patients using a Negative Binomial distribution\nset.seed(123)\nnum_patients &lt;- 50\nmean_colonies &lt;- 10  # Average number of bacterial colonies per patient\ndispersion_param &lt;- 3  # Dispersion parameter (greater dispersion leads to more clustering)\n\n# Generate data for the number of colonies per patient\ncolonies &lt;- rnbinom(num_patients, size = dispersion_param, mu = mean_colonies)\n\n# Summary of the simulated data\nsummary(colonies)\n\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;    1.00    5.00    9.00    9.32   12.75   22.00\n\n# Fit a Negative Binomial model to the data\nnb_model &lt;- glm.nb(colonies ~ 1)\n\n# View model results\nsummary(nb_model)\n\n#&gt; \n#&gt; Call:\n#&gt; glm.nb(formula = colonies ~ 1, init.theta = 4.368663822, link = log)\n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept)    2.232      0.082   27.22   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for Negative Binomial(4.3687) family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 53.936  on 49  degrees of freedom\n#&gt; Residual deviance: 53.936  on 49  degrees of freedom\n#&gt; AIC: 306.89\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 1\n#&gt; \n#&gt; \n#&gt;               Theta:  4.37 \n#&gt;           Std. Err.:  1.35 \n#&gt; \n#&gt;  2 x log-likelihood:  -302.885\n\n# Plot histogram of the data to visualize clustering\ncolonies |&gt; \n  as_tibble() |&gt; \n  ggplot(aes(x = value)) +\n  geom_histogram(bins = 10, fill = \"skyblue\", color = \"black\", alpha = 0.7) +\n  labs(title = \"Clustering of bacterial colonies\",\n       x = \"Number of bacterial colonies per patient\",\n       y = \"Frequency\")\n\n\n\n\n\n\n\n\nThe data are generated using a negative binomial distribution to reflect the fact that bacterial colonies cluster within certain patients. The dispersion parameter controls how much clustering there is (the smaller the parameter, the more clustering). The fitted model allows you to estimate the degree of clustering and assess whether the number of colonies per patient significantly deviates from a purely random (Poisson) distribution.\nIn practice, this kind of modeling can help hospitals or researchers understand the spread and concentration of infections in patient populations. It is also commonly used in ecological studies to model the distribution of species, such as insects or plants, in a habitat.\nOne-sample comparison\nFor a single-sample comparison, you compare the sample mean to a known or hypothesized population mean. You may use a Wald test or likelihood ratio test in the context of a negative binomial regression model to compare the sample mean with the hypothesized mean.\n\nExample 9: \nSuppose a hospital wants to check if the average number of emergency room (ER) visits due to asthma per month has increased compared to a hypothesized national mean. Historically, it is known that the average number of ER visits due to asthma per month is \\mu_0 = 30. The hospital collects data for 12 months and finds that the mean number of ER visits per month is 35. To compare whether the hospital’s average ER visits due to asthma differ significantly from the national average of 30 visits per month.\n\n\n# Simulate data for 12 months of ER visits (mean = 35, size parameter controls dispersion)\nset.seed(200)\ner_visits &lt;- rnbinom(12, size = 5, mu = 35)\n\n# Fit a Negative Binomial model and compare the mean to 30\nmean_visits &lt;- 30  # Hypothesized national mean\nnb_model &lt;- glm.nb(er_visits ~ 1)  # Null model (intercept-only)\n\n# Test if the mean differs from 30\nsummary(nb_model)\n\n#&gt; \n#&gt; Call:\n#&gt; glm.nb(formula = er_visits ~ 1, init.theta = 8.233978635, link = log)\n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept)   3.3142     0.1147    28.9   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for Negative Binomial(8.234) family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 11.962  on 11  degrees of freedom\n#&gt; Residual deviance: 11.962  on 11  degrees of freedom\n#&gt; AIC: 94.162\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 1\n#&gt; \n#&gt; \n#&gt;               Theta:  8.23 \n#&gt;           Std. Err.:  4.27 \n#&gt; \n#&gt;  2 x log-likelihood:  -90.162\n\n\nThe results will show whether the average number of ER visits significantly differs from the hypothesized mean.\nTwo-sample comparison\nFor a two-sample comparison, you compare the means of two independent samples.\nA common approach for comparing two means in the negative binomial distribution is using a negative binomial regression model. This is analogous to a two-sample t-test, but tailored for count data.\n\nExample 10: \nA clinical trial is conducted to compare the number of hospitalizations between two groups of patients: those receiving a new drug and those receiving a standard drug for chronic obstructive pulmonary disease. Over the course of a year, the hospital records the number of hospitalizations for both groups.To determine if the mean number of hospitalizations differs significantly between the two groups.\n\n\nset.seed(200)\n# Simulate data for two groups\ngroup &lt;- factor(rep(1:2, each = 100))  # Two groups\ncounts &lt;- rnbinom(200, size = 5, mu = if_else(group == 1, 10, 15))  # Negative Binomial counts\n\n# Fit a Negative Binomial regression model\nnb_model &lt;- glm.nb(counts ~ group)\n\n# View the summary of the model\nsummary(nb_model)\n\n#&gt; \n#&gt; Call:\n#&gt; glm.nb(formula = counts ~ group, init.theta = 5.66947187, link = log)\n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept)  2.26176    0.05297  42.701  &lt; 2e-16 ***\n#&gt; group2       0.35928    0.07278   4.937 7.95e-07 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for Negative Binomial(5.6695) family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 234.55  on 199  degrees of freedom\n#&gt; Residual deviance: 210.16  on 198  degrees of freedom\n#&gt; AIC: 1258.7\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 1\n#&gt; \n#&gt; \n#&gt;               Theta:  5.669 \n#&gt;           Std. Err.:  0.859 \n#&gt; \n#&gt;  2 x log-likelihood:  -1252.688\n\n\nThe coefficient for group will tell you whether the mean number of hospitalizations differs between the two groups. If the p-value for the group variable is less than 0.05, there is a statistically significant difference in hospitalizations between the new and standard drug groups.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Distributions of discrete variables</span>"
    ]
  },
  {
    "objectID": "discrete-data-dist.html#statistical-models-for-count-data",
    "href": "discrete-data-dist.html#statistical-models-for-count-data",
    "title": "10  Distributions of discrete variables",
    "section": "10.4 Statistical models for count data",
    "text": "10.4 Statistical models for count data\n\n10.4.1 Poisson regression\nA common model used to analyze count data, assuming that the mean is equal to the variance. It is appropriate when the count data are not overdispersed.\n\n\n10.4.2 Negative binomial regression\nUsed when count data exhibit overdispersion. This model is more flexible than Poisson regression, as it allows the variance to exceed the mean.\n\n\n10.4.3 Zero-inflated models\nIn some medical data, there may be an excess of zero counts (e.g., a large number of patients with no hospital visits). Zero-inflated Poisson (ZIP) or Zero-inflated Negative Binomial (ZINB) models can account for this excess of zeros.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Distributions of discrete variables</span>"
    ]
  },
  {
    "objectID": "chi-square-test.html",
    "href": "chi-square-test.html",
    "title": "11  Chi-square test",
    "section": "",
    "text": "11.1 Prerequisite\nlibrary(tidyverse)\nlibrary(rmarkdown)\nlibrary(coin)",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chi-square test</span>"
    ]
  },
  {
    "objectID": "chi-square-test.html#chi-square-test-for-2x2-tables",
    "href": "chi-square-test.html#chi-square-test-for-2x2-tables",
    "title": "11  Chi-square test",
    "section": "11.2 Chi-square test for 2x2 tables",
    "text": "11.2 Chi-square test for 2x2 tables\n\n11.2.1 Chi-square distribution\nThe Chi-square distribution is a probability distribution commonly used in hypothesis testing and confidence interval estimation for variance and categorical data analysis, particularly in the Chi-square test for independence and goodness-of-fit.\nThe probability density function of the Chi-square distribution with k degrees of freedom is:\n\nf(x; k) = \\frac{1}{2^{k/2} \\Gamma(k/2)} x^{(k/2 - 1)} e^{-x/2}\n\nwhere x \\geq 0, k is the degrees of freedom, \\Gamma is the Gamma function.\n\n\n11.2.2 Visualization\nThe shape of the Chi-square distribution is determined by its degrees of freedom. The code below will plot Chi-square distributions with different degrees of freedom to demonstrate how the shape of the distribution changes.\n\n\n\n\n\n\n\n\n\nIt is positively skewed for low degrees of freedom but becomes more symmetrical as the degrees of freedom increase. Since it’s a distribution of squared values, the Chi-square statistic is always non-negative, meaning it ranges from 0 to infinity.\n\n\n11.2.3 2x2 contingency table\n2x2 contingency table, also known as fourfold table , is a matrix used to display the frequencies of two categorical variables with two levels each (see Table 11.1 ).\n\n\n\n\nTable 11.1: Structure of a 2x2 contingency table\n\n\n\n\n\n\n\nevent\nnon-event\ntotal\n\n\n\n\nexposed\na\nb\na + b\n\n\nnon-exoposed\nc\nd\nc + d\n\n\ntotal\na + c\nb + d\nN = a + b + c + d\n\n\n\n\n\n\n\n\nwhere a is the number of events in the exposed group, b is the number of non-events in the exposed group, c is the number of events in the non-exposed group, d is the number of non-events in the non-exposed group.\nFor example, you might use a fourfold table to study whether a new treatment (exposure) improves the survival rates (event) compared to a standard treatment.\n\n\n\n\nTable 11.2: Heart attack outcomes by two treatment groups\n\n\n\n\n\n\n\n\n\n\n\n\n\nheart attack (survived)\nheart attack (died)\ntotal\n\n\n\n\nnew treatment\n30\n20\n50\n\n\nstandard treatment\n15\n35\n50\n\n\ntotal\n45\n55\n100\n\n\n\n\n\n\n\n\nChi-square statistic\nThe Chi-square statistic for a 2x2 table is calculated using the formula:\n\n\\chi^2 = \\frac{N(ad - bc)^2}{(a + b)(c + d)(a + c)(b + d)}\n\nwhere N is the total sample size, N = a + b + c + d, ad - bc is the product difference between rows and columns.\nYates’ continuity correction\nYates’ continuity correction is a statistical adjustment applied to the Chi-square test for 2x2 contingency tables to correct for the fact that the Chi-square test is an approximation of a continuous distribution, but the data in a contingency table are discrete. This correction makes the Chi-square test more conservative, reducing the likelihood of type I errors (false positives) by slightly lowering the Chi-square statistic.\nWith Yates’ correction, the formula for the Chi-square statistic is:\n\n\\chi^2 = \\frac{(|ad - bc| - 0.5N)^2 N}{(a+b)(c+d)(a+c)(b+d)}\n\n\nWhen to use Yates’ correction\n\nIt is generally recommended for 2x2 tables with small sample sizes, particularly when any expected frequency is less than 10 but greater than 5. It is automatically applied in chisq.test() for 2x2 tables unless specified otherwise.\nWhen you want to be more conservative and reduce the chance of false positives (finding a significant result when there is none), Yates’ correction can help.\n\nWhen not to use Yates’ correction\n\nWhen the sample size is large, or the expected frequencies in each cell are well above 5, Yates’ correction is not necessary and may be too conservative.\nYates’ correction is only relevant for 2x2 tables, so it’s ignored for larger contingency tables.\n\n\nIt is generally recommended for small sample sizes but can be unnecessary or even too conservative in larger samples.\n\n\n11.2.4 Chi-square test\nThe Chi-square test for 2x2 tables is commonly used to assess whether there is an association between two categorical variables. In medical research, this test is frequently applied to analyze whether exposure (e.g., treatment, risk factor) to a risk factor is associated with a particular outcome (e.g., disease, recovery).\nIn R, you can perform a Chi-square test for the 2x2 contingency table using the chisq.test() function or fisher.test() function.\nWhen to use chisq.test()\n\nChi-square test works well when the sample size is large enough, typically when the expected frequency in each cell of the contingency table is 5 or more.\nChi-square test provides an approximate result based on the Chi-square distribution. It is not suitable for small sample sizes, but it’s efficient for larger data sets.\nChi-square test assumes the total row and column frequencies can vary. It’s used when you don’t have fixed marginal totals.\n\nWhen to use fisher.test()\n\nFisher’s exact test (discussed in Section 11.4 )is recommended for small sample sizes, particularly when any expected cell count is less than 5. It calculates the exact p-value without relying on large sample approximations.\nFisher’s exact test assumes that both the row and column totals are fixed. This makes it a more conservative test compared to the Chi-square test.\nFisher’s test is preferred when dealing with sparse data (i.e., a lot of cells with small counts), as it doesn’t rely on assumptions of normality or expected frequencies.\n\n\nExample 1: \nTo determine if a new treatment for heart attack patients improves survival rates compared to a standard treatment, a total of 100 patients who have suffered heart attacks were randomly divided into two groups: 50 patients receive new treatment, 50 patients receive standard treatment. The outcome of interest is whether the patient survived (recovered or showed significant improvement) or died (passed away due to complications from the heart attack) after the treatment. The observed data is summarized in Table 11.2 . Is there a significant difference in survival rates between patients who received the new treatment and those who received the standard treatment?\n\n\n# Create a 2x2 contingency table\nmatrix(c(30, 20, 15, 35), nrow = 2, byrow = T) |&gt; \n  chisq.test(correct = F)\n\n#&gt; \n#&gt;  Pearson's Chi-squared test\n#&gt; \n#&gt; data:  matrix(c(30, 20, 15, 35), nrow = 2, byrow = T)\n#&gt; X-squared = 9.0909, df = 1, p-value = 0.002569\n\n\nThe calculated Chi-square statistic is 9.0909, p-value is 0.002569, less than the significance level 0.05. This indicates a statistically significant difference in survival rates between the two treatments.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chi-square test</span>"
    ]
  },
  {
    "objectID": "chi-square-test.html#chi-square-test-for-paired-2x2-tables",
    "href": "chi-square-test.html#chi-square-test-for-paired-2x2-tables",
    "title": "11  Chi-square test",
    "section": "11.3 Chi-square test for paired 2x2 tables",
    "text": "11.3 Chi-square test for paired 2x2 tables\nChi-square test for paired 2x2 table is typically used for analyzing paired categorical data, such as when the same subjects are measured before and after an intervention, or when two related groups are compared. In this case, the McNemar’s test is commonly applied for paired data rather than the standard Chi-square test, as it is designed specifically to handle dependent data (paired observations).\n\n11.3.1 McNemar’s Test\nMcNemar’s test focuses only on the off-diagonal cells b and c, which represent discordant pairs (i.e., patients who switched their outcomes between pre- and post-treatment). The cells a and d (concordant pairs) are ignored in this test because they reflect no change in outcome.\nThe test statistic without continuity correction:\n\n\\chi^2 = \\frac{(b - c)^2}{b + c},\n\nThe test statistic with continuity correction:\n\n\\chi^2 = \\frac{(|b - c| - 1)^2}{b + c}\n\nwhere b is the number of individuals who succeeded in pre-treatment but failed in post-treatment, c is the number of individuals who failed in pre-treatment but succeeded in post-treatment, 1 in the formula is a continuity correction applied for small sample sizes.\nThe continuity correction is especially important when b + c is small (e.g., less than 40). It helps to prevent overestimating the test statistic. For larger sample sizes, the correction has a smaller impact, and some recommend not applying it (i.e., setting correct = FALSE), as it can be overly conservative.\n\nExample 2: \nA study is conducted to evaluate the efficacy of a new vaccine in preventing the flu. The same group of individuals is tested before and after receiving the vaccine to see if they develop flu symptoms. The observed data is summarized in Table 11.3 . Determine whether the vaccine have a significant effect on reducing flu incidence.\n\n\n\n\n\nTable 11.3: Paired flu status before and after vaccination\n\n\n\n\n\n\n\n\n\n\n\n\n\nflu post-vaccine\nno flu post-vaccine\ntotal\n\n\n\n\nflue pre-vaccine\n10 (had flu both times)\n25 (had flu pre, no flu post)\n35\n\n\nno flu pre-vaccine\n5 (no flu pre, had flu post)\n60 (no flu both times)\n65\n\n\ntotal\n15\n85\n100\n\n\n\n\n\n\n\n\n\nmatrix(c(10, 25, 5, 60), nrow = 2, byrow = T) |&gt; \n  mcnemar.test(correct = T)\n\n#&gt; \n#&gt;  McNemar's Chi-squared test with continuity correction\n#&gt; \n#&gt; data:  matrix(c(10, 25, 5, 60), nrow = 2, byrow = T)\n#&gt; McNemar's chi-squared = 12.033, df = 1, p-value = 0.0005226\n\n\nIf the p-value from the McNemar’s test is less than the significance level (typically \\alpha = 0.05), you would reject the null hypothesis and conclude that the vaccine had a significant effect on reducing flu incidence. If the p-value is greater than the significance level, the null hypothesis is not rejected, and the data do not provide sufficient evidence to conclude that the vaccine significantly changed flu outcomes. Here the p-value is less than 0.05.\nThis type of paired data analysis is common in medical research, especially in pre-post studies where the same subjects are followed over time to assess the impact of an intervention (e.g., medication, surgery, or vaccines).",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chi-square test</span>"
    ]
  },
  {
    "objectID": "chi-square-test.html#sec-fisher-test",
    "href": "chi-square-test.html#sec-fisher-test",
    "title": "11  Chi-square test",
    "section": "11.4 Fisher’s exact test",
    "text": "11.4 Fisher’s exact test\nThe Fisher’s exact test is used to determine if there are nonrandom associations between two categorical variables, typically in a 2x2 contingency table (fourfold table). It is commonly applied when sample sizes are small, making the Chi-square test unreliable due to low expected frequencies in one or more cells.\nFisher’s exact test calculates the exact probability of obtaining a table at least as extreme as the one observed, assuming the null hypothesis is true. The probability is calculated based on the hypergeometric distribution, and Fisher’s test uses this distribution to calculate the p-value.\nThe formula for the probability of observing a given 2x2 table is:\n\nP = \\frac{(a+b)!(c+d)!(a+c)!(b+d)!}{a!b!c!d!N!}\n\nwhere N = a + b + c + d is the total sample size.\n\nExample 3: \nA clinical trial is conducted to evaluate the effectiveness of a new drug for treating hypertension. Patients are divided into treatment group and control group. After treatment, the number of patients whose blood pressure returned to normal (cured) and those whose blood pressure remained high (not cured) is recorded. The data collected is in . Determine if the new drug shows a statistically significant difference in its effectiveness compared to the control.\n\n\n\n\n\nTable 11.4: Structure of a 2x2 contingency table\n\n\n\n\n\n\n\nblood pressure normal\nBlood pressure high\ntotal\n\n\n\n\ntreatment\n8\n2\n10\n\n\ncontrol\n3\n7\n10d\n\n\ntotal\n11\n9\n20\n\n\n\n\n\n\n\n\nIn R, Fisher’s exact test can be performed using the fisher.test() function.\n\nmatrix(c(8, 2, 3, 7), nrow = 2, byrow = T) |&gt; \nfisher.test()\n\n#&gt; \n#&gt;  Fisher's Exact Test for Count Data\n#&gt; \n#&gt; data:  matrix(c(8, 2, 3, 7), nrow = 2, byrow = T)\n#&gt; p-value = 0.06978\n#&gt; alternative hypothesis: true odds ratio is not equal to 1\n#&gt; 95 percent confidence interval:\n#&gt;    0.8821175 127.0558418\n#&gt; sample estimates:\n#&gt; odds ratio \n#&gt;   8.153063\n\n\nThe result of Fisher’s Exact Test will give you a p-value. Here p-value is 0.06978, above 0.05, not reject the null hypothesis. This means the data do not provide sufficient evidence to conclude that the new drug shows a better effectiveness compared to control.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chi-square test</span>"
    ]
  },
  {
    "objectID": "chi-square-test.html#chi-square-test-for-rc-tables",
    "href": "chi-square-test.html#chi-square-test-for-rc-tables",
    "title": "11  Chi-square test",
    "section": "11.5 Chi-square test for R×C tables",
    "text": "11.5 Chi-square test for R×C tables\nThe Chi-square test for R×C contingency tables is used to assess whether there is an association between two categorical variables where one variable has R categories and the other has C categories. This is also called the Chi-square test of independence.\n\n11.5.1 Comparison of multiple sample rates\nWhen comparing multiple sample rates, several statistical methods are commonly used, including the Chi-square test and Cochran’s Q test. The choice of method depends on the structure of the data and the research question.\n\nChi-Square test for rates\nThe Chi-square test is widely used to compare rates across multiple independent samples. It tests whether the observed differences between sample proportions are statistically significant.\n\nExample 4: \nSuppose you want to compare recovery rates between three hospitals for a specific disease. The data is in Table 11.5 . Determine if the recovery rates are significantly different between these hospitals.\n\n\n\n\n\nTable 11.5: The recovery results of a disease in three hospitals\n\n\n\n\n\n\n\nrecovered\nnot revovered\n\n\n\n\nhospital A\n50\n20\n\n\nhospital B\n55\n15\n\n\nhospital C\n65\n5\n\n\n\n\n\n\n\n\nHere is chisq.test() is used to conduct this kind of test. In Section 11.6 , we reanalyze this example using function prop.test(), which gives the same result.\n\nmatrix(c(50, 20, 55, 15, 65, 5), nrow = 3, byrow = T) |&gt; \n  chisq.test(correct = F)\n\n#&gt; \n#&gt;  Pearson's Chi-squared test\n#&gt; \n#&gt; data:  matrix(c(50, 20, 55, 15, 65, 5), nrow = 3, byrow = T)\n#&gt; X-squared = 10.809, df = 2, p-value = 0.004497\n\n\nCochran’s Q test\nCochran’s Q test is an extension of the Chi-Square test, designed for comparing rates across multiple samples in paired or repeated measures settings. It is typically used with binary outcomes (e.g., “yes/no” or “success/failure”).\n\nExample 5: \nA study is conducted to test the effectiveness of a new drug across three time points (1 month, 3 months, and 6 months) on the same group of patients. The outcome is whether the treatment was successful (1) or not (0), recorded for 10 patients in Table 11.6 . Determine if there is a significant difference in success rates across the three time points.\n\n\n\n\n\nTable 11.6: The recovery results of a disease in three hospitals\n\n\n\n\n\n\nid\n1 month\n3 months\n6 months\n\n\n\n\n1\n1\n0\n1\n\n\n2\n0\n0\n1\n\n\n3\n1\n1\n1\n\n\n4\n0\n1\n0\n\n\n5\n1\n1\n1\n\n\n6\n1\n0\n1\n\n\n7\n0\n1\n0\n\n\n8\n1\n1\n1\n\n\n9\n1\n0\n0\n\n\n10\n1\n1\n1\n\n\n\n\n\n\n\n\n\ndata.frame(\n  id = factor(1:10),\n  time = factor(rep(c(\"1 Month\", \"3 Months\", \"6 Months\"), each = 10)),\n  outcome = c(1,0,1,0,1,1,0,1,1,1, 0,0,1,1,1,0,1,1,0,1, 1,0,0,1,0,1,1,1,0,1)\n) |&gt; \nsymmetry_test(outcome ~ time | id, data = _, teststat = \"quadratic\")\n\n#&gt; \n#&gt;  Asymptotic General Symmetry Test\n#&gt; \n#&gt; data:  outcome by\n#&gt;   time (1 Month, 3 Months, 6 Months) \n#&gt;   stratified by id\n#&gt; chi-squared = 0.28571, df = 2, p-value = 0.8669\n\n\nCochran-Armitage trend test\nIf you want to test whether there is a linear trend in the rates across ordered groups, you can use the Cochran-Armitage test for trend. This test is useful for ordered categorical data and examines if there is a significant linear trend in proportions.\n\nExample 6: \nSuppose you have data from a study on smoking and lung cancer risk. The groups are non-smokers, light smokers, and heavy smokers, and the outcome is the presence of lung cancer (yes/no). The data is listed in Table 11.7 . Determine if there’s a linear trend in cancer risk across the ordered smoking categories.\n\n\n\n\n\nTable 11.7: The data from a study on smoking and lung cancer risk\n\n\n\n\n\n\ngroup\ncancer\nno cancer\n\n\n\n\nnoon-smokers\n5\n95\n\n\nlight smokers\n15\n85\n\n\nheavy smokers\n40\n60\n\n\n\n\n\n\n\n\n\nwith(df, prop.trend.test(cancer, cancer + `no cancer`))\n\n#&gt; \n#&gt;  Chi-squared Test for Trend in Proportions\n#&gt; \n#&gt; data:  cancer out of cancer + `no cancer` ,\n#&gt;  using scores: 1 2 3\n#&gt; X-squared = 38.281, df = 1, p-value = 6.125e-10\n\n\nIt is specifically designed to detect linear trends in rates across ordered groups. This test is particularly useful when you have binary outcomes (e.g., success/failure) across different levels of an ordinal independent variable (e.g., increasing doses of a drug). It directly tests for a linear trend in the proportions of success.\n\n\n\n11.5.2 Comparison of multiple sample proportions\n\nExample 7: \nSuppose you are studying the distribution of a gene variant related to a certain disease in two groups: a healthy control group, and a disease group. You aim to compare the frequencies of the AA, Aa, and aa genotypes between these groups. The data is in Table 11.8 . Determine whether there is a significant difference in the distribution of genotypes between the healthy and disease groups.\n\n\n\n\n\nTable 11.8: The distribution of a gene variant in two groups\n\n\n\n\n\n\ngroup\nAA\nAa\naa\n\n\n\n\nhealth group\n40\n35\n25\n\n\ndisease group\n30\n45\n25\n\n\n\n\n\n\n\n\n\n# Create the data matrix\nmatrix(c(40, 35, 25, 30, 45, 25), nrow = 2, byrow = T) |&gt; \n  chisq.test(correct = F)\n\n#&gt; \n#&gt;  Pearson's Chi-squared test\n#&gt; \n#&gt; data:  matrix(c(40, 35, 25, 30, 45, 25), nrow = 2, byrow = T)\n#&gt; X-squared = 2.6786, df = 2, p-value = 0.262\n\n\nFor smaller sample sizes, Fisher’s Exact Test can be used.\nGenotyping is often used to study the distribution of specific genes in different populations, particularly when exploring the relationship between genetic variations and disease risk. For instance, certain gene alleles may be linked to chronic diseases like cardiovascular conditions or cancer. These analyses can help identify associations between specific genotypes and disease susceptibility. For example , comparing the distribution of APOE genotypes (APOE ε2, ε3, ε4) between patients with Alzheimer’s disease and healthy controls; Investigating differences in the INS gene (insulin gene) genotypes between diabetic and non-diabetic patients. Such analysis can help researchers understand the link between genetic variations and disease risk, providing insights into personalized medicine and genetic risk factors.\n\n\n11.5.3 Association Analysis of bivariate nominal data\nAssociation analysis of bivariate nominal data involves examining the relationship between two categorical variables. This is typically done using chi-square tests or Fisher’s exact tests to determine if there is a significant association between the two variables.\nIf the association is significant, and the strength of the relationship needs to be further analyzed, the Pearson contingency coefficient C need to be calculated.\nPearson’s contingency coefficient\nPearson’s contingency coefficient is a measure used to assess the strength of association between two categorical variables in a contingency table. It quantifies how strongly two variables are related based on the chi-square statistic. It is calculated using the following formula:\n\nC = \\sqrt{\\frac{\\chi^2}{\\chi^2 + n}}\n\nwhere C is the Pearson’s contingency coefficient, \\chi^2 is the chi-square statistic obtained from a chi-square test, n is the total sample size.\nThe value of C ranges from 0 \\leq C &lt; 1. Values closer to 1 indicate a stronger association between the variables, while values close to 0 indicate weak or no association. As the size of the contingency table (degrees of freedom) increases, the contingency coefficient tends to decrease.\n\nExample 8: \nResearchers aim to evaluate the effectiveness of two different antibiotics (A and B) for two types of infections (X and Y). The effectiveness is measured by whether patients recover, categorized as “recovered” or “not recovered.” The data is summarized in Table 11.9 . The goal is to determine if the effectiveness of the antibiotics differs significantly for different infection types.\n\n\n\n\n\nTable 11.9: The results of two different antibiotics for two types of infections\n\n\n\n\n\n\nantibiotic\ninfection type\nrecovered\nnot recovered\n\n\n\n\nA\nX\n35\n15\n\n\nA\nY\n25\n25\n\n\nB\nX\n40\n10\n\n\nB\nY\n30\n20\n\n\n\n\n\n\n\n\nTo evaluate whether the effectiveness of antibiotics A and B differs significantly for different infection types (X and Y), you can perform a chi-square test of independence on the contingency table. This will assess if there is a significant association between the type of antibiotic and the recovery status for the two infection types.\n\n# Create data\ndata &lt;- matrix(c(35, 15, 25, 25, 40, 10, 30, 20), nrow = 2, byrow = T)\n\nchisq.test(data)\n\n#&gt; \n#&gt;  Pearson's Chi-squared test\n#&gt; \n#&gt; data:  data\n#&gt; X-squared = 2.3434, df = 3, p-value = 0.5043\n\n# Calculate Pearson’s Contingency Coefficient\nDescTools::ContCoef(data)\n\n#&gt; [1] 0.1076172\n\n\nThe p-value is greater than your significance level 0.05, not reject the null hypothesis, indicating that the recovery status is not associated with the antibiotic and infection type.\nA little expansion*：\nTo analyze the interaction between antibiotics (A and B) and infection types (X and Y) on recovery outcomes (recovered or not recovered), a logistic regression model is appropriate. This model will allow you to test the interaction effect between the two categorical variables (antibiotics and infection types) and their influence on recovery.\nIn this case, you can model the recovery outcome as the dependent variable and include both antibiotics and infection types as independent variables, along with their interaction term.\n\n# Convert data to long format for logistic regression\ndf_long &lt;- df |&gt; \n  janitor::clean_names() |&gt; \n1  pivot_longer(\n    cols = c(recovered, not_recovered),\n    names_to = \"response\",\n    values_to = \"freq\"\n  ) |&gt;                                 \n  mutate(response = if_else(response == \"recovered\", 1, 0))\n  \n2df_expanded &lt;- df_long |&gt;\n  uncount(freq)\n\nmodel &lt;- glm(\n  response ~ antibiotic * infection_type, \n  data = df_expanded, \n  family = binomial(link = \"logit\")\n) \n\nsummary(model)\n\n\n1\n\nReshape the data to long format suitable for logistic regression\n\n2\n\nReplicate rows based on freq to get one row per individual (useful for logistic regression)\n\n\n\n\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = response ~ antibiotic * infection_type, family = binomial(link = \"logit\"), \n#&gt;     data = df_expanded)\n#&gt; \n#&gt; Coefficients:\n#&gt;                             Estimate Std. Error z value Pr(&gt;|z|)   \n#&gt; (Intercept)                   0.8473     0.3086   2.746  0.00604 **\n#&gt; antibioticB                   0.5390     0.4693   1.149  0.25075   \n#&gt; infection_typeY              -0.8473     0.4186  -2.024  0.04296 * \n#&gt; antibioticB:infection_typeY  -0.1335     0.6193  -0.216  0.82930   \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 258.98  on 199  degrees of freedom\n#&gt; Residual deviance: 247.74  on 196  degrees of freedom\n#&gt; AIC: 255.74\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 4\n\n\nThe logistic regression model is trying to assess the relationship between antibiotic type, infection type, and their interaction on patient recovery. Here’s a breakdown of the key results:\n\n\n\n\n\n\nInterpretation of logistic regression Results\n\n\n\n\n(Intercept): Estimate: 0.8473, p-value: 0.00604 (statistically significant)\n\nThis represents the log-odds of recovery for patients treated with antibiotic A for Infection X (the baseline group).\nA positive coefficient (0.8473) suggests that the odds of recovery are higher for this group.\nTo interpret this in terms of odds: \\exp(0.8473) \\approx 2.33 , meaning patients in the baseline group are 2.33 times more likely to recover.\n\nantibioticB: Estimate: 0.5390, p-value: 0.25075 (not statistically significant)\n\nThis is the difference in log-odds of recovery between antibiotic B and antibiotic A for Infection X.\nSince the p-value is not significant, there is no strong evidence that antibiotic B differs significantly from antibiotic A for Infection X.\n\ninfection_typeY: Estimate: -0.8473, p-value: 0.04296 (statistically significant)\n\nThis is the effect of having Infection Y instead of infection X when treated with antibiotic A.\nThe negative coefficient (-0.8473) indicates that patients with Infection Y are less likely to recover compared to those with infection X when treated with antibiotic A.\nIn terms of odds: \\exp(-0.8473) \\approx 0.43 , meaning patients with infection Y have about 43% the odds of recovery compared to those with infection X when treated with antibiotic A.\n\nantibioticB:infection_typeY (Interaction Term): Estimate: -0.1335, p-value: 0.82930 (not statistically significant)\n\nThis term represents the interaction between Antibiotic B and infection Y, meaning how the effect of antibiotic B changes for Infection Y compared to infection X.aThe non-significant p-value indicates that there is no strong evidence of an interaction effect between the type of antibiotic and the type of infection on recovery.\n\n\nSummary:\n\nAntibiotic: There is no statistically significant difference in recovery between antibiotic A and antibiotic B (p = 0.25075).\nInfection Type: Patients with infection Y are significantly less likely to recover compared to those with infection X (p = 0.04296).\nInteraction: The interaction between antibiotic type and infection type is not significant, meaning the effectiveness of the antibiotics does not seem to differ significantly between infection types (p = 0.82930).\n\nConclusion:\n\nThe type of infection has a significant impact on recovery, with infection X being more favorable for recovery than infection Y.\nHowever, there is no significant difference between antibiotic A and antibiotic B in terms of effectiveness.\nThere is also no significant interaction between antibiotic type and infection type, meaning the antibiotics perform similarly across infection types.\n\n\n\nTo visualize the interaction effect:\n\n# Add predictions to the expanded dataset\ndf_expanded  &lt;- df_expanded |&gt; \n  mutate(predicted = predict(model, type = \"response\"))\n\n# Plot the interaction between antibiotic and infection type\nggplot(df_expanded, aes(x = infection_type, y = predicted, color = antibiotic)) +\n  geom_point(position = position_dodge(width = 0.2), size = 1) +\n  geom_line(aes(group = antibiotic), position = position_dodge(width = 0.2)) +\n  labs(x = \"Infection type\", y = \"Predicted probability of recovery\") \n\n\n\n\n\n\n\nFigure 11.1: Interaction of antibiotics and infection types on recovery\n\n\n\n\n\n\n\n11.5.4 Linear trend test for two-way ordinal data\nThis test evaluates whether there is a linear trend between the categories in two ordinal variables, typically one variable representing treatment or groups and the other representing ordered outcomes. In this case, both “group” and “outcome” are ordinal variables.\n\nExample 9: \nSuppose you are studying the effect of different drug dosages on pain relief in patients. Patients are divided into four dosage groups (low dose, low-medium dose, medium-high dose, and high dose), and their pain relief is categorized into four levels (no relief, partial relief, moderate relief, and complete relief). This type of data is referred to as two-way ordinal data, listed in Table 11.10 . The aim is to assess whether there is a significant linear trend in pain relief as the drug dosage increases. For example, does increasing the dosage lead to a higher likelihood of better pain relief?\n\n\n\n\n\nTable 11.10: The association between different drug dosage and pain relief\n\n\n\n\n\n\n\n\n\n\n\n\n\ndosage\nno relief\npartial relief\nmoderate relief\ncomplete relief\n\n\n\n\nlow dose\n30\n25\n20\n10\n\n\nlow-medium dose\n20\n30\n25\n15\n\n\nmedium-high dose\n15\n35\n30\n20\n\n\nhigh dose\n10\n40\n35\n30\n\n\n\n\n\n\n\n\nThe Mantel-Haenszel chi-square statistic tests the alternative hypothesis that there is a linear association between the row variable and the column variable. Both variables must lie on an ordinal scale. Here we use the MHChisqTest() function in DescTools package.\n\nc(30, 25, 20, 10, 20, 30, 25, 15, 15, 35, 30, 20, 10, 40, 35, 30) |&gt; \n  matrix(nrow = 4, byrow = T) |&gt; \n  DescTools::MHChisqTest()\n\n#&gt; \n#&gt;  Mantel-Haenszel Chi-Square\n#&gt; \n#&gt; data:  matrix(c(30, 25, 20, 10, 20, 30, 25, 15, 15, 35, 30, 20, 10,     40, 35, 30), nrow = 4, byrow = T)\n#&gt; X-squared = 19.458, df = 1, p-value = 1.029e-05\n\n\nThe result shows chi-square statistic is 19.458, p-value less than 0.05, we can conclude that there is linear relationship between drug dosage and pain relief.\nAn alternative method is the logistic regression analysis using a binomial generalized linear model (GLM) with a logit link function.\n\ndf &lt;- tb |&gt; \n  janitor::clean_names() |&gt; \n  pivot_longer(\n    cols = contains(\"relief\"),\n    names_to = \"pain\",\n    values_to = \"freq\"\n  ) |&gt; \n  mutate(\n   pain = case_when(\n     pain == \"no_relief\" ~ 1,\n     pain == \"partial_relief\" ~ 2,\n     pain == \"moderate_relief\" ~ 3,\n     pain == \"complete_relief\" ~ 4\n   ),\n   dosage = case_when(\n     dosage == \"low dose\" ~ 1,\n     dosage == \"low-medium dose\" ~ 2,\n     dosage == \"medium-high dose\" ~ 3,\n     dosage == \"high dose\" ~ 4\n   ),\n   .keep = \"unused\"\n  ) |&gt; \n  mutate(\n1    dosage = factor(dosage, ordered = T),\n    pain = factor(pain, ordered = T)\n  ) |&gt; \n  uncount(freq)\n  \nglm(pain ~ dosage, data = df, family = binomial(link='logit')) |&gt; \n  summary()\n\n\n1\n\nmake sure dosage and pain are labeled as ordered factors\n\n\n\n\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = pain ~ dosage, family = binomial(link = \"logit\"), \n#&gt;     data = df)\n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept)  1.48622    0.13779  10.786  &lt; 2e-16 ***\n#&gt; dosage.L     1.27848    0.28214   4.531 5.86e-06 ***\n#&gt; dosage.Q    -0.01493    0.27557  -0.054    0.957    \n#&gt; dosage.C     0.06702    0.26884   0.249    0.803    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 381.85  on 389  degrees of freedom\n#&gt; Residual deviance: 358.21  on 386  degrees of freedom\n#&gt; AIC: 366.21\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 5\n\n\n\n\n\n\n\n\nResults interpretation\n\n\n\nCoefficients:\ndosage.L (Linear Contrast): The linear trend (dosage.L) has a positive coefficient (1.27848) and is highly significant (p-value = 5.86e-06), suggesting that as the dosage increases, the likelihood of pain relief also increases significantly in a linear fashion.\nGoodness-of-Fit:\nResidual deviance (358.21): The deviance with the dosage predictors included. Lower residual deviance suggests a better fit. AIC (366.21): AIC is used for model comparison; lower values indicate a better model fit.\nInterpretation:\nThe significant linear relationship between dosage and pain suggests that increasing the dosage is associated with a reduced likelihood of pain, but there is no evidence of a quadratic or cubic relationship. The model fits the data relatively well as indicated by the reduction in deviance, and dosage.L is the primary driver of the response variable (pain).",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chi-square test</span>"
    ]
  },
  {
    "objectID": "chi-square-test.html#sec-comp-multi-prop",
    "href": "chi-square-test.html#sec-comp-multi-prop",
    "title": "11  Chi-square test",
    "section": "11.6 Pairwise comparisons of multiple sample proportions",
    "text": "11.6 Pairwise comparisons of multiple sample proportions\nWhen comparing proportions across multiple samples, the goal is to determine whether there are significant differences in the rates or proportions between groups. This is commonly applied in medical research, genetics, or other fields where proportions, such as success rates, are of interest across multiple groups. When conducting multiple comparisons, the risk of a type I error increases.\n\n11.6.1 Pairwise comparison\nThe Bonferroni correction is used to adjust the significance level \\alpha to control for this risk, ensuring the results are reliable.\n\nExample 10: \nConducting pairwise comparisons for the data in Example 4 to determine whether the recovery rates are different between any two hospitals.\n\nHere we use the prop.test() function for testing the null that the proportions in several groups are the same.\n\ndata &lt;- matrix(c(50, 20, 55, 15, 65, 5), nrow = 3, byrow = T)\nrownames(data) &lt;- c(\"hospitalA\", \"hospitalB\", \"hospitalC\")\n\ndata |&gt;\n  prop.test(correct = F)\n\n#&gt; \n#&gt;  3-sample test for equality of proportions without continuity\n#&gt;  correction\n#&gt; \n#&gt; data:  data\n#&gt; X-squared = 10.809, df = 2, p-value = 0.004497\n#&gt; alternative hypothesis: two.sided\n#&gt; sample estimates:\n#&gt;    prop 1    prop 2    prop 3 \n#&gt; 0.7142857 0.7857143 0.9285714\n\n\nThe p-value is less than the significant level 0.05, reject the null hypothesis, indicating the recovery rates in the three hospitals are different. Next, we will make a pairwise comparison between the hospitals.\n\ndata |&gt;\n  pairwise.prop.test(p.adjust.method = \"bonferroni\")\n\n#&gt; \n#&gt;  Pairwise comparisons using Pairwise comparison of proportions \n#&gt; \n#&gt; data:  data \n#&gt; \n#&gt;           hospitalA hospitalB\n#&gt; hospitalB 1.000     -        \n#&gt; hospitalC 0.006     0.089    \n#&gt; \n#&gt; P value adjustment method: bonferroni\n\n\nThe result shows the p-values of different group pairs. Only the p-value between hospital A and hospital C is less than 0.05 (p-value = 0.006), saying the recovery rate between them is significantly different.\n\n\n11.6.2 Comparison with a control group\nHere we set the hospital C as the control group.\n\nalpha = 0.05\nk = 3\nalpha_adjusted &lt;- alpha / (2 * ( k- 1))\n\nchisq.test(data)\n\n#&gt; \n#&gt;  Pearson's Chi-squared test\n#&gt; \n#&gt; data:  data\n#&gt; X-squared = 10.809, df = 2, p-value = 0.004497\n\ncompare_pairs &lt;- list(\n  c(\"hospitalA\", \"hospitalC\"),\n  c(\"hospitalB\", \"hospitalC\")\n)\n\nfor (pair in compare_pairs) {\n  sub_data &lt;- data[match(pair, rownames(data)), ]\n  chi_squared_result &lt;- chisq.test(sub_data)\n  print(paste(\"Comparison between\", paste(pair, collapse=\" and \"), \":\"))\n  print(chi_squared_result)\n}\n\n#&gt; [1] \"Comparison between hospitalA and hospitalC :\"\n#&gt; \n#&gt;  Pearson's Chi-squared test with Yates' continuity correction\n#&gt; \n#&gt; data:  sub_data\n#&gt; X-squared = 9.5443, df = 1, p-value = 0.002006\n#&gt; \n#&gt; [1] \"Comparison between hospitalB and hospitalC :\"\n#&gt; \n#&gt;  Pearson's Chi-squared test with Yates' continuity correction\n#&gt; \n#&gt; data:  sub_data\n#&gt; X-squared = 4.725, df = 1, p-value = 0.02973\n\nprint(paste(\"The adjusted alpha is:\", paste(alpha_adjusted)))\n\n#&gt; [1] \"The adjusted alpha is: 0.0125\"",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chi-square test</span>"
    ]
  },
  {
    "objectID": "chi-square-test.html#chi-square-goodness-of-fit-test",
    "href": "chi-square-test.html#chi-square-goodness-of-fit-test",
    "title": "11  Chi-square test",
    "section": "11.7 Chi-square goodness-of-fit test",
    "text": "11.7 Chi-square goodness-of-fit test\nChi-square goodness-of-fit test is a statistical test used to determine whether observed frequencies conform to an expected theoretical distribution. This test is commonly used to check if data follows normal, uniform, Poisson, or other theoretical distributions.\nThe test is applicable in the following scenarios:\n\nTo test if a sample follows a theoretical distribution (e.g., normal, Poisson, etc.).\nTo compare observed frequency data with expected frequencies to identify deviations.\n\nThe test statistic is calculated as:\n\n\\chi^2 = \\sum_{i=1}^{k} \\frac{(O_i - E_i)^2}{E_i}\n\nO_i is the observed frequency for category i, E_i is the expected frequency for category i, k is the number of categories. The expected frequencies should be sufficiently large, typically at least 5 for each category, to ensure the validity of the chi-square test.\n\nExample 11: \nTo observe the spatial distribution of Keshan disease patients in a certain Keshan disease area, the investigators divided the region into 279 sampling units and recorded the cumulative number of cases in each unit over the years. The data are presented in Table 11.11 . Does this data follow a Poisson distribution?\n\n\n\n\n\nTable 11.11: The cumulative number of Keshan disease patients in each unit\n\n\n\n\n\n\nn_observed\nf_observed\n\n\n\n\n0\n26\n\n\n1\n51\n\n\n2\n75\n\n\n3\n63\n\n\n4\n38\n\n\n5\n17\n\n\n6\n5\n\n\n7\n3\n\n\n8\n1\n\n\n\n\n\n\n\n\nThe chisq.test() function can be used to perform a chi-square goodness-of-fit test.\n\nn_cases &lt;- df[[1]]\nf_observed &lt;- df[[2]]\n# Calculate the weighted average\nlambda &lt;- weighted.mean(n_cases, f_observed)\n\n# Calculated expected probability\np.expected &lt;- dpois(n_cases, lambda)\n\n# Combine the last three rows \nf &lt;- c(f_observed[1:6], sum(f_observed[7:9]))\np &lt;- c(p.expected[1:6], 1 - sum(p.expected[1:6]))\n\nchisq.test(x = f, p = p)\n\n#&gt; \n#&gt;  Chi-squared test for given probabilities\n#&gt; \n#&gt; data:  f\n#&gt; X-squared = 2.0355, df = 6, p-value = 0.9164\n\n\nHere p-value is 0.9164, greater than the significant level 0.1. If the p-value is greater than the significant level, fail to reject the null hypothesis, meaning the data might follow the expected distribution. If the calculated p-value is less than the significant level, reject the null hypothesis and conclude that the data does not follow the expected distribution.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chi-square test</span>"
    ]
  },
  {
    "objectID": "cox-regression.html",
    "href": "cox-regression.html",
    "title": "15  Cox regression",
    "section": "",
    "text": "Cox regression, also known as Cox proportional hazards regression, is a statistical method used to analyze the time until an event occurs, typically survival time. It is widely used in medical research to explore the relationship between the survival time of patients and one or more predictor variables. Unlike other models, Cox regression does not require assumptions about the baseline hazard function, making it a flexible tool for handling censored data where the exact time of the event may not be known for all subjects.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Cox regression</span>"
    ]
  },
  {
    "objectID": "ordered-cat-data.html",
    "href": "ordered-cat-data.html",
    "title": "18  Ordered categorical data",
    "section": "",
    "text": "18.1 Prerequisites\nlibrary(tidyverse)\n\n#&gt; ── Attaching core tidyverse packages ───────────────────── tidyverse 2.0.0 ──\n#&gt; ✔ dplyr     1.1.4     ✔ readr     2.1.5\n#&gt; ✔ forcats   1.0.0     ✔ stringr   1.5.1\n#&gt; ✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n#&gt; ✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n#&gt; ✔ purrr     1.0.2     \n#&gt; ── Conflicts ─────────────────────────────────────── tidyverse_conflicts() ──\n#&gt; ✖ dplyr::filter() masks stats::filter()\n#&gt; ✖ dplyr::lag()    masks stats::lag()\n#&gt; ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ordinal)\n\n#&gt; \n#&gt; Attaching package: 'ordinal'\n#&gt; \n#&gt; The following object is masked from 'package:dplyr':\n#&gt; \n#&gt;     slice\n\nlibrary(geepack)\nR codes for one-sample t-test is:\ndf &lt;- read_csv(\"datasets/ex18-01.csv\", , show_col_types = F)\nThe pain ratings are categorized as ordinal data, with measurements taken at multiple time points, making it repeated measures data. For such data, the Generalized Estimating Equations (GEE) approach is commonly used. It can handle ordinal data and accounts for the correlation within the data.\n下表是模型分析结果：\ndata &lt;- df |&gt; \n  pivot_longer(\n    cols = starts_with(\"t\"),\n    names_to = \"time\",\n    values_to = \"pain\",\n    names_prefix = \"t\"\n  ) |&gt; \n  mutate(\n    subject = as.integer(subject),\n    group = factor(group, labels = c(\"ctrl\", \"trt\")),\n    time = as.integer(time)\n  )\n\ngeeglm(\n  pain ~ time + group + time:group, \n  id = subject, \n  corstr = \"ar1\", \n  data = data\n) |&gt; \n  summary()\n\n#&gt; \n#&gt; Call:\n#&gt; geeglm(formula = pain ~ time + group + time:group, data = data, \n#&gt;     id = subject, corstr = \"ar1\")\n#&gt; \n#&gt;  Coefficients:\n#&gt;               Estimate  Std.err     Wald Pr(&gt;|W|)    \n#&gt; (Intercept)    2.48042  0.05591 1967.895  &lt; 2e-16 ***\n#&gt; time          -0.47973  0.02232  462.027  &lt; 2e-16 ***\n#&gt; grouptrt      -0.26915  0.09328    8.325  0.00391 ** \n#&gt; time:grouptrt  0.04365  0.03478    1.575  0.20946    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Correlation structure = ar1 \n#&gt; Estimated Scale Parameters:\n#&gt; \n#&gt;             Estimate Std.err\n#&gt; (Intercept)   0.4798  0.0302\n#&gt;   Link = identity \n#&gt; \n#&gt; Estimated Correlation Parameters:\n#&gt;       Estimate Std.err\n#&gt; alpha   0.5355 0.03477\n#&gt; Number of clusters:   149  Maximum cluster size: 5\nordgee(\n  ordered(pain) ~ time + group + time:group, \n  id = subject, \n  corstr = \"independence\", \n  data = data\n) |&gt; \n  summary()\n\n#&gt; \n#&gt; Call:\n#&gt; ordgee(formula = ordered(pain) ~ time + group + time:group, id = subject, \n#&gt;     data = data, corstr = \"independence\")\n#&gt; \n#&gt; Mean Model:\n#&gt;  Mean Link:                 logit \n#&gt;  Variance to Mean Relation: binomial \n#&gt; \n#&gt;  Coefficients:\n#&gt;                 estimate san.se wald p\n#&gt; Inter:0       -4.907e+01      0  Inf 0\n#&gt; Inter:1       -6.345e+01      0  Inf 0\n#&gt; Inter:2       -7.960e+01      0  Inf 0\n#&gt; Inter:3       -4.485e+15      0  Inf 0\n#&gt; time           2.493e+00      0  Inf 0\n#&gt; grouptrt      -1.437e+02      0  Inf 0\n#&gt; time:grouptrt  3.264e+01      0  Inf 0\n#&gt; \n#&gt; Scale is fixed.\n#&gt; \n#&gt; Correlation Model:\n#&gt;  Correlation Structure:     independence \n#&gt; \n#&gt; Returned Error Value:    0 \n#&gt; Number of clusters:   149   Maximum cluster size: 5\n模型分析了时间、处理以及时间和处理的交互作用对结果变量（疼痛程度）的影响：\n• Estimate: 2.565 • Standard Error: 0.055 • Statistic: 2,182.131 • p-value: 0.000\nInterpretation: The intercept represents the estimated average level of the response variable pain when both time and group are at their reference levels (e.g., at the baseline time point and in the control group). The p-value indicates that this estimate is highly statistically significant, meaning there’s strong evidence that the intercept is different from zero.\n• Estimate: -0.480 • Standard Error: 0.023 • Statistic: 422.960 • p-value: 0.000\nInterpretation: The coefficient for time represents the change in the response variable pain for each unit increase in time (e.g., for each additional time point). The negative estimate suggests that pain decreases over time. This effect is also highly statistically significant (p-value = 0.000).\n• Estimate: -0.304 • Standard Error: 0.090 • Statistic: 11.386 • p-value: 0.001\nInterpretation: The coefficient for group (experimental group) indicates the difference in the response variable pain between the experimental group and the reference group (likely the control group). The negative estimate suggests that pain is lower in the experimental group compared to the control group. This difference is statistically significant (p-value = 0.001).\n• Estimate: 0.038 • Standard Error: 0.035 • Statistic: 1.181 • p-value: 0.277\nInterpretation: The interaction term time:group represents how the effect of time on pain differs between the experimental group and the control group. The positive estimate suggests a slight increase in pain over time in the experimental group compared to the control group. However, the p-value of 0.277 indicates that this interaction is not statistically significant, meaning there’s no strong evidence that the effect of time on pain differs between the groups.\nSummary\n• Main Effects: Both time and group have statistically significant effects on pain. Pain decreases over time and is lower in the experimental group. • Interaction: The interaction between time and group is not statistically significant, suggesting that the rate of change in pain over time does not differ significantly between the experimental and control groups.\nclm(ordered(pain) ~ time * group, data = data) |&gt; \n  summary()\n\n#&gt; formula: ordered(pain) ~ time * group\n#&gt; data:    data\n#&gt; \n#&gt;  link  threshold nobs logLik  AIC     niter max.grad cond.H \n#&gt;  logit flexible  745  -696.28 1406.57 7(0)  1.24e-08 5.8e+02\n#&gt; \n#&gt; Coefficients:\n#&gt;               Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; time           -1.4000     0.0929  -15.07   &lt;2e-16 ***\n#&gt; grouptrt       -0.8344     0.2703   -3.09    0.002 ** \n#&gt; time:grouptrt   0.0953     0.1069    0.89    0.373    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Threshold coefficients:\n#&gt;     Estimate Std. Error z value\n#&gt; 0|1   -5.274      0.292  -18.05\n#&gt; 1|2   -3.755      0.256  -14.67\n#&gt; 2|3    0.209      0.186    1.13\n#&gt; 3|4    3.777      0.593    6.37\nThe choice between cumulative link mixed models (CLMM) and generalized estimating equations (GEE) depends on the specifics of your data and the goals of your analysis. Both methods are used for analyzing clustered or repeated measures data with ordinal outcomes, but they have different strengths and weaknesses.\nclmm(ordered(pain) ~ time * group + (1 | subject), data = data) |&gt; \n  summary()\n\n#&gt; Cumulative Link Mixed Model fitted with the Laplace approximation\n#&gt; \n#&gt; formula: ordered(pain) ~ time * group + (1 | subject)\n#&gt; data:    data\n#&gt; \n#&gt;  link  threshold nobs logLik  AIC     niter     max.grad cond.H \n#&gt;  logit flexible  745  -633.82 1283.64 484(1456) 3.41e-05 3.4e+02\n#&gt; \n#&gt; Random effects:\n#&gt;  Groups  Name        Variance Std.Dev.\n#&gt;  subject (Intercept) 2.89     1.7     \n#&gt; Number of groups:  subject 149 \n#&gt; \n#&gt; Coefficients:\n#&gt;               Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; time            -1.970      0.131  -15.03   &lt;2e-16 ***\n#&gt; grouptrt        -1.247      0.417   -2.99   0.0028 ** \n#&gt; time:grouptrt    0.123      0.122    1.01   0.3135    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Threshold coefficients:\n#&gt;     Estimate Std. Error z value\n#&gt; 0|1   -7.539      0.491  -15.37\n#&gt; 1|2   -5.397      0.420  -12.87\n#&gt; 2|3    0.252      0.295    0.85\n#&gt; 3|4    4.777      0.696    6.87\nCumulative Link Mixed Models (CLMM)\nPros:\n• Random Effects: CLMMs explicitly model random effects, which allows for subject-specific inferences. This means you can account for the variability between subjects, leading to more accurate estimates of the fixed effects. • Interpretation: CLMMs provide estimates for both fixed effects (population-level effects) and random effects (subject-level variability), which can be useful if you’re interested in understanding both individual and group-level patterns. • Likelihood-Based: CLMMs are likelihood-based models, which can provide more efficient estimates when the model assumptions are met.\nCons:\n• Computationally Intensive: CLMMs can be more computationally intensive, especially with large datasets or complex models with many random effects. • Assumptions: CLMMs assume a particular distribution (e.g., normally distributed random effects), and if this assumption is violated, the model may not perform well.\nGeneralized Estimating Equations (GEE)\nPros:\n• Robustness: GEE is robust to misspecification of the correlation structure within clusters. It focuses on estimating population-averaged effects rather than subject-specific effects, which can be more stable when model assumptions are not fully met. • Less Computationally Intensive: GEE tends to be less computationally demanding compared to CLMM, making it a better choice for large datasets. • Flexibility: GEE can handle various types of correlation structures, making it flexible for different kinds of clustered data.\nCons:\n• No Random Effects: GEE does not model random effects, which means it does not provide subject-specific inferences. It’s primarily focused on population-averaged effects. • Less Efficient with Small Samples: GEE might be less efficient with small sample sizes compared to CLMM because it does not fully account for subject-level variability.\nWhen to Choose Which:\n• Use CLMM if you are interested in subject-specific inferences, need to account for random effects, and are working with a relatively smaller dataset where computational demands are manageable. • Use GEE if you are more interested in population-averaged effects, have a large dataset, or want a method that is more robust to correlation structure misspecification.\nIn summary, if your primary interest is in understanding individual-level variability and you can handle the computational load, CLMM might be the better choice. If you’re more interested in population-level trends and want a more robust approach with less concern for the specific correlation structure, GEE would be a better option.\nordgee and geeglm are both functions in R used to fit models for clustered or repeated measures data, but they differ in the types of data they handle and the specific models they fit.\nordgee (Ordinal Generalized Estimating Equations)\n• Purpose: ordgee is specifically designed for fitting Generalized Estimating Equations (GEE) models to ordinal response data. This is useful when your outcome variable is ordinal (e.g., Likert scale data, ordered categorical data) and you have repeated measures or clustered data. • Model: ordgee accounts for the ordinal nature of the data by using a cumulative logit link function or other appropriate link functions for ordinal outcomes. • Usage: It’s particularly used in situations where the response variable has a natural order but the distances between the categories are not assumed to be equal. It extends the GEE framework to handle the special case of ordinal outcomes. • Example Scenario: If you’re analyzing patient satisfaction scores (e.g., 1 to 5) collected at multiple time points for different treatment groups, ordgee would be appropriate.\ngeeglm (Generalized Estimating Equations for Generalized Linear Models)\n• Purpose: geeglm is a more general function that fits Generalized Estimating Equations (GEE) models for a wide variety of outcome distributions (e.g., Gaussian, binomial, Poisson). It is not restricted to ordinal data and can handle any type of response variable (continuous, binary, count, etc.) that can be modeled with a generalized linear model (GLM). • Model: geeglm allows you to specify the link function and the error distribution appropriate for your data (e.g., logit for binary outcomes, identity for continuous outcomes). It estimates population-averaged effects rather than subject-specific effects. • Usage: It’s used for a broad range of data types and is versatile for many different kinds of clustered or repeated measures data. • Example Scenario: If you’re analyzing binary outcomes (e.g., success/failure) collected at multiple time points across different clusters (e.g., hospitals), geeglm would be the appropriate choice.\nKey Differences:\nWhen to Use Which:\n• Use ordgee when you have ordinal response data and need to account for repeated measures or clustering. • Use geeglm when your response variable is not ordinal, or you are dealing with binary, continuous, or count data, and you need to account for repeated measures or clustering.",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Ordered categorical data</span>"
    ]
  },
  {
    "objectID": "ordered-cat-data.html#prerequisites",
    "href": "ordered-cat-data.html#prerequisites",
    "title": "18  Ordered categorical data",
    "section": "",
    "text": "Example 1: \nA researcher randomly selected 149 patients who used a pain relief pump after gynecological abdominal surgery. The subjects were randomly divided them into a control group and an treatment group, with different postoperative care methods applied. The researcher observed the patients’ pain at five different time points and assessed the pain levels using a numerical rating scale. The question is whether the two methods differ in their effectiveness in relieving the patients’ pain.\n\n\n  Download data \n\n\n\n\n\n\n\n\n\n(Intercept)\n\n\n\n\ntime\n\n\n\n\ngroup (实验组)\n\n\n\n\ntime:group (时间和实验组的交互作用)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOutcome Type: • ordgee: Specifically for ordinal outcomes. • geeglm: For a wide range of outcome types (continuous, binary, count, etc.).\nLink Functions: • ordgee: Uses link functions appropriate for ordinal data (e.g., cumulative logit). • geeglm: Allows specification of various link functions depending on the distribution of the response variable.\nFlexibility: • ordgee: More specialized, focusing on ordinal outcomes. • geeglm: More flexible, applicable to various types of response variables.\nInterpretation: • ordgee: Focuses on the odds of being in a higher versus lower category of the ordinal outcome. • geeglm: Focuses on population-averaged effects across the levels of the outcome.",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Ordered categorical data</span>"
    ]
  },
  {
    "objectID": "quarto.html",
    "href": "quarto.html",
    "title": "19  Quarto",
    "section": "",
    "text": "19.1 GitHub Action\n呈现和发布内容有几种不同的方法。下面，我们将提供一个使用 GitHub Actions 和 GitHub Pages 发布内容的指南。",
    "crumbs": [
      "Communication",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "quarto.html#github-action",
    "href": "quarto.html#github-action",
    "title": "19  Quarto",
    "section": "",
    "text": "19.1.1 执行代码\n如果你愿意，可以配置一个 GitHub Action 来执行 R 代码作为渲染的一部分。这虽然是最好的方法，但当在 GitHub Actions 等 CI 服务中执行代码时，请考虑以下要求:\n\n你需要在 CI 环境中重新配置所有依赖项（包括 R 以及所需的正确版本的包）。\n如果你的代码需要任何特殊权限（例如访问数据库或网络）， CI 服务器上也需要具备这些权限。\n你的项目可能包含无法轻易执行的文档（例如使用旧版包的几年前的博客文章）。这些文档可能需要单独启用冻结功能，以防止它们在 CI 上执行。\n\n\n\n19.1.2 先决条件\n确保代码可以在 GitHub Action 中执行的最佳方法是为项目使用 renv 虚拟环境。以下是一个完整的 GitHub Action 示例，它安装 R 和 renv.lock 中的包依赖项，然后执行代码并将输出渲染到 GitHub Pages：\n\n\n\n\n\n\nTip\n\n\n\n在 RStudio 左下窗口的 Terminal 选项卡中依次运行以下 Git 命令：\n\ngit remote add origin https://github.com/qbgaoo/r4ms.git\ngit branch -M main\ngit push -u origin main\n\n\n\n在 Quarto 项目中新建文本文件，命名为 publish.yml（当然也可以是其他命字），保存路径为 .github/workflows/publish.yml，在文件中添加如下内容：\non:\n  push:\n    branches: main\n  pull_request:\n    branches: main\n  # to be able to trigger a manual build\n  workflow_dispatch:\n  schedule:\n    # run every day at 11 PM\n    - cron: '0 23 * * *'\n\nname: Render and deploy Book to Github\n\njobs:\n  build-deploy:\n    runs-on: ubuntu-latest\n    permissions:\n      contents: write\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v4\n\n      - name: Set up Quarto\n        uses: quarto-dev/quarto-actions/setup@v2\n\n      - name: Install R\n        uses: r-lib/actions/setup-r@v2\n        with:\n          r-version: '4.4.1'\n\n      - name: Install R Dependencies\n        uses: r-lib/actions/setup-renv@v2\n        with:\n          cache-version: 1\n\n      - name: Render and Publish\n        uses: quarto-dev/quarto-actions/publish@v2\n        with:\n          target: gh-pages\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n这是一个 GitHub Actions 工作流配置文件，保存后将文件提交到 GitHub中，触发文件中的工作流。",
    "crumbs": [
      "Communication",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Quarto</span>"
    ]
  },
  {
    "objectID": "pilot-project.html",
    "href": "pilot-project.html",
    "title": "20  A case to analyze",
    "section": "",
    "text": "20.1 Datasets\nIn this book, we used publicly available CDISC pilot study data, which is accessible through the CDISC GitHub repository. To streamline the process, we have downloaded all the datasets from the repository and converted them from .xpt format to .sas7bdat format for ease of use and compatibility. Then we stored them in the data/adam/ folder within this project. Additionally, The dataset structure adheres to the CDISC Analysis Data Model (ADaM) standard.\nThe SDTM-ADaM Pilot Project datasets were created to demonstrate the process of converting clinical trial data into formats that comply with the Study Data Tabulation Model (SDTM) and Analysis Data Model (ADaM) standards, which are set by the Clinical Data Interchange Standards Consortium (CDISC). These datasets are used to test, validate, and illustrate how to implement CDISC standards in real-world scenarios, helping pharmaceutical companies and regulatory agencies like the FDA ensure data quality and consistency in clinical trials.\nlibrary(haven)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nadsl &lt;- read_sas(\"data-adam/adsl.sas7bdat\")\n\nadsl |&gt; \n  select(ARM, ITTFL, EFFFL) |&gt; \n  filter(EFFFL == \"Y\") |&gt; \n  group_by(ARM) |&gt; \n  summarise(n = n())\n\n# A tibble: 3 × 2\n  ARM                      n\n  &lt;chr&gt;                &lt;int&gt;\n1 Placebo                 79\n2 Xanomeline High Dose    74\n3 Xanomeline Low Dose     81\nadsl |&gt; \n  glimpse()\n\nRows: 254\nColumns: 48\n$ STUDYID  &lt;chr&gt; \"CDISCPILOT01\", \"CDISCPILOT01\", \"CDISCPILOT01\", \"CDISCPILOT01…\n$ USUBJID  &lt;chr&gt; \"01-701-1015\", \"01-701-1023\", \"01-701-1028\", \"01-701-1033\", \"…\n$ SUBJID   &lt;chr&gt; \"1015\", \"1023\", \"1028\", \"1033\", \"1034\", \"1047\", \"1097\", \"1111…\n$ SITEID   &lt;chr&gt; \"701\", \"701\", \"701\", \"701\", \"701\", \"701\", \"701\", \"701\", \"701\"…\n$ SITEGR1  &lt;chr&gt; \"701\", \"701\", \"701\", \"701\", \"701\", \"701\", \"701\", \"701\", \"701\"…\n$ ARM      &lt;chr&gt; \"Placebo\", \"Placebo\", \"Xanomeline High Dose\", \"Xanomeline Low…\n$ TRT01P   &lt;chr&gt; \"Placebo\", \"Placebo\", \"Xanomeline High Dose\", \"Xanomeline Low…\n$ TRT01PN  &lt;dbl&gt; 0, 0, 81, 54, 81, 0, 54, 54, 54, 0, 0, 81, 81, 81, 0, 81, 81,…\n$ TRT01A   &lt;chr&gt; \"Placebo\", \"Placebo\", \"Xanomeline High Dose\", \"Xanomeline Low…\n$ TRT01AN  &lt;dbl&gt; 0, 0, 81, 54, 81, 0, 54, 54, 54, 0, 0, 81, 81, 81, 0, 81, 81,…\n$ TRTSDT   &lt;date&gt; 2014-01-02, 2012-08-05, 2013-07-19, 2014-03-18, 2014-07-01, …\n$ TRTEDT   &lt;date&gt; 2014-07-02, 2012-09-01, 2014-01-14, 2014-03-31, 2014-12-30, …\n$ TRTDUR   &lt;dbl&gt; 182, 28, 180, 14, 183, 26, 190, 10, 55, 182, 183, 183, 38, 18…\n$ AVGDD    &lt;dbl&gt; 0.0, 0.0, 77.7, 54.0, 76.9, 0.0, 54.0, 54.0, 54.0, 0.0, 0.0, …\n$ CUMDOSE  &lt;dbl&gt; 0, 0, 13986, 756, 14067, 0, 10260, 540, 2970, 0, 0, 14121, 26…\n$ AGE      &lt;dbl&gt; 63, 64, 71, 74, 77, 85, 68, 81, 84, 52, 84, 81, 75, 57, 79, 5…\n$ AGEGR1   &lt;chr&gt; \"&lt;65\", \"&lt;65\", \"65-80\", \"65-80\", \"65-80\", \"&gt;80\", \"65-80\", \"&gt;80…\n$ AGEGR1N  &lt;dbl&gt; 1, 1, 2, 2, 2, 3, 2, 3, 3, 1, 3, 3, 2, 1, 2, 1, 2, 2, 2, 3, 2…\n$ AGEU     &lt;chr&gt; \"YEARS\", \"YEARS\", \"YEARS\", \"YEARS\", \"YEARS\", \"YEARS\", \"YEARS\"…\n$ RACE     &lt;chr&gt; \"WHITE\", \"WHITE\", \"WHITE\", \"WHITE\", \"WHITE\", \"WHITE\", \"WHITE\"…\n$ RACEN    &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1…\n$ SEX      &lt;chr&gt; \"F\", \"M\", \"M\", \"M\", \"F\", \"F\", \"M\", \"F\", \"M\", \"M\", \"M\", \"F\", \"…\n$ ETHNIC   &lt;chr&gt; \"HISPANIC OR LATINO\", \"HISPANIC OR LATINO\", \"NOT HISPANIC OR …\n$ SAFFL    &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"…\n$ ITTFL    &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"…\n$ EFFFL    &lt;chr&gt; \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"Y\", \"…\n$ COMP8FL  &lt;chr&gt; \"Y\", \"N\", \"Y\", \"N\", \"Y\", \"N\", \"Y\", \"N\", \"Y\", \"Y\", \"Y\", \"Y\", \"…\n$ COMP16FL &lt;chr&gt; \"Y\", \"N\", \"Y\", \"N\", \"Y\", \"N\", \"Y\", \"N\", \"N\", \"Y\", \"Y\", \"Y\", \"…\n$ COMP24FL &lt;chr&gt; \"Y\", \"N\", \"Y\", \"N\", \"Y\", \"N\", \"Y\", \"N\", \"N\", \"Y\", \"Y\", \"Y\", \"…\n$ DISCONFL &lt;chr&gt; \"\", \"Y\", \"\", \"Y\", \"\", \"Y\", \"\", \"Y\", \"Y\", \"\", \"\", \"\", \"Y\", \"\",…\n$ DSRAEFL  &lt;chr&gt; \"\", \"Y\", \"\", \"\", \"\", \"Y\", \"\", \"Y\", \"Y\", \"\", \"\", \"\", \"Y\", \"\", …\n$ DTHFL    &lt;chr&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"…\n$ BMIBL    &lt;dbl&gt; 25.1, 30.4, 31.4, 28.8, 26.1, 30.4, 27.3, 23.9, 23.9, 21.9, 2…\n$ BMIBLGR1 &lt;chr&gt; \"25-&lt;30\", \"&gt;=30\", \"&gt;=30\", \"25-&lt;30\", \"25-&lt;30\", \"&gt;=30\", \"25-&lt;30…\n$ HEIGHTBL &lt;dbl&gt; 147.3, 162.6, 177.8, 175.3, 154.9, 148.6, 168.9, 158.2, 181.6…\n$ WEIGHTBL &lt;dbl&gt; 54.4, 80.3, 99.3, 88.5, 62.6, 67.1, 78.0, 59.9, 78.9, 71.2, 7…\n$ EDUCLVL  &lt;dbl&gt; 16, 14, 16, 12, 9, 8, 18, 22, 12, 14, 12, 10, 16, 15, 6, 16, …\n$ DISONSDT &lt;date&gt; 2010-04-30, 2006-03-11, 2009-12-16, 2009-08-02, 2011-09-29, …\n$ DURDIS   &lt;dbl&gt; 43.9, 76.4, 42.8, 55.3, 32.9, 42.0, 99.1, 40.7, 101.9, 44.2, …\n$ DURDSGR1 &lt;chr&gt; \"&gt;=12\", \"&gt;=12\", \"&gt;=12\", \"&gt;=12\", \"&gt;=12\", \"&gt;=12\", \"&gt;=12\", \"&gt;=12…\n$ VISIT1DT &lt;date&gt; 2013-12-26, 2012-07-22, 2013-07-11, 2014-03-10, 2014-06-24, …\n$ RFSTDTC  &lt;chr&gt; \"2014-01-02\", \"2012-08-05\", \"2013-07-19\", \"2014-03-18\", \"2014…\n$ RFENDTC  &lt;chr&gt; \"2014-07-02\", \"2012-09-02\", \"2014-01-14\", \"2014-04-14\", \"2014…\n$ VISNUMEN &lt;dbl&gt; 12, 5, 12, 5, 12, 6, 12, 4, 8, 12, 12, 12, 7, 12, 12, 7, 4, 7…\n$ RFENDT   &lt;date&gt; 2014-07-02, 2012-09-02, 2014-01-14, 2014-04-14, 2014-12-30, …\n$ DCDECOD  &lt;chr&gt; \"COMPLETED\", \"ADVERSE EVENT\", \"COMPLETED\", \"STUDY TERMINATED …\n$ DCREASCD &lt;chr&gt; \"Completed\", \"Adverse Event\", \"Completed\", \"Sponsor Decision\"…\n$ MMSETOT  &lt;dbl&gt; 23, 23, 23, 23, 21, 23, 10, 23, 20, 20, 19, 21, 22, 21, 10, 1…\nglimpse() makes it possible to see every column in a data frame. It’s a little like str() applied to a data frame but it tries to show you as much data as possible.\nOnce you read data in, the first step usually involves transforming it in some way to make it easier to work with in the rest of your analysis. Firstly, we use janitor::clean_names() to turn all column names of data frame adsl into snake case.",
    "crumbs": [
      "Communication",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>A case to analyze</span>"
    ]
  },
  {
    "objectID": "memo.html",
    "href": "memo.html",
    "title": "21  Memo for future programming",
    "section": "",
    "text": "Here is a typical example for generating data using rep() function and sample() function.\n\nset.seed(123)\n# a data fraome to store the simulation data\ndf &lt;- tibble(\n  antibiotic = rep(c(\"A\", \"B\"), each = 100),\n  infection_type = rep(c(\"X\", \"Y\"), times = 100),\n  recovered = sample(c(T, F), 200, replace = T, prob = c(0.7, 0.3))\n)\n# create the contingency table\ncontingency_table &lt;- table(df$antibiotic, df$infection_type, df$recovered)",
    "crumbs": [
      "Communication",
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Memo for future programming</span>"
    ]
  },
  {
    "objectID": "nonparametric-test.html#kruskal-wallis-test",
    "href": "nonparametric-test.html#kruskal-wallis-test",
    "title": "12  Nonparametric Tests",
    "section": "12.4 Kruskal-Wallis test",
    "text": "12.4 Kruskal-Wallis test\nThe Kruskal-Wallis test is a non-parametric method used to compare more than two independent samples to determine if they come from the same distribution. It is an extension of the Mann-Whitney U test (or Wilcoxon rank-sum test) to more than two groups. The Kruskal-Wallis test is useful when the assumptions of one-way ANOVA (such as normality and homogeneity of variance) are not met.\nThe Kruskal-Wallis test statistic H is calculated using the following formula:\n\nH = \\frac{12}{N(N+1)} \\sum_{i=1}^k \\frac{R_i^2}{n_i} - 3(N+1)\n\nWhere N is the total number of observations, k is the number of groups, R_i is the sum of ranks for the i-th group, n_i is the number of observations in the i-th group.\n\n12.4.1 Raw data\n\nExample 5: \nUsing three types of drugs, A, B, and C, to kill snails, 200 live snails were used for each batch. After applying the drugs, the number of dead snails in each batch was counted, and the mortality rate (%) was calculated. The data file can be downloaded from the button below. Determine if there is a significant difference in the effectiveness of the three drugs in killing snails.\n\n\n  Download data \n\n\ndf &lt;- read_csv(\n  \"datasets/ex12-05.csv\",\n  col_types = list(\n    drug = col_factor(),\n    mortality = col_double()\n  )\n)\nstr(df)\n\n#&gt; spc_tbl_ [15 × 2] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n#&gt;  $ drug     : Factor w/ 3 levels \"1\",\"2\",\"3\": 1 1 1 1 1 2 2 2 2 2 ...\n#&gt;  $ mortality: num [1:15] 32.5 35.5 40.5 46 49 16 20.5 22.5 29 36 ...\n#&gt;  - attr(*, \"spec\")=\n#&gt;   .. cols(\n#&gt;   ..   drug = col_factor(levels = NULL, ordered = FALSE, include_na = FALSE),\n#&gt;   ..   mortality = col_double()\n#&gt;   .. )\n#&gt;  - attr(*, \"problems\")=&lt;externalptr&gt;\n\n\n\nkruskal.test(mortality ~ drug, data = df)\n\n#&gt; \n#&gt;  Kruskal-Wallis rank sum test\n#&gt; \n#&gt; data:  mortality by drug\n#&gt; Kruskal-Wallis chi-squared = 9.74, df = 2, p-value = 0.007673\n\n\n\n\n12.4.2 Frequency table data and ordinal data\n\nExample 6: \nUsing three types of drugs, A, B, and C, to kill snails, 200 live snails were used for each batch. After applying the drugs, the number of dead snails in each batch was counted, and the mortality rate (%) was calculated. The data file can be downloaded from the button below. Determine if there is a significant difference in the effectiveness of the three drugs in killing snails.\n\n\n  Download data \n\n\ndf &lt;- read_csv(\n  \"datasets/ex12-06.csv\",\n  col_types = list(\n    group = col_factor(), \n    response = col_integer(),\n    freq = col_integer()\n  )\n)\nstr(df)\n\n#&gt; spc_tbl_ [12 × 3] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n#&gt;  $ group   : Factor w/ 3 levels \"1\",\"2\",\"3\": 1 1 1 1 2 2 2 2 3 3 ...\n#&gt;  $ response: int [1:12] 1 2 3 4 1 2 3 4 1 2 ...\n#&gt;  $ freq    : int [1:12] 49 31 5 15 45 9 22 4 15 28 ...\n#&gt;  - attr(*, \"spec\")=\n#&gt;   .. cols(\n#&gt;   ..   group = col_factor(levels = NULL, ordered = FALSE, include_na = FALSE),\n#&gt;   ..   response = col_integer(),\n#&gt;   ..   freq = col_integer()\n#&gt;   .. )\n#&gt;  - attr(*, \"problems\")=&lt;externalptr&gt;\n\n\n\ndf |&gt; \n  uncount(freq) |&gt; \n  kruskal.test(response ~ group, data = _)\n\n#&gt; \n#&gt;  Kruskal-Wallis rank sum test\n#&gt; \n#&gt; data:  response by group\n#&gt; Kruskal-Wallis chi-squared = 20.458, df = 2, p-value = 3.612e-05\n\n\n\n\n12.4.3 Nemenyi test\nThe Nemenyi test is a post-hoc test used to compare multiple groups after a non-parametric test, such as the Kruskal-Wallis test has shown a significant difference among groups. It is specifically designed for multiple comparisons between groups to determine which pairs of groups differ significantly from each other.\n\nExample 7: \nA study is to compare the survival days of mice after being inoculated with three different strains of typhoid bacillus. The data can be downloaded from the button below. The question is whether there is a difference in the survival days of mice inoculated with these three different strains of typhoid bacillus.\n\n\n  Download data \n\n\ndf &lt;- read_csv(\n  \"datasets/ex12-07.csv\",\n  col_types = list(\n    group = col_factor(), \n    time = col_integer()\n  )\n)\nstr(df)\n\n#&gt; spc_tbl_ [30 × 2] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n#&gt;  $ group: Factor w/ 3 levels \"1\",\"2\",\"3\": 1 1 1 1 1 1 1 1 1 1 ...\n#&gt;  $ time : int [1:30] 2 2 2 3 4 4 4 5 7 7 ...\n#&gt;  - attr(*, \"spec\")=\n#&gt;   .. cols(\n#&gt;   ..   group = col_factor(levels = NULL, ordered = FALSE, include_na = FALSE),\n#&gt;   ..   time = col_integer()\n#&gt;   .. )\n#&gt;  - attr(*, \"problems\")=&lt;externalptr&gt;\n\n\n\nkruskal.test(time ~ group, data = df)\n\n#&gt; \n#&gt;  Kruskal-Wallis rank sum test\n#&gt; \n#&gt; data:  time by group\n#&gt; Kruskal-Wallis chi-squared = 9.9405, df = 2, p-value = 0.006941\n\n\n\nNemenyiTest(time ~ group, data =df)\n\n#&gt; \n#&gt;  Nemenyi's test of multiple comparisons for independent samples (tukey)  \n#&gt; \n#&gt;     mean.rank.diff   pval    \n#&gt; 2-1     10.3777778 0.0278 *  \n#&gt; 3-1     10.8727273 0.0131 *  \n#&gt; 3-2      0.4949495 0.9914    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Nonparametric Tests</span>"
    ]
  },
  {
    "objectID": "nonparametric-test.html#friedman-test",
    "href": "nonparametric-test.html#friedman-test",
    "title": "12  Nonparametric Tests",
    "section": "12.5 Friedman test",
    "text": "12.5 Friedman test\nThe Friedman test is a non-parametric statistical test used to detect differences in treatments across multiple test attempts. It’s particularly useful for comparing three or more paired groups when the data is not normally distributed. The test is an alternative to the repeated measures ANOVA and is often applied in cases where the same subjects are used for each treatment.\n\nExample 8: \nEight subjects were exposed to four different sound frequencies under the same conditions and their response rates (%) were measured. The data can be downloaded from the button below. Is there a difference in their response rates to the four different sound frequencies?\n\n\n  Download data \n\n\ndf &lt;- read_csv(\"datasets/ex12-08.csv\", show_col_types = F)\nstr(df)\n\n#&gt; spc_tbl_ [8 × 4] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n#&gt;  $ freq_a: num [1:8] 8.4 11.6 9.4 9.8 8.3 8.6 8.9 7.8\n#&gt;  $ freq_b: num [1:8] 9.6 12.7 9.1 8.7 8 9.8 9 8.2\n#&gt;  $ freq_c: num [1:8] 9.8 11.8 10.4 9.9 8.6 9.6 10.6 8.5\n#&gt;  $ freq_d: num [1:8] 11.7 12 9.8 12 8.6 10.6 11.4 10.8\n#&gt;  - attr(*, \"spec\")=\n#&gt;   .. cols(\n#&gt;   ..   freq_a = col_double(),\n#&gt;   ..   freq_b = col_double(),\n#&gt;   ..   freq_c = col_double(),\n#&gt;   ..   freq_d = col_double()\n#&gt;   .. )\n#&gt;  - attr(*, \"problems\")=&lt;externalptr&gt;\n\n\nFirst, perform the Friedman test to determine if there are significant differences among groups using friedman.test() function.\n\ndf |&gt; \n  as.matrix() |&gt; \n  friedman.test()\n\n#&gt; \n#&gt;  Friedman rank sum test\n#&gt; \n#&gt; data:  as.matrix(df)\n#&gt; Friedman chi-squared = 15.152, df = 3, p-value = 0.001691\n\n\nHere is an alternative method based on the long format data.\n\ndf |&gt; \n  mutate(subject = c(1:8)) |&gt; \n  pivot_longer(\n    cols = contains(\"freq\"),\n    names_to = \"group\",\n    values_to = \"response\",\n    names_ptypes = list(group = factor())\n  ) |&gt; \n  friedman.test(response ~ group | subject, data = _)\n\n#&gt; \n#&gt;  Friedman rank sum test\n#&gt; \n#&gt; data:  response and group and subject\n#&gt; Friedman chi-squared = 15.152, df = 3, p-value = 0.001691\n\n\nAfter identifying significant differences with the Friedman test, you can use frdAllPairsNemenyiTest() to conduct pairwise comparisons. The test allows you to determine which pairs of groups are significantly different from each other, with multiple comparison corrections.\n\ndf |&gt; \n  mutate(subject = c(1:8)) |&gt; \n  pivot_longer(\n    cols = contains(\"freq\"),\n    names_to = \"group\",\n    values_to = \"response\",\n    names_ptypes = list(group = factor())\n  ) |&gt; \n  frdAllPairsNemenyiTest(response ~ group | subject, data = _) |&gt; \n  summary()\n#&gt; \n#&gt;  Pairwise comparisons using Nemenyi-Wilcoxon-Wilcox all-pairs test for a two-way balanced complete block design\n#&gt; data: response and group and subject\n#&gt; P value adjustment method: single-step\n#&gt; H0\n#&gt;                      q value  Pr(&gt;|q|)   \n#&gt; freq_b - freq_a == 0   1.369 0.7675424   \n#&gt; freq_c - freq_a == 0   3.423 0.0732221  .\n#&gt; freq_d - freq_a == 0   5.066 0.0019311 **\n#&gt; freq_c - freq_b == 0   2.054 0.4666088   \n#&gt; freq_d - freq_b == 0   3.697 0.0442915  *\n#&gt; freq_d - freq_c == 0   1.643 0.6509544\n#&gt; ---\n#&gt; Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Nonparametric Tests</span>"
    ]
  },
  {
    "objectID": "nonparametric-test.html#spearmans-rank-correlation",
    "href": "nonparametric-test.html#spearmans-rank-correlation",
    "title": "12  Nonparametric Tests",
    "section": "12.6 Spearman’s rank correlation",
    "text": "12.6 Spearman’s rank correlation\nMeasures the strength and direction of association between two ranked variables. Alternative to Pearson’s correlation when data are not linear or normally distributed.\n\nChi-Square Test: • Used for categorical data to test the association between two variables in a contingency table. • A nonparametric test that examines frequencies or counts.\nKolmogorov-Smirnov Test: • Used to compare a sample distribution with a reference probability distribution or to compare two sample distributions. • Often applied to assess goodness-of-fit.\n\nThese nonparametric tests provide flexibility in data analysis when the assumptions of parametric tests are violated.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Nonparametric Tests</span>"
    ]
  },
  {
    "objectID": "nonparametric-test.html#kolmogorov-smirnov-test",
    "href": "nonparametric-test.html#kolmogorov-smirnov-test",
    "title": "12  Nonparametric Tests",
    "section": "12.6 Kolmogorov-Smirnov Test",
    "text": "12.6 Kolmogorov-Smirnov Test\nThe Kolmogorov-Smirnov (K-S) test is a nonparametric test used to compare a sample distribution with a reference probability distribution or to compare two sample distributions. It assesses whether the samples come from the same distribution or not. The test is based on the maximum distance between the empirical distribution function of the sample and the cumulative distribution function of the reference distribution.\n\n12.6.1 One-sample K-S test\nCompares a sample to a known theoretical distribution (e.g., normal, uniform).\n\n\n12.6.2 Two-sample K-S test\nCompares two independent samples to determine if they come from the same distribution. • Null hypothesis: The two samples come from the same distribution.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Nonparametric Tests</span>"
    ]
  },
  {
    "objectID": "corr-and-reg.html",
    "href": "corr-and-reg.html",
    "title": "13  Correlation and regression",
    "section": "",
    "text": "13.1 Prerequisite\nlibrary(tidyverse)",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Correlation and regression</span>"
    ]
  },
  {
    "objectID": "reg-and-corr.html",
    "href": "reg-and-corr.html",
    "title": "13  Bivariate regression and correlation",
    "section": "",
    "text": "13.1 Prerequisite\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(patchwork)",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Bivariate regression and correlation</span>"
    ]
  },
  {
    "objectID": "reg-and-corr.html#simple-linear-regression",
    "href": "reg-and-corr.html#simple-linear-regression",
    "title": "13  Bivariate regression and correlation",
    "section": "13.2 Simple linear regression",
    "text": "13.2 Simple linear regression\n\n13.2.1 Key concepts\nSimple regression analysis involves modeling the relationship between a dependent variable and an independent variable. Generally, this relationship is formulated as a simple linear regression model:\n\nY = \\beta_0 + \\beta_1 X + \\epsilon\n\nwhere Y is the dependent (response) variable, X is the independent (predictor) variable, \\beta_0 is the intercept (value of Y when X is 0), \\beta_1 is the slope (the change in Y for each one-unit change in X), \\epsilon is the error term (difference between observed and predicted values).\nThe goal of regression analysis is to estimate the coefficients \\beta_0 and \\beta_1, fitting a linear equation to the data. It allows for predicting the value of Y based on a given value of X. Here is an example to illustrate the process of linear regression.\n\nExample 1: \nTo determine the relationship between a patient’s age and their systolic blood pressure, a researcher gathered data from a sample of patients, including their age and systolic blood pressure readings. The data can be downloaded from the button below, where age is the independent variable (X) and systolic blood pressure is the dependent variable (Y).\n\n\n  Download data \n\n\ndf &lt;- read_csv(\"datasets/ex13-01.csv\", show_col_types = F)\nstr(df)\n\n#&gt; spc_tbl_ [10 × 2] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n#&gt;  $ age: num [1:10] 25 30 45 50 60 70 80 20 65 73\n#&gt;  $ sbp: num [1:10] 120 126 130 139 145 150 160 116 142 148\n#&gt;  - attr(*, \"spec\")=\n#&gt;   .. cols(\n#&gt;   ..   age = col_double(),\n#&gt;   ..   sbp = col_double()\n#&gt;   .. )\n#&gt;  - attr(*, \"problems\")=&lt;externalptr&gt;\n\n\nA scatter plot in regression analysis visually represents the relationship between an independent variable X and a dependent variable Y. It helps in understanding whether the data fits a linear model, which is crucial for linear regression.\n\nggplot(df, aes(x = age, y =sbp)) +\n  geom_point(size = 1) + \n  geom_smooth(method = lm, formula = 'y ~ x', se = T) +\n  labs(x = \"Age\", y = \"Systolic Blood Pressure\") +\n  theme(\n    axis.title.x = element_text(size = 9),\n    axis.title.y = element_text(size = 9)\n  )\n\n\n\n\n\n\n\n\nEach point represents a data pair, showing the relationship between X and Y. The line added fits the data using least squares linear regression. The shaded area around the regression line represents the 95% confidence interval, indicating the uncertainty around the predicted regression line.\nIf the points cluster closely around the regression line, it indicates a strong linear relationship between the variables. A narrower confidence interval suggests more precise estimates for the regression line, while a wider interval implies greater uncertainty. This method provides a visual way to understand regression analysis and assess the fit of the linear model to the data.\n\n\n13.2.2 Parameter estimation\nThe slope \\beta_1 and intercept \\beta_0 are estimated using the least squares method, which minimize the sum of squared differences between observed and predicted values of Y. They can be calculated by the following derived formulas:\n\n\\beta_1 = \\frac{\\sum{(X_i - \\bar{X})(Y_i - \\bar{Y})}}{\\sum{(X_i - \\bar{X})^2}} \n\n\\beta_0 = \\bar{Y} - \\beta_1 \\bar{X}\n\nwhere \\bar{X} and \\bar{Y} are the mean values of X and Y , respectively, and X_i, Y_i are individual data points. Once you calculate the slope \\beta_1 and intercept \\beta_0 , you can plug them into the equation Y = \\beta_0 + \\beta_1 X to create your specific regression model.\nHere we use lm() function to fit a simple linear regression model to predict systolic blood pressure based on age.\n\nmodel &lt;- lm(sbp ~ age, data = df) \nsummary(model)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = sbp ~ age, data = df)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -4.2146 -2.5833  0.2063  2.4432  3.9962 \n#&gt; \n#&gt; Coefficients:\n#&gt;              Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept) 103.79438    2.65440   39.10 2.01e-10 ***\n#&gt; age           0.65262    0.04775   13.67 7.91e-07 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 3.047 on 8 degrees of freedom\n#&gt; Multiple R-squared:  0.9589, Adjusted R-squared:  0.9538 \n#&gt; F-statistic: 186.8 on 1 and 8 DF,  p-value: 7.907e-07\n\n\nThe output of summary(model) provides you the estimated coefficients for the intercept \\beta_0 and slope \\beta_1, which can be used to write the equation for your specific data. Based on the results above on this case, we can write the regression equation as:\n\nY = 103.79438 + 0.65262 \\ X\n\n\n\n13.2.3 Statistical inference\nStatistical inference in linear regression involves making conclusions about the population parameters based on the sample data. The goal is to test hypothesis and estimate confidence intervals to determine whether there is a significant relationship between the independent and dependent variables.\n\nHypothesis test\n\nTo assess whether there’s a significant linear relationship between X and Y, we perform a hypothesis test on the slope coefficient \\beta_1:\n\nH_0: \\beta_1 = 0, no relationship between X and Y.\nH_1: \\beta_1 \\neq 0, there is a relationship between X and Y.\n\nTo test the significance of the slope (\\beta_1), the t-statistic is calculated by:\n\nt = \\frac{\\hat{\\beta}_1}{\\text{SE}_{\\hat{\\beta}_1}}\n\nwhere \\hat{\\beta}_1 is point estimate of the true coefficient \\beta_1, \\text{SE}_{\\hat{\\beta}_1} is the standard error (\\text{SE}) of the slope \\hat{\\beta}_1 .\nThe F-statistic is calculated as:\n\nF = \\frac{\\text{SSR} / 1}{\\text{SSE} / (n - 2)}\n\nwhere SSR (sum of squares due to regression) is the variation in the dependent variable explained by the predictor, SSE (sum of squares of errors) is the variation in the dependent variable that is not explained by the model, n is the number of observations.\nThe output of summary(model) will give you the t-statistic and p-value of the t-test, as well as the F-statistic and its associated p-value from the F-test. For this case, the t-statistic is 13.67, and its p-value is 7.91e\\text{-}07. The F-statistic is 186.8, its p-value is 7.907e\\text{-}07. Both test methods indicate a statistically significant relationship between the independent and dependent variable.\n\nConfidence interval\n\nA confidence interval provides a range of values within which the true population parameter (\\beta_1) is likely to lie with a confidence level \\alpha:\n\n\\hat{\\beta}_1 \\pm t_{\\alpha/2} \\ \\text{SE}(\\hat{\\beta}_1)\n\nwhere t_{\\alpha/2} is the critical value from the t-distribution.\nYou can use the confint() function to calculate the confidence interval of the regression coefficient.\n\nconfint(model)\n\n#&gt;                  2.5 %      97.5 %\n#&gt; (Intercept) 97.6733240 109.9154372\n#&gt; age          0.5425101   0.7627262\n\n\nIf the confidence interval for the slope does not contain 0, it suggests that the relationship between X and Y is statistically significant. Here the confidence interval for age is [0.6328419, 0.8334338], not contain 0, the conclusion is consistent with the hypothesis test.\nStatistical inference allows us to generalize the findings from our sample to the larger population and make conclusions about the strength and significance of relationships in linear regression.\n\n\n13.2.4 Coefficient of determination\nThe coefficient of determination, commonly denoted as R^2 , is a statistical measure used in the context of regression analysis to assess the proportion of variance in the dependent variable that is predictable from the independent variable. In simple terms, it indicates the proportion of variance in Y explained by X. A value close to 1 indicates a strong linear relationship, while a value near 0 suggests a weak relationship.\nIn the case above, the R^2 value of 0.9589 indicates that 95.89% of the variance in systolic blood pressure is explained by the patient’s age.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Bivariate regression and correlation</span>"
    ]
  },
  {
    "objectID": "reg-and-corr.html#linear-correlation",
    "href": "reg-and-corr.html#linear-correlation",
    "title": "13  Bivariate regression and correlation",
    "section": "13.3 Linear correlation",
    "text": "13.3 Linear correlation\nLinear correlation measures the strength and direction of the linear relationship between two continuous variables. The most common measure of linear correlation is the Pearson correlation coefficient, denoted by r, which ranges from -1 to +1.\n\n13.3.1 Pearson correlation coefficient\nThe Pearson correlation coefficient is calculated as:\n\nr = \\frac{\\sum{(X_i - \\bar{X})(Y_i - \\bar{Y})}}{\\sqrt{\\sum{(X_i - \\bar{X})^2} \\sum{(Y_i - \\bar{Y})^2}}}\n\nwhere X_i and Y_i are the individual data points of the two variables, \\bar{X} and \\bar{Y} are the means of the two variables.\nLinear correlation, particularly using the Pearson correlation coefficient, helps quantify the strength and direction of a linear relationship between two continuous variables. Here are some plots with different values of r.\n\nset.seed(200)\n\n# Generating data, given sample size and r\ngenerate_data &lt;- function(n, r) {\n  x &lt;- rnorm(n)\n  y &lt;- r * x + sqrt(1 - r^2) * rnorm(n)\n  tibble(x = x, y = y)\n}\n\nn &lt;- 200\nr_values &lt;- c(1, 0.9, 0.6, -1, -0.9, -0.6)\n\nlapply(\n  r_values, \n  \\(r.value) {\n    ggplot(generate_data(n, r.value), aes(x = x, y = y)) +\n    geom_point(size = 0.2, color = if_else(r.value &gt; 0, \"blue\", \"#A039A0\")) +\n    ggtitle(paste(\"r = \", r.value))\n  }\n) |&gt; \n  wrap_plots(ncol = 3)\n\n\n\n\n\n\n\n\nBefore conducting correlation and regression analysis, it is crucial to plot a scatterplot. A scatterplot helps to visually examine the relationship between two variables, validate assumptions for analysis, determine if a linear relationship exists, and identify any outliers. The dataset of Anscombe’s quartet was created by the statistician Francis Anscombe to illustrate the importance of graphing data before analyzing it. This dataset that consists of four sets of data with nearly identical simple descriptive statistics (mean, variance, correlation coefficient, etc.), but they have very different distributions and visual characteristics, as shown in Figure 13.1 .\n\ndata(anscombe)\nanscombe_long &lt;- anscombe |&gt; \n  pivot_longer(\n    cols = everything(),\n    cols_vary = \"slowest\",\n    names_to = c(\".value\", \"data\"),\n    names_pattern = \"(.)(.)\"\n  )\n\n# Plot all four datasets\nggplot(anscombe_long, aes(x = x, y = y)) +\n  geom_point(size = 1.2) +\n  geom_smooth(method = 'lm', formula = 'y ~ x', se = F, linewidth = 0.6) +\n  facet_wrap(~ data, scales = \"free\") \n\n\n\n\n\n\n\nFigure 13.1: Anscombe’s Quartet with same r but different distributions\n\n\n\n\n\n\nExample 2: \nAssume a dataset where we have collected BMI and cholesterol readings for a group of patients. Let’s look at the correlation between BMI and cholesterol levels in a this study.\n\n\n  Download data \n\n\ndf &lt;- read_csv(\"datasets/ex13-02.csv\", show_col_types = F)\nstr(df)\n\n#&gt; spc_tbl_ [10 × 2] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n#&gt;  $ bmi : num [1:10] 22.1 27.3 30.5 25 28.2 32.1 21.7 26.9 29 33.2\n#&gt;  $ chol: num [1:10] 180 195 210 185 200 220 175 190 205 215\n#&gt;  - attr(*, \"spec\")=\n#&gt;   .. cols(\n#&gt;   ..   bmi = col_double(),\n#&gt;   ..   chol = col_double()\n#&gt;   .. )\n#&gt;  - attr(*, \"problems\")=&lt;externalptr&gt;\n\n\n\nggplot(df, aes(bmi, chol)) +\n  geom_point(size = 1) + \n  geom_smooth(method = lm, formula = 'y ~ x', se = T) +\n  labs(\n    x = \"BMI\",\n    y = \"Cholesterol\"\n  ) +\n  theme(\n    axis.title.x = element_text(size = 9),\n    axis.title.y = element_text(size = 9)\n  )\n\n\n\n\n\n\n\n\nHere we use cor() function to calculate the correlation coefficient:\n\nwith(df, cor(bmi, chol, method = \"pearson\")) \n\n#&gt; [1] 0.9769267\n\n\n\n\n13.3.2 Statistical inference\nIn linear correlation, we not only calculate the correlation coefficient but also test its significance, create confidence intervals, and interpret the result in the context of the data.\n\nHypothesis test\n\nTo make statistical inferences about the correlation, we test the following hypotheses:\n\nH_0 : There is no linear relationship between the two variables (ρ = 0).\nH_1 : There is a linear relationship between the two variables (ρ ≠ 0 for a two-tailed test, ρ &gt; 0 or ρ &lt; 0 for a one-tailed test).\n\nCalculate the t-statistic by:\n\nt = \\frac{r \\sqrt{n - 2}}{\\sqrt{1 - r^2}}\n\n\nConfidence interval\n\nThe confidence interval provides a range of values that likely contain the population correlation ρ. The formula for the confidence interval of the correlation coefficient depends on the Fisher transformation. First apply the Fisher transformation to r:\n\nz = \\frac{1}{2} \\ln \\left( \\frac{1 + r}{1 - r} \\right)\n\nThen construct the confidence interval for z:\n\nz \\pm z_{\\alpha/2} \\frac{1}{\\sqrt{n - 3}}\n\nFinally transform it back to the correlation scale:\n\nr = \\frac{e^{2z - 1}}{e^{2z + 1}}\n\nThe cor.test() function is used to test the association between two numeric variables, and when you specify method = \"pearson\", it performs a Pearson correlation test to measure the strength and direction of the linear relationship between them.\n\ncor.test(~ bmi + chol, data = df, method = \"pearson\")\n\n#&gt; \n#&gt;  Pearson's product-moment correlation\n#&gt; \n#&gt; data:  bmi and chol\n#&gt; t = 12.938, df = 8, p-value = 1.206e-06\n#&gt; alternative hypothesis: true correlation is not equal to 0\n#&gt; 95 percent confidence interval:\n#&gt;  0.9023104 0.9947088\n#&gt; sample estimates:\n#&gt;       cor \n#&gt; 0.9769267\n\n\nThis result indicates a strong positive correlation (r = 0.9769267), with a p-value of 1.206e\\text{-}06, which suggests the correlation is statistically significant. The t-statistic and 95% confidence interval is provided as well.\n\n\n13.3.3 Some considerations\nTo ensure the estimate and inference of linear regression model are valid, the following assumptions should be met:\n\nLinearity: The relationship between X and Y must be linear.\nIndependence: Observations must be independent of each other.\nHomoscedasticity: Constant variance of residuals across all levels of X.\nNormality of Residuals: The residuals (errors) should be approximately normally distributed.\n\nTo validly use the Pearson correlation coefficient and its statistical tests, the following assumptions should be met:\n\nLinearity: The relationship between the two variables must be linear.\nNormality: The two variables should be approximately normally distributed.\nHomoscedasticity: The variability in one variable should be the same at all levels of the other variable.\n\nResidual Plots in Regression\nUsing a residual plot is a key step in examining whether the data meets the assumptions required for a linear regression model. Residual plots display the residuals on the vertical axis and the predicted values or independent variable on the horizontal axis. Here is the residual plot on the basis of the data from Example 1.\n\ndf &lt;- read_csv(\"datasets/ex13-01.csv\", show_col_types = F)\n# Fit a linear regression model\nmodel &lt;- lm(sbp ~ age, data = df)\n\n# Create a residual plot using ggplot2\nggplot(df, aes(x = fitted(model), y = resid(model))) +\n  geom_point() +\n  geom_hline(yintercept = 0, color = \"red\") +\n  xlab(\"Fitted Values\") +\n  ylab(\"Residuals\")\n\n\n\n\n\n\n\n\nIf the residuals appear randomly scattered around zero with no clear patterns, the assumptions of linearity, homoscedasticity, and independence are likely satisfied. If any patterns or issues are identified, you may need to consider transforming variables or using a different model to better fit the data.\nIf these assumptions are violated, non-parametric alternatives such as Spearman’s rank correlation can be used.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Bivariate regression and correlation</span>"
    ]
  },
  {
    "objectID": "reg-and-corr.html#rank-correlation",
    "href": "reg-and-corr.html#rank-correlation",
    "title": "13  Bivariate regression and correlation",
    "section": "13.4 Rank correlation",
    "text": "13.4 Rank correlation\nRank correlation is a measure of the relationship between two variables based on the rank ordering of the data, rather than the raw data values. It assesses how well the relationship between two variables can be described by a monotonic function, meaning as one variable increases, the other tends to increase (positive correlation) or decrease (negative correlation). Rank correlation is especially useful when dealing with ordinal or non-normally distributed data, such as in patient ratings, clinical scores.\n\n13.4.1 Spearman’s rank correlation\nSpearman’s rank correlation coefficient r_s measures the strength and direction of the association between two ranked variables. It is a non-parametric measure, meaning it does not assume a linear relationship or normally distributed data.\n\nr_s = 1 - \\frac{6 \\sum d_i^2}{n(n^2 - 1)}\n\nwhere d_i is the difference between the ranks of corresponding values and n is the sample size.\n\n\n13.4.2 Spearman’s r_s for tied ranks\nWhen there are tied ranks, the denominator of the formula needs to be adjusted because the calculation assumes unique ranks. The correction ensures the ranks are properly weighted to reflect the presence of ties, improving the reliability of the coefficient. The correction involves calculating a factor that accounts for the groups of tied values.\nThe corrected formula for Spearman’s r_s with ties is:\n\nr_s = \\frac{n(n^2 - 1) - 6 \\sum d_i^2 - T_x - T_y}{\\sqrt{(n(n^2 - 1) - T_x)(n(n^2 - 1) - T_y)}}\n\nwhere T_X and T_Y are the tie correction factors for each variable, calculated as follows:\n\nT = \\sum (t_j^3 - t_j) / 12\n\nwhere t_j is the number of tied values in the j\\text{-}th group of ties. This sum is calculated over all groups of tied ranks.\n\nExample 3: \nConsider a study where researchers want to see the correlation between pain score and mobility score in patients with arthritis. They collected the scores from 15 patients, both are ranked from 1 to 10. The data can be accessed from the button below.\n\n\n  Download data \n\n\ndf &lt;- read_csv(\n  file = \"datasets/ex13-03.csv\",\n  col_types = list(\n    pain_score = col_integer(),\n    mobi_score = col_integer()\n  )\n)\nstr(df)\n\n#&gt; spc_tbl_ [15 × 2] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n#&gt;  $ pain_score: int [1:15] 3 5 6 8 7 9 2 4 5 6 ...\n#&gt;  $ mobi_score: int [1:15] 7 6 5 4 3 2 8 7 6 5 ...\n#&gt;  - attr(*, \"spec\")=\n#&gt;   .. cols(\n#&gt;   ..   pain_score = col_integer(),\n#&gt;   ..   mobi_score = col_integer()\n#&gt;   .. )\n#&gt;  - attr(*, \"problems\")=&lt;externalptr&gt;\n\n\nThe cor() function can be used to estimate Spearman’s rank correlation coefficient.\n\nwith(df, cor(pain_score, mobi_score, method = \"spearman\"))\n\n#&gt; [1] -0.952515\n\n\nYou can also use the cor.test() function to perform Spearman’s rank correlation test with method = \"spearman\".\n\ncor.test(~ pain_score + mobi_score, data = df, method = \"spearman\")\n\n#&gt; Warning in cor.test.default(x = mf[[1L]], y = mf[[2L]], ...): Cannot compute\n#&gt; exact p-value with ties\n\n\n#&gt; \n#&gt;  Spearman's rank correlation rho\n#&gt; \n#&gt; data:  pain_score and mobi_score\n#&gt; S = 1093.4, p-value = 4.378e-08\n#&gt; alternative hypothesis: true rho is not equal to 0\n#&gt; sample estimates:\n#&gt;       rho \n#&gt; -0.952515\n\n\nIn the output a warning occurs, noting that the data has ties. To correct for ties, you can set the argument exact = FALSE, which ensures the function handles ties and applies the necessary corrections.\n\ncor.test(~ pain_score + mobi_score, data = df, method = \"spearman\", exact = F)\n\n#&gt; \n#&gt;  Spearman's rank correlation rho\n#&gt; \n#&gt; data:  pain_score and mobi_score\n#&gt; S = 1093.4, p-value = 4.378e-08\n#&gt; alternative hypothesis: true rho is not equal to 0\n#&gt; sample estimates:\n#&gt;       rho \n#&gt; -0.952515\n\n\nThe result shows a strong monotonically negative relationship between the pain_score and mobi_score (r_s = -0.952515). The p-value is smaller than significance level 0.05, reject the null hypothesis and conclude that there is a significant monotonic relationship between the two variables. The test statistic used in determining the p-value is S = 1093.4.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Bivariate regression and correlation</span>"
    ]
  },
  {
    "objectID": "reg-and-corr.html#curve-fitting",
    "href": "reg-and-corr.html#curve-fitting",
    "title": "13  Bivariate regression and correlation",
    "section": "13.5 Curve fitting",
    "text": "13.5 Curve fitting\nCurve fitting is the process of finding a curve that best fits a given set of data points. It is commonly used when the relationship between the independent variable X and dependent variable Y is not linear, and a more complex model is needed to describe the data. Curve fitting can be used in many fields, including biology and the medical sciences.\n\n13.5.1 Types of curve fitting models\n\nLinear fit: A straight line that best describes the data. The equation is typically of the form y = a + bX . Example: Simple linear regression.\nPolynomial fit: A polynomial function is used to fit the data. The equation might look like Y = aX^2 + bX + c or higher-degree polynomials. Example: Quadratic, cubic, or higher-order polynomial fits.\nExponential fit: For exponential growth or decay, the model is typically of the form Y = a \\cdot e^{bx}. Example: Growth of populations or the decay of radioactive material.\nLogarithmic fit: A logarithmic function is used, of the form Y = a \\cdot \\ln(X) + b, where \\ln(X) is the natural logarithm of X.\nPower-Law fit: This follows the form y = ax^b, where the relationship between variables is a power function.\nSigmoidal fit: Sigmoid curves are often used for biological processes that show an S-shaped response, like population growth models. A common form is Y = \\frac{a}{1 + e^{-bX}}.\n\n\nExample 4: \nAssume we have a data from a study that tests different dosages of a drug and the corresponding effect on patients’ blood pressure reduction. The data can be downloaded from the button below, where dose represents the drug dose (mg) and response represents the percentage of blood pressure reduction. Let’s model the relationship between drug dosage and the body’s response.\n\n\n  Download data \n\n\n\n13.5.2 Steps for curve fitting\n\nData preparation\nEnsure you have collected data, typically with an independent variable X and a dependent variable Y.\n\ndf &lt;- read_csv(file = \"datasets/ex13-04.csv\", show_col_types = F)\nstr(df)\n\n#&gt; spc_tbl_ [10 × 2] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n#&gt;  $ dose    : num [1:10] 10 20 30 40 50 60 70 80 90 100\n#&gt;  $ response: num [1:10] 5 12 20 32 45 55 65 72 78 82\n#&gt;  - attr(*, \"spec\")=\n#&gt;   .. cols(\n#&gt;   ..   dose = col_double(),\n#&gt;   ..   response = col_double()\n#&gt;   .. )\n#&gt;  - attr(*, \"problems\")=&lt;externalptr&gt;\n\n\n\n\n\nData visualization\nUse a scatter plot to visualize the raw data and check for trends or outliers.\n\nggplot(df, aes(x = dose, y = response)) +\n  geom_point() + \n  geom_smooth(method = 'loess', formula = 'y ~ x', se = F)\n\n\n\n\n\n\n\n\nThe plot shows a S-shaped curve on the relationship between the two variables.\nChoose a model\nBased on the plot and theoretical background, the Logistic model Y = \\frac{1}{1 + e^{a + b\\ln(X)}} is adopted.\nPerform the Fitting\nUse statistical software to perform the fitting. You can use lm() for linear regression or nls() for nonlinear fitting. For nonlinear models, you usually need to provide initial parameter guesses. Parameters can be set based on data trends or prior experience. When using nonlinear fitting functions (e.g., nls()), initial parameter guesses are critical.\n\nmodel &lt;- nls(\n  response ~ (120 / (1 + exp(a + b * log(dose)))) , \n  data = df, \n  start = list(a = 8, b = -1)\n) \nsummary(model)\n\n#&gt; \n#&gt; Formula: response ~ (120/(1 + exp(a + b * log(dose))))\n#&gt; \n#&gt; Parameters:\n#&gt;   Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; a  8.12807    0.18750   43.35 8.84e-11 ***\n#&gt; b -1.94301    0.04505  -43.13 9.22e-11 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 1.262 on 8 degrees of freedom\n#&gt; \n#&gt; Number of iterations to convergence: 7 \n#&gt; Achieved convergence tolerance: 5.276e-06\n\n\nFrom the result we can write the fitted equation as:\n\nY = \\frac{120}{1 + e^{8.128-1.943\\ln(X)}}\n\nCheck the fit\nModel diagnostics: Use residual plots to check if the residuals meet assumptions such as independence and normality. Check the residuals to ensure that they are randomly scattered and do not show patterns that suggest a poor fit.\n\n# Create a residual plot using ggplot2\nggplot(df, aes(x = fitted(model), y = resid(model))) +\n  geom_point(size = 1) +\n  geom_hline(yintercept = 0, color = \"red\") +\n  xlab(\"Fitted Values\") +\n  ylab(\"Residuals\")\n\n\n\n\n\n\n\n\nGoodness of fit test: Use R-squared or adjusted R-squared to evaluate how well the linear model fits the data. For nonlinear models, you may use the sum of squared residuals or AIC/BIC values.\n\nAIC(model)\n\n#&gt; [1] 36.7957\n\n\nVisual inspection: Always plot the data with the fitted curve to visually check if the fit seems reasonable.\n\nggplot(df, aes(x = dose, y = response)) +\n  geom_point(size = 1) +\n  geom_line(aes(y = fitted(model)), color = \"#C0035490\", linewidth = 0.8) +\n  labs(title = \"NLS Model Fit\", x = \"Dose\", y = \"Response\")\n\n\n\n\n\n\n\n\nThis allows you to visualize both the raw data (points) and the fitted model (line) on the same plot.",
    "crumbs": [
      "Basic methods",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Bivariate regression and correlation</span>"
    ]
  },
  {
    "objectID": "multi-factor-anova.html",
    "href": "multi-factor-anova.html",
    "title": "14  Multifactor ANOVA",
    "section": "",
    "text": "14.1 Prerequisite\nlibrary(tidyverse)",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Multifactor ANOVA</span>"
    ]
  },
  {
    "objectID": "multi-factor-anova.html#factorial-design",
    "href": "multi-factor-anova.html#factorial-design",
    "title": "14  Multifactor ANOVA",
    "section": "14.2 Factorial design",
    "text": "14.2 Factorial design\nFactorial design is an experimental setup that involves two or more factors, each with multiple levels, and tests all possible combinations of these factors. This design is widely used in various fields like medicine, biology, and psychology to analyze the simultaneous influence of multiple factors on an outcome.\n\nExample 1: \nTo determine the relationship between a patient’s age and their systolic blood pressure, a researcher gathered data from a sample of patients, including their age and systolic blood pressure readings. The data can be downloaded from the button below, where age is the independent variable (X) and systolic blood pressure is the dependent variable (Y).\n\n\n  Download data",
    "crumbs": [
      "Advanced methods",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Multifactor ANOVA</span>"
    ]
  }
]